<!doctype html><html><head><title>arXiv @ 2024.03.10</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.10"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cond-mat.mtrl-sci (1) cs.AI (18) cs.AR (3) cs.CE (3) cs.CL (35) cs.CR (9) cs.CV (77) cs.CY (2) cs.DC (1) cs.DS (2) cs.ET (2) cs.GR (1) cs.GT (1) cs.HC (8) cs.IR (4) cs.IT (2) cs.LG (32) cs.MM (2) cs.NI (3) cs.PL (3) cs.RO (15) cs.SD (2) cs.SE (2) cs.SI (1) econ.EM (1) eess.AS (1) eess.IV (9) eess.SP (1) eess.SY (9) hep-ph (2) math.CO (4) math.NA (2) math.PR (1) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240310000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-10T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-10T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.10"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240310000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Sunday, Mar 10, 2024</p></div><div class=title><h1>arXiv @ 2024.03.10</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csai-18>cs.AI (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csar-3>cs.AR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csce-3>cs.CE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cscl-35>cs.CL (35)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cscr-9>cs.CR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cscv-77>cs.CV (77)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cset-2>cs.ET (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cshc-8>cs.HC (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csir-4>cs.IR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cslg-32>cs.LG (32)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csmm-2>cs.MM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cspl-3>cs.PL (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csro-15>cs.RO (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#csse-2>cs.SE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#econem-1>econ.EM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#eessiv-9>eess.IV (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#eesssy-9>eess.SY (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#hep-ph-2>hep-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#mathco-4>math.CO (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#mathpr-1>math.PR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#physicsapp-ph-1>physics.app-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#q-biope-1>q-bio.PE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#quant-ph-3>quant-ph (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#statap-1>stat.AP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/#statml-2>stat.ML (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td>2</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>BART</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>6</td><td>8</td><td>21</td><td>6</td><td>1</td></tr><tr><td>Black Box</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Causal Intervention</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Claude</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Code Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Convolution</td><td>2</td><td></td><td>8</td><td>3</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td></td><td>4</td><td>1</td><td>1</td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td>2</td><td>1</td><td>1</td></tr><tr><td>Diffusion Model</td><td>1</td><td></td><td>16</td><td>1</td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Domain Adaptation</td><td>1</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Emotion Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td>3</td><td>3</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Few-shot</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>2</td><td>11</td><td>11</td><td>4</td><td></td></tr><tr><td>Foundation Model</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>GPT</td><td>1</td><td>10</td><td></td><td></td><td>1</td></tr><tr><td>GPT-3</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>1</td><td>9</td><td></td><td></td><td>1</td></tr><tr><td>GPT-4 turbo</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Grammatical Error Correction</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Graph</td><td>5</td><td>1</td><td>3</td><td>4</td><td>1</td></tr><tr><td>Graph Convolutional Network</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Grounding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td></td><td>12</td><td>4</td><td></td></tr><tr><td>Knowledge Graph</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>LLaMA</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td>2</td><td></td><td></td><td>3</td></tr><tr><td>Large Language Model</td><td>11</td><td>35</td><td>9</td><td>8</td><td>4</td></tr><tr><td>Low-Resource</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Mathematical Reasoning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>5</td><td>6</td><td>11</td><td>4</td><td>2</td></tr><tr><td>Multiple Instance Learning</td><td></td><td></td><td>5</td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>8</td><td></td><td>1</td></tr><tr><td>Open-Domain Question Answering</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Prompt</td><td>1</td><td>12</td><td>12</td><td>1</td><td></td></tr><tr><td>Pruning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Question Answering</td><td></td><td>6</td><td>2</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>3</td><td>5</td><td>2</td><td></td><td>1</td></tr><tr><td>Reinforcement Learning</td><td>2</td><td>3</td><td></td><td>6</td><td></td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>2</td><td>3</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>10</td><td>3</td><td>1</td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentence Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td>1</td><td></td><td>3</td><td>6</td></tr><tr><td>Simulator</td><td>1</td><td>1</td><td></td><td>3</td><td>6</td></tr><tr><td>Sora</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Speech-to-Speech Translation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stance Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td></td><td>3</td><td>2</td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>1</td><td>7</td><td>2</td><td></td></tr><tr><td>Text Classification</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td>1</td><td>1</td><td>7</td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Transformer</td><td></td><td>3</td><td>13</td><td>4</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>5</td><td>2</td><td></td></tr><tr><td>Variational Autoencoder</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>3</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>2</td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-35>cs.CL (35)</h2><h3 id=135--1270-a-benchmark-of-domain-adapted-large-language-models-for-generating-brief-hospital-course-summaries-asad-aali-et-al-2024>(1/35 | 1/270) A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries (Asad Aali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asad Aali, Dave Van Veen, Yamin Ishraq Arefeen, Jason Hom, Christian Bluethgen, Eduardo Pontes Reis, Sergios Gatidis, Namuun Clifford, Joseph Daws, Arash S. Tehrani, Jangwon Kim, Akshay S. Chaudhari. (2024)<br><strong>A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries</strong><br><button class=copy-to-clipboard title="A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 123<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05720v1.pdf filename=2403.05720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of <b>LLMs</b> for BHC synthesis, we introduce a novel <b>benchmark</b> consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose <b>LLMs</b> and three healthcare-adapted <b>LLMs</b> to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply <b>prompting-based</b> (using <b>in-context</b> <b>learning)</b> and <b>fine-tuning-based</b> adaptation strategies to three open-source <b>LLMs</b> (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary <b>LLMs</b> <b>(GPT-3.5,</b> <b>GPT-4).</b> We quantitatively evaluate the performance of these <b>LLMs</b> across varying context-length inputs using conventional natural language similarity metrics. We further perform a qualitative study where five diverse clinicians blindly compare clinician-written BHCs and two <b>LLM-generated</b> BHCs for 30 samples across metrics of comprehensiveness, conciseness, factual correctness, and fluency. Overall, we present a new <b>benchmark</b> and pre-processed dataset for using <b>LLMs</b> in BHC synthesis from clinical notes. We observe high-quality <b>summarization</b> performance for both <b>in-context</b> <b>proprietary</b> and <b>fine-tuned</b> open-source <b>LLMs</b> using both quantitative metrics and a qualitative clinical reader study. We propose our work as a <b>benchmark</b> to motivate future works to adapt and assess the performance of <b>LLMs</b> in BHC synthesis.</p></p class="citation"></blockquote><h3 id=235--2270-to-err-is-human-but-llamas-can-learn-it-too-agnes-luhtaru-et-al-2024>(2/35 | 2/270) To Err Is Human, but Llamas Can Learn It Too (Agnes Luhtaru et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Agnes Luhtaru, Taido Purason, Martin Vainikko, Maksym Del, Mark Fishel. (2024)<br><strong>To Err Is Human, but Llamas Can Learn It Too</strong><br><button class=copy-to-clipboard title="To Err Is Human, but Llamas Can Learn It Too" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Grammatical Error Correction, Grammatical Error Correction, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05493v1.pdf filename=2403.05493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores enhancing <b>grammatical</b> <b>error</b> <b>correction</b> <b>(GEC)</b> through artificial error generation (AEG) using language models (LMs). Specifically, we <b>fine-tune</b> <b>Llama</b> 2-based LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next, we train <b>GEC</b> <b>Llama</b> models with the help of these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F0.5 points across all tested languages (German, Ukrainian, and Estonian). Moreover, we demonstrate that generating errors by <b>fine-tuning</b> smaller sequence-to-sequence models and <b>prompting</b> large commercial LMs <b>(GPT-3.5</b> and <b>GPT-4)</b> also results in synthetic errors beneficially affecting error generation models.</p></p class="citation"></blockquote><h3 id=335--3270-rat-retrieval-augmented-thoughts-elicit-context-aware-reasoning-in-long-horizon-generation-zihao-wang-et-al-2024>(3/35 | 3/270) RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation (Zihao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, Yitao Liang. (2024)<br><strong>RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation</strong><br><button class=copy-to-clipboard title="RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, Code Generation, Information Retrieval, Mathematical Reasoning, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05313v1.pdf filename=2403.05313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore how iterative revising a chain of thoughts with the help of <b>information</b> <b>retrieval</b> significantly improves <b>large</b> <b>language</b> <b>models&rsquo;</b> <b>reasoning</b> and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method &ndash; <em>retrieval-augmented thoughts</em> (RAT) &ndash; revises each thought step one by one with retrieved <b>information</b> <b>relevant</b> to the task query, the current and the past thought steps, after the initial <b>zero-shot</b> CoT is generated. Applying RAT to <b>GPT-3.5,</b> <b>GPT-4,</b> and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on <b>code</b> <b>generation,</b> 16.96% on <b>mathematical</b> <b>reasoning,</b> 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at <a href=https://craftjarvis.github.io/RAT>https://craftjarvis.github.io/RAT</a></p></p class="citation"></blockquote><h3 id=435--4270-gemini-15-unlocking-multimodal-understanding-across-millions-of-tokens-of-context-machel-reid-et-al-2024>(4/35 | 4/270) Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context (Machel Reid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, Luke Vilnis, Oscar Chang, Nobuyuki Morioka, George Tucker, Ce Zheng, Oliver Woodman, Nithya Attaluri, Tomas Kocisky, Evgenii Eltyshev, Xi Chen, Timothy Chung, Vittorio Selo, Siddhartha Brahma, Petko Georgiev, Ambrose Slone, Zhenkai Zhu, James Lottes, Siyuan Qiao, Ben Caine, Sebastian Riedel, Alex Tomala, Martin Chadwick, Juliette Love, Peter Choy, Sid Mittal, Neil Houlsby, Yunhao Tang, Matthew Lamm, Libin Bai, Qiao Zhang, Luheng He, Yong Cheng, Peter Humphreys, Yujia Li, Sergey Brin, Albin Cassirer, Yingjie Miao, Lukas Zilka, Taylor Tobin, Kelvin Xu, Lev Proleev, Daniel Sohn, Alberto Magni, Lisa Anne Hendricks, Isabel Gao, Santiago Ontañón, Oskar Bunyan, Nathan Byrd, Abhanshu Sharma, Biao Zhang, Mario Pinto, Rishika Sinha, Harsh Mehta, Dawei Jia, Sergi Caelles, Albert Webson, Alex Morris, Becca Roelofs, Yifan Ding, Robin Strudel, Xuehan Xiong, Marvin Ritter, Mostafa Dehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian Mentzer, Bibo Xu, YaGuang Li, Yujing Zhang, Tom Le Paine, Alex Goldin, Behnam Neyshabur, Kate Baumli, Anselm Levskaya, Michael Laskin, Wenhao Jia, Jack W. Rae, Kefan Xiao, Antoine He, Skye Giordano, Lakshman Yagati, Jean-Baptiste Lespiau, Paul Natsev, Sanjay Ganapathy, Fangyu Liu, Danilo Martins, Nanxin Chen, Yunhan Xu, Megan Barnes, Rhys May, Arpi Vezer, Junhyuk Oh, Ken Franko, Sophie Bridgers, Ruizhe Zhao, Boxi Wu, Basil Mustafa, Sean Sechrist, Emilio Parisotto, Thanumalayan Sankaranarayana Pillai, Chris Larkin, Chenjie Gu, Christina Sorokin, Maxim Krikun, Alexey Guseynov, Jessica Landon, Romina Datta, Alexander Pritzel, Phoebe Thacker, Fan Yang, Kevin Hui, Anja Hauth, Chih-Kuan Yeh, David Barker, Justin Mao-Jones, Sophia Austin, Hannah Sheahan, Parker Schuh, James Svensson, Rohan Jain, Vinay Ramasesh, Anton Briukhov, Da-Woon Chung, Tamara von Glehn, Christina Butterfield, Priya Jhakra, Matthew Wiethoff, Justin Frye, Jordan Grimstad, Beer Changpinyo, Charline Le Lan, Anna Bortsova, Yonghui Wu, Paul Voigtlaender, Tara Sainath, Charlotte Smith, Will Hawkins, Kris Cao, James Besley, Srivatsan Srinivasan, Mark Omernick, Colin Gaffney, Gabriela Surita, Ryan Burnell, Bogdan Damoc, Junwhan Ahn, Andrew Brock, Mantas Pajarskas, Anastasia Petrushkina, Seb Noury, Lorenzo Blanco, Kevin Swersky, Arun Ahuja, Thi Avrahami, Vedant Misra, Raoul de Liedekerke, Mariko Iinuma, Alex Polozov, Sarah York, George van den Driessche, Paul Michel, Justin Chiu, Rory Blevins, Zach Gleicher, Adrià Recasens, Alban Rrustemi, Elena Gribovskaya, Aurko Roy, Wiktor Gworek, Séb Arnold, Lisa Lee, James Lee-Thorp, Marcello Maggioni, Enrique Piqueras, Kartikeya Badola, Sharad Vikram, Lucas Gonzalez, Anirudh Baddepudi, Evan Senter, Jacob Devlin, James Qin, Michael Azzam, Maja Trebacz, Martin Polacek, Kashyap Krishnakumar, Shuo-yiin Chang, Matthew Tung, Ivo Penchev, Rishabh Joshi, Kate Olszewska, Carrie Muir, Mateo Wirth, Ale Jakse Hartman, Josh Newlan, Sheleem Kashem, Vijay Bolina, Elahe Dabir, Joost van Amersfoort, Zafarali Ahmed, James Cobon-Kerr, Aishwarya Kamath, Arnar Mar Hrafnkelsson, Le Hou, Ian Mackinnon, Alexandre Frechette, Eric Noland, Xiance Si, Emanuel Taropa, Dong Li, Phil Crone, Anmol Gulati, Sébastien Cevey, Jonas Adler, Ada Ma, David Silver, Simon Tokumine, Richard Powell, Stephan Lee, Michael Chang, Samer Hassan, Diana Mincu, Antoine Yang, Nir Levine, Jenny Brennan, Mingqiu Wang, Sarah Hodkinson, Jeffrey Zhao, Josh Lipschultz, Aedan Pope, Michael B. Chang, Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio Pardo, Seth Odoom, Mihaela Rosca, Cicero Nogueira dos Santos, Kedar Soparkar, Arthur Guez, Tom Hudson, Steven Hansen, Chulayuth Asawaroengchai, Ravi Addanki, Tianhe Yu, Wojciech Stokowiec, Mina Khan, Justin Gilmer, Jaehoon Lee, Carrie Grimes Bostock, Keran Rong, Jonathan Caton, Pedram Pejman, Filip Pavetic, Geoff Brown, Vivek Sharma, Mario Lučić, Rajkumar Samuel, Josip Djolonga, Amol Mandhane, Lars Lowe Sjösund, Elena Buchatskaya, Elspeth White, Natalie Clay, Jiepu Jiang, Hyeontaek Lim, Ross Hemsley, Jane Labanowski, Nicola De Cao, David Steiner, Sayed Hadi Hashemi, Jacob Austin, Anita Gergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant, Anders Andreassen, Carlos Araya, Nikhil Sethi, Rakesh Shivanna, Steven Hand, Ankur Bapna, Ali Khodaei, Antoine Miech, Garrett Tanzer, Andy Swing, Shantanu Thakoor, Zhufeng Pan, Zachary Nado, Stephanie Winkler, Dian Yu, Mohammad Saleh, Loren Maggiore, Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka, Amit Marathe, Vladimir Feinberg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller, Lexi Walker, Richard Tanburn, Mukarram Tariq, Disha Shrivastava, Fei Xia, Chung-Cheng Chiu, Zoe Ashwood, Khuslen Baatarsukh, Sina Samangooei, Fred Alcober, Axel Stjerngren, Paul Komarek, Katerina Tsihlas, Anudhyan Boral, Ramona Comanescu, Jeremy Chen, Ruibo Liu, Dawn Bloxwich, Charlie Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew Mauger, Xerxes Dotiwalla, Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Krishna Haridasan, Gabe Barth-Maron, Craig Swanson, Dominika Rogozińska, Alek Andreev, Paul Kishan Rubenstein, Ruoxin Sang, Dan Hurt, Gamaleldin Elsayed, Renshen Wang, Dave Lacey, Anastasija Ilić, Yao Zhao, Lora Aroyo, Chimezie Iwuanyanwu, Vitaly Nikolaev, Balaji Lakshminarayanan, Sadegh Jazayeri, Raphaël Lopez Kaufman, Mani Varadarajan, Chetan Tekur, Doug Fritz, Misha Khalman, David Reitter, Kingshuk Dasgupta, Shourya Sarcar, Tina Ornduff, Javier Snaider, Fantine Huot, Johnson Jia, Rupert Kemp, Nejc Trdin, Anitha Vijayakumar, Lucy Kim, Christof Angermueller, Li Lao, Tianqi Liu, Haibin Zhang, David Engel, Somer Greene, Anaïs White, Jessica Austin, Lilly Taylor, Shereen Ashraf, Dangyi Liu, Maria Georgaki, Irene Cai, Yana Kulizhskaya, Sonam Goenka, Brennan Saeta, Kiran Vodrahalli, Christian Frank, Dario de Cesare, Brona Robenek, Harry Richardson, Mahmoud Alnahlawi, Christopher Yew, Priya Ponnapalli, Marco Tagliasacchi, Alex Korchemniy, Yelin Kim, Dinghua Li, Bill Rosgen, Zoe Ashwood, Kyle Levin, Jeremy Wiesner, Praseem Banzal, Praveen Srinivasan, Hongkun Yu, Çağlar Ünlü, David Reid, Zora Tung, Daniel Finchelstein, Ravin Kumar, Andre Elisseeff, Jin Huang, Ming Zhang, Rui Zhu, Ricardo Aguilar, Mai Giménez, Jiawei Xia, Olivier Dousse, Willi Gierke, Soheil Hassas Yeganeh, Damion Yates, Komal Jalan, Lu Li, Eri Latorre-Chimoto, Duc Dung Nguyen, Ken Durden, Praveen Kallakuri, Yaxin Liu, Matthew Johnson, Tomy Tsai, Alice Talbert, Jasmine Liu, Alexander Neitz, Chen Elkind, Marco Selvi, Mimi Jasarevic, Livio Baldini Soares, Albert Cui, Pidong Wang, Alek Wenjiao Wang, Xinyu Ye, Krystal Kallarackal, Lucia Loher, Hoi Lam, Josef Broder, Dan Holtmann-Rice, Nina Martin, Bramandia Ramadhana, Daniel Toyama, Mrinal Shukla, Sujoy Basu, Abhi Mohan, Nick Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush Garg, Jane Park, DongHyun Choi, Diane Wu, Sankalp Singh, Zhishuai Zhang, Amir Globerson, Lily Yu, John Carpenter, Félix de Chaumont Quitry, Carey Radebaugh, Chu-Cheng Lin, Alex Tudor, Prakash Shroff, Drew Garmon, Dayou Du, Neera Vats, Han Lu, Shariq Iqbal, Alex Yakubovich, Nilesh Tripuraneni, James Manyika, Haroon Qureshi, Nan Hua, Christel Ngani, Maria Abi Raad, Hannah Forbes, Anna Bulanova, Jeff Stanway, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie Li, Balaji Venkatraman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian Tenney, Xihui Wu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach, Zhuyun Xiao, Peter Hawkins, Sid Dalmia, Clement Farabet, Pedro Valenzuela, Quan Yuan, Chris Welty, Ananth Agarwal, Mia Chen, Wooyeol Kim, Brice Hulse, Nandita Dukkipati, Adam Paszke, Andrew Bolt, Elnaz Davoodi, Kiam Choo, Jennifer Beattie, Jennifer Prendki, Harsha Vashisht, Rebeca Santamaria-Fernandez, Luis C. Cobo, Jarek Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin Ramirez, Matt Harvey, Tyler Liechty, Heiga Zen, Jeff Seibert, Clara Huiyi Hu, Mohamed Elhawaty, Andrey Khorlin, Maigo Le, Asaf Aharoni, Megan Li, Lily Wang, Sandeep Kumar, Alejandro Lince, Norman Casagrande, Jay Hoover, Dalia El Badawy, David Soergel, Denis Vnukov, Matt Miecnikowski, Jiri Simsa, Anna Koop, Praveen Kumar, Thibault Sellam, Daniel Vlasic, Samira Daruki, Nir Shabat, John Zhang, Guolong Su, Jiageng Zhang, Jeremiah Liu, Yi Sun, Evan Palmer, Alireza Ghaffarkhah, Xi Xiong, Victor Cotruta, Michael Fink, Lucas Dixon, Ashwin Sreevatsa, Adrian Goedeckemeyer, Alek Dimitriev, Mohsen Jafari, Remi Crocker, Nicholas FitzGerald, Aviral Kumar, Sanjay Ghemawat, Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev, Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn, Denese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim, Damien Vincent, Milad Nasr, Christopher A. Choquette-Choo, Reiko Tojo, Shawn Lu, Diego de Las Casas, Yuchung Cheng, Tolga Bolukbasi, Katherine Lee, Saaber Fatehi, Rajagopal Ananthanarayanan, Miteyan Patel, Charbel Kaed, Jing Li, Jakub Sygnowski, Shreyas Rammohan Belle, Zhe Chen, Jaclyn Konzelmann, Siim Põder, Roopal Garg, Vinod Koverkathu, Adam Brown, Chris Dyer, Rosanne Liu, Azade Nova, Jun Xu, Slav Petrov, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, Oriol Vinyals. (2024)<br><strong>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</strong><br><button class=copy-to-clipboard title="Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 99<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Claude, GPT, GPT-4, GPT-4 turbo, Gemini, Automatic Speech Recognition, Question Answering, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05530v1.pdf filename=2403.05530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this report, we present the latest model of the <b>Gemini</b> family, <b>Gemini</b> 1.5 Pro, a highly compute-efficient <b>multimodal</b> mixture-of-experts model capable of recalling and <b>reasoning</b> over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. <b>Gemini</b> 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document <b>QA,</b> long-video <b>QA</b> and long-context <b>ASR,</b> and matches or surpasses <b>Gemini</b> 1.0 Ultra&rsquo;s state-of-the-art performance across a broad set of <b>benchmarks.</b> Studying the limits of <b>Gemini</b> 1.5 Pro&rsquo;s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as <b>Claude</b> 2.1 (200k) and <b>GPT-4</b> <b>Turbo</b> (128k). Finally, we highlight surprising new capabilities of <b>large</b> <b>language</b> <b>models</b> at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.</p></p class="citation"></blockquote><h3 id=535--5270-bias-augmented-consistency-training-reduces-biased-reasoning-in-chain-of-thought-james-chua-et-al-2024>(5/35 | 5/270) Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought (James Chua et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James Chua, Edward Rees, Hunar Batra, Samuel R. Bowman, Julian Michael, Ethan Perez, Miles Turpin. (2024)<br><strong>Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought</strong><br><button class=copy-to-clipboard title="Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Unsupervised Learning, GPT, GPT-3, GPT-3.5, Question Answering, Reasoning, Chain-of-thought Prompt, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05518v1.pdf filename=2403.05518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>chain-of-thought</b> <b>prompting</b> (CoT) has the potential to improve the explainability of language model <b>reasoning,</b> it can systematically misrepresent the factors influencing models&rsquo; behavior&ndash;for example, rationalizing answers in line with a user&rsquo;s opinion without mentioning this bias. To mitigate this biased <b>reasoning</b> problem, we introduce bias-augmented consistency training (BCT), an <b>unsupervised</b> <b>fine-tuning</b> scheme that trains models to give consistent <b>reasoning</b> across <b>prompts</b> with and without biasing features. We construct a suite testing nine forms of biased <b>reasoning</b> on seven <b>question-answering</b> <b>tasks,</b> and find that applying BCT to <b>GPT-3.5-Turbo</b> with one bias reduces the rate of biased <b>reasoning</b> by 86% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased <b>reasoning</b> on held-out biases by an average of 37%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased <b>reasoning</b> from as-of-yet unknown biases and on tasks where supervision for ground truth <b>reasoning</b> is unavailable.</p></p class="citation"></blockquote><h3 id=635--6270-cross-lingual-transfer-or-machine-translation-on-data-augmentation-for-monolingual-semantic-textual-similarity-sho-hoshino-et-al-2024>(6/35 | 6/270) Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity (Sho Hoshino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sho Hoshino, Akihiko Kato, Soichiro Murakami, Peinan Zhang. (2024)<br><strong>Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity</strong><br><button class=copy-to-clipboard title="Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Data Augmentation, Fine-tuning, Zero-shot, Natural Language Inference, Natural Language Inference, Natural Language Understanding, Neural Machine Translation, Sentence Embedding, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05257v1.pdf filename=2403.05257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning better <b>sentence</b> <b>embeddings</b> leads to improved performance for <b>natural</b> <b>language</b> <b>understanding</b> tasks including semantic textual similarity (STS) and <b>natural</b> <b>language</b> <b>inference</b> <b>(NLI).</b> As prior studies leverage large-scale labeled <b>NLI</b> datasets for <b>fine-tuning</b> <b>masked</b> <b>language</b> <b>models</b> to yield <b>sentence</b> <b>embeddings,</b> task performance for languages other than English is often left behind. In this study, we directly compared two <b>data</b> <b>augmentation</b> techniques as potential solutions for monolingual STS: (a) cross-lingual transfer that exploits English resources alone as training <b>data</b> <b>to</b> yield non-English <b>sentence</b> <b>embeddings</b> as <b>zero-shot</b> inference, and (b) <b>machine</b> <b>translation</b> that coverts English <b>data</b> <b>into</b> pseudo non-English training <b>data</b> <b>in</b> advance. In our experiments on monolingual STS in Japanese and Korean, we find that the two <b>data</b> <b>techniques</b> yield performance on par. Rather, we find a superiority of the Wikipedia domain over the <b>NLI</b> domain for these languages, in contrast to prior studies that focused on <b>NLI</b> as training <b>data.</b> <b>Combining</b> our findings, we demonstrate that the cross-lingual transfer of Wikipedia <b>data</b> <b>exhibits</b> improved performance, and that native Wikipedia <b>data</b> <b>can</b> further improve performance for monolingual STS.</p></p class="citation"></blockquote><h3 id=735--7270-cant-remember-details-in-long-documents-you-need-some-rr-devanshu-agrawal-et-al-2024>(7/35 | 7/270) Can&rsquo;t Remember Details in Long Documents? You Need Some R&amp;R (Devanshu Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Devanshu Agrawal, Shang Gao, Martin Gajek. (2024)<br><strong>Can&rsquo;t Remember Details in Long Documents? You Need Some R&amp;R</strong><br><button class=copy-to-clipboard title="Can't Remember Details in Long Documents? You Need Some R&R" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: Claude, GPT, GPT-4, GPT-4 turbo, Question Answering, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05004v1.pdf filename=2403.05004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-context <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> hold promise for tasks such as <b>question-answering</b> <b>(QA)</b> over long documents, but they tend to miss important information in the middle of context documents (arXiv:2307.03172v3). Here, we introduce $\textit{R&amp;R}$ &ndash; a combination of two novel <b>prompt-based</b> methods called $\textit{reprompting}$ and $\textit{in-context retrieval}$ (ICR) &ndash; to alleviate this effect in document-based <b>QA.</b> In reprompting, we repeat the <b>prompt</b> instructions periodically throughout the context document to remind the <b>LLM</b> of its original task. In ICR, rather than instructing the <b>LLM</b> to answer the <b>question</b> <b>directly,</b> we instruct it to retrieve the top $k$ passage numbers most relevant to the given <b>question,</b> <b>which</b> are then used as an abbreviated context in a second <b>QA</b> <b>prompt.</b> We test R&amp;R with <b>GPT-4</b> <b>Turbo</b> and <b>Claude-2.1</b> on documents up to 80k tokens in length and observe a 16-point boost in <b>QA</b> accuracy on average. Our further analysis suggests that R&amp;R improves performance on long document-based <b>QA</b> because it reduces the distance between relevant context and the instructions. Finally, we show that compared to short-context chunkwise methods, R&amp;R enables the use of larger chunks that cost fewer <b>LLM</b> calls and output tokens, while minimizing the drop in accuracy.</p></p class="citation"></blockquote><h3 id=835--8270-will-gpt-4-run-doom-adrian-de-wynter-2024>(8/35 | 8/270) Will GPT-4 Run DOOM? (Adrian de Wynter, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian de Wynter. (2024)<br><strong>Will GPT-4 Run DOOM?</strong><br><button class=copy-to-clipboard title="Will GPT-4 Run DOOM?" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 70<br>Keywords: Reinforcement Learning, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05468v1.pdf filename=2403.05468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that <b>GPT-4&rsquo;s</b> <b>reasoning</b> and planning capabilities extend to the 1993 first-person shooter Doom. This <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> is able to run and play the game with only a few instructions, plus a textual description&ndash;generated by the model itself from screenshots&ndash;about the state of the game being observed. We find that <b>GPT-4</b> can play the game to a passable degree: it is able to manipulate doors, combat enemies, and perform pathing. More complex <b>prompting</b> strategies involving multiple model calls provide better results. While further work is required to enable the <b>LLM</b> to play the game as well as its classical, <b>reinforcement</b> <b>learning-based</b> counterparts, we note that <b>GPT-4</b> required no training, leaning instead on its own <b>reasoning</b> and observational capabilities. We hope our work pushes the boundaries on intelligent, <b>LLM-based</b> agents in video games. We conclude by discussing the ethical implications of our work.</p></p class="citation"></blockquote><h3 id=935--9270-diffchat-learning-to-chat-with-text-to-image-synthesis-models-for-interactive-image-creation-jiapeng-wang-et-al-2024>(9/35 | 9/270) DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation (Jiapeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiapeng Wang, Chengyu Wang, Tingfeng Cao, Jun Huang, Lianwen Jin. (2024)<br><strong>DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation</strong><br><button class=copy-to-clipboard title="DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 70<br>Keywords: Reinforcement Learning, Supervised Learning, Instruction Following, Text2image, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04997v1.pdf filename=2403.04997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DiffChat, a novel method to align <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to &ldquo;chat&rdquo; with <b>prompt-as-input</b> <b>Text-to-Image</b> Synthesis (TIS) models (e.g., Stable Diffusion) for interactive image creation. Given a raw prompt/image and a user-specified <b>instruction,</b> <b>DiffChat</b> can effectively make appropriate modifications and generate the target <b>prompt,</b> which can be leveraged to create the target image of high quality. To achieve this, we first collect an <b>instruction-following</b> <b>prompt</b> engineering dataset named InstructPE for the <b>supervised</b> training of DiffChat. Next, we propose a <b>reinforcement</b> <b>learning</b> framework with the feedback of three core criteria for image creation, i.e., aesthetics, user preference, and content integrity. It involves an action-space dynamic modification technique to obtain more relevant positive samples and harder negative samples during the off-policy sampling. Content integrity is also introduced into the value estimation function for further improvement of produced images. Our method can exhibit superior performance than baseline models and strong competitors based on both automatic and human evaluations, which fully demonstrates its effectiveness.</p></p class="citation"></blockquote><h3 id=1035--10270-harnessing-multi-role-capabilities-of-large-language-models-for-open-domain-question-answering-hongda-sun-et-al-2024>(10/35 | 10/270) Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering (Hongda Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo Shang, Rui Yan. (2024)<br><strong>Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering</strong><br><button class=copy-to-clipboard title="Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Open-Domain Question Answering, Open-Domain Question Answering, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05217v1.pdf filename=2403.05217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Open-domain</b> <b>question</b> <b>answering</b> <b>(ODQA)</b> has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) The \textit{retrieve-then-read} paradigm retrieves pertinent documents from an external corpus; and (2) the \textit{generate-then-read} paradigm employs <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the <b>ODQA</b> process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since <b>LLMs</b> exhibit their excellent capabilities to accomplish various tasks, we instruct <b>LLMs</b> to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the <b>ODQA</b> process. Furthermore, we introduce a novel <b>prompt</b> optimization algorithm to refine role-playing <b>prompts</b> and steer <b>LLMs</b> to produce higher-quality evidence and answers. Extensive experimental results on widely used <b>benchmarks</b> (NQ, WebQ, and TriviaQA) demonstrate that LLMQA achieves the best performance in terms of both answer accuracy and evidence quality, showcasing its potential for advancing <b>ODQA</b> research and applications.</p></p class="citation"></blockquote><h3 id=1135--11270-can-we-obtain-significant-success-in-rst-discourse-parsing-by-using-large-language-models-aru-maekawa-et-al-2024>(11/35 | 11/270) Can we obtain significant success in RST discourse parsing by using Large Language Models? (Aru Maekawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aru Maekawa, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura. (2024)<br><strong>Can we obtain significant success in RST discourse parsing by using Large Language Models?</strong><br><button class=copy-to-clipboard title="Can we obtain significant success in RST discourse parsing by using Large Language Models?" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, LLaMA, Large Language Model, Large Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05065v1.pdf filename=2403.05065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, decoder-only <b>pre-trained</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> with several tens of billion parameters, have significantly impacted a wide range of natural language processing (NLP) tasks. While encoder-only or encoder-decoder <b>pre-trained</b> <b>language</b> <b>models</b> have already proved to be effective in discourse parsing, the extent to which <b>LLMs</b> can perform this task remains an open research question. Therefore, this paper explores how beneficial such <b>LLMs</b> are for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing process for both fundamental top-down and bottom-up strategies is converted into <b>prompts,</b> which <b>LLMs</b> can work with. We employ <b>Llama</b> 2 and <b>fine-tune</b> it with QLoRA, which has fewer parameters that can be tuned. Experimental results on three <b>benchmark</b> datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate that <b>Llama</b> 2 with 70 billion parameters in the bottom-up strategy obtained state-of-the-art (SOTA) results with significant differences. Furthermore, our parsers demonstrated generalizability when evaluated on RST-DT, showing that, in spite of being trained with the GUM corpus, it obtained similar performances to those of existing parsers trained with RST-DT.</p></p class="citation"></blockquote><h3 id=1235--12270-cost-performance-optimization-for-processing-low-resource-language-tasks-using-commercial-llms-arijit-nag-et-al-2024>(12/35 | 12/270) Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs (Arijit Nag et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arijit Nag, Animesh Mukherjee, Niloy Ganguly, Soumen Chakrabarti. (2024)<br><strong>Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs</strong><br><button class=copy-to-clipboard title="Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: High-Resource, Low-Resource, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05434v1.pdf filename=2403.05434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> exhibit impressive zero/few-shot inference and generation quality for <b>high-resource</b> languages(HRLs). A few of them have been trained in <b>low-resource</b> languages (LRLs) and give decent performance. Owing to the prohibitive costs of training <b>LLMs,</b> they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the <b>LLM&rsquo;s</b> sub-word vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known <b>LLMs</b> produce more tokens for LRLs than HRLs. This is because most currently popular <b>LLMs</b> are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary <b>LLMs</b> while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the <b>LLM,</b> we consider code-mixing, translation, and transliteration of LRLs to HRLs. We perform an extensive study using the IndicXTREME dataset, covering 15 Indian languages, while using <b>GPT-4</b> (one of the costliest <b>LLM</b> services released so far) as a commercial <b>LLM.</b> We observe and analyze interesting patterns involving token count, cost,and quality across a multitude of languages and tasks. We show that choosing the best policy to interact with the <b>LLM</b> can reduce cost by 90% while giving better or comparable performance, compared to communicating with the <b>LLM</b> in the original LRL.</p></p class="citation"></blockquote><h3 id=1335--13270-the-impact-of-quantization-on-the-robustness-of-transformer-based-text-classifiers-seyed-parsa-neshaei-et-al-2024>(13/35 | 13/270) The Impact of Quantization on the Robustness of Transformer-based Text Classifiers (Seyed Parsa Neshaei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seyed Parsa Neshaei, Yasaman Boreshban, Gholamreza Ghassem-Sani, Seyed Abolghasem Mirroshandel. (2024)<br><strong>The Impact of Quantization on the Robustness of Transformer-based Text Classifiers</strong><br><button class=copy-to-clipboard title="The Impact of Quantization on the Robustness of Transformer-based Text Classifiers" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Adversarial Learning, Quantization, BERT, Transformer, Text Classification, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05365v1.pdf filename=2403.05365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> models have made remarkable advancements in various NLP areas. Nevertheless, these models often exhibit vulnerabilities when confronted with <b>adversarial</b> <b>attacks.</b> In this paper, we explore the effect of <b>quantization</b> on the robustness of <b>Transformer-based</b> models. <b>Quantization</b> usually involves mapping a high-precision real number to a lower-precision value, aiming at reducing the size of the model at hand. To the best of our knowledge, this work is the first application of <b>quantization</b> on the robustness of NLP models. In our experiments, we evaluate the impact of <b>quantization</b> on <b>BERT</b> and DistilBERT models in <b>text</b> <b>classification</b> using SST-2, Emotion, and MR datasets. We also evaluate the performance of these models against TextFooler, PWWS, and PSO <b>adversarial</b> <b>attacks.</b> Our findings show that <b>quantization</b> significantly improves (by an average of 18.68%) the <b>adversarial</b> <b>accuracy</b> of the models. Furthermore, we compare the effect of <b>quantization</b> versus that of the <b>adversarial</b> <b>training</b> approach on robustness. Our experiments indicate that <b>quantization</b> increases the robustness of the model by 18.80% on average compared to <b>adversarial</b> <b>training</b> without imposing any extra computational overhead during training. Therefore, our results highlight the effectiveness of <b>quantization</b> in improving the robustness of NLP models.</p></p class="citation"></blockquote><h3 id=1435--14270-explaining-pre-trained-language-models-with-attribution-scores-an-analysis-in-low-resource-settings-wei-zhou-et-al-2024>(14/35 | 14/270) Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings (Wei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Zhou, Heike Adel, Hendrik Schuff, Ngoc Thang Vu. (2024)<br><strong>Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings</strong><br><button class=copy-to-clipboard title="Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Low-Resource, Large Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05338v1.pdf filename=2403.05338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour. Currently, <b>prompt-based</b> models are gaining popularity, i.a., due to their easier adaptability in <b>low-resource</b> settings. However, the quality of attribution scores extracted from <b>prompt-based</b> models has not been investigated yet. In this work, we address this topic by analyzing attribution scores extracted from <b>prompt-based</b> models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from <b>fine-tuned</b> models and <b>large</b> <b>language</b> <b>models.</b> In contrast to previous work, we introduce training size as another dimension into the analysis. We find that using the <b>prompting</b> paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than <b>fine-tuning</b> the models in <b>low-resource</b> settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in terms of leading to more plausible and faithful explanations.</p></p class="citation"></blockquote><h3 id=1535--15270-erbench-an-entity-relationship-based-automatically-verifiable-hallucination-benchmark-for-large-language-models-jio-oh-et-al-2024>(15/35 | 15/270) ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models (Jio Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jio Oh, Soyeon Kim, Junseok Seo, Jindong Wang, Ruochen Xu, Xing Xie, Steven Euijong Whang. (2024)<br><strong>ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models</strong><br><button class=copy-to-clipboard title="ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05266v1.pdf filename=2403.05266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination <b>benchmarks</b> are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing <b>benchmarks</b> due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a <b>benchmark</b> based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of <b>LLMs.</b> Finally, ERBench supports continuous evaluation, <b>multimodal</b> questions, and various <b>prompt</b> engineering techniques. In our experiments, we construct an <b>LLM</b> <b>benchmark</b> using databases of multiple domains and make an extensive comparison of contemporary <b>LLMs.</b> We observe that better <b>LLMs</b> like <b>GPT-4</b> can handle a larger variety of question types, but are by no means perfect. Also, correct answers do not necessarily imply correct rationales, which is an important evaluation that ERBench does better than other <b>benchmarks</b> for various question types. Code is available at https: //github.com/DILAB-KAIST/ERBench.</p></p class="citation"></blockquote><h3 id=1635--16270-piperag-fast-retrieval-augmented-generation-via-algorithm-system-co-design-wenqi-jiang-et-al-2024>(16/35 | 16/270) PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design (Wenqi Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, Tim Kraska. (2024)<br><strong>PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design</strong><br><button class=copy-to-clipboard title="PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05676v1.pdf filename=2403.05676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> can enhance the generation quality of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> by incorporating external token databases. However, <b>retrievals</b> <b>from</b> <b>large</b> <b>databases</b> <b>can</b> constitute a substantial portion of the overall generation time, particularly when <b>retrievals</b> <b>are</b> <b>periodically</b> performed to align the retrieved content with the latest states of generation. In this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality. PipeRAG integrates (1) pipeline parallelism to enable concurrent <b>retrieval</b> <b>and</b> <b>generation</b> processes, (2) flexible <b>retrieval</b> <b>intervals</b> <b>to</b> maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance <b>retrieval</b> <b>quality</b> <b>and</b> latency based on the generation states and underlying hardware. Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6$\times$ speedup in end-to-end generation latency while improving generation quality. These promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of PipeRAG in future <b>RAG</b> systems.</p></p class="citation"></blockquote><h3 id=1735--17270-authorship-attribution-in-bangla-literature-aabl-via-transfer-learning-using-ulmfit-aisha-khatun-et-al-2024>(17/35 | 17/270) Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT (Aisha Khatun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aisha Khatun, Anisur Rahman, Md Saiful Islam, Hemayet Ahmed Chowdhury, Ayesha Tasnim. (2024)<br><strong>Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT</strong><br><button class=copy-to-clipboard title="Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Transfer Learning, LSTM, LSTM, Tokenization, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05519v1.pdf filename=2403.05519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Authorship Attribution is the task of creating an appropriate characterization of text that captures the authors&rsquo; writing style to identify the original author of a given piece of text. With increased anonymity on the internet, this task has become increasingly crucial in various security and plagiarism detection fields. Despite significant advancements in other languages such as English, Spanish, and Chinese, Bangla lacks comprehensive research in this field due to its complex linguistic feature and sentence structure. Moreover, existing systems are not scalable when the number of author increases, and the performance drops for small number of samples per author. In this paper, we propose the use of Average-Stochastic Gradient Descent Weight-Dropped <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(AWD-LSTM)</b> architecture and an effective <b>transfer</b> <b>learning</b> approach that addresses the problem of complex linguistic features extraction and scalability for authorship attribution in Bangla Literature (AABL). We analyze the effect of different <b>tokenization,</b> such as word, sub-word, and character level <b>tokenization,</b> and demonstrate the effectiveness of these <b>tokenizations</b> in the proposed model. Moreover, we introduce the publicly available Bangla Authorship Attribution Dataset of 16 authors (BAAD16) containing 17,966 sample texts and 13.4+ million words to solve the standard dataset scarcity problem and release six variations of <b>pre-trained</b> <b>language</b> <b>models</b> for use in any Bangla NLP downstream task. For evaluation, we used our developed BAAD16 dataset as well as other publicly available datasets. Empirically, our proposed model outperformed state-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset. Furthermore, we showed that the proposed system scales much better even with an increasing number of authors, and performance remains steady despite few training samples.</p></p class="citation"></blockquote><h3 id=1835--18270-chatasu-evoking-llms-reflexion-to-truly-understand-aspect-sentiment-in-dialogues-yiding-liu-et-al-2024>(18/35 | 18/270) ChatASU: Evoking LLM&rsquo;s Reflexion to Truly Understand Aspect Sentiment in Dialogues (Yiding Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiding Liu, Jingjing Wang, Jiamin Luo, Tao Zeng, Guodong Zhou. (2024)<br><strong>ChatASU: Evoking LLM&rsquo;s Reflexion to Truly Understand Aspect Sentiment in Dialogues</strong><br><button class=copy-to-clipboard title="ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05326v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05326v2.pdf filename=2403.05326v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., <b>Question-Answering</b> <b>and</b> Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore <b>LLMs&rsquo;</b> ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain <b>Reasoning</b> (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU. Specifically, this TSA treats the ACR task as an auxiliary task to boost the performance of the primary ASU task, and further integrates trusted learning into reflexion mechanisms to alleviate the <b>LLMs-intrinsic</b> factual hallucination problem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to evaluate TSA, and extensive experiments show that our proposed TSA can significantly outperform several state-of-the-art baselines, justifying the effectiveness of TSA to ChatASU and the importance of considering the coreference and hallucination issues in ChatASU.</p></p class="citation"></blockquote><h3 id=1935--19270-aclsum-a-new-dataset-for-aspect-based-summarization-of-scientific-publications-sotaro-takeshita-et-al-2024>(19/35 | 19/270) ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications (Sotaro Takeshita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, Simone Paolo Ponzetto. (2024)<br><strong>ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications</strong><br><button class=copy-to-clipboard title="ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Large Language Model, Large Language Model, Pre-trained Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05303v1.pdf filename=2403.05303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extensive efforts in the past have been directed toward the development of <b>summarization</b> datasets. However, a predominant number of these resources have been (semi)-automatically generated, typically through web data crawling, resulting in subpar resources for training and evaluating <b>summarization</b> systems, a quality compromise that is arguably due to the substantial costs associated with generating ground-truth summaries, particularly for diverse languages and specialized domains. To address this issue, we present ACLSum, a novel <b>summarization</b> dataset carefully crafted and evaluated by domain experts. In contrast to previous datasets, ACLSum facilitates multi-aspect <b>summarization</b> of scientific papers, covering challenges, approaches, and outcomes in depth. Through extensive experiments, we evaluate the quality of our resource and the performance of models based on <b>pretrained</b> <b>language</b> <b>models</b> and state-of-the-art <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Additionally, we explore the effectiveness of extractive versus abstractive <b>summarization</b> within the scholarly domain on the basis of automatically discovered aspects. Our results corroborate previous findings in the general domain and indicate the general superiority of end-to-end aspect-based <b>summarization.</b> Our data is released at <a href=https://github.com/sobamchan/aclsum>https://github.com/sobamchan/aclsum</a>.</p></p class="citation"></blockquote><h3 id=2035--20270-deep-prompt-multi-task-network-for-abuse-language-detection-jian-zhu-et-al-2024>(20/35 | 20/270) Deep Prompt Multi-task Network for Abuse Language Detection (Jian Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Zhu, Yuping Ruan, Jingfei Chang, Cheng Luo. (2024)<br><strong>Deep Prompt Multi-task Network for Abuse Language Detection</strong><br><button class=copy-to-clipboard title="Deep Prompt Multi-task Network for Abuse Language Detection" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05268v1.pdf filename=2403.05268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The detection of abusive language remains a long-standing challenge with the extensive use of social networks. The detection task of abusive language suffers from limited accuracy. We argue that the existing detection methods utilize the <b>fine-tuning</b> technique of the <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> to handle downstream tasks. Hence, these methods fail to stimulate the general knowledge of the <b>PLMs.</b> To address the problem, we propose a novel Deep <b>Prompt</b> Multi-task Network (DPMN) for abuse language detection. Specifically, DPMN first attempts to design two forms of deep <b>prompt</b> tuning and light <b>prompt</b> tuning for the <b>PLMs.</b> The effects of different <b>prompt</b> lengths, tuning strategies, and <b>prompt</b> initialization methods on detecting abusive language are studied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which can be used as a short text classifier. Eventually, DPMN utilizes multi-task learning to improve detection metrics further. The multi-task network has the function of transferring effective knowledge. The proposed DPMN is evaluated against eight typical methods on three public datasets: OLID, SOLID, and AbuseAnalyzer. The experimental results show that our DPMN outperforms the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2135--21270-towards-a-psychology-of-machines-large-language-models-predict-human-memory-markus-huff-et-al-2024>(21/35 | 21/270) Towards a Psychology of Machines: Large Language Models Predict Human Memory (Markus Huff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Markus Huff, Elanur Ulakçı. (2024)<br><strong>Towards a Psychology of Machines: Large Language Models Predict Human Memory</strong><br><button class=copy-to-clipboard title="Towards a Psychology of Machines: Large Language Models Predict Human Memory" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Generative AI, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05152v1.pdf filename=2403.05152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of <b>ChatGPT</b> to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., &ldquo;Because Bill drinks wine is never kept in the house&rdquo;) is facilitated by preceding them with contextually relevant information. Participants, both human and <b>ChatGPT,</b> were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., &ldquo;Bill has chronic alcoholism&rdquo;) or an unfitting context (e.g., &ldquo;Bill likes to play golf&rdquo;). We measured both human&rsquo;s and <b>ChatGPT&rsquo;s</b> ratings of sentence relatedness, <b>ChatGPT&rsquo;s</b> memorability ratings for the garden-path sentences, and humans&rsquo; spontaneous memory for the garden-path sentences. The results revealed a striking alignment between <b>ChatGPT&rsquo;s</b> assessments and human performance. Sentences deemed more related and assessed as being more memorable by <b>ChatGPT</b> were indeed better remembered by humans, even though <b>ChatGPT&rsquo;s</b> internal mechanisms likely differ significantly from human cognition. This finding, which was confirmed with a robustness check employing synonyms, underscores the potential of <b>generative</b> <b>AI</b> models to predict human performance accurately. We discuss the broader implications of these findings for leveraging <b>LLMs</b> in the development of psychological theories and for gaining a deeper understanding of human cognition.</p></p class="citation"></blockquote><h3 id=2235--22270-chatuie-exploring-chat-based-unified-information-extraction-using-large-language-models-jun-xu-et-al-2024>(22/35 | 22/270) ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models (Jun Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Xu, Mengshu Sun, Zhiqiang Zhang, Jun Zhou. (2024)<br><strong>ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models</strong><br><button class=copy-to-clipboard title="ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Information Retrieval, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05132v1.pdf filename=2403.05132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in <b>information</b> <b>extraction,</b> have certain limitations. Extracting structured <b>information</b> <b>from</b> natural language that deviates from known schemas or instructions has proven challenging for previous <b>prompt-based</b> methods. This motivated us to explore domain-specific modeling in chat-based language models as a solution for extracting structured <b>information</b> <b>from</b> natural language. In this paper, we present ChatUIE, an innovative unified <b>information</b> <b>extraction</b> framework built upon ChatGLM. Simultaneously, <b>reinforcement</b> <b>learning</b> is employed to improve and align various tasks that involve confusing and limited samples. Furthermore, we integrate generation constraints to address the issue of generating elements that are not present in the input. Our experimental results demonstrate that ChatUIE can significantly improve the performance of <b>information</b> <b>extraction</b> with a slight decrease in chatting ability.</p></p class="citation"></blockquote><h3 id=2335--23270-is-this-the-real-life-is-this-just-fantasy-the-misleading-success-of-simulating-social-interactions-with-llms-xuhui-zhou-et-al-2024>(23/35 | 23/270) Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs (Xuhui Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, Maarten Sap. (2024)<br><strong>Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs</strong><br><button class=copy-to-clipboard title="Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05020v1.pdf filename=2403.05020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have enabled richer social <b>simulations,</b> allowing for the study of various social phenomena with <b>LLM-based</b> agents. However, most work has used an omniscient perspective on these <b>simulations</b> (e.g., single <b>LLM</b> to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with <b>LLMs</b> in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient <b>simulations</b> improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our findings indicate that addressing information asymmetry remains a fundamental challenge for <b>LLM-based</b> agents.</p></p class="citation"></blockquote><h3 id=2435--24270-an-in-depth-evaluation-of-gpt-4-in-sentence-simplification-with-error-based-human-assessment-xuanxin-wu-et-al-2024>(24/35 | 24/270) An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment (Xuanxin Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanxin Wu, Yuki Arase. (2024)<br><strong>An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment</strong><br><button class=copy-to-clipboard title="An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04963v1.pdf filename=2403.04963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sentence simplification, which rewrites a sentence to be easier to read and understand, is a promising technique to help people with various reading difficulties. With the rise of advanced <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> evaluating their performance in sentence simplification has become imperative. Recent studies have used both automatic metrics and human evaluations to assess the simplification abilities of <b>LLMs.</b> However, the suitability of existing evaluation methodologies for <b>LLMs</b> remains in question. First, the suitability of current automatic metrics on <b>LLMs&rsquo;</b> simplification evaluation is still uncertain. Second, current human evaluation approaches in sentence simplification often fall into two extremes: they are either too superficial, failing to offer a clear understanding of the models&rsquo; performance, or overly detailed, making the annotation process complex and prone to inconsistency, which in turn affects the evaluation&rsquo;s reliability. To address these problems, this study provides in-depth insights into <b>LLMs&rsquo;</b> performance while ensuring the reliability of the evaluation. We design an error-based human annotation framework to assess the <b>GPT-4&rsquo;s</b> simplification capabilities. Results show that <b>GPT-4</b> generally generates fewer erroneous simplification outputs compared to the current state-of-the-art. However, <b>LLMs</b> have their limitations, as seen in <b>GPT-4&rsquo;s</b> struggles with lexical paraphrasing. Furthermore, we conduct meta-evaluations on widely used automatic metrics using our human annotations. We find that while these metrics are effective for significant quality differences, they lack sufficient sensitivity to assess the overall high-quality simplification by <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=2535--25270-towards-multimodal-sentiment-analysis-debiasing-via-bias-purification-dingkang-yang-et-al-2024>(25/35 | 25/270) Towards Multimodal Sentiment Analysis Debiasing via Bias Purification (Dingkang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingkang Yang, Mingcheng Li, Dongling Xiao, Yang Liu, Kun Yang, Zhaoyu Chen, Yuzheng Wang, Peng Zhai, Ke Li, Lihua Zhang. (2024)<br><strong>Towards Multimodal Sentiment Analysis Debiasing via Bias Purification</strong><br><button class=copy-to-clipboard title="Towards Multimodal Sentiment Analysis Debiasing via Bias Purification" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 32<br>Keywords: Graph, Benchmarking, Counter-factual, Multi-modal, Multi-modal, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05023v1.pdf filename=2403.05023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Sentiment</b> <b>Analysis</b> (MSA) aims to understand human intentions by integrating emotion-related clues from diverse modalities, such as visual, language, and audio. Unfortunately, the current MSA task invariably suffers from unplanned dataset biases, particularly <b>multimodal</b> utterance-level label bias and word-level context bias. These harmful biases potentially mislead models to focus on statistical shortcuts and spurious correlations, causing severe performance bottlenecks. To alleviate these issues, we present a <b>Multimodal</b> <b>Counterfactual</b> Inference <b>Sentiment</b> <b>(MCIS)</b> analysis framework based on causality rather than conventional likelihood. Concretely, we first formulate a causal <b>graph</b> to discover harmful biases from already-trained vanilla models. In the inference phase, given a factual <b>multimodal</b> input, MCIS imagines two <b>counterfactual</b> scenarios to purify and mitigate these biases. Then, MCIS can make unbiased decisions from biased observations by comparing factual and <b>counterfactual</b> outcomes. We conduct extensive experiments on several standard MSA <b>benchmarks.</b> Qualitative and quantitative results show the effectiveness of the proposed framework.</p></p class="citation"></blockquote><h3 id=2635--26270-rouge-k-do-your-summaries-have-keywords-sotaro-takeshita-et-al-2024>(26/35 | 26/270) ROUGE-K: Do Your Summaries Have Keywords? (Sotaro Takeshita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sotaro Takeshita, Simone Paolo Ponzetto, Kai Eckert. (2024)<br><strong>ROUGE-K: Do Your Summaries Have Keywords?</strong><br><button class=copy-to-clipboard title="ROUGE-K: Do Your Summaries Have Keywords?" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Transformer, Rouge, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05186v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05186v1.pdf filename=2403.05186v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Keywords, that is, content-relevant words in summaries play an important role in efficient information conveyance, making it critical to assess if system-generated summaries contain such informative words during evaluation. However, existing evaluation metrics for extreme <b>summarization</b> models do not pay explicit attention to keywords in summaries, leaving developers ignorant of their presence. To address this issue, we present a keyword-oriented evaluation metric, dubbed <b>ROUGE-K,</b> which provides a quantitative answer to the question of &ndash; \textit{How well do summaries include keywords?} Through the lens of this keyword-aware metric, we surprisingly find that a current strong baseline model often misses essential information in their summaries. Our analysis reveals that human annotators indeed find the summaries with more keywords to be more relevant to the source documents. This is an important yet previously overlooked aspect in evaluating <b>summarization</b> systems. Finally, to enhance keyword inclusion, we propose four approaches for incorporating word importance into a <b>transformer-based</b> model and experimentally show that it enables guiding models to include more keywords while keeping the overall quality. Our code is released at <a href=https://github.com/sobamchan/rougek>https://github.com/sobamchan/rougek</a>.</p></p class="citation"></blockquote><h3 id=2735--27270-tracing-the-roots-of-facts-in-multilingual-language-models-independent-shared-and-transferred-knowledge-xin-zhao-et-al-2024>(27/35 | 27/270) Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge (Xin Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhao, Naoki Yoshinaga, Daisuke Oba. (2024)<br><strong>Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge</strong><br><button class=copy-to-clipboard title="Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 25<br>Keywords: Low-Resource, Representation Learning, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05189v1.pdf filename=2403.05189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acquiring factual knowledge for language models (LMs) in <b>low-resource</b> languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and represent factual knowledge. Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual <b>BERT).</b> We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts. We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them. Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact <b>representation</b> <b>learning</b> in ML-LMs.</p></p class="citation"></blockquote><h3 id=2835--28270-seegull-multilingual-a-dataset-of-geo-culturally-situated-stereotypes-mukul-bhutani-et-al-2024>(28/35 | 28/270) SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes (Mukul Bhutani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mukul Bhutani, Kevin Robinson, Vinodkumar Prabhakaran, Shachi Dave, Sunipa Dev. (2024)<br><strong>SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes</strong><br><button class=copy-to-clipboard title="SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 20<br>Keywords: Fairness, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05696v1.pdf filename=2403.05696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While generative multilingual models are rapidly being deployed, their safety and <b>fairness</b> evaluations are largely limited to resources collected in English. This is especially problematic for evaluations targeting inherently socio-cultural phenomena such as stereotyping, where it is important to build multi-lingual resources that reflect the stereotypes prevalent in respective language communities. However, gathering these resources, at scale, in varied languages and regions pose a significant challenge as it requires broad socio-cultural knowledge and can also be prohibitively expensive. To overcome this critical gap, we employ a recently introduced approach that couples <b>LLM</b> generations for scale with culturally situated validations for reliability, and build SeeGULL Multilingual, a global-scale multilingual dataset of social stereotypes, containing over 25K stereotypes, spanning 20 languages, with human annotations across 23 regions, and demonstrate its utility in identifying gaps in model evaluations. Content warning: Stereotypes shared in this paper can be offensive.</p></p class="citation"></blockquote><h3 id=2935--29270-socialpet-socially-informed-pattern-exploiting-training-for-few-shot-stance-detection-in-social-media-parisa-jamadi-khiabani-et-al-2024>(29/35 | 29/270) SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot Stance Detection in Social Media (Parisa Jamadi Khiabani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parisa Jamadi Khiabani, Arkaitz Zubiaga. (2024)<br><strong>SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot Stance Detection in Social Media</strong><br><button class=copy-to-clipboard title="SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot Stance Detection in Social Media" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SI, cs.CL<br>Keyword Score: 20<br>Keywords: Few-shot, Stance Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05216v1.pdf filename=2403.05216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stance</b> <b>detection,</b> as the task of determining the viewpoint of a social media post towards a target as &lsquo;favor&rsquo; or &lsquo;against&rsquo;, has been understudied in the challenging yet realistic scenario where there is limited labeled data for a certain target. Our work advances research in <b>few-shot</b> <b>stance</b> <b>detection</b> by introducing SocialPET, a socially informed approach to leveraging language models for the task. Our proposed approach builds on the Pattern Exploiting Training (PET) technique, which addresses classification tasks as cloze questions through the use of language models. To enhance the approach with social awareness, we exploit the social network structure surrounding social media posts. We prove the effectiveness of SocialPET on two <b>stance</b> <b>datasets,</b> Multi-target and P-Stance, outperforming competitive <b>stance</b> <b>detection</b> models as well as the base model, PET, where the labeled instances for the target under study is as few as 100. When we delve into the results, we observe that SocialPET is comparatively strong in identifying instances of the `against&rsquo; class, where baseline models underperform.</p></p class="citation"></blockquote><h3 id=3035--30270-are-human-conversations-special-a-large-language-model-perspective-toshish-jawale-et-al-2024>(30/35 | 30/270) Are Human Conversations Special? A Large Language Model Perspective (Toshish Jawale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toshish Jawale, Chaitanya Animesh, Sekhar Vallath, Kartik Talamadupula, Larry Heck. (2024)<br><strong>Are Human Conversations Special? A Large Language Model Perspective</strong><br><button class=copy-to-clipboard title="Are Human Conversations Special? A Large Language Model Perspective" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05045v1.pdf filename=2403.05045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study analyzes changes in the attention mechanisms of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> when used to understand natural conversations between humans (human-human). We analyze three use cases of <b>LLMs:</b> interactions over web content, code, and mathematical texts. By analyzing attention distance, dispersion, and interdependency across these domains, we highlight the unique challenges posed by conversational data. Notably, conversations require nuanced handling of long-term contextual relationships and exhibit higher complexity through their attention patterns. Our findings reveal that while language models exhibit domain-specific attention behaviors, there is a significant gap in their ability to specialize in human conversations. Through detailed attention entropy analysis and t-SNE visualizations, we demonstrate the need for models trained with a diverse array of high-quality conversational data to enhance understanding and generation of human-like dialogue. This research highlights the importance of domain specialization in language models and suggests pathways for future advancement in modeling human conversational nuances.</p></p class="citation"></blockquote><h3 id=3135--31270-commitbench-a-benchmark-for-commit-message-generation-maximilian-schall-et-al-2024>(31/35 | 31/270) CommitBench: A Benchmark for Commit Message Generation (Maximilian Schall et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilian Schall, Tamara Czinczoll, Gerard de Melo. (2024)<br><strong>CommitBench: A Benchmark for Commit Message Generation</strong><br><button class=copy-to-clipboard title="CommitBench: A Benchmark for Commit Message Generation" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SE, cs.CL<br>Keyword Score: 16<br>Keywords: Benchmarking, Sample Size, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05188v1.pdf filename=2403.05188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Writing commit messages is a tedious daily task for many software developers, and often remains neglected. Automating this task has the potential to save time while ensuring that messages are informative. A high-quality dataset and an objective <b>benchmark</b> are vital preconditions for solid research and evaluation towards this goal. We show that existing datasets exhibit various problems, such as the quality of the commit selection, small <b>sample</b> <b>sizes,</b> duplicates, privacy issues, and missing licenses for redistribution. This can lead to unusable models and skewed evaluations, where inferior models achieve higher evaluation scores due to biases in the data. We compile a new large-scale dataset, CommitBench, adopting best practices for dataset creation. We <b>sample</b> <b>commits</b> from diverse projects with licenses that permit redistribution and apply our filtering and dataset enhancements to improve the quality of generated commit messages. We use CommitBench to compare existing models and show that other approaches are outperformed by a <b>Transformer</b> model pretrained on source code. We hope to accelerate future research by publishing the source code( <a href=https://github.com/Maxscha/commitbench>https://github.com/Maxscha/commitbench</a> ).</p></p class="citation"></blockquote><h3 id=3235--32270-generating-hard-negative-out-of-scope-data-with-chatgpt-for-intent-classification-zhijian-li-et-al-2024>(32/35 | 32/270) Generating Hard-Negative Out-of-Scope Data with ChatGPT for Intent Classification (Zhijian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhijian Li, Stefan Larson, Kevin Leach. (2024)<br><strong>Generating Hard-Negative Out-of-Scope Data with ChatGPT for Intent Classification</strong><br><button class=copy-to-clipboard title="Generating Hard-Negative Out-of-Scope Data with ChatGPT for Intent Classification" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05640v1.pdf filename=2403.05640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intent classifiers must be able to distinguish when a user&rsquo;s utterance does not belong to any supported intent to avoid producing incorrect and unrelated system responses. Although out-of-scope (OOS) detection for intent classifiers has been studied, previous work has not yet studied changes in classifier performance against hard-negative out-of-scope utterances (i.e., inputs that share common features with in-scope data, but are actually out-of-scope). We present an automated technique to generate hard-negative OOS data using <b>ChatGPT.</b> We use our technique to build five new hard-negative OOS datasets, and evaluate each against three <b>benchmark</b> intent classifiers. We show that classifiers struggle to correctly identify hard-negative OOS utterances more than general OOS utterances. Finally, we show that incorporating hard-negative OOS data for training improves model robustness when detecting hard-negative OOS data and general OOS data. Our technique, datasets, and evaluation address an important void in the field, offering a straightforward and inexpensive way to collect hard-negative OOS data and improve intent classifiers&rsquo; robustness.</p></p class="citation"></blockquote><h3 id=3335--33270-bayesian-preference-elicitation-with-language-models-kunal-handa-et-al-2024>(33/35 | 33/270) Bayesian Preference Elicitation with Language Models (Kunal Handa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunal Handa, Yarin Gal, Ellie Pavlick, Noah Goodman, Jacob Andreas, Alex Tamkin, Belinda Z. Li. (2024)<br><strong>Bayesian Preference Elicitation with Language Models</strong><br><button class=copy-to-clipboard title="Bayesian Preference Elicitation with Language Models" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05534v1.pdf filename=2403.05534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligning AI systems to users&rsquo; interests requires understanding and incorporating humans&rsquo; complex values and preferences. Recently, language models (LMs) have been used to gather information about the preferences of human users. This preference data can be used to <b>fine-tune</b> or guide other LMs and/or AI systems. However, LMs have been shown to struggle with crucial aspects of preference learning: quantifying uncertainty, modeling human mental states, and asking informative questions. These challenges have been addressed in other areas of machine learning, such as Bayesian Optimal Experimental Design (BOED), which focus on designing informative queries within a well-defined feature space. But these methods, in turn, are difficult to scale and apply to real-world problems where simply identifying the relevant features can be difficult. We introduce OPEN (Optimal Preference Elicitation with Natural language) a framework that uses BOED to guide the choice of informative questions and an LM to extract features and translate abstract BOED queries into natural language questions. By combining the flexibility of LMs with the rigor of BOED, OPEN can optimize the informativity of queries while remaining adaptable to real-world domains. In user studies, we find that OPEN outperforms existing LM- and BOED-based methods for preference elicitation.</p></p class="citation"></blockquote><h3 id=3435--34270-ffstc-fongbe-to-french-speech-translation-corpus-d-fortune-kponou-et-al-2024>(34/35 | 34/270) FFSTC: Fongbe to French Speech Translation Corpus (D. Fortune Kponou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>D. Fortune Kponou, Frejus A. A. Laleye, Eugene C. Ezin. (2024)<br><strong>FFSTC: Fongbe to French Speech Translation Corpus</strong><br><button class=copy-to-clipboard title="FFSTC: Fongbe to French Speech Translation Corpus" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Speech-to-Speech Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05488v1.pdf filename=2403.05488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce the Fongbe to French <b>Speech</b> <b>Translation</b> Corpus (FFSTC) for the first time. This corpus encompasses approximately 31 hours of collected Fongbe language content, featuring both French transcriptions and corresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset compiled through various collection methods and the efforts of dedicated individuals. Furthermore, we conduct baseline experiments using Fairseq&rsquo;s transformer_s and conformer models to evaluate data quality and validity. Our results indicate a score of 8.96 for the transformer_s model and 8.14 for the conformer model, establishing a baseline for the FFSTC corpus.</p></p class="citation"></blockquote><h3 id=3535--35270-rule-driven-news-captioning-ning-xu-et-al-2024>(35/35 | 35/270) Rule-driven News Captioning (Ning Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ning Xu, Tingting Zhang, Hongshuo Tian, An-An Liu. (2024)<br><strong>Rule-driven News Captioning</strong><br><button class=copy-to-clipboard title="Rule-driven News Captioning" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: BART<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05101v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05101v2.pdf filename=2403.05101v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>News captioning task aims to generate sentences by describing named entities or concrete events for an image with its news article. Existing methods have achieved remarkable results by relying on the large-scale pre-trained models, which primarily focus on the correlations between the input news content and the output predictions. However, the news captioning requires adhering to some fundamental rules of news reporting, such as accurately describing the individuals and actions associated with the event. In this paper, we propose the rule-driven news captioning method, which can generate image descriptions following designated rule signal. Specifically, we first design the news-aware semantic rule for the descriptions. This rule incorporates the primary action depicted in the image (e.g., &ldquo;performing&rdquo;) and the roles played by named entities involved in the action (e.g., &ldquo;Agent&rdquo; and &ldquo;Place&rdquo;). Second, we inject this semantic rule into the large-scale pre-trained model, <b>BART,</b> with the prefix-tuning strategy, where multiple encoder layers are embedded with news-aware semantic rule. Finally, we can effectively guide <b>BART</b> to generate news sentences that comply with the designated rule. Extensive experiments on two widely used datasets (i.e., GoodNews and NYTimes800k) demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h2 id=csai-18>cs.AI (18)</h2><h3 id=118--36270-deepseek-vl-towards-real-world-vision-language-understanding-haoyu-lu-et-al-2024>(1/18 | 36/270) DeepSeek-VL: Towards Real-World Vision-Language Understanding (Haoyu Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan. (2024)<br><strong>DeepSeek-VL: Towards Real-World Vision-Language Understanding</strong><br><button class=copy-to-clipboard title="DeepSeek-VL: Towards Real-World Vision-Language Understanding" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 83<br>Keywords: Optical Character Recognition, Benchmarking, Fine-tuning, Foundation Model, Chatbot, Instruction Tuning, Large Language Model, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05525v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05525v2.pdf filename=2403.05525v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DeepSeek-VL, an open-source <b>Vision-Language</b> (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions: We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, <b>OCR,</b> charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an <b>instruction</b> <b>tuning</b> dataset accordingly. The <b>fine-tuning</b> with this dataset substantially improves the model&rsquo;s user experience in practical applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. This design choice ensures the model&rsquo;s ability to capture critical semantic and detailed information across various visual tasks. We posit that a proficient <b>Vision-Language</b> Model should, foremost, possess strong language abilities. To ensure the preservation of <b>LLM</b> capabilities during pretraining, we investigate an effective VL pretraining strategy by integrating <b>LLM</b> training from the beginning and carefully managing the competitive dynamics observed between vision and language modalities. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a <b>vision-language</b> <b>chatbot</b> in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language <b>benchmarks</b> at the same model size while maintaining robust performance on language-centric <b>benchmarks.</b> We have made both 1.3B and 7B models publicly accessible to foster innovations based on this <b>foundation</b> <b>model.</b></p></p class="citation"></blockquote><h3 id=218--37270-bjtt-a-large-scale-multimodal-dataset-for-traffic-prediction-chengyang-zhang-et-al-2024>(2/18 | 37/270) BjTT: A Large-scale Multimodal Dataset for Traffic Prediction (Chengyang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengyang Zhang, Yong Zhang, Qitan Shao, Bo Li, Yisheng Lv, Xinglin Piao, Baocai Yin. (2024)<br><strong>BjTT: A Large-scale Multimodal Dataset for Traffic Prediction</strong><br><button class=copy-to-clipboard title="BjTT: A Large-scale Multimodal Dataset for Traffic Prediction" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 62<br>Keywords: Diffusion Model, Graph Convolutional Network, Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05029v1.pdf filename=2403.05029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) limited performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation, and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first <b>diffusion</b> <b>model</b> for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a <b>diffusion</b> <b>model</b> with the <b>Graph</b> <b>Convolutional</b> <b>Network</b> <b>(GCN)</b> to extract spatial correlations of traffic data. In addition, we construct a large dataset containing text-traffic pairs for the TTG task. We <b>benchmarked</b> our model qualitatively and quantitatively on the released dataset. The experimental results indicate that ChatTraffic can generate realistic traffic situations from the text. Our code and dataset are available at <a href=https://github.com/ChyaZhang/ChatTraffic>https://github.com/ChyaZhang/ChatTraffic</a>.</p></p class="citation"></blockquote><h3 id=318--38270-can-large-language-models-play-games-a-case-study-of-a-self-play-approach-hongyi-guo-et-al-2024>(3/18 | 38/270) Can Large Language Models Play Games? A Case Study of A Self-Play Approach (Hongyi Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyi Guo, Zhihan Liu, Yufeng Zhang, Zhaoran Wang. (2024)<br><strong>Can Large Language Models Play Games? A Case Study of A Self-Play Approach</strong><br><button class=copy-to-clipboard title="Can Large Language Models Play Games? A Case Study of A Self-Play Approach" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 60<br>Keywords: Pruning, Simulation, Simulator, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05632v1.pdf filename=2403.05632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While <b>LLMs</b> have proven beneficial as decision-making aids, their reliability is hampered by limitations in <b>reasoning,</b> hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic <b>pruning</b> and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters <b>LLMs</b> with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize <b>LLMs</b> as both action pruners and proxies for value functions without the need for additional training. We theoretically prove that the suboptimality of the estimated value in our proposed method scales with $\tilde{\mathcal O}\Bigl(\frac{|\tilde {\mathcal A}|}{\sqrt{N}} + \epsilon_\mathrm{pruner} + \epsilon_\mathrm{critic}\Bigr)$, where (N) is the number of <b>simulations,</b> $|\tilde {\mathcal A}|$ is the cardinality of the pruned action space by <b>LLM,</b> and $\epsilon_\mathrm{pruner}$ and $\epsilon_\mathrm{critic}$ quantify the errors incurred by adopting <b>LLMs</b> as action space pruner and value function proxy, respectively. Our experiments in chess and go demonstrate the capability of our method to address challenges beyond the scope of MCTS and improve the performance of the directly application of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=418--39270-decomposing-vision-based-llm-predictions-for-auto-evaluation-with-gpt-4-qingqing-zhu-et-al-2024>(4/18 | 39/270) Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4 (Qingqing Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingqing Zhu, Benjamin Hou, Tejas S. Mathai, Pritam Mukherjee, Qiao Jin, Xiuying Chen, Zhizheng Wang, Ruida Cheng, Ronald M. Summers, Zhiyong Lu. (2024)<br><strong>Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4</strong><br><button class=copy-to-clipboard title="Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs.AI<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05680v1.pdf filename=2403.05680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The volume of CT exams being done in the world has been rising every year, which has led to radiologist burn-out. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have the potential to reduce their burden, but their adoption in the clinic depends on radiologist trust, and easy evaluation of generated content. Presently, many automated methods are available to evaluate the reports generated for chest radiographs, but such an approach is not available for CT presently. In this paper, we propose a novel evaluation framework to judge the capabilities of <b>vision-language</b> <b>LLMs</b> in generating accurate summaries of CT-based abnormalities. CT slices containing an abnormality (e.g., lesion) were input to a vision-based <b>LLM</b> <b>(GPT-4V,</b> LLaVA-Med, and RadFM), and it generated a free-text summary of the predicted characteristics of the abnormality. Next, a <b>GPT-4</b> model decomposed the summary into specific aspects (body part, location, type, and attributes), automatically evaluated the characteristics against the ground-truth, and generated a score for each aspect based on its clinical relevance and factual accuracy. These scores were then contrasted against those obtained from a clinician, and a high correlation ( 85%, p &lt; .001) was observed. Although <b>GPT-4V</b> outperformed other models in our evaluation, it still requires overall improvement. Our evaluation method offers valuable insights into the specific areas that need the most enhancement, guiding future development in this field.</p></p class="citation"></blockquote><h3 id=518--40270-tuning-free-accountable-intervention-for-llm-deployment----a-metacognitive-approach-zhen-tan-et-al-2024>(5/18 | 40/270) Tuning-Free Accountable Intervention for LLM Deployment &ndash; A Metacognitive Approach (Zhen Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Tan, Jie Peng, Tianlong Chen, Huan Liu. (2024)<br><strong>Tuning-Free Accountable Intervention for LLM Deployment &ndash; A Metacognitive Approach</strong><br><button class=copy-to-clipboard title="Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 50<br>Keywords: Few-shot, Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05636v1.pdf filename=2403.05636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have catalyzed transformative advances across a spectrum of natural language processing tasks through <b>few-shot</b> or <b>zero-shot</b> <b>prompting,</b> bypassing the need for parameter tuning. While convenient, this modus operandi aggravates <code>hallucination'' concerns, particularly given the enigmatic </code>black-box&rsquo;&rsquo; nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences. In contrast, human decision-making relies on nuanced cognitive processes, such as the ability to sense and adaptively correct misjudgments through conceptual understanding. Drawing inspiration from human cognition, we propose an innovative \textit{metacognitive} approach, dubbed \textbf{CLEAR}, to equip <b>LLMs</b> with capabilities for self-aware error identification and correction. Our framework facilitates the construction of concept-specific sparse subnetworks that illuminate transparent decision pathways. This provides a novel interface for model \textit{intervention} after deployment. Our intervention offers compelling advantages: (\textit{i})~at deployment or inference time, our metacognitive <b>LLMs</b> can self-consciously identify potential mispredictions with minimum human involvement, (\textit{ii})~the model has the capability to self-correct its errors efficiently, obviating the need for additional tuning, and (\textit{iii})~the rectification procedure is not only self-explanatory but also user-friendly, enhancing the interpretability and accessibility of the model. By integrating these metacognitive features, our approach pioneers a new path toward engendering greater trustworthiness and accountability in the deployment of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=618--41270-tell-me-the-truth-a-system-to-measure-the-trustworthiness-of-large-language-models-carlo-lipizzi-2024>(6/18 | 41/270) Tell me the truth: A system to measure the trustworthiness of Large Language Models (Carlo Lipizzi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlo Lipizzi. (2024)<br><strong>Tell me the truth: A system to measure the trustworthiness of Large Language Models</strong><br><button class=copy-to-clipboard title="Tell me the truth: A system to measure the trustworthiness of Large Language Models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs.AI<br>Keyword Score: 48<br>Keywords: Graph, Fine-tuning, Knowledge Graph, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04964v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04964v2.pdf filename=2403.04964v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> have taken the front seat in most of the news since November 2022, when <b>ChatGPT</b> was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), <b>ChatGPT-4</b> showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. &lsquo;24 study by JAMA Pediatrics found that <b>ChatGPT</b> has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is &ldquo;trust&rdquo;? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a <b>knowledge</b> <b>graph</b> of the domain. The approach is a process with humans in the loop to validate the representation of the domain and to <b>fine-tune</b> the system. Measuring the trustworthiness would be essential for all the entities operating in critical environments, such as healthcare, defense, finance, but it would be very relevant for all the users of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=718--42270-sora-as-an-agi-world-model-a-complete-survey-on-text-to-video-generation-joseph-cho-et-al-2024>(7/18 | 42/270) Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation (Joseph Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng, Lik-Hang Lee, Tae-Ho Kim, Choong Seon Hong, Chaoning Zhang. (2024)<br><strong>Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation</strong><br><button class=copy-to-clipboard title="Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs.AI<br>Keyword Score: 30<br>Keywords: Sora, Generative AI, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05131v1.pdf filename=2403.05131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-video generation marks a significant frontier in the rapidly evolving domain of <b>generative</b> <b>AI,</b> integrating advancements in <b>text-to-image</b> synthesis, video captioning, and text-guided editing. This survey critically examines the progression of text-to-video technologies, focusing on the shift from traditional <b>generative</b> <b>models</b> to the cutting-edge <b>Sora</b> model, highlighting developments in scalability and generalizability. Distinguishing our analysis from prior works, we offer an in-depth exploration of the technological frameworks and evolutionary pathways of these models. Additionally, we delve into practical applications and address ethical and technological challenges such as the inability to perform multiple entity handling, comprehend causal-effect learning, understand physical interaction, perceive object scaling and proportioning, and combat object hallucination which is also a long-standing problem in <b>generative</b> <b>models.</b> Our comprehensive discussion covers the topic of enablement of text-to-video generation models as human-assistive tools and world models, as well as eliciting model&rsquo;s shortcomings and summarizing future improvement direction that mainly centers around training datasets and evaluation metrics (both automatic and human-centered). Aimed at both newcomers and seasoned researchers, this survey seeks to catalyze further innovation and discussion in the growing field of text-to-video generation, paving the way for more reliable and practical <b>generative</b> <b>artificial</b> intelligence technologies.</p></p class="citation"></blockquote><h3 id=818--43270-from-chain-to-tree-refining-chain-like-rules-into-tree-like-rules-on-knowledge-graphs-wangtao-sun-et-al-2024>(8/18 | 43/270) From Chain to Tree: Refining Chain-like Rules into Tree-like Rules on Knowledge Graphs (Wangtao Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wangtao Sun, Shizhu He, Jun Zhao, Kang Liu. (2024)<br><strong>From Chain to Tree: Refining Chain-like Rules into Tree-like Rules on Knowledge Graphs</strong><br><button class=copy-to-clipboard title="From Chain to Tree: Refining Chain-like Rules into Tree-like Rules on Knowledge Graphs" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Grounding, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05130v1.pdf filename=2403.05130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With good explanatory power and controllability, rule-based methods play an important role in many tasks such as <b>knowledge</b> <b>reasoning</b> and decision support. However, existing studies primarily focused on learning chain-like rules, which limit their semantic expressions and accurate prediction abilities. As a result, chain-like rules usually fire on the incorrect <b>grounding</b> values, producing inaccurate or even erroneous <b>reasoning</b> results. In this paper, we propose the concept of tree-like rules on <b>knowledge</b> <b>graphs</b> to expand the application scope and improve the <b>reasoning</b> ability of rule-based methods. Meanwhile, we propose an effective framework for refining chain-like rules into tree-like rules. Experimental comparisons on four public datasets show that the proposed framework can easily adapt to other chain-like rule induction methods and the refined tree-like rules consistently achieve better performances than chain-like rules on link prediction. The data and code of this paper can be available at <a href=https://anonymous.4open.science/r/tree-rule-E3CD/>https://anonymous.4open.science/r/tree-rule-E3CD/</a>.</p></p class="citation"></blockquote><h3 id=918--44270-tapilot-crossing-benchmarking-and-evolving-llms-towards-interactive-data-analysis-agents-jinyang-li-et-al-2024>(9/18 | 44/270) Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents (Jinyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyang Li, Nan Huo, Yan Gao, Jiayi Shi, Yingxiu Zhao, Ge Qu, Yurong Wu, Chenhao Ma, Jian-Guang Lou, Reynold Cheng. (2024)<br><strong>Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents</strong><br><button class=copy-to-clipboard title="Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05307v1.pdf filename=2403.05307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interactive Data Analysis, the collaboration between humans and <b>LLM</b> agents, enables real-time data exploration for informed decision-making. The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> agents in this task. To mitigate this issue, we introduce Tapilot-Crossing, a new <b>benchmark</b> to evaluate <b>LLM</b> agents on interactive data analysis. Tapilot-Crossing contains 1024 interactions, covering 4 practical scenarios: Normal, Action, Private, and Private Action. Notably, Tapilot-Crossing is constructed by an economical multi-agent environment, Decision Company, with few human efforts. We evaluate popular and advanced <b>LLM</b> agents in Tapilot-Crossing, which underscores the challenges of interactive data analysis. Furthermore, we propose Adaptive Interaction Reflection (AIR), a self-generated reflection strategy that guides <b>LLM</b> agents to learn from successful history. Experiments demonstrate that Air can evolve <b>LLMs</b> into effective interactive data analysis agents, achieving a relative performance improvement of up to 44.5%.</p></p class="citation"></blockquote><h3 id=1018--45270-towards-multimodal-human-intention-understanding-debiasing-via-subject-deconfounding-dingkang-yang-et-al-2024>(10/18 | 45/270) Towards Multimodal Human Intention Understanding Debiasing via Subject-Deconfounding (Dingkang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingkang Yang, Dongling Xiao, Ke Li, Yuzheng Wang, Zhaoyu Chen, Jinjie Wei, Lihua Zhang. (2024)<br><strong>Towards Multimodal Human Intention Understanding Debiasing via Subject-Deconfounding</strong><br><button class=copy-to-clipboard title="Towards Multimodal Human Intention Understanding Debiasing via Subject-Deconfounding" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 22<br>Keywords: Graph, Benchmarking, Causal Intervention, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05025v1.pdf filename=2403.05025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> intention understanding (MIU) is an indispensable component of human expression analysis (e.g., sentiment or humor) from heterogeneous modalities, including visual postures, linguistic contents, and acoustic behaviors. Existing works invariably focus on designing sophisticated structures or fusion strategies to achieve impressive improvements. Unfortunately, they all suffer from the subject variation problem due to data distribution discrepancies among subjects. Concretely, MIU models are easily misled by distinct subjects with different expression customs and characteristics in the training data to learn subject-specific spurious correlations, significantly limiting performance and generalizability across uninitiated subjects.Motivated by this observation, we introduce a recapitulative <b>causal</b> <b>graph</b> to formulate the MIU procedure and analyze the confounding effect of subjects. Then, we propose SuCI, a simple yet effective <b>causal</b> <b>intervention</b> module to disentangle the impact of subjects acting as unobserved confounders and achieve model training via true <b>causal</b> <b>effects.</b> As a plug-and-play component, SuCI can be widely applied to most methods that seek unbiased predictions. Comprehensive experiments on several MIU <b>benchmarks</b> clearly demonstrate the effectiveness of the proposed module.</p></p class="citation"></blockquote><h3 id=1118--46270-algorithmic-identification-of-essential-exogenous-nodes-for-causal-sufficiency-in-brain-networks-abdolmahdi-bagheri-et-al-2024>(11/18 | 46/270) Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks (Abdolmahdi Bagheri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdolmahdi Bagheri, Mahdi Dehshiri, Babak Nadjar Araabi, Alireza Akhondi Asl. (2024)<br><strong>Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks</strong><br><button class=copy-to-clipboard title="Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05407v1.pdf filename=2403.05407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the investigation of any causal mechanisms, such as the brain&rsquo;s causal networks, the assumption of causal sufficiency plays a critical role. Notably, neglecting this assumption can result in significant errors, a fact that is often disregarded in the causal analysis of brain networks. In this study, we propose an algorithmic identification approach for determining essential exogenous nodes that satisfy the critical need for causal sufficiency to adhere to it in such inquiries. Our approach consists of three main steps: First, by capturing the essence of the Peter-Clark (PC) algorithm, we conduct independence tests for pairs of regions within a network, as well as for the same pairs conditioned on nodes from other networks. Next, we distinguish candidate confounders by analyzing the differences between the conditional and unconditional results, using the Kolmogorov-Smirnov test. Subsequently, we utilize Non-Factorized identifiable <b>Variational</b> <b>Autoencoders</b> (NF-iVAE) along with the Correlation Coefficient index (CCI) metric to identify the confounding variables within these candidate nodes. Applying our method to the Human Connectome Projects (HCP) movie-watching task data, we demonstrate that while interactions exist between dorsal and ventral regions, only dorsal regions serve as confounders for the visual networks, and vice versa. These findings align consistently with those resulting from the neuroscientific perspective. Finally, we show the reliability of our results by testing 30 independent runs for NF-iVAE initialization.</p></p class="citation"></blockquote><h3 id=1218--47270-predicting-single-cell-drug-sensitivity-by-adaptive-weighted-feature-for-adversarial-multi-source-domain-adaptation-wei-duan-et-al-2024>(12/18 | 47/270) Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for Adversarial Multi-source Domain Adaptation (Wei Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Duan, Hui Liu. (2024)<br><strong>Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for Adversarial Multi-source Domain Adaptation</strong><br><button class=copy-to-clipboard title="Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for Adversarial Multi-source Domain Adaptation" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Autoencoder, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05260v1.pdf filename=2403.05260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of single-cell sequencing technology had promoted the generation of a large amount of single-cell transcriptional profiles, providing valuable opportunities to explore drug-resistant cell subpopulations in a tumor. However, the drug sensitivity data in single-cell level is still scarce to date, pressing an urgent and highly challenging task for computational prediction of the drug sensitivity to individual cells. This paper proposed scAdaDrug, a multi-source adaptive weighting model to predict single-cell drug sensitivity. We used an <b>autoencoder</b> to extract <b>domain-invariant</b> <b>features</b> related to drug sensitivity from multiple source <b>domains</b> <b>by</b> exploiting adversarial <b>domain</b> <b>adaptation.</b> Especially, we introduced an adaptive weight generator to produce importance-aware and mutual independent weights, which could adaptively modulate the embedding of each sample in dimension-level for both source and target <b>domains.</b> <b>Extensive</b> experimental results showed that our model achieved state-of-the-art performance in predicting drug sensitivity on sinle-cell datasets, as well as on cell line and patient datasets.</p></p class="citation"></blockquote><h3 id=1318--48270-rlperi-accelerating-visual-perimetry-test-with-reinforcement-learning-and-convolutional-feature-extraction-tanvi-verma-et-al-2024>(13/18 | 48/270) RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning and Convolutional Feature Extraction (Tanvi Verma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanvi Verma, Linh Le Dinh, Nicholas Tan, Xinxing Xu, Chingyu Cheng, Yong Liu. (2024)<br><strong>RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning and Convolutional Feature Extraction</strong><br><button class=copy-to-clipboard title="RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning and Convolutional Feature Extraction" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Convolution, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05112v1.pdf filename=2403.05112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual perimetry is an important eye examination that helps detect vision problems caused by ocular or neurological conditions. During the test, a patient&rsquo;s gaze is fixed at a specific location while light stimuli of varying intensities are presented in central and peripheral vision. Based on the patient&rsquo;s responses to the stimuli, the visual field mapping and sensitivity are determined. However, maintaining high levels of concentration throughout the test can be challenging for patients, leading to increased examination times and decreased accuracy. In this work, we present RLPeri, a <b>reinforcement</b> <b>learning-based</b> approach to optimize visual perimetry testing. By determining the optimal sequence of locations and initial stimulus values, we aim to reduce the examination time without compromising accuracy. Additionally, we incorporate reward shaping techniques to further improve the testing performance. To monitor the patient&rsquo;s responses over time during testing, we represent the test&rsquo;s state as a pair of 3D matrices. We apply two different <b>convolutional</b> kernels to extract spatial features across locations as well as features across different stimulus values for each location. Through experiments, we demonstrate that our approach results in a 10-20% reduction in examination time while maintaining the accuracy as compared to state-of-the-art methods. With the presented approach, we aim to make visual perimetry testing more efficient and patient-friendly, while still providing accurate results.</p></p class="citation"></blockquote><h3 id=1418--49270-efficient-public-health-intervention-planning-using-decomposition-based-decision-focused-learning-sanket-shah-et-al-2024>(14/18 | 49/270) Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning (Sanket Shah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanket Shah, Arun Suggala, Milind Tambe, Aparna Taneja. (2024)<br><strong>Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning</strong><br><button class=copy-to-clipboard title="Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05683v1.pdf filename=2403.05683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The declining participation of beneficiaries over time is a key concern in public health programs. A popular strategy for improving retention is to have health workers `intervene&rsquo; on beneficiaries at risk of dropping out. However, the availability and time of these health workers are limited resources. As a result, there has been a line of research on optimizing these limited intervention resources using Restless Multi-Armed <b>Bandits</b> (RMABs). The key technical barrier to using this framework in practice lies in the need to estimate the beneficiaries&rsquo; RMAB parameters from historical data. Recent research has shown that Decision-Focused Learning (DFL), which focuses on maximizing the beneficiaries&rsquo; adherence rather than predictive accuracy, improves the performance of intervention targeting using RMABs. Unfortunately, these gains come at a high computational cost because of the need to solve and evaluate the RMAB in each DFL training step. In this paper, we provide a principled way to exploit the structure of RMABs to speed up intervention planning by cleverly decoupling the planning for different beneficiaries. We use real-world data from an Indian NGO, ARMMAN, to show that our approach is up to two orders of magnitude faster than the state-of-the-art approach while also yielding superior model performance. This would enable the NGO to scale up deployments using DFL to potentially millions of mothers, ultimately advancing progress toward UNSDG 3.1.</p></p class="citation"></blockquote><h3 id=1518--50270-a-feature-based-generalizable-prediction-model-for-both-perceptual-and-abstract-reasoning-quan-do-et-al-2024>(15/18 | 50/270) A Feature-based Generalizable Prediction Model for Both Perceptual and Abstract Reasoning (Quan Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quan Do, Thomas M. Morin, Chantal E. Stern, Michael E. Hasselmo. (2024)<br><strong>A Feature-based Generalizable Prediction Model for Both Perceptual and Abstract Reasoning</strong><br><button class=copy-to-clipboard title="A Feature-based Generalizable Prediction Model for Both Perceptual and Abstract Reasoning" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI, q-bio-NC<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05641v1.pdf filename=2403.05641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A hallmark of human intelligence is the ability to infer abstract rules from limited experience and apply these rules to unfamiliar situations. This capacity is widely studied in the visual domain using the Raven&rsquo;s Progressive Matrices. Recent advances in deep learning have led to multiple artificial neural network models matching or even surpassing human performance. However, while humans can identify and express the rule underlying these tasks with little to no exposure, contemporary neural networks often rely on massive pattern-based training and cannot express or extrapolate the rule inferred from the task. Furthermore, most Raven&rsquo;s Progressive Matrices or Raven-like tasks used for neural network training used symbolic representations, whereas humans can flexibly switch between symbolic and continuous perceptual representations. In this work, we present an algorithmic approach to rule detection and application using feature detection, affine transformation estimation and search. We applied our model to a simplified Raven&rsquo;s Progressive Matrices task, previously designed for behavioral testing and neuroimaging in humans. The model exhibited one-shot learning and achieved near human-level performance in the symbolic <b>reasoning</b> condition of the simplified task. Furthermore, the model can express the relationships discovered and generate multi-step predictions in accordance with the underlying rule. Finally, the model can reason using continuous patterns. We discuss our results and their relevance to studying abstract <b>reasoning</b> in humans, as well as their implications for improving intelligent machines.</p></p class="citation"></blockquote><h3 id=1618--51270-multi-agent-reinforcement-learning-with-a-hierarchy-of-reward-machines-xuejing-zheng-et-al-2024>(16/18 | 51/270) Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines (Xuejing Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuejing Zheng, Chao Yu. (2024)<br><strong>Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines</strong><br><button class=copy-to-clipboard title="Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-MA, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07005v1.pdf filename=2403.07005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the cooperative Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL) problems using Reward Machines (RMs) to specify the reward functions such that the prior knowledge of high-level events in a task can be leveraged to facilitate the learning efficiency. Unlike the existing work that RMs have been incorporated into MARL for task decomposition and policy learning in relatively simple domains or with an assumption of independencies among the agents, we present Multi-Agent <b>Reinforcement</b> <b>Learning</b> with a Hierarchy of RMs (MAHRM) that is capable of dealing with more complex scenarios when the events among agents can occur concurrently and the agents are highly interdependent. MAHRM exploits the relationship of high-level events to decompose a task into a hierarchy of simpler subtasks that are assigned to a small group of agents, so as to reduce the overall computational complexity. Experimental results in three cooperative MARL domains show that MAHRM outperforms other MARL methods using the same prior knowledge of high-level events.</p></p class="citation"></blockquote><h3 id=1718--52270-mmoe-robust-spoiler-detection-with-multi-modal-information-and-domain-aware-mixture-of-experts-zinan-zeng-et-al-2024>(17/18 | 52/270) MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts (Zinan Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zinan Zeng, Sen Ye, Zijian Cai, Heng Wang, Yuhan Liu, Qinghua Zheng, Minnan Luo. (2024)<br><strong>MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts</strong><br><button class=copy-to-clipboard title="MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 6<br>Keywords: Graph, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05265v1.pdf filename=2403.05265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online movie review websites are valuable for information and discussion about movies. However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task. Previous methods simply focus on reviews&rsquo; text content, ignoring the heterogeneity of information in the platform. For instance, the metadata and the corresponding user&rsquo;s information of a review could be helpful. Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods. To this end, we propose MMoE, a <b>multi-modal</b> network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization. MMoE first extracts <b>graph,</b> text, and meta feature from the user-movie network, the review&rsquo;s textual content, and the review&rsquo;s metadata respectively. To handle genre-specific spoilers, we then adopt Mixture-of-Experts architecture to process information in three modalities to promote robustness. Finally, we use an expert fusion layer to integrate the features from different perspectives and make predictions based on the fused embedding. Experiments demonstrate that MMoE achieves state-of-the-art performance on two widely-used spoiler detection datasets, surpassing previous SOTA methods by 2.56% and 8.41% in terms of accuracy and F1-score. Further experiments also demonstrate MMoE&rsquo;s superiority in robustness and generalization.</p></p class="citation"></blockquote><h3 id=1818--53270-looking-ahead-to-avoid-being-late-solving-hard-constrained-traveling-salesman-problem-jingxiao-chen-et-al-2024>(18/18 | 53/270) Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling Salesman Problem (Jingxiao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingxiao Chen, Ziqin Gong, Minghuan Liu, Jun Wang, Yong Yu, Weinan Zhang. (2024)<br><strong>Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling Salesman Problem</strong><br><button class=copy-to-clipboard title="Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling Salesman Problem" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05318v1.pdf filename=2403.05318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many real-world problems can be formulated as a constrained Traveling Salesman Problem (TSP). However, the constraints are always complex and numerous, making the TSPs challenging to solve. When the number of complicated constraints grows, it is time-consuming for traditional heuristic algorithms to avoid illegitimate outcomes. Learning-based methods provide an alternative to solve TSPs in a soft manner, which also supports GPU acceleration to generate solutions quickly. Nevertheless, the soft manner inevitably results in difficulty solving hard-constrained problems with learning algorithms, and the conflicts between legality and optimality may substantially affect the optimality of the solution. To overcome this problem and to have an effective solution against hard constraints, we proposed a novel learning-based method that uses looking-ahead information as the feature to improve the legality of TSP with Time Windows (TSPTW) solutions. Besides, we constructed TSPTW datasets with hard constraints in order to accurately evaluate and <b>benchmark</b> the statistical performance of various approaches, which can serve the community for future research. With comprehensive experiments on diverse datasets, MUSLA outperforms existing baselines and shows generalizability potential.</p></p class="citation"></blockquote><h2 id=cscr-9>cs.CR (9)</h2><h3 id=19--54270-dp-tabicl-in-context-learning-with-differentially-private-tabular-data-alycia-n-carey-et-al-2024>(1/9 | 54/270) DP-TabICL: In-Context Learning with Differentially Private Tabular Data (Alycia N. Carey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alycia N. Carey, Karuna Bhaila, Kennedy Edemacu, Xintao Wu. (2024)<br><strong>DP-TabICL: In-Context Learning with Differentially Private Tabular Data</strong><br><button class=copy-to-clipboard title="DP-TabICL: In-Context Learning with Differentially Private Tabular Data" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 80<br>Keywords: Fine-tuning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05681v1.pdf filename=2403.05681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> <b>(ICL)</b> enables <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to adapt to new tasks by conditioning on demonstrations of question-answer pairs and it has been shown to have comparable performance to costly model retraining and <b>fine-tuning.</b> Recently, <b>ICL</b> has been extended to allow tabular data to be used as demonstration examples by serializing individual records into natural language formats. However, it has been shown that <b>LLMs</b> can leak information contained in <b>prompts,</b> and since tabular data often contain sensitive information, understanding how to protect the underlying tabular data used in <b>ICL</b> is a critical area of research. This work serves as an initial investigation into how to use <b>differential</b> <b>privacy</b> (DP) &ndash; the long-established gold standard for data privacy and anonymization &ndash; to protect tabular data used in <b>ICL.</b> Specifically, we investigate the application of DP mechanisms for private tabular <b>ICL</b> via data privatization prior to serialization and <b>prompting.</b> We formulate two private <b>ICL</b> frameworks with provable privacy guarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios via injecting noise into individual records or group statistics, respectively. We evaluate our DP-based frameworks on eight real-world tabular datasets and across multiple <b>ICL</b> and DP settings. Our evaluations show that DP-based <b>ICL</b> can protect the privacy of the underlying tabular data while achieving comparable performance to non-LLM baselines, especially under high privacy regimes.</p></p class="citation"></blockquote><h3 id=29--55270-defending-against-unforeseen-failure-modes-with-latent-adversarial-training-stephen-casper-et-al-2024>(2/9 | 55/270) Defending Against Unforeseen Failure Modes with Latent Adversarial Training (Stephen Casper et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephen Casper, Lennart Schulze, Oam Patel, Dylan Hadfield-Menell. (2024)<br><strong>Defending Against Unforeseen Failure Modes with Latent Adversarial Training</strong><br><button class=copy-to-clipboard title="Defending Against Unforeseen Failure Modes with Latent Adversarial Training" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Text Classification, Text Generation, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05030v1.pdf filename=2403.05030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and <b>adversarial</b> <b>training</b> (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent <b>adversarial</b> <b>training</b> (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of <b>adversarial</b> <b>attacks.</b> We show in image classification, <b>text</b> <b>classification,</b> and <b>text</b> <b>generation</b> tasks that LAT usually improves both robustness and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.</p></p class="citation"></blockquote><h3 id=39--56270-secgpt-an-execution-isolation-architecture-for-llm-based-systems-yuhao-wu-et-al-2024>(3/9 | 56/270) SecGPT: An Execution Isolation Architecture for LLM-Based Systems (Yuhao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal. (2024)<br><strong>SecGPT: An Execution Isolation Architecture for LLM-Based Systems</strong><br><button class=copy-to-clipboard title="SecGPT: An Execution Isolation Architecture for LLM-Based Systems" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-CY, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04960v1.pdf filename=2403.04960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> extended as systems, such as <b>ChatGPT,</b> have begun supporting third-party applications. These <b>LLM</b> apps leverage the de facto natural language-based automated execution paradigm of <b>LLMs:</b> that is, apps and their interactions are defined in natural language, provided access to user data, and allowed to freely interact with each other and the system. These <b>LLM</b> app ecosystems resemble the settings of earlier computing platforms, where there was insufficient isolation between apps and the system. Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users. In this paper, we propose SecGPT, an architecture for <b>LLM-based</b> systems that aims to mitigate the security and privacy issues that arise with the execution of third-party apps. SecGPT&rsquo;s key idea is to isolate the execution of apps and more precisely mediate their interactions outside of their isolated environments. We evaluate SecGPT against a number of case study attacks and demonstrate that it protects against many security, privacy, and safety issues that exist in non-isolated <b>LLM-based</b> systems. The performance overhead incurred by SecGPT to improve security is under 0.3x for three-quarters of the tested queries. To foster follow-up research, we release SecGPT&rsquo;s source code at <a href=https://github.com/llm-platform-security/SecGPT>https://github.com/llm-platform-security/SecGPT</a>.</p></p class="citation"></blockquote><h3 id=49--57270-on-protecting-the-data-privacy-of-large-language-models-llms-a-survey-biwei-yan-et-al-2024>(4/9 | 57/270) On Protecting the Data Privacy of Large Language Models (LLMs): A Survey (Biwei Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, Xiuzheng Cheng. (2024)<br><strong>On Protecting the Data Privacy of Large Language Models (LLMs): A Survey</strong><br><button class=copy-to-clipboard title="On Protecting the Data Privacy of Large Language Models (LLMs): A Survey" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05156v1.pdf filename=2403.05156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are complex artificial intelligence systems capable of understanding, generating and translating human language. They learn language patterns by analyzing <b>large</b> <b>amounts</b> <b>of</b> text data, allowing them to perform writing, conversation, summarizing and other language tasks. When <b>LLMs</b> process and generate <b>large</b> <b>amounts</b> <b>of</b> data, there is a risk of leaking sensitive information, which may threaten data privacy. This paper concentrates on elucidating the data privacy concerns associated with <b>LLMs</b> to foster a comprehensive understanding. Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within <b>LLMs.</b> Subsequently, we conduct an assessment of the privacy protection mechanisms employed by <b>LLMs</b> at various stages, followed by a detailed examination of their efficacy and constraints. Finally, the discourse extends to delineate the challenges encountered and outline prospective directions for advancement in the realm of <b>LLM</b> privacy protection.</p></p class="citation"></blockquote><h3 id=59--58270-exploring-the-adversarial-frontier-quantifying-robustness-via-adversarial-hypervolume-ping-guo-et-al-2024>(5/9 | 58/270) Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume (Ping Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ping Guo, Cheng Gong, Xi Lin, Zhiyuan Yang, Qingfu Zhang. (2024)<br><strong>Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume</strong><br><button class=copy-to-clipboard title="Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05100v1.pdf filename=2403.05100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The escalating threat of <b>adversarial</b> <b>attacks</b> on deep learning models, particularly in security-critical fields, has underscored the need for robust deep learning systems. Conventional robustness evaluations have relied on <b>adversarial</b> <b>accuracy,</b> which measures a model&rsquo;s performance under a specific perturbation intensity. However, this singular metric does not fully encapsulate the overall resilience of a model against varying degrees of perturbation. To address this gap, we propose a new metric termed <b>adversarial</b> <b>hypervolume,</b> assessing the robustness of deep learning models comprehensively over a range of perturbation intensities from a multi-objective optimization standpoint. This metric allows for an in-depth comparison of defense mechanisms and recognizes the trivial improvements in robustness afforded by less potent defensive strategies. Additionally, we adopt a novel training algorithm that enhances <b>adversarial</b> <b>robustness</b> uniformly across various perturbation intensities, in contrast to methods narrowly focused on optimizing <b>adversarial</b> <b>accuracy.</b> Our extensive empirical studies validate the effectiveness of the <b>adversarial</b> <b>hypervolume</b> metric, demonstrating its ability to reveal subtle differences in robustness that <b>adversarial</b> <b>accuracy</b> overlooks. This research contributes a new measure of robustness and establishes a standard for assessing and <b>benchmarking</b> the resilience of current and future defensive models against <b>adversarial</b> <b>threats.</b></p></p class="citation"></blockquote><h3 id=69--59270-inception-attacks-immersive-hijacking-in-virtual-reality-systems-zhuolin-yang-et-al-2024>(6/9 | 59/270) Inception Attacks: Immersive Hijacking in Virtual Reality Systems (Zhuolin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuolin Yang, Cathy Yuanchen Li, Arman Bhalla, Ben Y. Zhao, Haitao Zheng. (2024)<br><strong>Inception Attacks: Immersive Hijacking in Virtual Reality Systems</strong><br><button class=copy-to-clipboard title="Inception Attacks: Immersive Hijacking in Virtual Reality Systems" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05721v1.pdf filename=2403.05721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in virtual reality (VR) system provide fully immersive interactions that connect users with online resources, applications, and each other. Yet these immersive interfaces can make it easier for users to fall prey to a new type of security attacks. We introduce the inception attack, where an attacker controls and manipulates a user&rsquo;s interaction with their VR environment and applications, by trapping them inside a malicious VR application that masquerades as the full VR system. Once trapped in an &ldquo;inception VR layer&rdquo;, all of the user&rsquo;s interactions with remote servers, network applications, and other VR users can be recorded or modified without their knowledge. This enables traditional attacks (recording passwords and modifying user actions in flight), as well as VR interaction attacks, where (with <b>generative</b> <b>AI</b> tools) two VR users interacting can experience two dramatically different conversations. In this paper, we introduce inception attacks and their design, and describe our implementation that works on all Meta Quest VR headsets. Our implementation of inception attacks includes a cloned version of the Meta Quest browser that can modify data as it&rsquo;s displayed to the user, and alter user input en route to the server (e.g. modify amount of $ transferred in a banking session). Our implementation also includes a cloned VRChat app, where an attacker can eavesdrop and modify live audio between two VR users. We then conduct a study on users with a range of VR experiences, execute the inception attack during their session, and debrief them about their experiences. Only 37% of users noticed the momentary visual &ldquo;glitch&rdquo; when the inception attack began, and all but 1 user attributed it to imperfections in the VR platform. Finally, we consider and discuss efficacy and tradeoffs for a wide range of potential inception defenses.</p></p class="citation"></blockquote><h3 id=79--60270-vspace-voting-in-a-scalable-privacy-aware-and-confidential-election-se-elnour-et-al-2024>(7/9 | 60/270) vSPACE: Voting in a Scalable, Privacy-Aware and Confidential Election (Se Elnour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Se Elnour, William J Buchanan, Paul Keating, Mwrwan Abubakar, Sirag Elnour. (2024)<br><strong>vSPACE: Voting in a Scalable, Privacy-Aware and Confidential Election</strong><br><button class=copy-to-clipboard title="vSPACE: Voting in a Scalable, Privacy-Aware and Confidential Election" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CY, cs.CR<br>Keyword Score: 10<br>Keywords: Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05275v1.pdf filename=2403.05275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The vSPACE experimental proof-of-concept (PoC) on the TrueElect[Anon][Creds] protocol presents a novel approach to secure, private, and scalable elections, extending the TrueElect and ElectAnon protocols with the integration of AnonCreds SSI (Self-Sovereign Identity). Such a protocol PoC is situated within a <b>Zero-Trust</b> <b>Architecture</b> (ZTA) and leverages confidential computing, continuous authentication, multi-party computation (MPC), and well-architected framework (WAF) principles to address the challenges of cybersecurity, privacy, and trust over IP (ToIP) protection. Employing a Kubernetes confidential cluster within an Enterprise-Scale Landing Zone (ESLZ), vSPACE integrates Distributed Ledger Technology (DLT) for immutable and certifiable audit trails. The Infrastructure as Code (IaC) model ensures rapid deployment, consistent management, and adherence to security standards, making vSPACE a future-proof solution for digital voting systems.</p></p class="citation"></blockquote><h3 id=89--61270-elections-in-the-post-quantum-era-is-the-complexity-shield-strong-enough-šimon-schierreich-2024>(8/9 | 61/270) Elections in the Post-Quantum Era: Is the Complexity Shield Strong Enough? (Šimon Schierreich, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Šimon Schierreich. (2024)<br><strong>Elections in the Post-Quantum Era: Is the Complexity Shield Strong Enough?</strong><br><button class=copy-to-clipboard title="Elections in the Post-Quantum Era: Is the Complexity Shield Strong Enough?" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CY, cs-GT, cs.CR<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05273v1.pdf filename=2403.05273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The election, a cornerstone of democracy, is one of the best-recognizable symbols of democratic governance. Voters&rsquo; confidence in elections is essential, and these days, we can watch practically in live broadcast what consequences distrust in the <b>fairness</b> of elections may have. From the times of the celebrated Gibbard-Satterthwaite theorem, it is well-known in the social-choice community that most voting systems are vulnerable to the efforts of various players to influence elections. Luckily for us, computing such influence to affect election outcomes is a hard problem from the computational complexity perspective. This intractability is regarded as a ``complexity shield&rsquo;&rsquo; that secures voting rules against this malicious behavior. In this work, we consider quantum computers to be a new threat to the complexity shield described above, as they break out of standard computing paradigms and unlock additional computational resources. To this end, we provide an overview of possible attacks on election, discuss the abilities of quantum computing, and chart possible directions for future research in this area.</p></p class="citation"></blockquote><h3 id=99--62270-private-count-release-a-simple-and-scalable-approach-for-private-data-analytics-ryan-rogers-2024>(9/9 | 62/270) Private Count Release: A Simple and Scalable Approach for Private Data Analytics (Ryan Rogers, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Rogers. (2024)<br><strong>Private Count Release: A Simple and Scalable Approach for Private Data Analytics</strong><br><button class=copy-to-clipboard title="Private Count Release: A Simple and Scalable Approach for Private Data Analytics" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05073v1.pdf filename=2403.05073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a data analytics system that ensures accurate counts can be released with <b>differential</b> <b>privacy</b> and minimal onboarding effort while showing instances that outperform other approaches that require more onboarding effort. The primary difference between our proposal and existing approaches is that it does not rely on user contribution bounds over distinct elements, i.e. $\ell_0$-sensitivity bounds, which can significantly bias counts. Contribution bounds for $\ell_0$-sensitivity have been considered as necessary to ensure <b>differential</b> <b>privacy,</b> but we show that this is actually not necessary and can lead to releasing more results that are more accurate. We require minimal hyperparameter tuning and demonstrate results on several publicly available dataset. We hope that this approach will help <b>differential</b> <b>privacy</b> scale to many different data analytics applications.</p></p class="citation"></blockquote><h2 id=csir-4>cs.IR (4)</h2><h3 id=14--63270-cfairllm-consumer-fairness-evaluation-in-large-language-model-recommender-system-yashar-deldjoo-et-al-2024>(1/4 | 63/270) CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System (Yashar Deldjoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yashar Deldjoo, Tommaso di Noia. (2024)<br><strong>CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System</strong><br><button class=copy-to-clipboard title="CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 70<br>Keywords: Fairness, Recommendation, Recommender System, ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05668v1.pdf filename=2403.05668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of <b>recommender</b> <b>systems,</b> the integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> such as <b>ChatGPT</b> marks a new era, introducing the concept of <b>Recommendation</b> via <b>LLM</b> (RecLLM). While these advancements promise unprecedented personalization and efficiency, they also bring to the fore critical concerns regarding <b>fairness,</b> particularly in how <b>recommendations</b> might inadvertently perpetuate or amplify biases associated with sensitive user attributes. In order to address these concerns, our study introduces a comprehensive evaluation framework, CFaiRLLM, aimed at evaluating (and thereby mitigating) biases on the consumer side within RecLLMs. Our research methodically assesses the <b>fairness</b> of RecLLMs by examining how <b>recommendations</b> might vary with the inclusion of sensitive attributes such as gender, age, and their intersections, through both similarity alignment and true preference alignment. By analyzing <b>recommendations</b> generated under different conditions-including the use of sensitive attributes in user <b>prompts-our</b> framework identifies potential biases in the <b>recommendations</b> provided. A key part of our study involves exploring how different detailed strategies for constructing user profiles (random, top-rated, recent) impact the alignment between <b>recommendations</b> made without consideration of sensitive attributes and those that are sensitive-attribute-aware, highlighting the bias mechanisms within RecLLMs. The findings in our study highlight notable disparities in the <b>fairness</b> of <b>recommendations,</b> particularly when sensitive attributes are integrated into the <b>recommendation</b> process, either individually or in combination. The analysis demonstrates that the choice of user profile sampling strategy plays a significant role in affecting <b>fairness</b> outcomes, highlighting the complexity of achieving fair <b>recommendations</b> in the era of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=24--64270-aligning-large-language-models-for-controllable-recommendations-wensheng-lu-et-al-2024>(2/4 | 64/270) Aligning Large Language Models for Controllable Recommendations (Wensheng Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wensheng Lu, Jianxun Lian, Wei Zhang, Guanghua Li, Mingyang Zhou, Hao Liao, Xing Xie. (2024)<br><strong>Aligning Large Language Models for Controllable Recommendations</strong><br><button class=copy-to-clipboard title="Aligning Large Language Models for Controllable Recommendations" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: 68T50, cs-AI, cs-IR, cs.IR<br>Keyword Score: 70<br>Keywords: Recommendation, Recommender System, Reinforcement Learning, Supervised Learning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05063v1.pdf filename=2403.05063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by the exceptional general intelligence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> researchers have begun to explore their application in pioneering the next generation of <b>recommender</b> <b>systems</b> - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into <b>LLMs</b> to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of <b>supervised</b> <b>learning</b> tasks, augmented with labels derived from a conventional <b>recommender</b> <b>model,</b> aimed at explicitly improving <b>LLMs&rsquo;</b> proficiency in adhering to <b>recommendation-specific</b> instructions. Subsequently, we develop a <b>reinforcement</b> <b>learning-based</b> alignment procedure to further strengthen <b>LLMs&rsquo;</b> aptitude in responding to users&rsquo; intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method markedly advances the capability of <b>LLMs</b> to comply with instructions within <b>recommender</b> <b>systems,</b> while sustaining a high level of accuracy performance.</p></p class="citation"></blockquote><h3 id=34--65270-personalized-audiobook-recommendations-at-spotify-through-graph-neural-networks-marco-de-nadai-et-al-2024>(3/4 | 65/270) Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks (Marco De Nadai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco De Nadai, Francesco Fabbri, Paul Gigioli, Alice Wang, Ang Li, Fabrizio Silvestri, Laura Kim, Shawn Lin, Vladan Radosavljevic, Sandeep Ghael, David Nyhan, Hugues Bouchard, Mounia Lalmas-Roelleke, Andreas Damianou. (2024)<br><strong>Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05185v1.pdf filename=2403.05185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the ever-evolving digital audio landscape, Spotify, well-known for its music and talk content, has recently introduced audiobooks to its vast user base. While promising, this move presents significant challenges for personalized <b>recommendations.</b> Unlike music and podcasts, audiobooks, initially available for a fee, cannot be easily skimmed before purchase, posing higher stakes for the relevance of <b>recommendations.</b> Furthermore, introducing a new content type into an existing platform confronts extreme data sparsity, as most users are unfamiliar with this new content type. Lastly, recommending content to millions of users requires the model to react fast and be scalable. To address these challenges, we leverage podcast and music user preferences and introduce 2T-HGNN, a scalable <b>recommendation</b> system comprising Heterogeneous <b>Graph</b> <b>Neural</b> <b>Networks</b> (HGNNs) and a Two Tower (2T) model. This novel approach uncovers nuanced item relationships while ensuring low latency and complexity. We decouple users from the HGNN <b>graph</b> <b>and</b> <b>propose</b> an innovative multi-link neighbor sampler. These choices, together with the 2T component, significantly reduce the complexity of the HGNN model. Empirical evaluations involving millions of users show significant improvement in the quality of personalized <b>recommendations,</b> resulting in a +46% increase in new audiobooks start rate and a +23% boost in streaming rates. Intriguingly, our model&rsquo;s impact extends beyond audiobooks, benefiting established products like podcasts.</p></p class="citation"></blockquote><h3 id=44--66270-multi-tower-multi-interest-recommendation-with-user-representation-repel-tianyu-xiong-et-al-2024>(4/4 | 66/270) Multi-Tower Multi-Interest Recommendation with User Representation Repel (Tianyu Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyu Xiong, Xiaohan Yu. (2024)<br><strong>Multi-Tower Multi-Interest Recommendation with User Representation Repel</strong><br><button class=copy-to-clipboard title="Multi-Tower Multi-Interest Recommendation with User Representation Repel" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: H-3-3, cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05122v1.pdf filename=2403.05122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of information overload, the value of <b>recommender</b> <b>systems</b> has been profoundly recognized in academia and industry alike. Multi-interest sequential <b>recommendation,</b> in particular, is a subfield that has been receiving increasing attention in recent years. By generating multiple-user representations, multi-interest learning models demonstrate superior expressiveness than single-user representation models, both theoretically and empirically. Despite major advancements in the field, three major issues continue to plague the performance and adoptability of multi-interest learning methods, the difference between training and deployment objectives, the inability to access item information, and the difficulty of industrial adoption due to its single-tower architecture. We address these challenges by proposing a novel multi-tower multi-interest framework with user representation repel. Experimental results across multiple large-scale industrial datasets proved the effectiveness and generalizability of our proposed framework.</p></p class="citation"></blockquote><h2 id=hep-ph-2>hep-ph (2)</h2><h3 id=12--67270-omnijet-α-the-first-cross-task-foundation-model-for-particle-physics-joschka-birk-et-al-2024>(1/2 | 67/270) OmniJet-$α$: The first cross-task foundation model for particle physics (Joschka Birk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joschka Birk, Anna Hallin, Gregor Kasieczka. (2024)<br><strong>OmniJet-$α$: The first cross-task foundation model for particle physics</strong><br><button class=copy-to-clipboard title="OmniJet-$α$: The first cross-task foundation model for particle physics" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ph<br>Categories: cs-LG, hep-ex, hep-ph, hep-ph, physics-data-an<br>Keyword Score: 70<br>Keywords: Fine-tuning, Foundation Model, Supervised Learning, Transfer Learning, Unsupervised Learning, Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05618v1.pdf filename=2403.05618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> are multi-dataset and multi-task machine learning methods that once pre-trained can be <b>fine-tuned</b> for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data. We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with <b>transformer</b> architectures (the common backbone of <b>foundation</b> <b>models).</b> These measures motivate the choice of a higher-fidelity <b>tokenization</b> compared to previous works. Finally, we demonstrate <b>transfer</b> <b>learning</b> between an <b>unsupervised</b> problem (jet generation) and a classic <b>supervised</b> task (jet tagging) with our new OmniJet-$\alpha$ model. This is the first successful <b>transfer</b> <b>between</b> two different and actively studied classes of tasks and constitutes a major step in the building of <b>foundation</b> <b>models</b> for particle physics.</p></p class="citation"></blockquote><h3 id=22--68270-jet-discrimination-with-quantum-complete-graph-neural-network-yi-an-chen-et-al-2024>(2/2 | 68/270) Jet Discrimination with Quantum Complete Graph Neural Network (Yi-An Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-An Chen, Kai-Feng Chen. (2024)<br><strong>Jet Discrimination with Quantum Complete Graph Neural Network</strong><br><button class=copy-to-clipboard title="Jet Discrimination with Quantum Complete Graph Neural Network" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ph<br>Categories: cs-LG, hep-ph, hep-ph, quant-ph<br>Keyword Score: 16<br>Keywords: Graph, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04990v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04990v2.pdf filename=2403.04990v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning, particularly deep neural networks, has been widely utilized in high energy physics and has shown remarkable results in various applications. Moreover, the concept of machine learning has been extended to quantum computers, giving rise to a new research area known as quantum machine learning. In this paper, we propose a novel variational quantum circuit model, Quantum Complete <b>Graph</b> <b>Neural</b> <b>Network</b> (QCGNN), designed for learning complete <b>graphs.</b> <b>We</b> <b>argue</b> that QCGNN has a polynomial speedup against its classical counterpart, due to the property of quantum parallelism. In this paper, we study the application of QCGNN through the challenging jet discrimination, where the jets are represented with complete <b>graphs.</b> <b>Subsequently,</b> <b>we</b> conduct a comparative analysis with classical <b>graph</b> <b>neural</b> <b>networks</b> to establish a <b>benchmark.</b></p></p class="citation"></blockquote><h2 id=cslg-32>cs.LG (32)</h2><h3 id=132--69270-unfamiliar-finetuning-examples-control-how-language-models-hallucinate-katie-kang-et-al-2024>(1/32 | 69/270) Unfamiliar Finetuning Examples Control How Language Models Hallucinate (Katie Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, Sergey Levine. (2024)<br><strong>Unfamiliar Finetuning Examples Control How Language Models Hallucinate</strong><br><button class=copy-to-clipboard title="Unfamiliar Finetuning Examples Control How Language Models Hallucinate" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Supervised Learning, Massive Multitask Language Understanding (MMLU), Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05612v1.pdf filename=2403.05612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts. In this work, we explore the underlying mechanisms that govern how <b>finetuned</b> <b>LLMs</b> hallucinate. Our investigation reveals an interesting pattern: as inputs become more unfamiliar, <b>LLM</b> outputs tend to default towards a <code>hedged'' prediction, whose form is determined by how the unfamiliar examples in the &lt;b>finetuning&lt;/b> data are &lt;b>supervised.&lt;/b> Thus, by strategically modifying these examples' supervision, we can control &lt;b>LLM&lt;/b> predictions for unfamiliar inputs (e.g., teach them to say </code>I don&rsquo;t know&rsquo;&rsquo;). Based on these principles, we develop an RL approach that more reliably mitigates hallucinations for long-form generation tasks, by tackling the challenges presented by reward model hallucinations. We validate our findings with a series of controlled experiments in multiple-choice <b>QA</b> on <b>MMLU,</b> as well as long-form biography and book/movie plot generation tasks.</p></p class="citation"></blockquote><h3 id=232--70270-overcoming-reward-overoptimization-via-adversarial-policy-optimization-with-lightweight-uncertainty-estimation-xiaoying-zhang-et-al-2024>(2/32 | 70/270) Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation (Xiaoying Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoying Zhang, Jean-Francois Ton, Wei Shen, Hongning Wang, Yang Liu. (2024)<br><strong>Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation</strong><br><button class=copy-to-clipboard title="Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05171v1.pdf filename=2403.05171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model&rsquo;s predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR <b>summarization</b> datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently resulting in enhanced performance as evaluated through human-assisted evaluation.</p></p class="citation"></blockquote><h3 id=332--71270-simple-multigraph-convolution-networks-danyang-wu-et-al-2024>(3/32 | 71/270) Simple Multigraph Convolution Networks (Danyang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danyang Wu, Xinjie Shen, Jitao Lu, Jin Xu, Feiping Nie. (2024)<br><strong>Simple Multigraph Convolution Networks</strong><br><button class=copy-to-clipboard title="Simple Multigraph Convolution Networks" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 56<br>Keywords: Message-Passing, Graph, Benchmarking, Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05014v1.pdf filename=2403.05014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing multigraph <b>convolution</b> <b>methods</b> either ignore the cross-view interaction among multiple <b>graphs,</b> or induce extremely high computational cost due to standard cross-view polynomial operators. To alleviate this problem, this paper proposes a Simple MultiGraph <b>Convolution</b> <b>Networks</b> (SMGCN) which first extracts consistent cross-view topology from multigraphs including edge-level and subgraph-level topology, then performs polynomial expansion based on raw multigraphs and consistent topologies. In theory, SMGCN utilizes the consistent topologies in polynomial expansion rather than standard cross-view polynomial expansion, which performs credible cross-view spatial <b>message-passing,</b> follows the spectral <b>convolution</b> <b>paradigm,</b> and effectively reduces the complexity of standard polynomial expansion. In the <b>simulations,</b> experimental results demonstrate that SMGCN achieves state-of-the-art performance on ACM and DBLP multigraph <b>benchmark</b> datasets. Our codes are available at <a href=https://github.com/frinkleko/SMGCN>https://github.com/frinkleko/SMGCN</a>.</p></p class="citation"></blockquote><h3 id=432--72270-simulating-battery-powered-tinyml-systems-optimised-using-reinforcement-learning-in-image-based-anomaly-detection-jared-m-ping-et-al-2024>(4/32 | 72/270) Simulating Battery-Powered TinyML Systems Optimised using Reinforcement Learning in Image-Based Anomaly Detection (Jared M. Ping et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jared M. Ping, Ken J. Nixon. (2024)<br><strong>Simulating Battery-Powered TinyML Systems Optimised using Reinforcement Learning in Image-Based Anomaly Detection</strong><br><button class=copy-to-clipboard title="Simulating Battery-Powered TinyML Systems Optimised using Reinforcement Learning in Image-Based Anomaly Detection" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Anomaly Detection, Benchmarking, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05106v1.pdf filename=2403.05106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in Tiny Machine Learning (TinyML) have bolstered the creation of smart industry solutions, including smart agriculture, healthcare and smart cities. Whilst related research contributes to enabling TinyML solutions on constrained hardware, there is a need to amplify real-world applications by optimising energy consumption in battery-powered systems. The work presented extends and contributes to TinyML research by optimising battery-powered image-based <b>anomaly</b> <b>detection</b> Internet of Things (IoT) systems. Whilst previous work in this area has yielded the capabilities of on-device inferencing and training, there has yet to be an investigation into optimising the management of such capabilities using machine learning approaches, such as <b>Reinforcement</b> <b>Learning</b> (RL), to improve the deployment battery life of such systems. Using modelled <b>simulations,</b> the battery life effects of an RL algorithm are <b>benchmarked</b> against static and dynamic optimisation approaches, with the foundation laid for a hardware <b>benchmark</b> to follow. It is shown that using RL within a TinyML-enabled IoT system to optimise the system operations, including cloud <b>anomaly</b> <b>processing</b> and on-device training, yields an improved battery life of 22.86% and 10.86% compared to static and dynamic optimisation approaches respectively. The proposed solution can be deployed to resource-constrained hardware, given its low memory footprint of 800 B, which could be further reduced. This further facilitates the real-world deployment of such systems, including key sectors such as smart agriculture.</p></p class="citation"></blockquote><h3 id=532--73270-spectral-invariant-learning-for-dynamic-graphs-under-distribution-shifts-zeyang-zhang-et-al-2024>(5/32 | 73/270) Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (Zeyang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyang Zhang, Xin Wang, Ziwei Zhang, Zhou Qin, Weigao Wen, Hui Xue, Haoyang Li, Wenwu Zhu. (2024)<br><strong>Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts</strong><br><button class=copy-to-clipboard title="Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Node Classification, Graph, Graph Neural Network, Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05026v1.pdf filename=2403.05026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic <b>graph</b> <b>neural</b> <b>networks</b> (DyGNNs) currently struggle with handling <b>distribution</b> <b>shifts</b> that are inherent in dynamic <b>graphs.</b> <b>Existing</b> <b>work</b> on DyGNNs with <b>out-of-distribution</b> settings only focuses on the time domain, failing to handle cases involving <b>distribution</b> <b>shifts</b> in the spectral domain. In this paper, we discover that there exist cases with <b>distribution</b> <b>shifts</b> unobservable in the time domain while observable in the spectral domain, and propose to study <b>distribution</b> <b>shifts</b> on dynamic <b>graphs</b> <b>in</b> <b>the</b> spectral domain for the first time. However, this investigation poses two key challenges: i) it is non-trivial to capture different <b>graph</b> <b>patterns</b> <b>that</b> are driven by various frequency components entangled in the spectral domain; and ii) it remains unclear how to handle <b>distribution</b> <b>shifts</b> with the discovered spectral patterns. To address these challenges, we propose Spectral Invariant Learning for Dynamic <b>Graphs</b> <b>under</b> <b>Distribution</b> <b>Shifts</b> (SILD), which can handle <b>distribution</b> <b>shifts</b> on dynamic <b>graphs</b> <b>by</b> <b>capturing</b> and utilizing invariant and variant spectral patterns. Specifically, we first design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums, allowing the mixed dynamic <b>graph</b> <b>patterns</b> <b>to</b> be transformed into separate frequency components. We then develop a disentangled spectrum mask to filter <b>graph</b> <b>dynamics</b> <b>from</b> various frequency components and discover the invariant and variant spectral patterns. Finally, we propose invariant spectral filtering, which encourages the model to rely on invariant patterns for generalization under <b>distribution</b> <b>shifts.</b> Experimental results on synthetic and real-world dynamic <b>graph</b> <b>datasets</b> <b>demonstrate</b> the superiority of our method for both <b>node</b> <b>classification</b> and link prediction tasks under <b>distribution</b> <b>shifts.</b></p></p class="citation"></blockquote><h3 id=632--74270-provable-multi-party-reinforcement-learning-with-diverse-human-feedback-huiying-zhong-et-al-2024>(6/32 | 74/270) Provable Multi-Party Reinforcement Learning with Diverse Human Feedback (Huiying Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huiying Zhong, Zhun Deng, Weijie J. Su, Zhiwei Steven Wu, Linjun Zhang. (2024)<br><strong>Provable Multi-Party Reinforcement Learning with Diverse Human Feedback</strong><br><button class=copy-to-clipboard title="Provable Multi-Party Reinforcement Learning with Diverse Human Feedback" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 40<br>Keywords: Fairness, Meta Learning, Reinforcement Learning, Reinforcement Learning from Human Feedback<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05006v1.pdf filename=2403.05006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> with human feedback <b>(RLHF)</b> is an emerging paradigm to align models with human preferences. Typically, <b>RLHF</b> aggregates preferences from multiple individuals who have diverse viewpoints that may conflict with each other. Our work \textit{initiates} the theoretical study of multi-party <b>RLHF</b> that explicitly models the diverse preferences of multiple individuals. We show how traditional <b>RLHF</b> approaches can fail since learning a single reward function cannot capture and balance the preferences of multiple individuals. To overcome such limitations, we incorporate <b>meta-learning</b> <b>to</b> learn multiple preferences and adopt different social welfare functions to aggregate the preferences across multiple parties. We focus on the offline learning setting and establish sample complexity bounds, along with efficiency and <b>fairness</b> guarantees, for optimizing diverse social welfare functions such as Nash, Utilitarian, and Leximin welfare functions. Our results show a separation between the sample complexities of multi-party <b>RLHF</b> and traditional single-party <b>RLHF.</b> Furthermore, we consider a reward-free setting, where each individual&rsquo;s preference is no longer consistent with a reward model, and give pessimistic variants of the von Neumann Winner based on offline preference data. Taken together, our work showcases the advantage of multi-party <b>RLHF</b> but also highlights its more demanding statistical complexity.</p></p class="citation"></blockquote><h3 id=732--75270-benchmarking-large-language-models-for-molecule-prediction-tasks-zhiqiang-zhong-et-al-2024>(7/32 | 75/270) Benchmarking Large Language Models for Molecule Prediction Tasks (Zhiqiang Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin. (2024)<br><strong>Benchmarking Large Language Models for Molecule Prediction Tasks</strong><br><button class=copy-to-clipboard title="Benchmarking Large Language Models for Molecule Prediction Tasks" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 39<br>Keywords: Graph, Benchmarking, Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05075v1.pdf filename=2403.05075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> stand at the forefront of a number of Natural Language Processing (NLP) tasks. Despite the widespread adoption of <b>LLMs</b> in NLP, much of their potential in broader fields remains largely unexplored, and significant limitations persist in their design and implementation. Notably, <b>LLMs</b> struggle with structured data, such as <b>graphs,</b> and often falter when tasked with answering domain-specific questions requiring deep expertise, such as those in biology and chemistry. In this paper, we explore a fundamental question: Can <b>LLMs</b> effectively handle molecule prediction tasks? Rather than pursuing top-tier performance, our goal is to assess how <b>LLMs</b> can contribute to diverse molecule tasks. We identify several classification and regression prediction tasks across six standard molecule datasets. Subsequently, we carefully design a set of <b>prompts</b> to query <b>LLMs</b> on these tasks and compare their performance with existing Machine Learning (ML) models, which include text-based models and those specifically designed for analysing the geometric structure of molecules. Our investigation reveals several key insights: Firstly, <b>LLMs</b> generally lag behind ML models in achieving competitive performance on molecule tasks, particularly when compared to models adept at capturing the geometric structure of molecules, highlighting the constrained ability of <b>LLMs</b> to comprehend <b>graph</b> data. Secondly, <b>LLMs</b> show promise in enhancing the performance of ML models when used collaboratively. Lastly, we engage in a discourse regarding the challenges and promising avenues to harness <b>LLMs</b> for molecule prediction tasks. The code and models are available at <a href=https://github.com/zhiqiangzhongddu/LLMaMol>https://github.com/zhiqiangzhongddu/LLMaMol</a>.</p></p class="citation"></blockquote><h3 id=832--76270-denoising-autoregressive-representation-learning-yazhe-li-et-al-2024>(8/32 | 76/270) Denoising Autoregressive Representation Learning (Yazhe Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yazhe Li, Jorg Bornschein, Ting Chen. (2024)<br><strong>Denoising Autoregressive Representation Learning</strong><br><button class=copy-to-clipboard title="Denoising Autoregressive Representation Learning" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 35<br>Keywords: Diffusion Model, Fine-tuning, Representation Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05196v1.pdf filename=2403.05196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore a new generative approach for learning visual <b>representations.</b> <b>Our</b> method, DARL, employs a decoder-only <b>Transformer</b> to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong <b>representations.</b> <b>To</b> enhance the image generation ability, we replace the MSE loss with the <b>diffusion</b> <b>objective</b> by using a denoising patch decoder. We show that the learned <b>representation</b> <b>can</b> be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image <b>diffusion</b> <b>models.</b> Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the <b>fine-tuning</b> protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths of autoregressive and denoising <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=932--77270-unity-by-diversity-improved-representation-learning-in-multimodal-vaes-thomas-m-sutter-et-al-2024>(9/32 | 77/270) Unity by Diversity: Improved Representation Learning in Multimodal VAEs (Thomas M. Sutter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas M. Sutter, Yang Meng, Norbert Fortin, Julia E. Vogt, Stephan Mandt. (2024)<br><strong>Unity by Diversity: Improved Representation Learning in Multimodal VAEs</strong><br><button class=copy-to-clipboard title="Unity by Diversity: Improved Representation Learning in Multimodal VAEs" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 34<br>Keywords: Autoencoder, Benchmarking, Multi-modal, Multi-modal, Representation Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05300v1.pdf filename=2403.05300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Variational</b> <b>Autoencoders</b> for <b>multimodal</b> data hold promise for many tasks in data analysis, such as <b>representation</b> <b>learning,</b> conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared <b>representation.</b> <b>Such</b> architectures impose hard constraints on the model. In this work, we show that a better latent <b>representation</b> <b>can</b> be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality&rsquo;s latent <b>representation</b> <b>towards</b> a shared aggregate posterior. This approach results in a superior latent <b>representation</b> <b>and</b> allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple <b>benchmark</b> datasets and a challenging real-world neuroscience data set, we show improved learned latent <b>representations</b> <b>and</b> imputation of missing data modalities compared to existing methods.</p></p class="citation"></blockquote><h3 id=1032--78270-unsupervised-graph-neural-architecture-search-with-disentangled-self-supervision-zeyang-zhang-et-al-2024>(10/32 | 78/270) Unsupervised Graph Neural Architecture Search with Disentangled Self-supervision (Zeyang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyang Zhang, Xin Wang, Ziwei Zhang, Guangyao Shen, Shiqi Shen, Wenwu Zhu. (2024)<br><strong>Unsupervised Graph Neural Architecture Search with Disentangled Self-supervision</strong><br><button class=copy-to-clipboard title="Unsupervised Graph Neural Architecture Search with Disentangled Self-supervision" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Self-supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05064v1.pdf filename=2403.05064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The existing <b>graph</b> neural architecture search (GNAS) methods heavily rely on <b>supervised</b> labels during the search process, failing to handle ubiquitous scenarios where supervisions are not available. In this paper, we study the problem of <b>unsupervised</b> <b>graph</b> neural architecture search, which remains unexplored in the literature. The key problem is to discover the latent <b>graph</b> factors that drive the formation of <b>graph</b> data as well as the underlying relations between the factors and the optimal neural architectures. Handling this problem is challenging given that the latent <b>graph</b> factors together with architectures are highly entangled due to the nature of the <b>graph</b> and the complexity of the neural architecture search process. To address the challenge, we propose a novel Disentangled <b>Self-supervised</b> <b>Graph</b> Neural Architecture Search (DSGAS) model, which is able to discover the optimal architectures capturing various latent <b>graph</b> factors in a <b>self-supervised</b> fashion based on unlabeled <b>graph</b> data. Specifically, we first design a disentangled <b>graph</b> super-network capable of incorporating multiple architectures with factor-wise disentanglement, which are optimized simultaneously. Then, we estimate the performance of architectures under different factors by our proposed <b>self-supervised</b> training with joint architecture-graph disentanglement. Finally, we propose a contrastive search with architecture augmentations to discover architectures with factor-specific expertise. Extensive experiments on 11 real-world datasets demonstrate that the proposed model is able to achieve state-of-the-art performance against several baseline methods in an <b>unsupervised</b> manner.</p></p class="citation"></blockquote><h3 id=1132--79270-augmentations-vs-algorithms-what-works-in-self-supervised-learning-warren-morningstar-et-al-2024>(11/32 | 79/270) Augmentations vs Algorithms: What Works in Self-Supervised Learning (Warren Morningstar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Warren Morningstar, Alex Bijamov, Chris Duvarney, Luke Friedman, Neha Kalibhat, Luyang Liu, Philip Mansfield, Renan Rojas-Gomez, Karan Singhal, Bradley Green, Sushant Prakash. (2024)<br><strong>Augmentations vs Algorithms: What Works in Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="Augmentations vs Algorithms: What Works in Self-Supervised Learning" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Data Augmentation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05726v1.pdf filename=2403.05726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the relative effects of <b>data</b> <b>augmentations,</b> pretraining algorithms, and model architectures in <b>Self-Supervised</b> <b>Learning</b> (SSL). While the recent literature in this space leaves the impression that the pretraining algorithm is of critical importance to performance, understanding its effect is complicated by the difficulty in making objective and direct comparisons between methods. We propose a new framework which unifies many seemingly disparate SSL methods into a single shared template. Using this framework, we identify aspects in which methods differ and observe that in addition to changing the pretraining algorithm, many works also use new <b>data</b> <b>augmentations</b> or more powerful model architectures. We compare several popular SSL methods using our framework and find that many algorithmic additions, such as prediction networks or new losses, have a minor impact on downstream task performance (often less than $1%$), while enhanced augmentation techniques offer more significant performance improvements ($2-4%$). Our findings challenge the premise that SSL is being driven primarily by algorithmic improvements, and suggest instead a bitter lesson for SSL: that augmentation diversity and <b>data</b> <b>/</b> model scale are more critical contributors to recent advances in <b>self-supervised</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=1232--80270-tune-without-validation-searching-for-learning-rate-and-weight-decay-on-training-sets-lorenzo-brigato-et-al-2024>(12/32 | 80/270) Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets (Lorenzo Brigato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Brigato, Stavroula Mougiakakou. (2024)<br><strong>Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets</strong><br><button class=copy-to-clipboard title="Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05532v1.pdf filename=2403.05532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Tune without Validation (Twin), a pipeline for tuning learning rate and weight decay without validation sets. We leverage a recent theoretical framework concerning learning phases in hypothesis space to devise a heuristic that predicts what hyper-parameter (HP) combinations yield better generalization. Twin performs a grid search of trials according to an early-/non-early-stopping scheduler and then segments the region that provides the best results in terms of training loss. Among these trials, the weight norm strongly correlates with predicting generalization. To assess the effectiveness of Twin, we run extensive experiments on 20 image classification datasets and train several families of deep networks, including <b>convolutional,</b> <b>transformer,</b> and feed-forward models. We demonstrate proper HP selection when training from scratch and <b>fine-tuning,</b> emphasizing small-sample scenarios.</p></p class="citation"></blockquote><h3 id=1332--81270-gear-an-efficient-kv-cache-compression-recipe-for-near-lossless-generative-inference-of-llm-hao-kang-et-al-2024>(13/32 | 81/270) GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM (Hao Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao. (2024)<br><strong>GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM</strong><br><button class=copy-to-clipboard title="GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05527v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05527v2.pdf filename=2403.05527v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Key-value (KV) caching has become the de-facto to accelerate generation speed for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> inference. However, the growing cache demand with increasing sequence length has transformed <b>LLM</b> inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies <b>quantization</b> to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the <b>quantization</b> error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x. Our code is publicly available at <a href=https://github.com/HaoKang-Timmy/GEAR>https://github.com/HaoKang-Timmy/GEAR</a>.</p></p class="citation"></blockquote><h3 id=1432--82270-considering-nonstationary-within-multivariate-time-series-with-variational-hierarchical-transformer-for-forecasting-muyao-wang-et-al-2024>(14/32 | 82/270) Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting (Muyao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muyao Wang, Wenchao Chen, Bo Chen. (2024)<br><strong>Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting</strong><br><button class=copy-to-clipboard title="Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Probabilistic Model, Transformer, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05406v1.pdf filename=2403.05406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The forecasting of Multivariate Time Series <b>(MTS)</b> has long been an important but challenging task. Due to the non-stationary problem across long-distance time steps, previous studies primarily adopt stationarization method to attenuate the non-stationary problem of the original series for better predictability. However, existing methods always adopt the stationarized series, which ignores the inherent non-stationarity, and has difficulty in modeling <b>MTS</b> with complex distributions due to the lack of stochasticity. To tackle these problems, we first develop a powerful hierarchical <b>probabilistic</b> <b>generative</b> module to consider the non-stationarity and stochastic characteristics within <b>MTS,</b> and then combine it with <b>transformer</b> for a well-defined variational generative dynamic model named Hierarchical Time series Variational <b>Transformer</b> (HTV-Trans), which recovers the intrinsic non-stationary information into temporal dependencies. Being a powerful <b>probabilistic</b> <b>model,</b> HTV-Trans is utilized to learn expressive representations of <b>MTS</b> and applied to forecasting tasks. Extensive experiments on diverse datasets show the efficiency of HTV-Trans on <b>MTS</b> forecasting tasks</p></p class="citation"></blockquote><h3 id=1532--83270-adversarial-sparse-teacher-defense-against-distillation-based-model-stealing-attacks-using-adversarial-examples-eda-yilmaz-et-al-2024>(15/32 | 83/270) Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples (Eda Yilmaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eda Yilmaz, Hacer Yalim Keles. (2024)<br><strong>Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples</strong><br><button class=copy-to-clipboard title="Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05181v1.pdf filename=2403.05181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> facilitates the transfer of discriminative capabilities from an advanced teacher model to a simpler student model, ensuring performance enhancement without compromising accuracy. It is also exploited for model stealing attacks, where adversaries use <b>KD</b> to mimic the functionality of a teacher model. Recent developments in this domain have been influenced by the Stingy Teacher model, which provided empirical analysis showing that sparse outputs can significantly degrade the performance of student models. Addressing the risk of intellectual property leakage, our work introduces an approach to train a teacher model that inherently protects its logits, influenced by the Nasty Teacher concept. Differing from existing methods, we incorporate sparse outputs of adversarial examples with standard training data to strengthen the teacher&rsquo;s defense against student <b>distillation.</b> Our approach carefully reduces the relative entropy between the original and adversarially perturbed outputs, allowing the model to produce adversarial logits with minimal impact on overall performance. The source codes will be made publicly available soon.</p></p class="citation"></blockquote><h3 id=1632--84270-adaptive-split-learning-over-energy-constrained-wireless-edge-networks-zuguang-li-et-al-2024>(16/32 | 84/270) Adaptive Split Learning over Energy-Constrained Wireless Edge Networks (Zuguang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zuguang Li, Wen Wu, Shaohua Wu, Wei Wang. (2024)<br><strong>Adaptive Split Learning over Energy-Constrained Wireless Edge Networks</strong><br><button class=copy-to-clipboard title="Adaptive Split Learning over Energy-Constrained Wireless Edge Networks" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NI, cs.LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05158v1.pdf filename=2403.05158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Split learning (SL) is a promising approach for training artificial intelligence (AI) models, in which devices collaborate with a server to train an AI model in a distributed manner, based on a same fixed split point. However, due to the device heterogeneity and variation of channel conditions, this way is not optimal in training delay and energy consumption. In this paper, we design an adaptive split learning (ASL) scheme which can dynamically select split points for devices and allocate computing resource for the server in wireless edge networks. We formulate an optimization problem to minimize the average training latency subject to long-term energy consumption constraint. The difficulties in solving this problem are the lack of future information and mixed integer programming (MIP). To solve it, we propose an online algorithm leveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP problem only with the current information. Then, a two-layer optimization method is proposed to solve the MIP problem. Extensive <b>simulation</b> results demonstrate that the ASL scheme can reduce the average training delay and energy consumption by 53.7% and 22.1%, respectively, as compared to the existing SL schemes.</p></p class="citation"></blockquote><h3 id=1732--85270-ectonas-evolutionary-cross-topology-neural-architecture-search-elisabeth-j-schiessler-et-al-2024>(17/32 | 85/270) ECToNAS: Evolutionary Cross-Topology Neural Architecture Search (Elisabeth J. Schiessler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elisabeth J. Schiessler, Roland C. Aydin, Christian J. Cyron. (2024)<br><strong>ECToNAS: Evolutionary Cross-Topology Neural Architecture Search</strong><br><button class=copy-to-clipboard title="ECToNAS: Evolutionary Cross-Topology Neural Architecture Search" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 20<br>Keywords: MNIST, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05123v1.pdf filename=2403.05123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present ECToNAS, a cost-efficient evolutionary cross-topology neural architecture search algorithm that does not require any pre-trained meta controllers. Our framework is able to select suitable network architectures for different tasks and hyperparameter settings, independently performing cross-topology optimisation where required. It is a hybrid approach that fuses training and topology optimisation together into one lightweight, resource-friendly process. We demonstrate the validity and power of this approach with six standard data sets (CIFAR-10, CIFAR-100, EuroSAT, Fashion <b>MNIST,</b> <b>MNIST,</b> SVHN), showcasing the algorithm&rsquo;s ability to not only optimise the topology within an architectural type, but also to dynamically add and remove <b>convolutional</b> cells when and where required, thus crossing boundaries between different network types. This enables researchers without a background in machine learning to make use of appropriate model types and topologies and to apply machine learning methods in their domains, with a computationally cheap, easy-to-use cross-topology neural architecture search framework that fully encapsulates the topology optimisation within the training process.</p></p class="citation"></blockquote><h3 id=1832--86270-reset--distill-a-recipe-for-overcoming-negative-transfer-in-continual-reinforcement-learning-hongjoon-ahn-et-al-2024>(18/32 | 86/270) Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning (Hongjoon Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongjoon Ahn, Jinu Hyeon, Youngmin Oh, Bosun Hwang, Taesup Moon. (2024)<br><strong>Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05066v1.pdf filename=2403.05066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We argue that one of the main obstacles for developing effective Continual <b>Reinforcement</b> <b>Learning</b> (CRL) algorithms is the negative transfer issue occurring when the new task to learn arrives. Through comprehensive experimental validation, we demonstrate that such issue frequently exists in CRL and cannot be effectively addressed by several recent work on mitigating plasticity loss of RL agents. To that end, we develop Reset & <b>Distill</b> (R&amp;D), a simple yet highly effective method, to overcome the negative transfer problem in CRL. R&amp;D combines a strategy of resetting the agent&rsquo;s online actor and critic networks to learn a new task and an offline learning step for <b>distilling</b> the knowledge from the online actor and previous expert&rsquo;s action probabilities. We carried out extensive experiments on long sequence of Meta-World tasks and show that our method consistently outperforms recent baselines, achieving significantly higher success rates across a range of tasks. Our findings highlight the importance of considering negative transfer in CRL and emphasize the need for robust strategies like R&amp;D to mitigate its detrimental effects.</p></p class="citation"></blockquote><h3 id=1932--87270-quantifying-manifolds-do-the-manifolds-learned-by-generative-adversarial-networks-converge-to-the-real-data-manifold-anupam-chaudhuri-et-al-2024>(19/32 | 87/270) Quantifying Manifolds: Do the manifolds learned by Generative Adversarial Networks converge to the real data manifold (Anupam Chaudhuri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anupam Chaudhuri, Anj Simmons, Mohamed Abdelrazek. (2024)<br><strong>Quantifying Manifolds: Do the manifolds learned by Generative Adversarial Networks converge to the real data manifold</strong><br><button class=copy-to-clipboard title="Quantifying Manifolds: Do the manifolds learned by Generative Adversarial Networks converge to the real data manifold" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05033v1.pdf filename=2403.05033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents our experiments to quantify the manifolds learned by ML models (in our experiment, we use a <b>GAN</b> model) as they train. We compare the manifolds learned at each epoch to the real manifolds representing the real data. To quantify a manifold, we study the intrinsic dimensions and topological features of the manifold learned by the ML model, how these metrics change as we continue to train the model, and whether these metrics convergence over the course of training to the metrics of the real data manifold.</p></p class="citation"></blockquote><h3 id=2032--88270-poly-view-contrastive-learning-amitis-shidani-et-al-2024>(20/32 | 88/270) Poly-View Contrastive Learning (Amitis Shidani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amitis Shidani, Devon Hjelm, Jason Ramapuram, Russ Webb, Eeshan Gunesh Dhekane, Dan Busbridge. (2024)<br><strong>Poly-View Contrastive Learning</strong><br><button class=copy-to-clipboard title="Poly-View Contrastive Learning" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 15<br>Keywords: Contrastive Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05490v1.pdf filename=2403.05490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>learning</b> typically matches pairs of related views among a number of unrelated negative views. Views can be generated (e.g. by augmentations) or be observed. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new <b>representation</b> <b>learning</b> objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view <b>contrastive</b> <b>models</b> trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that <b>contrastive</b> <b>models</b> require large batch sizes and many training epochs.</p></p class="citation"></blockquote><h3 id=2132--89270-overcoming-data-inequality-across-domains-with-semi-supervised-domain-generalization-jinha-park-et-al-2024>(21/32 | 89/270) Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization (Jinha Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinha Park, Wonguk Cho, Taesup Kim. (2024)<br><strong>Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization</strong><br><button class=copy-to-clipboard title="Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05209v1.pdf filename=2403.05209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While there have been considerable advancements in machine learning driven by extensive datasets, a significant disparity still persists in the availability of data across various sources and populations. This inequality across domains poses challenges in modeling for those with limited data, which can lead to profound practical and ethical concerns. In this paper, we address a representative case of data inequality problem across domains termed <b>Semi-Supervised</b> <b>Domain</b> Generalization (SSDG), in which only one domain is labeled while the rest are unlabeled. We propose a novel algorithm, ProUD, which can effectively learn domain-invariant features via domain-aware prototypes along with progressive generalization via uncertainty-adaptive mixing of labeled and unlabeled domains. Our experiments on three different <b>benchmark</b> datasets demonstrate the effectiveness of ProUD, outperforming all baseline models including single domain generalization and <b>semi-supervised</b> <b>learning.</b> Source code will be released upon acceptance of the paper.</p></p class="citation"></blockquote><h3 id=2232--90270-a-concept-based-interpretable-model-for-the-diagnosis-of-choroid-neoplasias-using-multimodal-data-yifan-wu-et-al-2024>(22/32 | 90/270) A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data (Yifan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Wu, Yang Liu, Yue Yang, Michael S. Yao, Wenli Yang, Xuehui Shi, Lihong Yang, Dongjun Li, Yueming Liu, James C. Gee, Xuan Yang, Wenbin Wei, Shi Gu. (2024)<br><strong>A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data</strong><br><button class=copy-to-clipboard title="A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 11<br>Keywords: Black Box, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05606v1.pdf filename=2403.05606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diagnosing rare diseases presents a common challenge in clinical practice, necessitating the expertise of specialists for accurate identification. The advent of machine learning offers a promising solution, while the development of such technologies is hindered by the scarcity of data on rare conditions and the demand for models that are both interpretable and trustworthy in a clinical context. Interpretable AI, with its capacity for human-readable outputs, can facilitate validation by clinicians and contribute to medical education. In the current work, we focus on choroid neoplasias, the most prevalent form of eye cancer in adults, albeit rare with 5.1 per million. We built the so-far largest dataset consisting of 750 patients, incorporating three distinct imaging modalities collected from 2004 to 2022. Our work introduces a concept-based interpretable model that distinguishes between three types of choroidal tumors, integrating insights from domain experts via radiological reports. Remarkably, this model not only achieves an F1 score of 0.91, rivaling that of <b>black-box</b> <b>models,</b> but also boosts the diagnostic accuracy of junior doctors by 42%. This study highlights the significant potential of interpretable machine learning in improving the diagnosis of rare diseases, laying a groundwork for future breakthroughs in medical AI that could tackle a wider array of complex health scenarios.</p></p class="citation"></blockquote><h3 id=2332--91270-mathtttsgt-stochastic-time-series-modeling-with-transformer-łukasz-kuciński-et-al-2024>(23/32 | 91/270) $\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer (Łukasz Kuciński et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Łukasz Kuciński, Witold Drzewakowski, Mateusz Olko, Piotr Kozakowski, Łukasz Maziarka, Marta Emilia Nowakowska, Łukasz Kaiser, Piotr Miłoś. (2024)<br><strong>$\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer</strong><br><button class=copy-to-clipboard title="$\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05713v1.pdf filename=2403.05713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series methods are of fundamental importance in virtually any field of science that deals with temporally structured data. Recently, there has been a surge of deterministic <b>transformer</b> models with time series-specific architectural biases. In this paper, we go in a different direction by introducing $\mathtt{tsGT}$, a stochastic time series model built on a general-purpose <b>transformer</b> architecture. We focus on using a well-known and theoretically justified rolling window backtesting and evaluation protocol. We show that $\mathtt{tsGT}$ outperforms the state-of-the-art models on MAD and RMSE, and surpasses its stochastic peers on QL and CRPS, on four commonly used datasets. We complement these results with a detailed analysis of $\mathtt{tsGT}$&rsquo;s ability to model the data distribution and predict marginal quantile values.</p></p class="citation"></blockquote><h3 id=2432--92270-shielded-deep-reinforcement-learning-for-complex-spacecraft-tasking-robert-reed-et-al-2024>(24/32 | 92/270) Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking (Robert Reed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Reed, Hanspeter Schaub, Morteza Lahijanian. (2024)<br><strong>Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking</strong><br><button class=copy-to-clipboard title="Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05693v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05693v2.pdf filename=2403.05693v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous spacecraft control via Shielded Deep <b>Reinforcement</b> <b>Learning</b> (SDRL) has become a rapidly growing research area. However, the construction of shields and the definition of tasking remains informal, resulting in policies with no guarantees on safety and ambiguous goals for the RL agent. In this paper, we first explore the use of formal languages, namely Linear Temporal Logic (LTL), to formalize spacecraft tasks and safety requirements. We then define a manner in which to construct a reward function from a co-safe LTL specification automatically for effective training in SDRL framework. We also investigate methods for constructing a shield from a safe LTL specification for spacecraft applications and propose three designs that provide probabilistic guarantees. We show how these shields interact with different policies and the flexibility of the reward structure through several experiments.</p></p class="citation"></blockquote><h3 id=2532--93270-what-is-different-between-these-datasets-varun-babbar-et-al-2024>(25/32 | 93/270) What is different between these datasets? (Varun Babbar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Varun Babbar, Zhicheng Guo, Cynthia Rudin. (2024)<br><strong>What is different between these datasets?</strong><br><button class=copy-to-clipboard title="What is different between these datasets?" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05652v1.pdf filename=2403.05652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of machine learning models heavily depends on the quality of input data, yet real-world applications often encounter various data-related challenges. One such challenge could arise when curating training data or deploying the model in the real world - two comparable datasets in the same domain may have different <b>distributions.</b> <b>While</b> numerous techniques exist for detecting <b>distribution</b> <b>shifts,</b> the literature lacks comprehensive approaches for explaining dataset differences in a human-understandable manner. To address this gap, we propose a suite of interpretable methods (toolbox) for comparing two datasets. We demonstrate the versatility of our approach across diverse data modalities, including tabular data, language, images, and signals in both low and high-dimensional settings. Our methods not only outperform comparable and related approaches in terms of explanation quality and correctness, but also provide actionable, complementary insights to understand and mitigate dataset differences effectively.</p></p class="citation"></blockquote><h3 id=2632--94270-recovery-guarantees-of-unsupervised-neural-networks-for-inverse-problems-trained-with-gradient-descent-nathan-buskulic-et-al-2024>(26/32 | 94/270) Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent (Nathan Buskulic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Buskulic, Jalal Fadili, Yvain Quéau. (2024)<br><strong>Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent</strong><br><button class=copy-to-clipboard title="Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05395v1.pdf filename=2403.05395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advanced machine learning methods, and more prominently neural networks, have become standard to solve inverse problems over the last years. However, the theoretical recovery guarantees of such methods are still scarce and difficult to achieve. Only recently did <b>unsupervised</b> methods such as Deep Image Prior (DIP) get equipped with convergence and recovery guarantees for generic loss functions when trained through gradient flow with an appropriate initialization. In this paper, we extend these results by proving that these guarantees hold true when using gradient descent with an appropriately chosen step-size/learning rate. We also show that the discretization only affects the overparametrization bound for a two-layer DIP network by a constant and thus that the different guarantees found for the gradient flow will hold for gradient descent.</p></p class="citation"></blockquote><h3 id=2732--95270-switching-the-loss-reduces-the-cost-in-batch-reinforcement-learning-alex-ayoub-et-al-2024>(27/32 | 95/270) Switching the Loss Reduces the Cost in Batch Reinforcement Learning (Alex Ayoub et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James McInerney, Dawen Liang, Nathan Kallus, Csaba Szepesvári. (2024)<br><strong>Switching the Loss Reduces the Cost in Batch Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Switching the Loss Reduces the Cost in Batch Reinforcement Learning" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05385v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05385v3.pdf filename=2403.05385v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch <b>reinforcement</b> <b>learning</b> (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.</p></p class="citation"></blockquote><h3 id=2832--96270-leveraging-continuous-time-to-understand-momentum-when-training-diagonal-linear-networks-hristo-papazov-et-al-2024>(28/32 | 96/270) Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks (Hristo Papazov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hristo Papazov, Scott Pesme, Nicolas Flammarion. (2024)<br><strong>Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks</strong><br><button class=copy-to-clipboard title="Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05293v1.pdf filename=2403.05293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we investigate the effect of momentum on the optimisation trajectory of gradient descent. We leverage a <b>continuous-time</b> <b>approach</b> in the analysis of momentum gradient descent with step size $\gamma$ and momentum parameter $\beta$ that allows us to identify an intrinsic quantity $\lambda = \frac{ \gamma }{ (1 - \beta)^2 }$ which uniquely defines the optimisation path and provides a simple acceleration rule. When training a $2$-layer diagonal linear network in an overparametrised regression setting, we characterise the recovered solution through an implicit regularisation problem. We then prove that small values of $\lambda$ help to recover sparse solutions. Finally, we give similar but weaker results for stochastic momentum gradient descent. We provide numerical experiments which support our claims.</p></p class="citation"></blockquote><h3 id=2932--97270-fairness-aware-interpretable-modeling-faim-for-trustworthy-machine-learning-in-healthcare-mingxuan-liu-et-al-2024>(29/32 | 97/270) Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare (Mingxuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxuan Liu, Yilin Ning, Yuhe Ke, Yuqing Shang, Bibhas Chakraborty, Marcus Eng Hock Ong, Roger Vaughan, Nan Liu. (2024)<br><strong>Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare</strong><br><button class=copy-to-clipboard title="Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05235v1.pdf filename=2403.05235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model <b>fairness.</b> We propose an interpretable framework - <b>Fairness-Aware</b> Interpretable Modeling (FAIM), to improve model <b>fairness</b> without compromising performance, featuring an interactive interface to identify a &ldquo;fairer&rdquo; model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized <b>fairness.</b> We demonstrated FAIM&rsquo;s value in reducing sex and race biases by predicting hospital admission with two real-world databases, MIMIC-IV-ED and SGH-ED. We show that for both datasets, FAIM models not only exhibited satisfactory discriminatory performance but also significantly mitigated biases as measured by well-established <b>fairness</b> metrics, outperforming commonly used bias-mitigation methods. Our approach demonstrates the feasibility of improving <b>fairness</b> without sacrificing performance and provides an a modeling mode that invites domain experts to engage, fostering a multidisciplinary effort toward tailored AI <b>fairness.</b></p></p class="citation"></blockquote><h3 id=3032--98270-continual-learning-and-catastrophic-forgetting-gido-m-van-de-ven-et-al-2024>(30/32 | 98/270) Continual Learning and Catastrophic Forgetting (Gido M. van de Ven et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gido M. van de Ven, Nicholas Soures, Dhireesha Kudithipudi. (2024)<br><strong>Continual Learning and Catastrophic Forgetting</strong><br><button class=copy-to-clipboard title="Continual Learning and Catastrophic Forgetting" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, q-bio-NC, stat-ML<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05175v1.pdf filename=2403.05175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This book chapter delves into the dynamics of <b>continual</b> <b>learning,</b> which is the process of incrementally learning from a non-stationary stream of data. Although <b>continual</b> <b>learning</b> is a natural skill for the human brain, it is very challenging for artificial neural networks. An important reason is that, when learning something new, these networks tend to quickly and drastically forget what they had learned before, a phenomenon known as catastrophic forgetting. Especially in the last decade, <b>continual</b> <b>learning</b> has become an extensively studied topic in deep learning. This book chapter reviews the insights that this field has generated.</p></p class="citation"></blockquote><h3 id=3132--99270-vtrust-controllable-value-function-based-subset-selection-for-data-centric-trustworthy-ai-soumi-das-et-al-2024>(31/32 | 99/270) VTruST: Controllable value function based subset selection for Data-Centric Trustworthy AI (Soumi Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soumi Das, Shubhadip Nag, Shreyyash Sharma, Suparna Bhattacharya, Sourangshu Bhattacharya. (2024)<br><strong>VTruST: Controllable value function based subset selection for Data-Centric Trustworthy AI</strong><br><button class=copy-to-clipboard title="VTruST: Controllable value function based subset selection for Data-Centric Trustworthy AI" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05174v1.pdf filename=2403.05174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trustworthy AI is crucial to the widespread adoption of AI in high-stakes applications with <b>fairness,</b> robustness, and accuracy being some of the key trustworthiness metrics. In this work, we propose a controllable framework for data-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the trade-offs between the different trustworthiness metrics of the constructed training datasets. A key challenge in implementing an efficient DCTAI framework is to design an online value-function-based training data subset selection algorithm. We pose the training data valuation and subset selection problem as an online sparse approximation formulation. We propose a novel online version of the Orthogonal Matching Pursuit (OMP) algorithm for solving this problem. Experimental results show that VTruST outperforms the state-of-the-art baselines on social, image, and scientific datasets. We also show that the data values generated by VTruST can provide effective data-centric explanations for different trustworthiness metrics.</p></p class="citation"></blockquote><h3 id=3232--100270-synthetic-data-generation-for-system-identification-leveraging-knowledge-transfer-from-similar-systems-dario-piga-et-al-2024>(32/32 | 100/270) Synthetic data generation for system identification: leveraging knowledge transfer from similar systems (Dario Piga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dario Piga, Matteo Rufolo, Gabriele Maroni, Manas Mejari, Marco Forgione. (2024)<br><strong>Synthetic data generation for system identification: leveraging knowledge transfer from similar systems</strong><br><button class=copy-to-clipboard title="Synthetic data generation for system identification: leveraging knowledge transfer from similar systems" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05164v1.pdf filename=2403.05164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenge of overfitting in the learning of dynamical systems by introducing a novel approach for the generation of synthetic data, aimed at enhancing model generalization and robustness in scenarios characterized by data scarcity. Central to the proposed methodology is the concept of <b>knowledge</b> <b>transfer</b> from systems within the same class. Specifically, synthetic data is generated through a pre-trained meta-model that describes a broad class of systems to which the system of interest is assumed to belong. Training data serves a dual purpose: firstly, as input to the pre-trained meta model to discern the system&rsquo;s dynamics, enabling the prediction of its behavior and thereby generating synthetic output sequences for new input sequences; secondly, in conjunction with synthetic data, to define the loss function used for model estimation. A validation dataset is used to tune a scalar hyper-parameter balancing the relative importance of training and synthetic data in the definition of the loss function. The same validation set can be also used for other purposes, such as early stopping during the training, fundamental to avoid overfitting in case of small-size training datasets. The efficacy of the approach is shown through a numerical example that highlights the advantages of integrating synthetic data into the system identification process.</p></p class="citation"></blockquote><h2 id=eessiv-9>eess.IV (9)</h2><h3 id=19--101270-hybridized-convolutional-neural-networks-and-long-short-term-memory-for-improved-alzheimers-disease-diagnosis-from-mri-scans-maleka-khatun-et-al-2024>(1/9 | 101/270) Hybridized Convolutional Neural Networks and Long Short-Term Memory for Improved Alzheimer&rsquo;s Disease Diagnosis from MRI Scans (Maleka Khatun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maleka Khatun, Md Manowarul Islam, Habibur Rahman Rifat, Md. Shamim Bin Shahid, Md. Alamin Talukder, Md Ashraf Uddin. (2024)<br><strong>Hybridized Convolutional Neural Networks and Long Short-Term Memory for Improved Alzheimer&rsquo;s Disease Diagnosis from MRI Scans</strong><br><button class=copy-to-clipboard title="Hybridized Convolutional Neural Networks and Long Short-Term Memory for Improved Alzheimer's Disease Diagnosis from MRI Scans" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 70<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transfer Learning, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05353v1.pdf filename=2403.05353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brain-related diseases are more sensitive than other diseases due to several factors, including the complexity of surgical procedures, high costs, and other challenges. Alzheimer&rsquo;s disease is a common brain disorder that causes memory loss and the shrinking of brain cells. Early detection is critical for providing proper treatment to patients. However, identifying Alzheimer&rsquo;s at an early stage using manual scanning of CT or MRI scans is challenging. Therefore, researchers have delved into the exploration of computer-aided systems, employing Machine Learning and Deep Learning methodologies, which entail the training of datasets to detect Alzheimer&rsquo;s disease. This study aims to present a hybrid model that combines a <b>CNN</b> model&rsquo;s feature extraction capabilities with an <b>LSTM</b> model&rsquo;s detection capabilities. This study has applied the <b>transfer</b> <b>learning</b> called VGG16 in the hybrid model to extract features from MRI images. The <b>LSTM</b> detects features between the <b>convolution</b> layer and the fully connected layer. The output layer of the fully connected layer uses the softmax function. The training of the hybrid model involved utilizing the ADNI dataset. The trial findings revealed that the model achieved a level of accuracy of 98.8%, a sensitivity rate of 100%, and a specificity rate of 76%. The proposed hybrid model outperforms its contemporary <b>CNN</b> counterparts, showcasing a superior performance.</p></p class="citation"></blockquote><h3 id=29--102270-c2p-gcn-cell-to-patch-graph-convolutional-network-for-colorectal-cancer-grading-sudipta-paul-et-al-2024>(2/9 | 102/270) C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer Grading (Sudipta Paul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sudipta Paul, Bulent Yener, Amanda W. Lund. (2024)<br><strong>C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer Grading</strong><br><button class=copy-to-clipboard title="C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer Grading" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 43<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04962v1.pdf filename=2403.04962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph-based</b> <b>learning</b> <b>approaches,</b> due to their ability to encode tissue/organ structure information, are increasingly favored for grading colorectal cancer histology images. Recent <b>graph-based</b> <b>techniques</b> <b>involve</b> dividing whole slide images (WSIs) into smaller or medium-sized patches, and then building <b>graphs</b> <b>on</b> <b>each</b> patch for direct use in training. This method, however, fails to capture the tissue structure information present in an entire WSI and relies on training from a significantly large dataset of image patches. In this paper, we propose a novel cell-to-patch <b>graph</b> <b>convolutional</b> <b>network</b> (C2P-GCN), which is a two-stage <b>graph</b> <b>formation-based</b> <b>approach.</b> In the first stage, it forms a patch-level <b>graph</b> <b>based</b> <b>on</b> the cell organization on each patch of a WSI. In the second stage, it forms an image-level <b>graph</b> <b>based</b> <b>on</b> a similarity measure between patches of a WSI considering each patch as a node of a <b>graph.</b> <b>This</b> <b>graph</b> <b>representation</b> <b>is</b> then fed into a multi-layer <b>GCN-based</b> classification network. Our approach, through its dual-phase <b>graph</b> <b>construction,</b> <b>effectively</b> gathers local structural details from individual patches and establishes a meaningful connection among all patches across a WSI. As C2P-GCN integrates the structural data of an entire WSI into a single <b>graph,</b> <b>it</b> <b>allows</b> our model to work with significantly fewer training data compared to the latest models for colorectal cancer. Experimental validation of C2P-GCN on two distinct colorectal cancer datasets demonstrates the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=39--103270-spatial-aware-transformer-gru-framework-for-enhanced-glaucoma-diagnosis-from-3d-oct-imaging-mona-ashtari-majlan-et-al-2024>(3/9 | 103/270) Spatial-aware Transformer-GRU Framework for Enhanced Glaucoma Diagnosis from 3D OCT Imaging (Mona Ashtari-Majlan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mona Ashtari-Majlan, Mohammad Mahdi Dehshibi, David Masip. (2024)<br><strong>Spatial-aware Transformer-GRU Framework for Enhanced Glaucoma Diagnosis from 3D OCT Imaging</strong><br><button class=copy-to-clipboard title="Spatial-aware Transformer-GRU Framework for Enhanced Glaucoma Diagnosis from 3D OCT Imaging" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Graph Attention Networks, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05702v1.pdf filename=2403.05702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Glaucoma, a leading cause of irreversible blindness, necessitates early detection for accurate and timely intervention to prevent irreversible <b>vision</b> <b>loss.</b> In this study, we present a novel deep learning framework that leverages the diagnostic value of 3D Optical Coherence Tomography (OCT) imaging for automated glaucoma detection. In this framework, we integrate a pre-trained <b>Vision</b> <b>Transformer</b> on retinal data for rich slice-wise feature extraction and a bidirectional <b>Gated</b> Recurrent Unit for capturing inter-slice spatial dependencies. This dual-component approach enables comprehensive analysis of local nuances and global structural integrity, crucial for accurate glaucoma diagnosis. Experimental results on a large dataset demonstrate the superior performance of the proposed method over state-of-the-art ones, achieving an F1-score of 93.58%, Matthews Correlation Coefficient (MCC) of 73.54%, and AUC of 95.24%. The framework&rsquo;s ability to leverage the valuable information in 3D OCT data holds significant potential for enhancing clinical decision support systems and improving patient outcomes in glaucoma management.</p></p class="citation"></blockquote><h3 id=49--104270-a-data-augmentation-pipeline-to-generate-synthetic-labeled-datasets-of-3d-echocardiography-images-using-a-gan-cristiana-tiago-et-al-2024>(4/9 | 104/270) A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of 3D Echocardiography Images using a GAN (Cristiana Tiago et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristiana Tiago, Andrew Gilbert, Ahmed S. Beela, Svein Arne Aase, Sten Roar Snare, Jurica Sprem. (2024)<br><strong>A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of 3D Echocardiography Images using a GAN</strong><br><button class=copy-to-clipboard title="A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of 3D Echocardiography Images using a GAN" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05384v1.pdf filename=2403.05384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to privacy issues and limited amount of publicly available labeled datasets in the domain of medical imaging, we propose an image generation pipeline to synthesize 3D echocardiographic images with corresponding ground truth labels, to alleviate the need for <b>data</b> <b>collection</b> and for laborious and error-prone human labeling of images for subsequent Deep Learning (DL) tasks. The proposed method utilizes detailed anatomical segmentations of the heart as ground truth label sources. This initial dataset is combined with a second dataset made up of real 3D echocardiographic images to train a <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)</b> to synthesize realistic 3D cardiovascular Ultrasound images paired with ground truth labels. To generate the synthetic 3D dataset, the trained <b>GAN</b> uses high resolution anatomical models from Computed Tomography (CT) as input. A qualitative analysis of the synthesized images showed that the main structures of the heart are well delineated and closely follow the labels obtained from the anatomical models. To assess the usability of these synthetic images for DL tasks, segmentation algorithms were trained to delineate the left ventricle, left atrium, and myocardium. A quantitative analysis of the 3D segmentations given by the models trained with the synthetic images indicated the potential use of this <b>GAN</b> approach to generate 3D synthetic <b>data,</b> <b>use</b> the <b>data</b> <b>to</b> train DL models for different clinical tasks, and therefore tackle the problem of scarcity of 3D labeled echocardiography datasets.</p></p class="citation"></blockquote><h3 id=59--105270-lightm-unet-mamba-assists-in-lightweight-unet-for-medical-image-segmentation-weibin-liao-et-al-2024>(5/9 | 105/270) LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation (Weibin Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weibin Liao, Yinghao Zhu, Xinyuan Wang, Chengwei Pan, Yasha Wang, Liantao Ma. (2024)<br><strong>LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Transformer, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05246v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05246v2.pdf filename=2403.05246v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>UNet and its variants have been widely used in medical image segmentation. However, these models, especially those based on <b>Transformer</b> architectures, pose challenges due to their large number of parameters and computational loads, making them unsuitable for mobile health applications. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as competitive alternatives to <b>CNN</b> and <b>Transformer</b> architectures. Building upon this, we employ Mamba as a lightweight substitute for <b>CNN</b> and <b>Transformer</b> within UNet, aiming at tackling challenges <b>stemming</b> from computational resource limitations in real medical settings. To this end, we introduce the Lightweight Mamba UNet (LightM-UNet) that integrates Mamba and UNet in a lightweight framework. Specifically, LightM-UNet leverages the Residual Vision Mamba Layer in a pure Mamba fashion to extract deep semantic features and model long-range spatial dependencies, with linear computational complexity. Extensive experiments conducted on two real-world 2D/3D datasets demonstrate that LightM-UNet surpasses existing state-of-the-art literature. Notably, when compared to the renowned nnU-Net, LightM-UNet achieves superior segmentation performance while drastically reducing parameter and computation costs by 116x and 21x, respectively. This highlights the potential of Mamba in facilitating model lightweighting. Our code implementation is publicly available at <a href=https://github.com/MrBlankness/LightM-UNet>https://github.com/MrBlankness/LightM-UNet</a>.</p></p class="citation"></blockquote><h3 id=69--106270-fedfms-exploring-federated-foundation-models-for-medical-image-segmentation-yuxi-liu-et-al-2024>(6/9 | 106/270) FedFMS: Exploring Federated Foundation Models for Medical Image Segmentation (Yuxi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxi Liu, Guibo Luo, Yuesheng Zhu. (2024)<br><strong>FedFMS: Exploring Federated Foundation Models for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="FedFMS: Exploring Federated Foundation Models for Medical Image Segmentation" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: I-4-6; I-2-11, cs-CV, cs-DC, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Federated Learning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05408v1.pdf filename=2403.05408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation is crucial for clinical diagnosis. The Segmentation Anything Model (SAM) serves as a powerful <b>foundation</b> <b>model</b> for visual segmentation and can be adapted for medical image segmentation. However, medical imaging data typically contain privacy-sensitive information, making it challenging to train <b>foundation</b> <b>models</b> with centralized storage and sharing. To date, there are few <b>foundation</b> <b>models</b> tailored for medical image deployment within the <b>federated</b> <b>learning</b> framework, and the segmentation performance, as well as the efficiency of communication and training, remain unexplored. In response to these issues, we developed <b>Federated</b> <b>Foundation</b> <b>models</b> for Medical image Segmentation (FedFMS), which includes the <b>Federated</b> <b>SAM</b> (FedSAM) and a communication and training-efficient <b>Federated</b> <b>SAM</b> with Medical SAM Adapter (FedMSA). Comprehensive experiments on diverse datasets are conducted to investigate the performance disparities between centralized training and <b>federated</b> <b>learning</b> across various configurations of FedFMS. The experiments revealed that FedFMS could achieve performance comparable to models trained via centralized training methods while maintaining privacy. Furthermore, FedMSA demonstrated the potential to enhance communication and training efficiency. Our model implementation codes are available at <a href=https://github.com/LIU-YUXI/FedFMS>https://github.com/LIU-YUXI/FedFMS</a>.</p></p class="citation"></blockquote><h3 id=79--107270-a-probabilistic-hadamard-u-net-for-mri-bias-field-correction-xin-zhu-et-al-2024>(7/9 | 107/270) A Probabilistic Hadamard U-Net for MRI Bias Field Correction (Xin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhu, Hongyi Pan, Yury Velichko, Adam B. Murphy, Ashley Ross, Baris Turkbey, Ahmet Enis Cetin, Ulas Bagci. (2024)<br><strong>A Probabilistic Hadamard U-Net for MRI Bias Field Correction</strong><br><button class=copy-to-clipboard title="A Probabilistic Hadamard U-Net for MRI Bias Field Correction" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05024v1.pdf filename=2403.05024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Magnetic field inhomogeneity correction remains a challenging task in MRI analysis. Most established techniques are designed for brain MRI by supposing that image intensities in the identical tissue follow a uniform distribution. Such an assumption cannot be easily applied to other organs, especially those that are small in size and heterogeneous in texture (large variations in intensity), such as the prostate. To address this problem, this paper proposes a probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field correction. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the low-frequency scalar field, multiplied by the original input to obtain the prototypical corrected image. HU-Net converts the input image from the time domain into the frequency domain via Hadamard transform. In the frequency domain, high-frequency components are eliminated using the trainable filter (scaling layer), hard-thresholding layer, and sparsity penalty. Next, a conditional <b>variational</b> <b>autoencoder</b> is used to encode possible bias field-corrected variants into a low-dimensional latent space. Random samples drawn from latent space are then incorporated with a prototypical corrected image to generate multiple plausible images. Experimental results demonstrate the effectiveness of PHU-Net in correcting bias-field in prostate MRI with a fast inference speed. It has also been shown that prostate MRI segmentation accuracy improves with the high-quality corrected images from PHU-Net. The code will be available in the final version of this manuscript.</p></p class="citation"></blockquote><h3 id=89--108270-dudouninext-dual-domain-unified-hybrid-model-for-single-and-multi-contrast-undersampled-mri-reconstruction-ziqi-gao-et-al-2024>(8/9 | 108/270) DuDoUniNeXt: Dual-domain unified hybrid model for single and multi-contrast undersampled MRI reconstruction (Ziqi Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqi Gao, Yue Zhang, Xinwen Liu, Kaiyan Li, S. Kevin Zhou. (2024)<br><strong>DuDoUniNeXt: Dual-domain unified hybrid model for single and multi-contrast undersampled MRI reconstruction</strong><br><button class=copy-to-clipboard title="DuDoUniNeXt: Dual-domain unified hybrid model for single and multi-contrast undersampled MRI reconstruction" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05256v1.pdf filename=2403.05256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to incorporate a reference image of auxiliary modality to guide the reconstruction process of the target modality. Known MC reconstruction methods perform well with a fully sampled reference image, but usually exhibit inferior performance, compared to single-contrast (SC) methods, when the reference image is missing or of low quality. To address this issue, we propose DuDoUniNeXt, a unified dual-domain MRI reconstruction network that can accommodate to scenarios involving absent, low-quality, and high-quality reference images. DuDoUniNeXt adopts a hybrid backbone that combines <b>CNN</b> and ViT, enabling specific adjustment of image domain and k-space reconstruction. Specifically, an adaptive coarse-to-fine feature fusion module (AdaC2F) is devised to dynamically process the information from reference images of varying qualities. Besides, a partially shared shallow feature extractor (PaSS) is proposed, which uses shared and distinct parameters to handle consistent and discrepancy information among contrasts. Experimental results demonstrate that the proposed model surpasses state-of-the-art SC and MC models significantly. Ablation studies show the effectiveness of the proposed hybrid backbone, AdaC2F, PaSS, and the dual-domain unified learning scheme.</p></p class="citation"></blockquote><h3 id=99--109270-noise-level-adaptive-diffusion-model-for-robust-reconstruction-of-accelerated-mri-shoujin-huang-et-al-2024>(9/9 | 109/270) Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI (Shoujin Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shoujin Huang, Guanxiong Luo, Xi Wang, Ziran Chen, Yuwan Wang, Huaishui Yang, Pheng-Ann Heng, Lingyan Zhang, Mengye Lyu. (2024)<br><strong>Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI</strong><br><button class=copy-to-clipboard title="Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05245v1.pdf filename=2403.05245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In general, <b>diffusion</b> <b>model-based</b> MRI reconstruction methods incrementally remove artificially added noise while imposing data consistency to reconstruct the underlying images. However, real-world MRI acquisitions already contain inherent noise due to thermal fluctuations. This phenomenon is particularly notable when using ultra-fast, high-resolution imaging sequences for advanced research, or using low-field systems favored by low- and middle-income countries. These common scenarios can lead to sub-optimal performance or complete failure of existing <b>diffusion</b> <b>model-based</b> reconstruction techniques. Specifically, as the artificially added noise is gradually removed, the inherent MRI noise becomes increasingly pronounced, making the actual noise level inconsistent with the predefined denoising schedule and consequently inaccurate image reconstruction. To tackle this problem, we propose a posterior sampling strategy with a novel NoIse Level Adaptive Data Consistency (Nila-DC) operation. Extensive experiments are conducted on two public datasets and an in-house clinical dataset with field strength ranging from 0.3T to 3T, showing that our method surpasses the state-of-the-art MRI reconstruction methods, and is highly robust against various noise levels. The code will be released after review.</p></p class="citation"></blockquote><h2 id=cscv-77>cs.CV (77)</h2><h3 id=177--110270-tracking-meets-lora-faster-training-larger-model-stronger-performance-liting-lin-et-al-2024>(1/77 | 110/270) Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance (Liting Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liting Lin, Heng Fan, Zhipeng Zhang, Yaowei Wang, Yong Xu, Haibin Ling. (2024)<br><strong>Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance</strong><br><button class=copy-to-clipboard title="Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Convolution, Fine-tuning, Fine-tuning, Transformer, Large Language Model, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05231v1.pdf filename=2403.05231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the Parameter-Efficient <b>Fine-Tuning</b> (PEFT) in <b>large</b> <b>language</b> <b>models,</b> we propose LoRAT, a method that unveils the power of larger <b>Vision</b> <b>Transformers</b> (ViT) for tracking within laboratory-level resources. The essence of our work lies in adapting LoRA, a technique that <b>fine-tunes</b> a small subset of model parameters without adding inference latency, to the domain of visual tracking. However, unique challenges and potential domain gaps make this transfer not as easy as the first intuition. Firstly, a <b>transformer-based</b> tracker constructs unshared position embedding for template and search image. This poses a challenge for the transfer of LoRA, usually requiring consistency in the design when applied to the pre-trained backbone, to downstream tasks. Secondly, the inductive bias inherent in <b>convolutional</b> heads diminishes the effectiveness of parameter-efficient <b>fine-tuning</b> in tracking models. To overcome these limitations, we first decouple the position embeddings in <b>transformer-based</b> trackers into shared spatial ones and independent type ones. The shared embeddings, which describe the absolute coordinates of multi-resolution images (namely, the template and search images), are inherited from the pre-trained backbones. In contrast, the independent embeddings indicate the sources of each token and are learned from scratch. Furthermore, we design an anchor-free head solely based on a multilayer perceptron (MLP) to adapt PETR, enabling better performance with less computational overhead. With our design, 1) it becomes practical to train trackers with the ViT-g backbone on GPUs with only memory of 25.8GB (batch size of 16); 2) we reduce the training time of the L-224 variant from 35.0 to 10.8 GPU hours; 3) we improve the LaSOT SUC score from 0.703 to 0.743 with the L-224 variant; 4) we fast the inference speed of the L-224 variant from 52 to 119 FPS. Code and models will be released.</p></p class="citation"></blockquote><h3 id=277--111270-beyond-finite-data-towards-data-free-out-of-distribution-generalization-via-extrapolation-yijiang-li-et-al-2024>(2/77 | 111/270) Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation (Yijiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijiang Li, Sucheng Ren, Weipeng Deng, Yuzhi Xu, Ying Gao, Edith Ngai, Haohan Wang. (2024)<br><strong>Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation</strong><br><button class=copy-to-clipboard title="Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, Out-of-distribution, Supervised Learning, Reasoning, Text2image, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05523v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05523v2.pdf filename=2403.05523v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) generalization is a favorable yet challenging property for deep neural networks. The core challenges lie in the limited availability of source domains that help models learn an invariant representation from the spurious features. Various domain augmentation have been proposed but largely rely on interpolating existing domains and frequently face difficulties in creating truly &ldquo;novel&rdquo; domains. Humans, on the other hand, can easily extrapolate novel domains, thus, an intriguing question arises: How can neural networks extrapolate like humans and achieve OOD generalization? We introduce a novel approach to domain extrapolation that leverages <b>reasoning</b> ability and the extensive knowledge encapsulated within <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to synthesize entirely new domains. Starting with the class of interest, we query the <b>LLMs</b> to extract relevant knowledge for these novel domains. We then bridge the gap between the text-centric knowledge derived from <b>LLMs</b> and the pixel input space of the model using <b>text-to-image</b> generation techniques. By augmenting the training set of domain generalization datasets with high-fidelity, photo-realistic images of these new domains, we achieve significant improvements over all existing methods, as demonstrated in both single and multi-domain generalization across various <b>benchmarks.</b> With the ability to extrapolate any domains for any class, our method has the potential to learn a generalized model for any task without any data. To illustrate, we put forth a much more difficult setting termed, data-free domain generalization, that aims to learn a generalized model in the absence of any collected data. Our empirical findings support the above argument and our methods exhibit commendable performance in this setting, even surpassing the <b>supervised</b> setting by approximately 1-2% on datasets such as VLCS.</p></p class="citation"></blockquote><h3 id=377--112270-sirst-5k-exploring-massive-negatives-synthesis-with-self-supervised-learning-for-robust-infrared-small-target-detection-yahao-lu-et-al-2024>(3/77 | 112/270) SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised Learning for Robust Infrared Small Target Detection (Yahao Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yahao Lu, Yupei Lin, Han Wu, Xiaoyu Xian, Yukai Shi, Liang Lin. (2024)<br><strong>SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised Learning for Robust Infrared Small Target Detection</strong><br><button class=copy-to-clipboard title="SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised Learning for Robust Infrared Small Target Detection" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Object Detection, Convolution, Convolutional Neural Network, Self-supervised Learning, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05416v1.pdf filename=2403.05416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single-frame infrared small target (SIRST) detection aims to recognize small targets from clutter backgrounds. Recently, <b>convolutional</b> <b>neural</b> <b>networks</b> have achieved significant advantages in general <b>object</b> <b>detection.</b> With the development of <b>Transformer,</b> the scale of SIRST models is constantly increasing. Due to the limited training samples, performance has not been improved accordingly. The quality, quantity, and diversity of the infrared dataset are critical to the detection of small targets. To highlight this issue, we propose a negative sample augmentation method in this paper. Specifically, a negative augmentation approach is proposed to generate massive negatives for <b>self-supervised</b> <b>learning.</b> Firstly, we perform a sequential noise modeling technology to generate realistic infrared data. Secondly, we fuse the extracted noise with the original data to facilitate diversity and fidelity in the generated data. Lastly, we proposed a negative augmentation strategy to enrich diversity as well as maintain semantic invariance. The proposed algorithm produces a synthetic SIRST-5K dataset, which contains massive pseudo-data and corresponding labels. With a rich diversity of infrared small target data, our algorithm significantly improves the model performance and convergence speed. Compared with other state-of-the-art (SOTA) methods, our method achieves outstanding performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection over union (IoU).</p></p class="citation"></blockquote><h3 id=477--113270-exploring-robust-features-for-few-shot-object-detection-in-satellite-imagery-xavier-bou-et-al-2024>(4/77 | 113/270) Exploring Robust Features for Few-Shot Object Detection in Satellite Imagery (Xavier Bou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xavier Bou, Gabriele Facciolo, Rafael Grompone von Gioi, Jean-Michel Morel, Thibaud Ehret. (2024)<br><strong>Exploring Robust Features for Few-Shot Object Detection in Satellite Imagery</strong><br><button class=copy-to-clipboard title="Exploring Robust Features for Few-Shot Object Detection in Satellite Imagery" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Object Detection, Few-shot, Fine-tuning, Supervised Learning, Image2text, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05381v1.pdf filename=2403.05381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The goal of this paper is to perform <b>object</b> <b>detection</b> in satellite imagery with only a few examples, thus enabling users to specify any <b>object</b> <b>class</b> with minimal annotation. To this end, we explore recent methods and ideas from open-vocabulary detection for the remote sensing domain. We develop a <b>few-shot</b> <b>object</b> <b>detector</b> based on a traditional two-stage architecture, where the classification block is replaced by a prototype-based classifier. A large-scale pre-trained model is used to build class-reference embeddings or prototypes, which are compared to region proposal contents for label prediction. In addition, we propose to <b>fine-tune</b> prototypes on available training images to boost performance and learn differences between similar classes, such as aircraft types. We perform extensive evaluations on two remote sensing datasets containing challenging and rare <b>objects.</b> <b>Moreover,</b> we study the performance of both visual and <b>image-text</b> features, namely DINOv2 and CLIP, including two CLIP models specifically tailored for remote sensing applications. Results indicate that visual features are largely superior to <b>vision-language</b> models, as the latter lack the necessary domain-specific vocabulary. Lastly, the developed detector outperforms fully <b>supervised</b> and <b>few-shot</b> methods evaluated on the SIMD and DIOR datasets, despite minimal training parameters.</p></p class="citation"></blockquote><h3 id=577--114270-debiasing-large-visual-language-models-yi-fan-zhang-et-al-2024>(5/77 | 114/270) Debiasing Large Visual Language Models (Yi-Fan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-Fan Zhang, Weichen Yu, Qingsong Wen, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan. (2024)<br><strong>Debiasing Large Visual Language Models</strong><br><button class=copy-to-clipboard title="Debiasing Large Visual Language Models" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Fairness, Question Answering, Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05262v1.pdf filename=2403.05262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realms of computer vision and natural language processing, <b>Large</b> <b>Vision-Language</b> <b>Models</b> (LVLMs) have become indispensable tools, proficient in generating textual descriptions based on visual inputs. Despite their advancements, our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> prior rather than the input image. Our empirical experiments underscore the persistence of this bias, as LVLMs often provide confident answers even in the absence of relevant images or given incongruent visual input. To rectify these biases and redirect the model&rsquo;s focus toward vision information, we introduce two simple, training-free strategies. Firstly, for tasks such as classification or multi-choice <b>question-answering</b> <b>(QA),</b> we propose a <code>calibration'' step through affine transformation to adjust the output distribution. This </code>Post-Hoc debias&rsquo;&rsquo; approach ensures uniform scores for each answer when the image is absent, serving as an effective regularization technique to alleviate the influence of <b>LLM</b> priors. For more intricate open-ended generation tasks, we extend this method to ``Debias sampling&rsquo;&rsquo;, drawing inspirations from contrastive decoding methods. Furthermore, our investigation sheds light on the instability of LVLMs across various decoding configurations. Through systematic exploration of different settings, we significantly enhance performance, surpassing reported results and raising concerns about the <b>fairness</b> of existing evaluations. Comprehensive experiments substantiate the effectiveness of our proposed strategies in mitigating biases. These strategies not only prove beneficial in minimizing hallucinations but also contribute to the generation of more helpful and precise illustrations.</p></p class="citation"></blockquote><h3 id=677--115270-ella-equip-diffusion-models-with-llm-for-enhanced-semantic-alignment-xiwei-hu-et-al-2024>(6/77 | 115/270) ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment (Xiwei Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Gang Yu. (2024)<br><strong>ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment</strong><br><button class=copy-to-clipboard title="ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Diffusion Model, Graph, Benchmarking, Text2image, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05135v1.pdf filename=2403.05135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have demonstrated remarkable performance in the domain of <b>text-to-image</b> generation. However, most widely used models still employ CLIP as their text encoder, which constrains their ability to comprehend dense <b>prompts,</b> encompassing multiple objects, detailed attributes, complex relationships, long-text alignment, etc. In this paper, we introduce an Efficient <b>Large</b> <b>Language</b> <b>Model</b> Adapter, termed ELLA, which equips <b>text-to-image</b> <b>diffusion</b> <b>models</b> with powerful <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> to enhance text alignment without training of either U-Net or <b>LLM.</b> To seamlessly bridge two pre-trained models, we investigate a range of semantic alignment connector designs and propose a novel module, the Timestep-Aware Semantic Connector (TSC), which dynamically extracts timestep-dependent conditions from <b>LLM.</b> Our approach adapts semantic features at different stages of the denoising process, assisting <b>diffusion</b> <b>models</b> in interpreting lengthy and intricate <b>prompts</b> over sampling timesteps. Additionally, ELLA can be readily incorporated with community models and tools to improve their <b>prompt-following</b> capabilities. To assess <b>text-to-image</b> models in dense <b>prompt</b> following, we introduce Dense <b>Prompt</b> <b>Graph</b> <b>Benchmark</b> (DPG-Bench), a challenging <b>benchmark</b> consisting of 1K dense <b>prompts.</b> Extensive experiments demonstrate the superiority of ELLA in dense <b>prompt</b> following compared to state-of-the-art methods, particularly in multiple object compositions involving diverse attributes and relationships.</p></p class="citation"></blockquote><h3 id=777--116270-federated-learning-method-for-preserving-privacy-in-face-recognition-system-enoch-solomon-et-al-2024>(7/77 | 116/270) Federated Learning Method for Preserving Privacy in Face Recognition System (Enoch Solomon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enoch Solomon, Abraham Woubie. (2024)<br><strong>Federated Learning Method for Preserving Privacy in Face Recognition System</strong><br><button class=copy-to-clipboard title="Federated Learning Method for Preserving Privacy in Face Recognition System" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Face Recognition, Federated Learning, Generative Adversarial Network, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05344v1.pdf filename=2403.05344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The state-of-the-art <b>face</b> <b>recognition</b> systems are typically trained on a single computer, utilizing extensive image datasets collected from various number of users. However, these datasets often contain sensitive personal information that users may hesitate to disclose. To address potential privacy concerns, we explore the application of <b>federated</b> <b>learning,</b> both with and without secure aggregators, in the context of both <b>supervised</b> and <b>unsupervised</b> <b>face</b> <b>recognition</b> systems. <b>Federated</b> <b>learning</b> facilitates the training of a shared model without necessitating the sharing of individual private data, achieving this by training models on decentralized edge devices housing the data. In our proposed system, each edge device independently trains its own model, which is subsequently transmitted either to a secure aggregator or directly to the central server. To introduce diverse data without the need for data transmission, we employ <b>generative</b> <b>adversarial</b> <b>networks</b> to generate imposter data at the edge. Following this, the secure aggregator or central server combines these individual models to construct a global model, which is then relayed back to the edge devices. Experimental findings based on the CelebA datasets reveal that employing <b>federated</b> <b>learning</b> in both <b>supervised</b> and <b>unsupervised</b> <b>face</b> <b>recognition</b> systems offers dual benefits. Firstly, it safeguards privacy since the original data remains on the edge devices. Secondly, the experimental results demonstrate that the aggregated model yields nearly identical performance compared to the individual models, particularly when the <b>federated</b> <b>model</b> does not utilize a secure aggregator. Hence, our results shed light on the practical challenges associated with privacy-preserving <b>face</b> <b>image</b> training, particularly in terms of the balance between privacy and accuracy.</p></p class="citation"></blockquote><h3 id=877--117270-fine-tuning-a-multiple-instance-learning-feature-extractor-with-masked-context-modelling-and-knowledge-distillation-juan-i-pisula-et-al-2024>(8/77 | 117/270) Fine-tuning a Multiple Instance Learning Feature Extractor with Masked Context Modelling and Knowledge Distillation (Juan I. Pisula et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan I. Pisula, Katarzyna Bozek. (2024)<br><strong>Fine-tuning a Multiple Instance Learning Feature Extractor with Masked Context Modelling and Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Fine-tuning a Multiple Instance Learning Feature Extractor with Masked Context Modelling and Knowledge Distillation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05325v1.pdf filename=2403.05325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The first step in <b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) algorithms for Whole Slide Image (WSI) classification consists of tiling the input image into smaller patches and computing their feature vectors produced by a pre-trained feature extractor model. Feature extractor models that were pre-trained with supervision on ImageNet have proven to transfer well to this domain, however, this pre-training task does not take into account that visual information in neighboring patches is highly correlated. Based on this observation, we propose to increase downstream MIL classification by <b>fine-tuning</b> the feature extractor model using \textit{Masked Context Modelling with <b>Knowledge</b> <b>Distillation}.</b> In this task, the feature extractor model is <b>fine-tuned</b> by predicting masked patches in a bigger context window. Since reconstructing the input image would require a powerful image generation model, and our goal is not to generate realistically looking image patches, we predict instead the feature vectors produced by a larger teacher network. A single epoch of the proposed task suffices to increase the downstream performance of the feature-extractor model when used in a MIL scenario, even capable of outperforming the downstream performance of the teacher model, while being considerably smaller and requiring a fraction of its compute.</p></p class="citation"></blockquote><h3 id=977--118270-radardistill-boosting-radar-based-object-detection-performance-via-knowledge-distillation-from-lidar-features-geonho-bang-et-al-2024>(9/77 | 118/270) RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features (Geonho Bang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geonho Bang, Kwangjin Choi, Jisong Kim, Dongsuk Kum, Jun Won Choi. (2024)<br><strong>RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features</strong><br><button class=copy-to-clipboard title="RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Object Detection, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05061v1.pdf filename=2403.05061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The inherent noisy and sparse characteristics of radar data pose challenges in finding effective representations for 3D <b>object</b> <b>detection.</b> In this paper, we propose RadarDistill, a novel <b>knowledge</b> <b>distillation</b> <b>(KD)</b> method, which can improve the representation of radar data by leveraging LiDAR data. RadarDistill successfully transfers desirable characteristics of LiDAR features into radar features using three key components: Cross-Modality Alignment (CMA), Activation-based Feature <b>Distillation</b> (AFD), and Proposal-based Feature <b>Distillation</b> (PFD). CMA enhances the density of radar features through multiple layers of dilation operations, effectively addressing the challenges of inefficient <b>knowledge</b> <b>transfer</b> from LiDAR to radar. AFD is designed to transfer <b>knowledge</b> <b>from</b> significant areas of the LiDAR features, specifically those regions where activation intensity exceeds a predetermined threshold. PFD guides the radar network to mimic LiDAR network features in the <b>object</b> <b>proposals</b> for accurately detected results while moderating features for misdetected proposals like false positives. Our comparative analyses conducted on the nuScenes datasets demonstrate that RadarDistill achieves state-of-the-art (SOTA) performance for radar-only <b>object</b> <b>detection</b> task, recording 20.5% in mAP and 43.7% in NDS. Also, RadarDistill significantly improves the performance of the camera-radar fusion model.</p></p class="citation"></blockquote><h3 id=1077--119270-histgen-histopathology-report-generation-via-local-global-feature-encoding-and-cross-modal-context-interaction-zhengrui-guo-et-al-2024>(10/77 | 119/270) HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction (Zhengrui Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengrui Guo, Jiabo Ma, Yingxue Xu, Yihui Wang, Liansheng Wang, Hao Chen. (2024)<br><strong>HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction</strong><br><button class=copy-to-clipboard title="HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Multiple Instance Learning, Transfer Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05396v1.pdf filename=2403.05396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a <b>multiple</b> <b>instance</b> <b>learning-empowered</b> framework for histopathology report generation together with the first <b>benchmark</b> dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a region-to-slide perspective. Meanwhile, a cross-modal context module is proposed to explicitly facilitate alignment and interaction between distinct modalities, effectively bridging the gap between the extensive visual sequences of WSIs and corresponding highly <b>summarized</b> reports. Experimental results on WSI report generation show the proposed model outperforms state-of-the-art (SOTA) models by a large margin. Moreover, the results of <b>fine-tuning</b> our model on cancer subtyping and survival analysis tasks further demonstrate superior performance compared to SOTA methods, showcasing strong <b>transfer</b> <b>learning</b> capability. Dataset, model weights, and source code are available in <a href=https://github.com/dddavid4real/HistGen>https://github.com/dddavid4real/HistGen</a>.</p></p class="citation"></blockquote><h3 id=1177--120270-jointmotion-joint-self-supervision-for-joint-motion-prediction-royden-wagner-et-al-2024>(11/77 | 120/270) JointMotion: Joint Self-supervision for Joint Motion Prediction (Royden Wagner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Royden Wagner, Ömer Şahin Taş, Marvin Klemp, Carlos Fernandez. (2024)<br><strong>JointMotion: Joint Self-supervision for Joint Motion Prediction</strong><br><button class=copy-to-clipboard title="JointMotion: Joint Self-supervision for Joint Motion Prediction" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Self-supervised Learning, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05489v1.pdf filename=2403.05489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present JointMotion, a <b>self-supervised</b> <b>learning</b> method for joint motion prediction in autonomous driving. Our method includes a scene-level objective connecting motion and environments, and an instance-level objective to refine learned representations. Our evaluations show that these objectives are complementary and outperform recent contrastive and autoencoding methods as pre-training for joint motion prediction. Furthermore, JointMotion adapts to all common types of environment representations used for motion prediction (i.e., agent-centric, scene-centric, and pairwise relative), and enables effective <b>transfer</b> <b>learning</b> between the Waymo Open Motion and the Argoverse 2 Forecasting datasets. Notably, our method improves the joint final displacement error of Wayformer, Scene <b>Transformer,</b> and HPTR by 3%, 7%, and 11%, respectively.</p></p class="citation"></blockquote><h3 id=1277--121270-rethinking-transformers-pre-training-for-multi-spectral-satellite-imagery-mubashir-noman-et-al-2024>(12/77 | 121/270) Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery (Mubashir Noman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mubashir Noman, Muzammal Naseer, Hisham Cholakkal, Rao Muhammad Anwar, Salman Khan, Fahad Shahbaz Khan. (2024)<br><strong>Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery</strong><br><button class=copy-to-clipboard title="Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Unsupervised Learning, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05419v1.pdf filename=2403.05419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>unsupervised</b> <b>learning</b> have demonstrated the ability of large vision models to achieve promising results on downstream tasks by pre-training on large amount of unlabelled data. Such pre-training techniques have also been explored recently in the remote sensing domain due to the availability of large amount of unlabelled data. Different from standard natural image datasets, remote sensing data is acquired from various sensor technologies and exhibit diverse range of scale variations as well as modalities. Existing satellite image pre-training methods either ignore the scale information present in the remote sensing imagery or restrict themselves to use only a single type of data modality. In this paper, we re-visit <b>transformers</b> pre-training and leverage multi-scale information that is effectively utilized with multiple modalities. Our proposed approach, named SatMAE++, performs multi-scale pre-training and utilizes <b>convolution</b> based upsampling blocks to reconstruct the image at higher scales making it extensible to include more scales. Compared to existing works, the proposed SatMAE++ with multi-scale pre-training is equally effective for both optical as well as multi-spectral imagery. Extensive experiments on six datasets reveal the merits of proposed contributions, leading to state-of-the-art performance on all datasets. SatMAE++ achieves mean average precision (mAP) gain of 2.5% for multi-label classification task on BigEarthNet dataset. Our code and pre-trained models are available at \url{https://github.com/techmn/satmae_pp}.</p></p class="citation"></blockquote><h3 id=1377--122270-self-supervised-multiple-instance-learning-for-acute-myeloid-leukemia-classification-salome-kazeminia-et-al-2024>(13/77 | 122/270) Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification (Salome Kazeminia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salome Kazeminia, Max Joosten, Dragan Bosnacki, Carsten Marr. (2024)<br><strong>Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification</strong><br><button class=copy-to-clipboard title="Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Multiple Instance Learning, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05379v1.pdf filename=2403.05379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated disease diagnosis using medical image analysis relies on deep learning, often requiring large labeled datasets for <b>supervised</b> model training. Diseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and costly annotations on a single-cell level. <b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) addresses weakly labeled scenarios but necessitates powerful encoders typically trained with labeled data. In this study, we explore <b>Self-Supervised</b> <b>Learning</b> (SSL) as a pre-training approach for MIL-based AML subtype classification from blood smears, removing the need for labeled data during encoder training. We investigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and compare their performance against <b>supervised</b> pre-training. Our findings show that SSL-pretrained encoders achieve comparable performance, showcasing the potential of SSL in MIL. This breakthrough offers a cost-effective and data-efficient solution, propelling the field of AI-based disease diagnosis.</p></p class="citation"></blockquote><h3 id=1477--123270-peeb-part-based-image-classifiers-with-an-explainable-and-editable-language-bottleneck-thang-m-pham-et-al-2024>(14/77 | 123/270) PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck (Thang M. Pham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thang M. Pham, Peijie Chen, Tin Nguyen, Seunghyun Yoon, Trung Bui, Anh Nguyen. (2024)<br><strong>PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck</strong><br><button class=copy-to-clipboard title="PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Supervised Learning, Supervised Learning, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05297v1.pdf filename=2403.05297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>CLIP-based classifiers rely on the <b>prompt</b> containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a <b>zero-shot</b> setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the <b>supervised-learning</b> <b>setting</b> (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compared to concept bottleneck models, PEEB is also the state-of-the-art in both <b>zero-shot</b> and <b>supervised</b> <b>learning</b> settings.</p></p class="citation"></blockquote><h3 id=1577--124270-towards-effective-usage-of-human-centric-priors-in-diffusion-models-for-text-based-human-image-generation-junyan-wang-et-al-2024>(15/77 | 124/270) Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation (Junyan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyan Wang, Zhenhong Sun, Zhiyu Tan, Xuanbai Chen, Weihua Chen, Hao Li, Cheng Zhang, Yang Song. (2024)<br><strong>Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation</strong><br><button class=copy-to-clipboard title="Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05239v1.pdf filename=2403.05239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vanilla <b>text-to-image</b> <b>diffusion</b> <b>models</b> struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by <b>fine-tuning</b> the model with extra images or adding additional controls &ndash; human-centric priors such as pose or depth maps &ndash; during the image generation phase. This paper explores the integration of these human-centric priors directly into the model <b>fine-tuning</b> stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual <b>prompts</b> within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during <b>fine-tuning,</b> we introduce scale-aware and step-wise constraints within the <b>diffusion</b> <b>process,</b> according to an in-depth analysis of the cross-attention layer. Extensive experiments show that our method largely improves over state-of-the-art <b>text-to-image</b> models to synthesize high-quality human images based on user-written <b>prompts.</b> Project page: \url{https://hcplayercvpr2024.github.io}.</p></p class="citation"></blockquote><h3 id=1677--125270-improving-diffusion-models-for-virtual-try-on-yisol-choi-et-al-2024>(16/77 | 125/270) Improving Diffusion Models for Virtual Try-on (Yisol Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, Jinwoo Shin. (2024)<br><strong>Improving Diffusion Models for Virtual Try-on</strong><br><button class=copy-to-clipboard title="Improving Diffusion Models for Virtual Try-on" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Generative Adversarial Network, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05139v1.pdf filename=2403.05139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers image-based virtual try-on, which renders an image of a person wearing a curated garment, given a pair of images depicting the person and the garment, respectively. Previous works adapt existing exemplar-based inpainting <b>diffusion</b> <b>models</b> for virtual try-on to improve the naturalness of the generated visuals compared to other methods (e.g., <b>GAN-based),</b> but they fail to preserve the identity of the garments. To overcome this limitation, we propose a novel <b>diffusion</b> <b>model</b> that improves garment fidelity and generates authentic virtual try-on images. Our method, coined IDM-VTON, uses two different modules to encode the semantics of garment image; given the base UNet of the <b>diffusion</b> <b>model,</b> 1) the high-level semantics extracted from a visual encoder are fused to the cross-attention layer, and then 2) the low-level features extracted from parallel UNet are fused to the <b>self-attention</b> layer. In addition, we provide detailed textual <b>prompts</b> for both garment and person images to enhance the authenticity of the generated visuals. Finally, we present a customization method using a pair of person-garment images, which significantly improves fidelity and authenticity. Our experimental results show that our method outperforms previous approaches (both <b>diffusion-based</b> <b>and</b> <b>GAN-based)</b> in preserving garment details and generating authentic virtual try-on images, both qualitatively and quantitatively. Furthermore, the proposed customization method demonstrates its effectiveness in a real-world scenario.</p></p class="citation"></blockquote><h3 id=1777--126270-spectrum-translation-for-refinement-of-image-generation-stig-based-on-contrastive-learning-and-spectral-filter-profile-seokjun-lee-et-al-2024>(17/77 | 126/270) Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile (Seokjun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seokjun Lee, Seung-Won Jung, Hyunseok Seo. (2024)<br><strong>Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile</strong><br><button class=copy-to-clipboard title="Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Contrastive Learning, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05093v1.pdf filename=2403.05093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, image generation and synthesis have remarkably progressed with <b>generative</b> <b>models.</b> <b>Despite</b> photo-realistic results, intrinsic discrepancies are still observed in the frequency domain. The spectral discrepancy appeared not only in <b>generative</b> <b>adversarial</b> <b>networks</b> but in <b>diffusion</b> <b>models.</b> In this study, we propose a framework to effectively mitigate the disparity in frequency domain of the generated images to improve <b>generative</b> <b>performance</b> <b>of</b> both <b>GAN</b> and <b>diffusion</b> <b>models.</b> This is realized by spectrum translation for the refinement of image generation (STIG) based on <b>contrastive</b> <b>learning.</b> We adopt theoretical logic of frequency components in various <b>generative</b> <b>networks.</b> <b>The</b> key idea, here, is to refine the spectrum of the generated image via the concept of image-to-image translation and <b>contrastive</b> <b>learning</b> in terms of digital signal processing. We evaluate our framework across eight fake image datasets and various cutting-edge models to demonstrate the effectiveness of STIG. Our framework outperforms other cutting-edges showing significant decreases in FID and log frequency distance of spectrum. We further emphasize that STIG improves image quality by decreasing the spectral anomaly. Additionally, validation results present that the frequency-based deepfake detector confuses more in the case where fake spectrums are manipulated by STIG.</p></p class="citation"></blockquote><h3 id=1877--127270-instructgie-towards-generalizable-image-editing-zichong-meng-et-al-2024>(18/77 | 127/270) InstructGIE: Towards Generalizable Image Editing (Zichong Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zichong Meng, Changdi Yang, Jun Liu, Hao Tang, Pu Zhao, Yanzhi Wang. (2024)<br><strong>InstructGIE: Towards Generalizable Image Editing</strong><br><button class=copy-to-clipboard title="InstructGIE: Towards Generalizable Image Editing" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05018v1.pdf filename=2403.05018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in image editing have been driven by the development of denoising <b>diffusion</b> <b>models,</b> marking a significant leap forward in this field. Despite these advances, the generalization capabilities of recent image editing approaches remain constrained. In response to this challenge, our study introduces a novel image editing framework with enhanced generalization robustness by boosting <b>in-context</b> <b>learning</b> capability and unifying language instruction. This framework incorporates a module specifically optimized for image editing tasks, leveraging the VMamba Block and an editing-shift matching strategy to augment <b>in-context</b> <b>learning.</b> Furthermore, we unveil a selective area-matching technique specifically engineered to address and rectify corrupted details in generated images, such as human facial features, to further improve the quality. Another key innovation of our approach is the integration of a language unification technique, which aligns language embeddings with editing semantics to elevate the quality of image editing. Moreover, we compile the first dataset for image editing with visual <b>prompts</b> and editing instructions that could be used to enhance <b>in-context</b> <b>capability.</b> Trained on this dataset, our methodology not only achieves superior synthesis quality for trained tasks, but also demonstrates robust generalization capability across unseen vision tasks through tailored <b>prompts.</b></p></p class="citation"></blockquote><h3 id=1977--128270-xpsr-cross-modal-priors-for-diffusion-based-image-super-resolution-yunpeng-qu-et-al-2024>(19/77 | 128/270) XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution (Yunpeng Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunpeng Qu, Kun Yuan, Kai Zhao, Qizhi Xie, Jinhua Hao, Ming Sun, Chao Zhou. (2024)<br><strong>XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution</strong><br><button class=copy-to-clipboard title="XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Diffusion Model, Knowledge Distillation, Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05049v1.pdf filename=2403.05049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion-based</b> <b>methods,</b> endowed with a formidable generative prior, have received increasing attention in Image Super-Resolution (ISR) recently. However, as low-resolution (LR) images often undergo severe degradation, it is challenging for ISR models to perceive the semantic and degradation information, resulting in restoration images with incorrect content or unrealistic artifacts. To address these issues, we propose a \textit{Cross-modal Priors for Super-Resolution (XPSR)} framework. Within XPSR, to acquire precise and comprehensive semantic conditions for the <b>diffusion</b> <b>model,</b> cutting-edge <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) are utilized. To facilitate better fusion of cross-modal priors, a \textit{Semantic-Fusion Attention} is raised. To <b>distill</b> semantic-preserved information instead of undesired degradations, a \textit{Degradation-Free Constraint} is attached between LR and its high-resolution (HR) counterpart. Quantitative and qualitative results show that XPSR is capable of generating high-fidelity and high-realism images across synthetic and real-world datasets. Codes will be released at \url{https://github.com/qyp2000/XPSR}.</p></p class="citation"></blockquote><h3 id=2077--129270-med3dinsight-enhancing-3d-medical-image-understanding-with-2d-multi-modal-large-language-models-qiuhui-chen-et-al-2024>(20/77 | 129/270) Med3DInsight: Enhancing 3D Medical Image Understanding with 2D Multi-Modal Large Language Models (Qiuhui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiuhui Chen, Huping Ye, Yi Hong. (2024)<br><strong>Med3DInsight: Enhancing 3D Medical Image Understanding with 2D Multi-Modal Large Language Models</strong><br><button class=copy-to-clipboard title="Med3DInsight: Enhancing 3D Medical Image Understanding with 2D Multi-Modal Large Language Models" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Convolution, Multi-modal, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05141v1.pdf filename=2403.05141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding 3D medical image volumes is a critical task in the medical domain. However, existing 3D <b>convolution</b> and <b>transformer-based</b> methods have limited semantic understanding of an image volume and also need a <b>large</b> <b>set</b> <b>of</b> volumes for training. Recent advances in <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) provide a new and promising way to understand images with the help of text descriptions. However, most current MLLMs are designed for 2D natural images. To enhance the 3D medical image understanding with 2D MLLMs, we propose a novel pre-training framework called Med3DInsight, which marries existing 3D image encoders with 2D MLLMs and bridges them via a designed Plane-Slice-Aware <b>Transformer</b> (PSAT) module. Extensive experiments demonstrate our SOTA performance on two downstream segmentation and classification tasks, including three public datasets with CT and MRI modalities and comparison to more than ten baselines. Med3DInsight can be easily integrated into any current 3D medical image understanding network and improves its performance by a good margin.</p></p class="citation"></blockquote><h3 id=2177--130270-attention-guided-feature-distillation-for-semantic-segmentation-amir-m-mansourian-et-al-2024>(21/77 | 130/270) Attention-guided Feature Distillation for Semantic Segmentation (Amir M. Mansourian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir M. Mansourian, Arya Jalali, Rozhan Ahmadi, Shohreh Kasaei. (2024)<br><strong>Attention-guided Feature Distillation for Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Attention-guided Feature Distillation for Semantic Segmentation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05451v1.pdf filename=2403.05451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In contrast to existing complex methodologies commonly employed for <b>distilling</b> knowledge from a teacher to a student, the pro-posed method showcases the efficacy of a simple yet powerful method for utilizing refined feature maps to transfer attention. The proposed method has proven to be effective in <b>distilling</b> rich information, outperforming existing methods in semantic segmentation as a dense prediction task. The proposed Attention-guided Feature <b>Distillation</b> (AttnFD) method, em-ploys the <b>Convolutional</b> Block Attention Module (CBAM), which refines feature maps by taking into account both channel-specific and spatial information content. By only using the Mean Squared Error (MSE) loss function between the refined feature maps of the teacher and the student,AttnFD demonstrates outstanding performance in semantic segmentation, achieving state-of-the-art results in terms of mean Intersection over Union (mIoU) on the PascalVoc 2012 and Cityscapes datasets. The Code is available at <a href=https://github.com/AmirMansurian/AttnFD>https://github.com/AmirMansurian/AttnFD</a>.</p></p class="citation"></blockquote><h3 id=2277--131270-videoelevator-elevating-video-generation-quality-with-versatile-text-to-image-diffusion-models-yabo-zhang-et-al-2024>(22/77 | 131/270) VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models (Yabo Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yabo Zhang, Yuxiang Wei, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji, Wangmeng Zuo. (2024)<br><strong>VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05438v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05438v1.pdf filename=2403.05438v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> (T2I) have demonstrated unprecedented capabilities in creating realistic and aesthetic images. On the contrary, text-to-video <b>diffusion</b> <b>models</b> (T2V) still lag far behind in frame quality and text alignment, owing to insufficient quality and quantity of training videos. In this paper, we introduce VideoElevator, a training-free and plug-and-play method, which elevates the performance of T2V using superior capabilities of T2I. Different from conventional T2V sampling (i.e., temporal and spatial modeling), VideoElevator explicitly decomposes each sampling step into temporal motion refining and spatial quality elevating. Specifically, temporal motion refining uses encapsulated T2V to enhance temporal consistency, followed by inverting to the noise distribution required by T2I. Then, spatial quality elevating harnesses inflated T2I to directly predict less noisy latent, adding more photo-realistic details. We have conducted experiments in extensive <b>prompts</b> under the combination of various T2V and T2I. The results show that VideoElevator not only improves the performance of T2V baselines with foundational T2I, but also facilitates stylistic video synthesis with personalized T2I. Our code is available at <a href=https://github.com/YBYBZhang/VideoElevator>https://github.com/YBYBZhang/VideoElevator</a>.</p></p class="citation"></blockquote><h3 id=2377--132270-vlm-pl-advanced-pseudo-labeling-approach-class-incremental-object-detection-with-vision-language-model-junsu-kim-et-al-2024>(23/77 | 132/270) VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model (Junsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junsu Kim, Yunhoe Ku, Jihyeon Kim, Junuk Cha, Seungryul Baek. (2024)<br><strong>VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model</strong><br><button class=copy-to-clipboard title="VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05346v1.pdf filename=2403.05346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of Class Incremental <b>Object</b> <b>Detection</b> (CIOD), creating models that can continuously learn like humans is a major challenge. Pseudo-labeling methods, although initially powerful, struggle with multi-scenario incremental learning due to their tendency to forget past knowledge. To overcome this, we introduce a new approach called <b>Vision-Language</b> Model assisted Pseudo-Labeling (VLM-PL). This technique uses <b>Vision-Language</b> Model (VLM) to verify the correctness of pseudo ground-truths (GTs) without requiring additional model training. VLM-PL starts by deriving pseudo GTs from a pre-trained detector. Then, we generate custom queries for each pseudo GT using carefully designed <b>prompt</b> templates that combine image and text features. This allows the VLM to classify the correctness through its responses. Furthermore, VLM-PL integrates refined pseudo and real GTs from upcoming training, effectively combining new and old knowledge. Extensive experiments conducted on the Pascal VOC and MS COCO datasets not only highlight VLM-PL&rsquo;s exceptional performance in multi-scenario but also illuminate its effectiveness in dual-scenario by achieving state-of-the-art results in both.</p></p class="citation"></blockquote><h3 id=2477--133270-mammil-multiple-instance-learning-for-whole-slide-images-with-state-space-models-zijie-fang-et-al-2024>(24/77 | 133/270) MamMIL: Multiple Instance Learning for Whole Slide Images with State Space Models (Zijie Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijie Fang, Yifeng Wang, Zhi Wang, Jian Zhang, Xiangyang Ji, Yongbing Zhang. (2024)<br><strong>MamMIL: Multiple Instance Learning for Whole Slide Images with State Space Models</strong><br><button class=copy-to-clipboard title="MamMIL: Multiple Instance Learning for Whole Slide Images with State Space Models" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Multiple Instance Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05160v1.pdf filename=2403.05160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, pathological diagnosis, the gold standard for cancer diagnosis, has achieved superior performance by combining the <b>Transformer</b> with the <b>multiple</b> <b>instance</b> <b>learning</b> (MIL) framework using whole slide images (WSIs). However, the giga-pixel nature of WSIs poses a great challenge for the quadratic-complexity <b>self-attention</b> mechanism in <b>Transformer</b> to be applied in MIL. Existing studies usually use linear attention to improve computing efficiency but inevitably bring performance bottlenecks. To tackle this challenge, we propose a MamMIL framework for WSI classification by cooperating the selective structured state space model (i.e., Mamba) with MIL for the first time, enabling the modeling of instance dependencies while maintaining linear complexity. Specifically, to solve the problem that Mamba can only conduct unidirectional one-dimensional (1D) sequence modeling, we innovatively introduce a bidirectional state space model and a 2D context-aware block to enable MamMIL to learn the bidirectional instance dependencies with 2D spatial relationships. Experiments on two datasets show that MamMIL can achieve advanced classification performance with smaller memory footprints than the state-of-the-art MIL frameworks based on the <b>Transformer.</b> The code will be open-sourced if accepted.</p></p class="citation"></blockquote><h3 id=2577--134270-clip-gaze-towards-general-gaze-estimation-via-visual-linguistic-model-pengwei-yin-et-al-2024>(25/77 | 134/270) CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model (Pengwei Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengwei Yin, Guanzhong Zeng, Jingjing Wang, Di Xie. (2024)<br><strong>CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model</strong><br><button class=copy-to-clipboard title="CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Prompt, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05124v1.pdf filename=2403.05124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaze estimation methods often experience significant performance degradation when evaluated across different domains, due to the domain gap between the testing and training data. Existing methods try to address this issue using various domain generalization approaches, but with little success because of the limited diversity of gaze datasets, such as appearance, wearable, and image quality. To overcome these limitations, we propose a novel framework called CLIP-Gaze that utilizes a pre-trained <b>vision-language</b> model to leverage its transferable knowledge. Our framework is the first to leverage the <b>vision-and-language</b> cross-modality approach for gaze estimation task. Specifically, we extract gaze-relevant feature by pushing it away from gaze-irrelevant features which can be flexibly constructed via language descriptions. To learn more suitable <b>prompts,</b> we propose a personalized context optimization method for text <b>prompt</b> tuning. Furthermore, we utilize the relationship among gaze samples to refine the distribution of gaze-relevant features, thereby improving the generalization capability of the gaze estimation model. Extensive experiments demonstrate the excellent performance of CLIP-Gaze over existing methods on four cross-domain evaluations.</p></p class="citation"></blockquote><h3 id=2677--135270-cogview3-finer-and-faster-text-to-image-generation-via-relay-diffusion-wendi-zheng-et-al-2024>(26/77 | 135/270) CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion (Wendi Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, Jie Tang. (2024)<br><strong>CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion</strong><br><button class=copy-to-clipboard title="CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05121v1.pdf filename=2403.05121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>text-to-image</b> generative systems have been largely driven by <b>diffusion</b> <b>models.</b> However, single-stage <b>text-to-image</b> <b>diffusion</b> <b>models</b> still face challenges, in terms of computational efficiency and the refinement of image details. To tackle the issue, we propose CogView3, an innovative cascaded framework that enhances the performance of <b>text-to-image</b> <b>diffusion.</b> <b>CogView3</b> is the first model implementing relay <b>diffusion</b> <b>in</b> the realm of <b>text-to-image</b> generation, executing the task by first creating low-resolution images and subsequently applying relay-based super-resolution. This methodology not only results in competitive <b>text-to-image</b> outputs but also greatly reduces both training and inference costs. Our experimental results demonstrate that CogView3 outperforms SDXL, the current state-of-the-art open-source <b>text-to-image</b> <b>diffusion</b> <b>model,</b> by 77.0% in human evaluations, all while requiring only about 1/2 of the inference time. The <b>distilled</b> variant of CogView3 achieves comparable performance while only utilizing 1/10 of the inference time by SDXL.</p></p class="citation"></blockquote><h3 id=2777--136270-face2diffusion-for-fast-and-editable-face-personalization-kaede-shiohara-et-al-2024>(27/77 | 136/270) Face2Diffusion for Fast and Editable Face Personalization (Kaede Shiohara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaede Shiohara, Toshihiko Yamasaki. (2024)<br><strong>Face2Diffusion for Fast and Editable Face Personalization</strong><br><button class=copy-to-clipboard title="Face2Diffusion for Fast and Editable Face Personalization" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05094v1.pdf filename=2403.05094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Face personalization aims to insert specific faces, taken from images, into pretrained <b>text-to-image</b> <b>diffusion</b> <b>models.</b> However, it is still challenging for previous methods to preserve both the identity similarity and editability due to overfitting to training samples. In this paper, we propose Face2Diffusion (F2D) for high-editability face personalization. The core idea behind F2D is that removing identity-irrelevant information from the training pipeline prevents the overfitting problem and improves editability of encoded faces. F2D consists of the following three novel components: 1) Multi-scale identity encoder provides well-disentangled identity features while keeping the benefits of multi-scale information, which improves the diversity of camera poses. 2) Expression guidance disentangles face expressions from identities and improves the controllability of face expressions. 3) Class-guided denoising regularization encourages models to learn how faces should be denoised, which boosts the text-alignment of backgrounds. Extensive experiments on the FaceForensics++ dataset and diverse <b>prompts</b> demonstrate our method greatly improves the trade-off between the identity- and text-fidelity compared to previous state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2877--137270-promptiqa-boosting-the-performance-and-generalization-for-no-reference-image-quality-assessment-via-prompts-zewen-chen-et-al-2024>(28/77 | 137/270) PromptIQA: Boosting the Performance and Generalization for No-Reference Image Quality Assessment via Prompts (Zewen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zewen Chen, Haina Qin, Juan Wang, Chunfeng Yuan, Bing Li, Weiming Hu, Liang Wang. (2024)<br><strong>PromptIQA: Boosting the Performance and Generalization for No-Reference Image Quality Assessment via Prompts</strong><br><button class=copy-to-clipboard title="PromptIQA: Boosting the Performance and Generalization for No-Reference Image Quality Assessment via Prompts" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04993v1.pdf filename=2403.04993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the diversity of assessment requirements in various application scenarios for the IQA task, existing IQA methods struggle to directly adapt to these varied requirements after training. Thus, when facing new requirements, a typical approach is <b>fine-tuning</b> these models on datasets specifically created for those requirements. However, it is time-consuming to establish IQA datasets. In this work, we propose a <b>Prompt-based</b> IQA (PromptIQA) that can directly adapt to new requirements without <b>fine-tuning</b> after training. On one hand, it utilizes a short sequence of Image-Score Pairs (ISP) as <b>prompts</b> for targeted predictions, which significantly reduces the dependency on the <b>data</b> <b>requirements.</b> On the other hand, PromptIQA is trained on a mixed dataset with two proposed <b>data</b> <b>augmentation</b> strategies to learn diverse requirements, thus enabling it to effectively adapt to new requirements. Experiments indicate that the PromptIQA outperforms SOTA methods with higher performance and better generalization. The code will be available.</p></p class="citation"></blockquote><h3 id=2977--138270-pipsus-self-supervised-dense-point-tracking-in-ultrasound-wanwen-chen-et-al-2024>(29/77 | 138/270) PIPsUS: Self-Supervised Dense Point Tracking in Ultrasound (Wanwen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanwen Chen, Adam Schmidt, Eitan Prisman, Septimiu E Salcudean. (2024)<br><strong>PIPsUS: Self-Supervised Dense Point Tracking in Ultrasound</strong><br><button class=copy-to-clipboard title="PIPsUS: Self-Supervised Dense Point Tracking in Ultrasound" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04969v1.pdf filename=2403.04969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Finding point-level correspondences is a fundamental problem in ultrasound (US), since it can enable US landmark tracking for intraoperative image guidance in different surgeries, including head and neck. Most existing US tracking methods, e.g., those based on optical flow or feature matching, were initially designed for RGB images before being applied to US. Therefore domain shift can impact their performance. Training could be <b>supervised</b> by ground-truth correspondences, but these are expensive to acquire in US. To solve these problems, we propose a <b>self-supervised</b> pixel-level tracking model called PIPsUS. Our model can track an arbitrary number of points in one forward pass and exploits temporal information by considering multiple, instead of just consecutive, frames. We developed a new <b>self-supervised</b> training strategy that utilizes a long-term point-tracking model trained for RGB images as a teacher to guide the model to learn realistic motions and use <b>data</b> <b>augmentation</b> to enforce tracking from US appearance. We evaluate our method on neck and oral US and echocardiography, showing higher point tracking accuracy when compared with fast normalized cross-correlation and tuned optical flow. Code will be available once the paper is accepted.</p></p class="citation"></blockquote><h3 id=3077--139270-stereodiffusion-training-free-stereo-image-generation-using-latent-diffusion-models-lezhong-wang-et-al-2024>(30/77 | 139/270) StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models (Lezhong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, Siavash Arjomand Bigdeli. (2024)<br><strong>StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04965v1.pdf filename=2403.04965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The demand for stereo images increases as manufacturers launch more XR devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is trainning free, remarkably straightforward to use, and it seamlessly integrates into the original Stable <b>Diffusion</b> <b>model.</b> Our method modifies the latent variable to provide an end-to-end, lightweight capability for fast generation of stereo image pairs, without the need for <b>fine-tuning</b> model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and <b>Self-Attention</b> Layers Modification methods to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the stereo generation process, achieving state-of-the-art scores in various quantitative evaluations.</p></p class="citation"></blockquote><h3 id=3177--140270-contrastdiagnosis-enhancing-interpretability-in-lung-nodule-diagnosis-using-contrastive-learning-chenglong-wang-et-al-2024>(31/77 | 140/270) ContrastDiagnosis: Enhancing Interpretability in Lung Nodule Diagnosis Using Contrastive Learning (Chenglong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenglong Wang, Yinqiao Yi, Yida Wang, Chengxiu Zhang, Yun Liu, Kensaku Mori, Mei Yuan, Guang Yang. (2024)<br><strong>ContrastDiagnosis: Enhancing Interpretability in Lung Nodule Diagnosis Using Contrastive Learning</strong><br><button class=copy-to-clipboard title="ContrastDiagnosis: Enhancing Interpretability in Lung Nodule Diagnosis Using Contrastive Learning" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Black Box, Contrastive Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05280v1.pdf filename=2403.05280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the ongoing development of deep learning, an increasing number of AI models have surpassed the performance levels of human clinical practitioners. However, the prevalence of AI diagnostic products in actual clinical practice remains significantly lower than desired. One crucial reason for this gap is the so-called `black box&rsquo; nature of AI models. Clinicians&rsquo; distrust of <b>black</b> <b>box</b> models has directly hindered the clinical deployment of AI products. To address this challenge, we propose ContrastDiagnosis, a straightforward yet effective interpretable diagnosis framework. This framework is designed to introduce inherent transparency and provide extensive post-hoc explainability for deep learning model, making them more suitable for clinical medical diagnosis. ContrastDiagnosis incorporates a <b>contrastive</b> <b>learning</b> mechanism to provide a case-based <b>reasoning</b> diagnostic rationale, enhancing the model&rsquo;s transparency and also offers post-hoc interpretability by highlighting similar areas. High diagnostic accuracy was achieved with AUC of 0.977 while maintain a high transparency and explainability.</p></p class="citation"></blockquote><h3 id=3277--141270-scene-graph-aided-radiology-report-generation-jun-wang-et-al-2024>(32/77 | 141/270) Scene Graph Aided Radiology Report Generation (Jun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Wang, Lixing Zhu, Abhir Bhalerao, Yulan He. (2024)<br><strong>Scene Graph Aided Radiology Report Generation</strong><br><button class=copy-to-clipboard title="Scene Graph Aided Radiology Report Generation" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Graph, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05687v1.pdf filename=2403.05687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radiology report generation (RRG) methods often lack sufficient medical <b>knowledge</b> <b>to</b> produce clinically accurate reports. The scene <b>graph</b> contains rich information to describe the objects in an image. We explore enriching the medical <b>knowledge</b> <b>for</b> RRG via a scene <b>graph,</b> which has not been done in the current RRG literature. To this end, we propose the Scene <b>Graph</b> aided RRG (SGRRG) network, a framework that generates region-level visual features, predicts anatomical attributes, and leverages an automatically generated scene <b>graph,</b> thus achieving medical <b>knowledge</b> <b>distillation</b> in an end-to-end manner. SGRRG is composed of a dedicated scene <b>graph</b> encoder responsible for translating the scene <b>graph,</b> and a scene <b>graph-aided</b> decoder that takes advantage of both patch-level and region-level visual information. A fine-grained, sentence-level attention method is designed to better dis-till the scene <b>graph</b> information. Extensive experiments demonstrate that SGRRG outperforms previous state-of-the-art methods in report generation and can better capture abnormal findings.</p></p class="citation"></blockquote><h3 id=3377--142270-omnicount-multi-label-object-counting-with-semantic-geometric-priors-anindya-mondal-et-al-2024>(33/77 | 142/270) OmniCount: Multi-label Object Counting with Semantic-Geometric Priors (Anindya Mondal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anindya Mondal, Sauradip Nag, Xiatian Zhu, Anjan Dutta. (2024)<br><strong>OmniCount: Multi-label Object Counting with Semantic-Geometric Priors</strong><br><button class=copy-to-clipboard title="OmniCount: Multi-label Object Counting with Semantic-Geometric Priors" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV, eess-SP<br>Keyword Score: 23<br>Keywords: Benchmarking, Visual Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05435v1.pdf filename=2403.05435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object counting is pivotal for understanding the composition of scenes. Previously, this task was dominated by class-specific methods, which have gradually evolved into more adaptable class-agnostic strategies. However, these strategies come with their own set of limitations, such as the need for manual exemplar input and multiple passes for multiple categories, resulting in significant inefficiencies. This paper introduces a new, more practical approach enabling simultaneous counting of multiple object categories using an open vocabulary framework. Our solution, OmniCount, stands out by using semantic and geometric insights from pre-trained models to count multiple categories of objects as specified by users, all without additional training. OmniCount distinguishes itself by generating precise object masks and leveraging point <b>prompts</b> via the Segment Anything Model for efficient counting. To evaluate OmniCount, we created the OmniCount-191 <b>benchmark,</b> a first-of-its-kind dataset with multi-label object counts, including points, bounding boxes, and <b>VQA</b> annotations. Our comprehensive evaluation in OmniCount-191, alongside other leading <b>benchmarks,</b> demonstrates OmniCount&rsquo;s exceptional performance, significantly outpacing existing solutions and heralding a new era in object counting technology.</p></p class="citation"></blockquote><h3 id=3477--143270-part-aware-personalized-segment-anything-model-for-patient-specific-segmentation-chenhui-zhao-et-al-2024>(34/77 | 143/270) Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation (Chenhui Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhui Zhao, Liyue Shen. (2024)<br><strong>Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation</strong><br><button class=copy-to-clipboard title="Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05433v1.pdf filename=2403.05433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Precision medicine, such as patient-adaptive treatments utilizing medical images, poses new challenges for image segmentation algorithms due to (1) the large variability across different patients and (2) the limited availability of annotated data for each patient. In this work, we propose a data-efficient segmentation method to address these challenges, namely Part-aware Personalized Segment Anything Model (P^2SAM). Without any model <b>fine-tuning,</b> P^2SAM enables seamless adaptation to any new patients relying only on one-shot patient-specific data. We introduce a novel part-aware <b>prompt</b> mechanism to select multiple-point <b>prompts</b> based on part-level features of the one-shot data. To further promote the robustness of the selected <b>prompt,</b> we propose a retrieval approach to handle outlier <b>prompts.</b> Extensive experiments demonstrate that P^2SAM improves the performance by +8.0% and +2.0% mean Dice score within two patient-specific segmentation settings, and exhibits impressive generality across different application domains, e.g., +6.4% mIoU on the PerSeg <b>benchmark.</b> Code will be released upon acceptance.</p></p class="citation"></blockquote><h3 id=3577--144270-diffsf-diffusion-models-for-scene-flow-estimation-yushan-zhang-et-al-2024>(35/77 | 144/270) DiffSF: Diffusion Models for Scene Flow Estimation (Yushan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushan Zhang, Bastian Wandt, Maria Magnusson, Michael Felsberg. (2024)<br><strong>DiffSF: Diffusion Models for Scene Flow Estimation</strong><br><button class=copy-to-clipboard title="DiffSF: Diffusion Models for Scene Flow Estimation" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05327v1.pdf filename=2403.05327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene flow estimation is an essential ingredient for a variety of real-world applications, especially for autonomous agents, such as self-driving cars and robots. While recent scene flow estimation approaches achieve a reasonable accuracy, their applicability to real-world systems additionally benefits from a reliability measure. Aiming at improving accuracy while additionally providing an estimate for uncertainty, we propose DiffSF that combines <b>transformer-based</b> scene flow estimation with denoising <b>diffusion</b> <b>models.</b> In the <b>diffusion</b> <b>process,</b> the ground truth scene flow vector field is gradually perturbed by adding Gaussian noise. In the reverse process, starting from randomly sampled Gaussian noise, the scene flow vector field prediction is recovered by conditioning on a source and a target point cloud. We show that the <b>diffusion</b> <b>process</b> greatly increases the robustness of predictions compared to prior approaches resulting in state-of-the-art performance on standard scene flow estimation <b>benchmarks.</b> Moreover, by sampling multiple times with different initial states, the denoising process predicts multiple hypotheses, which enables measuring the output uncertainty, allowing our approach to detect a majority of the inaccurate predictions.</p></p class="citation"></blockquote><h3 id=3677--145270-synthetic-privileged-information-enhances-medical-image-representation-learning-lucas-farndale-et-al-2024>(36/77 | 145/270) Synthetic Privileged Information Enhances Medical Image Representation Learning (Lucas Farndale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Farndale, Chris Walsh, Robert Insall, Ke Yuan. (2024)<br><strong>Synthetic Privileged Information Enhances Medical Image Representation Learning</strong><br><button class=copy-to-clipboard title="Synthetic Privileged Information Enhances Medical Image Representation Learning" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, q-bio-TO<br>Keyword Score: 21<br>Keywords: Multi-modal, Multi-modal, Representation Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05220v1.pdf filename=2403.05220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>self-supervised</b> <b>representation</b> <b>learning</b> has consistently proven to be a highly effective method in medical image analysis, offering strong task performance and producing biologically informed insights. However, these methods heavily rely on large, paired datasets, which is prohibitive for their use in scenarios where paired data does not exist, or there is only a small amount available. In contrast, image generation methods can work well on very small datasets, and can find mappings between unpaired datasets, meaning an effectively unlimited amount of paired synthetic data can be generated. In this work, we demonstrate that <b>representation</b> <b>learning</b> can be significantly improved by synthetically generating paired information, both compared to training on either single-modality (up to 4.4x error reduction) or authentic <b>multi-modal</b> paired datasets (up to 5.6x error reduction).</p></p class="citation"></blockquote><h3 id=3777--146270-micro-fracture-detection-in-photovoltaic-cells-with-hardware-constrained-devices-and-computer-vision-booy-vitas-faassen-et-al-2024>(37/77 | 146/270) Micro-Fracture Detection in Photovoltaic Cells with Hardware-Constrained Devices and Computer Vision (Booy Vitas Faassen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Booy Vitas Faassen, Jorge Serrano, Paul D. Rosero-Montalvo. (2024)<br><strong>Micro-Fracture Detection in Photovoltaic Cells with Hardware-Constrained Devices and Computer Vision</strong><br><button class=copy-to-clipboard title="Micro-Fracture Detection in Photovoltaic Cells with Hardware-Constrained Devices and Computer Vision" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cond-mat-mtrl-sci, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05694v1.pdf filename=2403.05694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Solar energy is rapidly becoming a robust renewable energy source to conventional finite resources such as fossil fuels. It is harvested using interconnected photovoltaic panels, typically built with crystalline silicon cells, i.e. semiconducting materials that convert effectively the solar radiation into electricity. However, crystalline silicon is fragile and vulnerable to cracking over time or in predictive maintenance tasks, which can lead to electric isolation of parts of the solar cell and even failure, thus affecting the panel performance and reducing electricity generation. This work aims to developing a system for detecting cell cracks in solar panels to anticipate and alaert of a potential failure of the photovoltaic system by using computer vision techniques. Three scenarios are defined where these techniques will bring value. In scenario A, images are taken manually and the system detecting failures in the solar cells is not subject to any computationa constraints. In scenario B, an Edge device is placed near the solar farm, able to make inferences. Finally, in scenario C, a small microcontroller is placed in a drone flying over the solar farm and making inferences about the solar cells&rsquo; states. Three different architectures are found the most suitable solutions, one for each scenario, namely the InceptionV3 model, an EfficientNetB0 model shrunk into full integer <b>quantization,</b> and a customized <b>CNN</b> architechture built with VGG16 blocks.</p></p class="citation"></blockquote><h3 id=3877--147270-semantic-feature-learning-for-universal-unsupervised-cross-domain-retrieval-lixu-wang-et-al-2024>(38/77 | 147/270) Semantic Feature Learning for Universal Unsupervised Cross-Domain Retrieval (Lixu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lixu Wang, Xinyu Du, Qi Zhu. (2024)<br><strong>Semantic Feature Learning for Universal Unsupervised Cross-Domain Retrieval</strong><br><button class=copy-to-clipboard title="Semantic Feature Learning for Universal Unsupervised Cross-Domain Retrieval" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05690v1.pdf filename=2403.05690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-domain retrieval (CDR), as a crucial tool for numerous technologies, is finding increasingly broad applications. However, existing efforts face several major issues, with the most critical being the need for accurate supervision, which often demands costly resources and efforts. Cutting-edge studies focus on achieving <b>unsupervised</b> CDR but typically assume that the category spaces across domains are identical, an assumption that is often unrealistic in real-world scenarios. This is because only through dedicated and comprehensive analysis can the category spaces of different domains be confirmed as identical, which contradicts the premise of <b>unsupervised</b> scenarios. Therefore, in this work, we introduce the problem of Universal <b>Unsupervised</b> Cross-Domain Retrieval (U^2CDR) for the first time and design a two-stage semantic feature learning framework to address it. In the first stage, a cross-domain unified prototypical structure is established under the guidance of an instance-prototype-mixed contrastive loss and a semantic-enhanced loss, to counteract category space differences. In the second stage, through a modified <b>adversarial</b> <b>training</b> mechanism, we ensure minimal changes for the established prototypical structure during domain alignment, enabling more accurate nearest-neighbor searching. Extensive experiments across multiple datasets and scenarios, including closet, partial, and open-set CDR, demonstrate that our approach significantly outperforms existing state-of-the-art CDR works and some potentially effective studies from other topics in solving U^2CDR challenges.</p></p class="citation"></blockquote><h3 id=3977--148270-dualbev-cnn-is-all-you-need-in-view-transformation-peidong-li-et-al-2024>(39/77 | 148/270) DualBEV: CNN is All You Need in View Transformation (Peidong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peidong Li, Wancheng Shen, Qihao Huang, Dixiao Cui. (2024)<br><strong>DualBEV: CNN is All You Need in View Transformation</strong><br><button class=copy-to-clipboard title="DualBEV: CNN is All You Need in View Transformation" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05402v1.pdf filename=2403.05402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Camera-based Bird&rsquo;s-Eye-View (BEV) perception often struggles between adopting 3D-to-2D or 2D-to-3D view transformation (VT). The 3D-to-2D VT typically employs resource intensive <b>Transformer</b> to establish robust correspondences between 3D and 2D feature, while the 2D-to-3D VT utilizes the Lift-Splat-Shoot (LSS) pipeline for real-time application, potentially missing distant information. To address these limitations, we propose DualBEV, a unified framework that utilizes a shared <b>CNN-based</b> feature transformation incorporating three probabilistic measurements for both strategies. By considering dual-view correspondences in one-stage, DualBEV effectively bridges the gap between these strategies, harnessing their individual strengths. Our method achieves state-of-the-art performance without <b>Transformer,</b> delivering comparable efficiency to the LSS approach, with 55.2% mAP and 63.4% NDS on the nuScenes test set. Code will be released at <a href=https://github.com/PeidongLi/DualBEV>https://github.com/PeidongLi/DualBEV</a>.</p></p class="citation"></blockquote><h3 id=4077--149270-frequency-adaptive-dilated-convolution-for-semantic-segmentation-linwei-chen-et-al-2024>(40/77 | 149/270) Frequency-Adaptive Dilated Convolution for Semantic Segmentation (Linwei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linwei Chen, Lin Gu, Ying Fu. (2024)<br><strong>Frequency-Adaptive Dilated Convolution for Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Frequency-Adaptive Dilated Convolution for Semantic Segmentation" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05369v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05369v2.pdf filename=2403.05369v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dilated <b>convolution,</b> which expands the receptive field by inserting gaps between its consecutive elements, is widely employed in computer vision. In this study, we propose three strategies to improve individual phases of dilated <b>convolution</b> from the view of spectrum analysis. Departing from the conventional practice of fixing a global dilation rate as a hyperparameter, we introduce Frequency-Adaptive Dilated <b>Convolution</b> (FADC), which dynamically adjusts dilation rates spatially based on local frequency components. Subsequently, we design two plug-in modules to directly enhance effective bandwidth and receptive field size. The Adaptive Kernel (AdaKern) module decomposes <b>convolution</b> weights into low-frequency and high-frequency components, dynamically adjusting the ratio between these components on a per-channel basis. By increasing the high-frequency part of <b>convolution</b> weights, AdaKern captures more high-frequency components, thereby improving effective bandwidth. The Frequency Selection (FreqSelect) module optimally balances high- and low-frequency components in feature representations through spatially variant reweighting. It suppresses high frequencies in the background to encourage FADC to learn a larger dilation, thereby increasing the receptive field for an expanded scope. Extensive experiments on segmentation and <b>object</b> <b>detection</b> consistently validate the efficacy of our approach. The code is publicly available at \url{https://github.com/Linwei-Chen/FADC}.</p></p class="citation"></blockquote><h3 id=4177--150270-evaluating-text-to-image-generative-models-an-empirical-study-on-human-image-synthesis-muxi-chen-et-al-2024>(41/77 | 150/270) Evaluating Text-to-Image Generative Models: An Empirical Study on Human Image Synthesis (Muxi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muxi Chen, Yi Liu, Jian Yi, Changran Xu, Qiuxia Lai, Hongliang Wang, Tsung-Yi Ho, Qiang Xu. (2024)<br><strong>Evaluating Text-to-Image Generative Models: An Empirical Study on Human Image Synthesis</strong><br><button class=copy-to-clipboard title="Evaluating Text-to-Image Generative Models: An Empirical Study on Human Image Synthesis" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fairness, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05125v1.pdf filename=2403.05125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present an empirical study introducing a nuanced evaluation framework for <b>text-to-image</b> (T2I) generative models, applied to human image synthesis. Our framework categorizes evaluations into two distinct groups: first, focusing on image qualities such as aesthetics and realism, and second, examining text conditions through concept coverage and <b>fairness.</b> We introduce an innovative aesthetic score prediction model that assesses the visual appeal of generated images and unveils the first dataset marked with low-quality regions in generated human images to facilitate automatic defect detection. Our exploration into concept coverage probes the model&rsquo;s effectiveness in interpreting and rendering text-based concepts accurately, while our analysis of <b>fairness</b> reveals biases in model outputs, with an emphasis on gender, race, and age. While our study is grounded in human imagery, this dual-faceted approach is designed with the flexibility to be applicable to other forms of image generation, enhancing our understanding of generative models and paving the way to the next generation of more sophisticated, contextually aware, and ethically attuned generative models. We will release our code, the data used for evaluating generative models and the dataset annotated with defective areas soon.</p></p class="citation"></blockquote><h3 id=4277--151270-apple-adversarial-privacy-aware-perturbations-on-latent-embedding-for-unfairness-mitigation-zikang-xu-et-al-2024>(42/77 | 151/270) APPLE: Adversarial Privacy-aware Perturbations on Latent Embedding for Unfairness Mitigation (Zikang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zikang Xu, Fenghe Tang, Quan Quan, Qingsong Yao, S. Kevin Zhou. (2024)<br><strong>APPLE: Adversarial Privacy-aware Perturbations on Latent Embedding for Unfairness Mitigation</strong><br><button class=copy-to-clipboard title="APPLE: Adversarial Privacy-aware Perturbations on Latent Embedding for Unfairness Mitigation" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fairness, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05114v1.pdf filename=2403.05114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring <b>fairness</b> in deep-learning-based segmentors is crucial for health equity. Much effort has been dedicated to mitigating unfairness in the training datasets or procedures. However, with the increasing prevalence of <b>foundation</b> <b>models</b> in medical image analysis, it is hard to train fair models from scratch while preserving utility. In this paper, we propose a novel method, Adversarial Privacy-aware Perturbations on Latent Embedding (APPLE), that can improve the <b>fairness</b> of deployed segmentors by introducing a small latent feature perturber without updating the weights of the original model. By adding perturbation to the latent vector, APPLE decorates the latent vector of segmentors such that no <b>fairness-related</b> features can be passed to the decoder of the segmentors while preserving the architecture and parameters of the segmentor. Experiments on two segmentation datasets and five segmentors (three U-Net-like and two SAM-like) illustrate the effectiveness of our proposed method compared to several unfairness mitigation methods.</p></p class="citation"></blockquote><h3 id=4377--152270-crm-single-image-to-3d-textured-mesh-with-convolutional-reconstruction-model-zhengyi-wang-et-al-2024>(43/77 | 152/270) CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model (Zhengyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, Jun Zhu. (2024)<br><strong>CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model</strong><br><button class=copy-to-clipboard title="CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05034v1.pdf filename=2403.05034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feed-forward 3D generative models like the Large Reconstruction Model (LRM) have demonstrated exceptional generation speed. However, the <b>transformer-based</b> methods do not leverage the geometric priors of the triplane component in their architecture, often leading to sub-optimal quality given the limited size of 3D data and slow training. In this work, we present the <b>Convolutional</b> Reconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D generative model. Recognizing the limitations posed by sparse 3D data, we highlight the necessity of integrating geometric priors into network design. CRM builds on the key observation that the visualization of triplane exhibits spatial correspondence of six orthographic images. First, it generates six orthographic view images from a single input image, then feeds these images into a <b>convolutional</b> U-Net, leveraging its strong pixel-level alignment capabilities and significant bandwidth to create a high-resolution triplane. CRM further employs Flexicubes as geometric representation, facilitating direct end-to-end optimization on textured meshes. Overall, our model delivers a high-fidelity textured mesh from an image in just 10 seconds, without any test-time optimization.</p></p class="citation"></blockquote><h3 id=4477--153270-actformer-scalable-collaborative-perception-via-active-queries-suozhi-huang-et-al-2024>(44/77 | 153/270) ActFormer: Scalable Collaborative Perception via Active Queries (Suozhi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suozhi Huang, Juexiao Zhang, Yiming Li, Chen Feng. (2024)<br><strong>ActFormer: Scalable Collaborative Perception via Active Queries</strong><br><button class=copy-to-clipboard title="ActFormer: Scalable Collaborative Perception via Active Queries" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04968v1.pdf filename=2403.04968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collaborative perception leverages rich visual observations from multiple robots to extend a single robot&rsquo;s perception ability beyond its field of view. Many prior works receive messages broadcast from all collaborators, leading to a scalability challenge when dealing with a large number of robots and sensors. In this work, we aim to address \textit{scalable camera-based collaborative perception} with a <b>Transformer-based</b> architecture. Our key idea is to enable a single robot to intelligently discern the relevance of the collaborators and their associated cameras according to a learned spatial prior. This proactive understanding of the visual features&rsquo; relevance does not require the transmission of the features themselves, enhancing both communication and computation efficiency. Specifically, we present ActFormer, a <b>Transformer</b> that learns bird&rsquo;s eye view (BEV) representations by using predefined BEV queries to interact with multi-robot multi-camera inputs. Each BEV query can actively select relevant cameras for information aggregation based on pose information, instead of interacting with all cameras indiscriminately. Experiments on the V2X-Sim dataset demonstrate that ActFormer improves the detection performance from 29.89% to 45.15% in terms of <a href=mailto:AP@0.7>AP@0.7</a> with about 50% fewer queries, showcasing the effectiveness of ActFormer in multi-agent collaborative 3D <b>object</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=4577--154270-learning-to-rematch-mismatched-pairs-for-robust-cross-modal-retrieval-haochen-han-et-al-2024>(45/77 | 154/270) Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval (Haochen Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochen Han, Qinghua Zheng, Guang Dai, Minnan Luo, Jingdong Wang. (2024)<br><strong>Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval</strong><br><button class=copy-to-clipboard title="Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05105v1.pdf filename=2403.05105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collecting well-matched multimedia datasets is crucial for training cross-modal retrieval models. However, in real-world scenarios, massive <b>multimodal</b> data are harvested from the Internet, which inevitably contains Partially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data will remarkably harm the cross-modal retrieval performance. Previous efforts tend to mitigate this problem by estimating a soft correspondence to down-weight the contribution of PMPs. In this paper, we aim to address this challenge from a new perspective: the potential semantic similarity among unpaired samples makes it possible to excavate useful knowledge from mismatched pairs. To achieve this, we propose L2RM, a general framework based on Optimal Transport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to generate refined alignments by seeking a minimal-cost transport plan across different modalities. To formalize the rematching idea in OT, first, we propose a <b>self-supervised</b> cost function that automatically learns from explicit similarity-cost mapping relation. Second, we present to model a partial OT problem while restricting the transport among false positives to further boost refined alignments. Extensive experiments on three <b>benchmarks</b> demonstrate our L2RM significantly improves the robustness against PMPs for existing models. The code is available at <a href=https://github.com/hhc1997/L2RM>https://github.com/hhc1997/L2RM</a>.</p></p class="citation"></blockquote><h3 id=4677--155270-probabilistic-image-driven-traffic-modeling-via-remote-sensing-scott-workman-et-al-2024>(46/77 | 155/270) Probabilistic Image-Driven Traffic Modeling via Remote Sensing (Scott Workman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Scott Workman, Armin Hadzic. (2024)<br><strong>Probabilistic Image-Driven Traffic Modeling via Remote Sensing</strong><br><button class=copy-to-clipboard title="Probabilistic Image-Driven Traffic Modeling via Remote Sensing" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05521v1.pdf filename=2403.05521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work addresses the task of modeling spatiotemporal traffic patterns directly from overhead imagery, which we refer to as image-driven traffic modeling. We extend this line of work and introduce a <b>multi-modal,</b> multi-task <b>transformer-based</b> segmentation architecture that can be used to create dense city-scale traffic models. Our approach includes a geo-temporal positional encoding module for integrating geo-temporal context and a probabilistic objective function for estimating traffic speeds that naturally models temporal variations. We evaluate our method extensively using the Dynamic Traffic Speeds (DTS) <b>benchmark</b> dataset and significantly improve the state-of-the-art. Finally, we introduce the DTS++ dataset to support mobility-related location adaptation experiments.</p></p class="citation"></blockquote><h3 id=4777--156270-benchmarking-micro-action-recognition-dataset-methods-and-applications-dan-guo-et-al-2024>(47/77 | 156/270) Benchmarking Micro-action Recognition: Dataset, Methods, and Applications (Dan Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dan Guo, Kun Li, Bin Hu, Yan Zhang, Meng Wang. (2024)<br><strong>Benchmarking Micro-action Recognition: Dataset, Methods, and Applications</strong><br><button class=copy-to-clipboard title="Benchmarking Micro-action Recognition: Dataset, Methods, and Applications" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05234v1.pdf filename=2403.05234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Micro-action is an imperceptible non-verbal behaviour characterised by low-intensity movement. It offers insights into the feelings and intentions of individuals and is important for human-oriented applications such as <b>emotion</b> <b>recognition</b> and psychological assessment. However, the identification, differentiation, and understanding of micro-actions pose challenges due to the imperceptible and inaccessible nature of these subtle human behaviors in everyday life. In this study, we innovatively collect a new micro-action dataset designated as Micro-action-52 (MA-52), and propose a <b>benchmark</b> named micro-action network (MANet) for micro-action recognition (MAR) task. Uniquely, MA-52 provides the whole-body perspective including gestures, upper- and lower-limb movements, attempting to reveal comprehensive micro-action cues. In detail, MA-52 contains 52 micro-action categories along with seven body part labels, and encompasses a full array of realistic and natural micro-actions, accounting for 205 participants and 22,422 video instances collated from the psychological interviews. Based on the proposed dataset, we assess MANet and other nine prevalent action recognition methods. MANet incorporates squeeze-and excitation (SE) and temporal shift module (TSM) into the ResNet architecture for modeling the spatiotemporal characteristics of micro-actions. Then a joint-embedding loss is designed for semantic matching between video and action labels; the loss is used to better distinguish between visually similar yet distinct micro-action categories. The extended application in <b>emotion</b> <b>recognition</b> has demonstrated one of the important values of our proposed dataset and method. In the future, further exploration of human behaviour, <b>emotion,</b> <b>and</b> psychological assessment will be conducted in depth. The dataset and source code are released at <a href=https://github.com/VUT-HFUT/Micro-Action>https://github.com/VUT-HFUT/Micro-Action</a>.</p></p class="citation"></blockquote><h3 id=4877--157270-3d-face-reconstruction-using-a-spectral-based-graph-convolution-encoder-haoxin-xu-et-al-2024>(48/77 | 157/270) 3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder (Haoxin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxin Xu, Zezheng Zhao, Yuxin Cao, Chunyu Chen, Hao Ge, Ziyao Liu. (2024)<br><strong>3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder</strong><br><button class=copy-to-clipboard title="3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05218v1.pdf filename=2403.05218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular 3D face reconstruction plays a crucial role in avatar generation, with significant demand in web-related applications such as generating virtual financial advisors in FinTech. Current reconstruction methods predominantly rely on deep learning techniques and employ 2D self-supervision as a means to guide model learning. However, these methods encounter challenges in capturing the comprehensive 3D structural information of the face due to the utilization of 2D images for model training purposes. To overcome this limitation and enhance the reconstruction of 3D structural features, we propose an innovative approach that integrates existing 2D features with 3D features to guide the model learning process. Specifically, we introduce the 3D-ID Loss, which leverages the high-dimensional structure features extracted from a Spectral-Based <b>Graph</b> <b>Convolution</b> Encoder applied to the facial mesh. This approach surpasses the sole reliance on the 3D information provided by the facial mesh vertices coordinates. Our model is trained using 2D-3D data pairs from a combination of datasets and achieves state-of-the-art performance on the NoW <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=4977--158270-feature-cam-interpretable-ai-in-image-classification-frincy-clement-et-al-2024>(49/77 | 158/270) Feature CAM: Interpretable AI in Image Classification (Frincy Clement et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frincy Clement, Ji Yang, Irene Cheng. (2024)<br><strong>Feature CAM: Interpretable AI in Image Classification</strong><br><button class=copy-to-clipboard title="Feature CAM: Interpretable AI in Image Classification" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 15<br>Keywords: Black Box, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05658v1.pdf filename=2403.05658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Networks have often been called the <b>black</b> <b>box</b> because of the complex, deep architecture and non-transparency presented by the inner layers. There is a lack of trust to use Artificial Intelligence in critical and high-precision fields such as security, finance, health, and manufacturing industries. A lot of focused work has been done to provide interpretable models, intending to deliver meaningful insights into the thoughts and behavior of neural networks. In our research, we compare the state-of-the-art methods in the Activation-based methods (ABM) for interpreting predictions of <b>CNN</b> models, specifically in the application of Image Classification. We then extend the same for eight <b>CNN-based</b> architectures to compare the differences in visualization and thus interpretability. We introduced a novel technique Feature CAM, which falls in the perturbation-activation combination, to create fine-grained, class-discriminative visualizations. The resulting saliency maps from our experiments proved to be 3-4 times better human interpretable than the state-of-the-art in ABM. At the same time it reserves machine interpretability, which is the average confidence scores in classification.</p></p class="citation"></blockquote><h3 id=5077--159270-uforecon-generalizable-sparse-view-surface-reconstruction-from-arbitrary-and-unfavorable-sets-youngju-na-et-al-2024>(50/77 | 159/270) UFORecon: Generalizable Sparse-View Surface Reconstruction from Arbitrary and UnFavOrable Sets (Youngju Na et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youngju Na, Woo Jae Kim, Kyu Beom Han, Suhyeon Ha, Sung-eui Yoon. (2024)<br><strong>UFORecon: Generalizable Sparse-View Surface Reconstruction from Arbitrary and UnFavOrable Sets</strong><br><button class=copy-to-clipboard title="UFORecon: Generalizable Sparse-View Surface Reconstruction from Arbitrary and UnFavOrable Sets" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05086v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05086v2.pdf filename=2403.05086v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalizable neural implicit surface reconstruction aims to obtain an accurate underlying <b>geometry</b> given a limited number of multi-view images from unseen scenes. However, existing methods select only informative and relevant views using predefined scores for training and testing phases. This constraint renders the model impractical in real-world scenarios, where the availability of favorable combinations cannot always be ensured. We introduce and validate a view-combination score to indicate the effectiveness of the input view combination. We observe that previous methods output degenerate solutions under arbitrary and unfavorable sets. Building upon this finding, we propose UFORecon, a robust view-combination generalizable surface reconstruction framework. To achieve this, we apply cross-view matching <b>transformers</b> to model interactions between source images and build correlation frustums to capture global correlations. Additionally, we explicitly encode pairwise feature similarities as view-consistent priors. Our proposed framework significantly outperforms previous methods in terms of view-combination generalizability and also in the conventional generalizable protocol trained with favorable view-combinations. The code is available at <a href=https://github.com/Youngju-Na/UFORecon>https://github.com/Youngju-Na/UFORecon</a>.</p></p class="citation"></blockquote><h3 id=5177--160270-audio-synchronized-visual-animation-lin-zhang-et-al-2024>(51/77 | 160/270) Audio-Synchronized Visual Animation (Lin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Zhang, Shentong Mo, Yijing Zhang, Pedro Morgado. (2024)<br><strong>Audio-Synchronized Visual Animation</strong><br><button class=copy-to-clipboard title="Audio-Synchronized Visual Animation" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05659v1.pdf filename=2403.05659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current visual generation methods can produce high quality videos guided by texts. However, effectively controlling object dynamics remains a challenge. This work explores audio as a cue to generate temporally synchronized image animations. We introduce Audio Synchronized Visual Animation (ASVA), a task animating a static image to demonstrate motion dynamics, temporally guided by audio clips across multiple classes. To this end, we present AVSync15, a dataset curated from VGGSound with videos featuring synchronized audio visual events across 15 categories. We also present a <b>diffusion</b> <b>model,</b> AVSyncD, capable of generating dynamic animations guided by audios. Extensive evaluations validate AVSync15 as a reliable <b>benchmark</b> for synchronized generation and demonstrate our models superior performance. We further explore AVSyncDs potential in a variety of audio synchronized generation tasks, from generating full videos without a base image to controlling object motions with various sounds. We hope our established <b>benchmark</b> can open new avenues for controllable visual generation. More videos on project webpage <a href=https://lzhangbj.github.io/projects/asva/asva.html>https://lzhangbj.github.io/projects/asva/asva.html</a>.</p></p class="citation"></blockquote><h3 id=5277--161270-tell-dont-show-language-guidance-eases-transfer-across-domains-in-images-and-videos-tarun-kalluri-et-al-2024>(52/77 | 161/270) Tell, Don&rsquo;t Show!: Language Guidance Eases Transfer Across Domains in Images and Videos (Tarun Kalluri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tarun Kalluri, Bodhisattwa Prasad Majumder, Manmohan Chandraker. (2024)<br><strong>Tell, Don&rsquo;t Show!: Language Guidance Eases Transfer Across Domains in Images and Videos</strong><br><button class=copy-to-clipboard title="Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05535v1.pdf filename=2403.05535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While <b>unsupervised</b> adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiveness. To further extend the scope of our study beyond images, we introduce a new <b>benchmark</b> to study ego-exo transfer in videos and find that our language-aided LaGTran yields significant gains in this highly challenging and non-trivial transfer setting. Code, models, and proposed datasets are publicly available at <a href=https://tarun005.github.io/lagtran/>https://tarun005.github.io/lagtran/</a>.</p></p class="citation"></blockquote><h3 id=5377--162270-multiple-instance-learning-with-random-sampling-for-whole-slide-image-classification-h-keshvarikhojasteh-et-al-2024>(53/77 | 162/270) Multiple Instance Learning with random sampling for Whole Slide Image Classification (H. Keshvarikhojasteh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H. Keshvarikhojasteh, J. P. W. Pluim, M. Veta. (2024)<br><strong>Multiple Instance Learning with random sampling for Whole Slide Image Classification</strong><br><button class=copy-to-clipboard title="Multiple Instance Learning with random sampling for Whole Slide Image Classification" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multiple Instance Learning, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05351v1.pdf filename=2403.05351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In computational pathology, random sampling of patches during training of <b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) methods is computationally efficient and serves as a regularization strategy. Despite its promising benefits, questions concerning performance trends for varying <b>sample</b> <b>sizes</b> and its influence on model interpretability remain. Addressing these, we reach an optimal performance enhancement of 1.7% using thirty percent of patches on the CAMELYON16 dataset, and 3.7% with only eight <b>samples</b> <b>on</b> the TUPAC16 dataset. We also find interpretability effects are strongly dataset-dependent, with interpretability impacted on CAMELYON16, while remaining unaffected on TUPAC16. This reinforces that both the performance and interpretability relationships with sampling are closely task-specific. End-to-end training with 1024 <b>samples</b> <b>reveals</b> improvements across both datasets compared to pre-extracted features, further highlighting the potential of this efficient approach.</p></p class="citation"></blockquote><h3 id=5477--163270-laneptrnet-revisiting-lane-detection-as-point-voting-and-grouping-on-curves-jiayan-cao-et-al-2024>(54/77 | 163/270) LanePtrNet: Revisiting Lane Detection as Point Voting and Grouping on Curves (Jiayan Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayan Cao, Xueyu Zhu, Cheng Qian. (2024)<br><strong>LanePtrNet: Revisiting Lane Detection as Point Voting and Grouping on Curves</strong><br><button class=copy-to-clipboard title="LanePtrNet: Revisiting Lane Detection as Point Voting and Grouping on Curves" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05155v1.pdf filename=2403.05155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lane detection plays a critical role in the field of autonomous driving. Prevailing methods generally adopt basic concepts (anchors, key points, etc.) from <b>object</b> <b>detection</b> and segmentation tasks, while these approaches require manual adjustments for curved <b>objects,</b> <b>involve</b> exhaustive searches on predefined anchors, require complex post-processing steps, and may lack flexibility when applied to real-world scenarios.In this paper, we propose a novel approach, LanePtrNet, which treats lane detection as a process of point voting and grouping on ordered sets: Our method takes backbone features as input and predicts a curve-aware centerness, which represents each lane as a point and assigns the most probable center point to it. A novel point sampling method is proposed to generate a set of candidate points based on the votes received. By leveraging features from local neighborhoods, and cross-instance attention score, we design a grouping module that further performs lane-wise <b>clustering</b> between neighboring and seeding points. Furthermore, our method can accommodate a point-based framework, (PointNet++ series, etc.) as an alternative to the backbone. This flexibility enables effortless extension to 3D lane detection tasks. We conduct comprehensive experiments to validate the effectiveness of our proposed approach, demonstrating its superior performance.</p></p class="citation"></blockquote><h3 id=5577--164270-agile-multi-source-free-domain-adaptation-xinyao-li-et-al-2024>(55/77 | 164/270) Agile Multi-Source-Free Domain Adaptation (Xinyao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyao Li, Jingjing Li, Fengling Li, Lei Zhu, Ke Lu. (2024)<br><strong>Agile Multi-Source-Free Domain Adaptation</strong><br><button class=copy-to-clipboard title="Agile Multi-Source-Free Domain Adaptation" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05062v1.pdf filename=2403.05062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficiently utilizing rich knowledge in pretrained models has become a critical topic in the era of large models. This work focuses on adaptively utilizing knowledge from multiple source-pretrained models to an unlabeled target <b>domain</b> <b>without</b> accessing the source data. Despite being a practically useful setting, existing methods require extensive parameter tuning over each source model, which is computationally expensive when facing abundant source <b>domains</b> <b>or</b> larger source models. To address this challenge, we propose a novel approach which is free of the parameter tuning over source backbones. Our technical contribution lies in the Bi-level ATtention ENsemble (Bi-ATEN) module, which learns both intra-domain weights and inter-domain ensemble weights to achieve a fine balance between instance specificity and <b>domain</b> <b>consistency.</b> By slightly tuning source bottlenecks, we achieve comparable or even superior performance on a challenging <b>benchmark</b> DomainNet with less than 3% trained parameters and 8 times of throughput compared with SOTA method. Furthermore, with minor modifications, the proposed module can be easily equipped to existing methods and gain more than 4% performance boost. Code is available at <a href=https://github.com/TL-UESTC/Bi-ATEN>https://github.com/TL-UESTC/Bi-ATEN</a>.</p></p class="citation"></blockquote><h3 id=5677--165270-dyronet-a-low-rank-adapter-enhanced-dynamic-routing-network-for-streaming-perception-xiang-huang-et-al-2024>(56/77 | 165/270) DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception (Xiang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Huang, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Wangmeng Xiang, Baigui Sun, Xiao Wu. (2024)<br><strong>DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception</strong><br><button class=copy-to-clipboard title="DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05050v1.pdf filename=2403.05050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving systems demand real-time, accurate perception to navigate complex environments. Addressing this, we introduce the Dynamic Router Network (DyRoNet), a framework that innovates with low-rank dynamic routing for enhanced streaming perception. By integrating specialized pre-trained branch networks, <b>fine-tuned</b> for various environmental conditions, DyRoNet achieves a balance between latency and precision. Its core feature, the speed router module, intelligently directs input data to the best-suited branch network, optimizing performance. The extensive evaluations reveal that DyRoNet adapts effectively to multiple branch selection strategies, setting a new <b>benchmark</b> in performance across a range of scenarios. DyRoNet not only establishes a new <b>benchmark</b> for streaming perception but also provides valuable engineering insights for future work. More project information is available at <a href=https://tastevision.github.io/DyRoNet/>https://tastevision.github.io/DyRoNet/</a></p></p class="citation"></blockquote><h3 id=5777--166270-beyond-mot-semantic-multi-object-tracking-yunhao-li-et-al-2024>(57/77 | 166/270) Beyond MOT: Semantic Multi-Object Tracking (Yunhao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunhao Li, Hao Wang, Xue Ma, Jiali Yao, Shaohua Dong, Heng Fan, Libo Zhang. (2024)<br><strong>Beyond MOT: Semantic Multi-Object Tracking</strong><br><button class=copy-to-clipboard title="Beyond MOT: Semantic Multi-Object Tracking" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05021v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05021v2.pdf filename=2403.05021v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current multi-object tracking (MOT) aims to predict trajectories of targets (i.e.,&ldquo;where&rdquo;) in videos. Yet, knowing merely &ldquo;where&rdquo; is insufficient in many crucial applications. In comparison, semantic understanding such as fine-grained behaviors, interactions, and overall <b>summarized</b> captions (i.e., &ldquo;what&rdquo;) from videos, associated with &ldquo;where&rdquo;, is highly-desired for comprehensive video analysis. Thus motivated, we introduce Semantic Multi-Object Tracking (SMOT), that aims to estimate object trajectories and meanwhile understand semantic details of associated trajectories including instance captions, instance interactions, and overall video captions, integrating &ldquo;where&rdquo; and &ldquo;what&rdquo; for tracking. In order to foster the exploration of SMOT, we propose BenSMOT, a large-scale <b>Benchmark</b> for Semantic MOT. Specifically, BenSMOT comprises 3,292 videos with 151K frames, covering various scenarios for semantic tracking of humans. BenSMOT provides annotations for the trajectories of targets, along with associated instance captions in natural language, instance interactions, and overall caption for each video sequence. To our best knowledge, BenSMOT is the first publicly available <b>benchmark</b> for SMOT. Besides, to encourage future research, we present a novel tracker named SMOTer, which is specially designed and end-to-end trained for SMOT, showing promising performance. By releasing BenSMOT, we expect to go beyond conventional MOT by predicting &ldquo;where&rdquo; and &ldquo;what&rdquo; for SMOT, opening up a new direction in tracking for video understanding. Our BenSMOT and SMOTer will be released.</p></p class="citation"></blockquote><h3 id=5877--167270-diffclass-diffusion-based-class-incremental-learning-zichong-meng-et-al-2024>(58/77 | 167/270) DiffClass: Diffusion-Based Class Incremental Learning (Zichong Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zichong Meng, Jie Zhang, Changdi Yang, Zheng Zhan, Pu Zhao, Yanzhi WAng. (2024)<br><strong>DiffClass: Diffusion-Based Class Incremental Learning</strong><br><button class=copy-to-clipboard title="DiffClass: Diffusion-Based Class Incremental Learning" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05016v1.pdf filename=2403.05016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class Incremental Learning (CIL) is challenging due to catastrophic forgetting. On top of that, Exemplar-free Class Incremental Learning is even more challenging due to forbidden access to previous task data. Recent exemplar-free CIL methods attempt to mitigate catastrophic forgetting by synthesizing previous task data. However, they fail to overcome the catastrophic forgetting due to the inability to deal with the significant domain gap between real and synthetic data. To overcome these issues, we propose a novel exemplar-free CIL method. Our method adopts multi-distribution matching (MDM) <b>diffusion</b> <b>models</b> to unify quality and bridge domain gaps among all domains of training data. Moreover, our approach integrates selective synthetic image augmentation (SSIA) to expand the distribution of the training data, thereby improving the model&rsquo;s plasticity and reinforcing the performance of our method&rsquo;s ultimate component, multi-domain adaptation (MDA). With the proposed integrations, our method then reformulates exemplar-free CIL into a multi-domain adaptation problem to implicitly address the domain gap problem to enhance model stability during incremental training. Extensive experiments on <b>benchmark</b> class incremental datasets and settings demonstrate that our method excels previous exemplar-free CIL methods and achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=5977--168270-unlocking-the-potential-of-multimodal-unified-discrete-representation-through-training-free-codebook-optimization-and-hierarchical-alignment-hai-huang-et-al-2024>(59/77 | 168/270) Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment (Hai Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hai Huang, Yan Xia, Shengpeng Ji, Shulei Wang, Hanting Wang, Jieming Zhu, Zhenhua Dong, Zhou Zhao. (2024)<br><strong>Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment</strong><br><button class=copy-to-clipboard title="Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 11<br>Keywords: Multi-modal, Multi-modal, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05168v1.pdf filename=2403.05168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>representation</b> <b>learning</b> have demonstrated the significance of <b>multimodal</b> alignment. The Dual Cross-modal Information Disentanglement (DCID) model, utilizing a unified codebook, shows promising results in achieving fine-grained <b>representation</b> <b>and</b> cross-modal generalization. However, it is still hindered by equal treatment of all channels and neglect of minor event information, resulting in interference from irrelevant channels and limited performance in fine-grained tasks. Thus, in this work, We propose a Training-free Optimization of Codebook (TOC) method to enhance model performance by selecting important channels in the unified space without retraining. Additionally, we introduce the Hierarchical Dual Cross-modal Information Disentanglement (H-DCID) approach to extend information separation and alignment to two levels, capturing more cross-modal details. The experiment results demonstrate significant improvements across various downstream tasks, with TOC contributing to an average improvement of 1.70% for DCID on four tasks, and H-DCID surpassing DCID by an average of 3.64%. The combination of TOC and H-DCID further enhances performance, exceeding DCID by 4.43%. These findings highlight the effectiveness of our methods in facilitating robust and nuanced cross-modal learning, opening avenues for future enhancements. The source code and pre-trained models can be accessed at <a href=https://github.com/haihuangcode/TOC_H-DCID>https://github.com/haihuangcode/TOC_H-DCID</a>.</p></p class="citation"></blockquote><h3 id=6077--169270-not-just-birds-and-cars-generic-scalable-and-explainable-models-for-professional-visual-recognition-junde-wu-et-al-2024>(60/77 | 169/270) Not just Birds and Cars: Generic, Scalable and Explainable Models for Professional Visual Recognition (Junde Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junde Wu, Jiayuan Zhu, Min Xu, Yueming Jin. (2024)<br><strong>Not just Birds and Cars: Generic, Scalable and Explainable Models for Professional Visual Recognition</strong><br><button class=copy-to-clipboard title="Not just Birds and Cars: Generic, Scalable and Explainable Models for Professional Visual Recognition" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05703v1.pdf filename=2403.05703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Some visual recognition tasks are more challenging then the general ones as they require professional categories of images. The previous efforts, like fine-grained vision classification, primarily introduced models tailored to specific tasks, like identifying bird species or car brands with limited scalability and generalizability. This paper aims to design a scalable and explainable model to solve Professional Visual Recognition tasks from a generic standpoint. We introduce a biologically-inspired structure named Pro-NeXt and reveal that Pro-NeXt exhibits substantial generalizability across diverse professional fields such as fashion, medicine, and art-areas previously considered disparate. Our basic-sized Pro-NeXt-B surpasses all preceding task-specific models across 12 distinct datasets within 5 diverse domains. Furthermore, we find its good scaling property that scaling up Pro-NeXt in depth and width with increasing GFlops can consistently enhances its accuracy. Beyond scalability and adaptability, the intermediate features of Pro-NeXt achieve reliable <b>object</b> <b>detection</b> and segmentation performance without extra training, highlighting its solid explainability. We will release the code to foster further research in this area.</p></p class="citation"></blockquote><h3 id=6177--170270-generalized-correspondence-matching-via-flexible-hierarchical-refinement-and-patch-descriptor-distillation-yu-han-et-al-2024>(61/77 | 170/270) Generalized Correspondence Matching via Flexible Hierarchical Refinement and Patch Descriptor Distillation (Yu Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Han, Ziwei Long, Yanting Zhang, Jin Wu, Zhijun Fang, Rui Fan. (2024)<br><strong>Generalized Correspondence Matching via Flexible Hierarchical Refinement and Patch Descriptor Distillation</strong><br><button class=copy-to-clipboard title="Generalized Correspondence Matching via Flexible Hierarchical Refinement and Patch Descriptor Distillation" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05388v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05388v1.pdf filename=2403.05388v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Correspondence matching plays a crucial role in numerous robotics applications. In comparison to conventional hand-crafted methods and recent data-driven approaches, there is significant interest in plug-and-play algorithms that make full use of pre-trained backbone networks for multi-scale feature extraction and leverage hierarchical refinement strategies to generate matched correspondences. The primary focus of this paper is to address the limitations of deep feature matching (DFM), a state-of-the-art (SoTA) plug-and-play correspondence matching approach. First, we eliminate the pre-defined threshold employed in the hierarchical refinement process of DFM by leveraging a more flexible nearest neighbor search strategy, thereby preventing the exclusion of repetitive yet valid matches during the early stages. Our second technical contribution is the integration of a patch descriptor, which extends the applicability of DFM to accommodate a wide range of backbone networks pre-trained across diverse computer vision tasks, including image classification, semantic segmentation, and stereo matching. Taking into account the practical applicability of our method in real-world robotics applications, we also propose a novel patch descriptor <b>distillation</b> strategy to further reduce the computational complexity of correspondence matching. Extensive experiments conducted on three public datasets demonstrate the superior performance of our proposed method. Specifically, it achieves an overall performance in terms of mean matching accuracy of 0.68, 0.92, and 0.95 with respect to the tolerances of 1, 3, and 5 pixels, respectively, on the HPatches dataset, outperforming all other SoTA algorithms. Our source code, demo video, and supplement are publicly available at mias.group/GCM.</p></p class="citation"></blockquote><h3 id=6277--171270-enhancing-plausibility-evaluation-for-generated-designs-with-denoising-autoencoder-jiajie-fan-et-al-2024>(62/77 | 171/270) Enhancing Plausibility Evaluation for Generated Designs with Denoising Autoencoder (Jiajie Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajie Fan, Amal Trigui, Thomas Bäck, Hao Wang. (2024)<br><strong>Enhancing Plausibility Evaluation for Generated Designs with Denoising Autoencoder</strong><br><button class=copy-to-clipboard title="Enhancing Plausibility Evaluation for Generated Designs with Denoising Autoencoder" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05352v1.pdf filename=2403.05352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A great interest has arisen in using Deep Generative Models (DGM) for generative design. When assessing the quality of the generated designs, human designers focus more on structural plausibility, e.g., no missing component, rather than visual artifacts, e.g., noises in the images. Meanwhile, commonly used metrics such as Fr'echet Inception Distance (FID) may not evaluate accurately as they tend to penalize visual artifacts instead of structural implausibility. As such, FID might not be suitable to assess the performance of DGMs for a generative design task. In this work, we propose to encode the input designs with a simple Denoising <b>Autoencoder</b> (DAE) and measure the distribution distance in the latent space thereof. We experimentally test our DAE-based metrics with FID and other state-of-the-art metrics on three data sets: compared to FID and some more recent works, e.g., FD$_\text{DINO-V2}$ and topology distance, DAE-based metrics can effectively detect implausible structures and are more consistent with structural inspection by human experts.</p></p class="citation"></blockquote><h3 id=6377--172270-cross-modal-and-uni-modal-soft-label-alignment-for-image-text-retrieval-hailang-huang-et-al-2024>(63/77 | 172/270) Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval (Hailang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hailang Huang, Zhijie Nie, Ziqiao Wang, Ziyu Shang. (2024)<br><strong>Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval</strong><br><button class=copy-to-clipboard title="Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05261v1.pdf filename=2403.05261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>image-text</b> retrieval methods have demonstrated impressive performance in recent years. However, they still face two problems: the inter-modal matching missing problem and the intra-modal semantic loss problem. These problems can significantly affect the accuracy of <b>image-text</b> retrieval. To address these challenges, we propose a novel method called Cross-modal and Uni-modal Soft-label Alignment (CUSA). Our method leverages the power of uni-modal pre-trained models to provide soft-label supervision signals for the <b>image-text</b> retrieval model. Additionally, we introduce two alignment techniques, Cross-modal Soft-label Alignment (CSA) and Uni-modal Soft-label Alignment (USA), to overcome false negatives and enhance similarity recognition between uni-modal samples. Our method is designed to be plug-and-play, meaning it can be easily applied to existing <b>image-text</b> retrieval models without changing their original architectures. Extensive experiments on various <b>image-text</b> retrieval models and datasets, we demonstrate that our method can consistently improve the performance of <b>image-text</b> retrieval and achieve new state-of-the-art results. Furthermore, our method can also boost the uni-modal retrieval performance of <b>image-text</b> retrieval models, enabling it to achieve universal retrieval. The code and supplementary files can be found at <a href=https://github.com/lerogo/aaai24_itr_cusa>https://github.com/lerogo/aaai24_itr_cusa</a>.</p></p class="citation"></blockquote><h3 id=6477--173270-hide-in-thicket-generating-imperceptible-and-rational-adversarial-perturbations-on-3d-point-clouds-tianrui-lou-et-al-2024>(64/77 | 173/270) Hide in Thicket: Generating Imperceptible and Rational Adversarial Perturbations on 3D Point Clouds (Tianrui Lou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianrui Lou, Xiaojun Jia, Jindong Gu, Li Liu, Siyuan Liang, Bangyan He, Xiaochun Cao. (2024)<br><strong>Hide in Thicket: Generating Imperceptible and Rational Adversarial Perturbations on 3D Point Clouds</strong><br><button class=copy-to-clipboard title="Hide in Thicket: Generating Imperceptible and Rational Adversarial Perturbations on 3D Point Clouds" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05247v1.pdf filename=2403.05247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>attack</b> methods based on point manipulation for 3D point cloud classification have revealed the fragility of 3D models, yet the <b>adversarial</b> <b>examples</b> they produce are easily perceived or defended against. The trade-off between the imperceptibility and <b>adversarial</b> <b>strength</b> leads most point attack methods to inevitably introduce easily detectable outlier points upon a successful attack. Another promising strategy, shape-based attack, can effectively eliminate outliers, but existing methods often suffer significant reductions in imperceptibility due to irrational deformations. We find that concealing deformation perturbations in areas insensitive to human eyes can achieve a better trade-off between imperceptibility and <b>adversarial</b> <b>strength,</b> specifically in parts of the object surface that are complex and exhibit drastic curvature changes. Therefore, we propose a novel shape-based <b>adversarial</b> <b>attack</b> method, HiT-ADV, which initially conducts a two-stage search for attack regions based on saliency and imperceptibility scores, and then adds deformation perturbations in each attack region using Gaussian kernel functions. Additionally, HiT-ADV is extendable to physical attack. We propose that by employing benign resampling and benign rigid transformations, we can further enhance physical <b>adversarial</b> <b>strength</b> with little sacrifice to imperceptibility. Extensive experiments have validated the superiority of our method in terms of <b>adversarial</b> <b>and</b> imperceptible properties in both digital and physical spaces. Our code is avaliable at: <a href=https://github.com/TRLou/HiT-ADV>https://github.com/TRLou/HiT-ADV</a>.</p></p class="citation"></blockquote><h3 id=6577--174270-learning-expressive-and-generalizable-motion-features-for-face-forgery-detection-jingyi-zhang-et-al-2024>(65/77 | 174/270) Learning Expressive And Generalizable Motion Features For Face Forgery Detection (Jingyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyi Zhang, Peng Zhang, Jingjing Wang, Di Xie, Shiliang Pu. (2024)<br><strong>Learning Expressive And Generalizable Motion Features For Face Forgery Detection</strong><br><button class=copy-to-clipboard title="Learning Expressive And Generalizable Motion Features For Face Forgery Detection" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05172v1.pdf filename=2403.05172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous face forgery detection methods mainly focus on appearance features, which may be easily attacked by sophisticated manipulation. Considering the majority of current face manipulation methods generate fake faces based on a single frame, which do not take frame consistency and coordination into consideration, artifacts on frame sequences are more effective for face forgery detection. However, current sequence-based face forgery detection methods use general video classification networks directly, which discard the special and discriminative motion information for face manipulation detection. To this end, we propose an effective sequence-based forgery detection framework based on an existing video classification method. To make the motion features more expressive for manipulation detection, we propose an alternative motion consistency block instead of the original motion features module. To make the learned features more generalizable, we propose an auxiliary <b>anomaly</b> <b>detection</b> block. With these two specially designed improvements, we make a general video classification network achieve promising results on three popular face forgery datasets.</p></p class="citation"></blockquote><h3 id=6677--175270-diffult-how-to-make-diffusion-model-useful-for-long-tail-recognition-jie-shao-et-al-2024>(66/77 | 175/270) DiffuLT: How to Make Diffusion Model Useful for Long-tail Recognition (Jie Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Shao, Ke Zhu, Hanxiao Zhang, Jianxin Wu. (2024)<br><strong>DiffuLT: How to Make Diffusion Model Useful for Long-tail Recognition</strong><br><button class=copy-to-clipboard title="DiffuLT: How to Make Diffusion Model Useful for Long-tail Recognition" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05170v1.pdf filename=2403.05170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a new pipeline for long-tail (LT) recognition. Instead of re-weighting or re-sampling, we utilize the long-tailed dataset itself to generate a balanced proxy that can be optimized through cross-entropy (CE). Specifically, a randomly initialized <b>diffusion</b> <b>model,</b> trained exclusively on the long-tailed dataset, is employed to synthesize new samples for underrepresented classes. Then, we utilize the inherent information in the original dataset to filter out harmful samples and keep the useful ones. Our strategy, <b>Diffusion</b> <b>model</b> for Long-Tail recognition (DiffuLT), represents a pioneering utilization of generative models in long-tail recognition. DiffuLT achieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, surpassing the best competitors with non-trivial margins. Abundant ablations make our pipeline interpretable, too. The whole generation pipeline is done without any external data or pre-trained model weights, making it highly generalizable to real-world long-tailed settings.</p></p class="citation"></blockquote><h3 id=6777--176270-gsedit-efficient-text-guided-editing-of-3d-objects-via-gaussian-splatting-francesco-palandra-et-al-2024>(67/77 | 176/270) GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting (Francesco Palandra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Palandra, Andrea Sanchietti, Daniele Baieri, Emanuele Rodolà. (2024)<br><strong>GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting</strong><br><button class=copy-to-clipboard title="GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T45, I-2-10; I-3-8, cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05154v1.pdf filename=2403.05154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present GSEdit, a pipeline for text-guided 3D object editing based on Gaussian Splatting models. Our method enables the editing of the style and appearance of 3D objects without altering their main details, all in a matter of minutes on consumer hardware. We tackle the problem by leveraging Gaussian splatting to represent 3D scenes, and we optimize the model while progressively varying the image supervision by means of a pretrained image-based <b>diffusion</b> <b>model.</b> The input object may be given as a 3D triangular mesh, or directly provided as Gaussians from a generative model such as DreamGaussian. GSEdit ensures consistency across different viewpoints, maintaining the integrity of the original object&rsquo;s information. Compared to previously proposed methods relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making 3D editing tasks much faster. Our editing process is refined via the application of the SDS loss, ensuring that our edits are both precise and accurate. Our comprehensive evaluation demonstrates that GSEdit effectively alters object shape and appearance following the given textual instructions while preserving their coherence and detail.</p></p class="citation"></blockquote><h3 id=6877--177270-enhancing-texture-generation-with-high-fidelity-using-advanced-texture-priors-kuo-xu-et-al-2024>(68/77 | 177/270) Enhancing Texture Generation with High-Fidelity Using Advanced Texture Priors (Kuo Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kuo Xu, Maoyu Wang, Muyu Wang, Lincong Feng, Tianhui Zhang, Xiaoli Liu. (2024)<br><strong>Enhancing Texture Generation with High-Fidelity Using Advanced Texture Priors</strong><br><button class=copy-to-clipboard title="Enhancing Texture Generation with High-Fidelity Using Advanced Texture Priors" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05102v1.pdf filename=2403.05102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent advancements in 2D generation technology have sparked a widespread discussion on using 2D priors for 3D shape and texture content generation. However, these methods often overlook the subsequent user operations, such as texture aliasing and blurring that occur when the user acquires the 3D model and simplifies its structure. Traditional graphics methods partially alleviate this issue, but recent texture synthesis technologies fail to ensure consistency with the original model&rsquo;s appearance and cannot achieve high-fidelity restoration. Moreover, background noise frequently arises in high-resolution texture synthesis, limiting the practical application of these generation technologies.In this work, we propose a high-resolution and high-fidelity texture restoration technique that uses the rough texture as the initial input to enhance the consistency between the synthetic texture and the initial texture, thereby overcoming the issues of aliasing and blurring caused by the user&rsquo;s structure simplification operations. Additionally, we introduce a background noise smoothing technique based on a <b>self-supervised</b> scheme to address the noise problem in current high-resolution texture synthesis schemes. Our approach enables high-resolution texture synthesis, paving the way for high-definition and high-detail texture synthesis technology. Experiments demonstrate that our scheme outperforms currently known schemes in high-fidelity texture recovery under high-resolution conditions.</p></p class="citation"></blockquote><h3 id=6977--178270-improving-diffusion-based-generative-models-via-approximated-optimal-transport-daegyu-kim-et-al-2024>(69/77 | 178/270) Improving Diffusion-Based Generative Models via Approximated Optimal Transport (Daegyu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daegyu Kim, Jooyoung Choi, Chaehun Shin, Uiwon Hwang, Sungroh Yoon. (2024)<br><strong>Improving Diffusion-Based Generative Models via Approximated Optimal Transport</strong><br><button class=copy-to-clipboard title="Improving Diffusion-Based Generative Models via Approximated Optimal Transport" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05069v1.pdf filename=2403.05069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the Approximated Optimal Transport (AOT) technique, a novel training scheme for <b>diffusion-based</b> <b>generative</b> models. Our approach aims to approximate and integrate optimal transport into the training process, significantly enhancing the ability of <b>diffusion</b> <b>models</b> to estimate the denoiser outputs accurately. This improvement leads to ODE trajectories of <b>diffusion</b> <b>models</b> with lower curvature and reduced truncation errors during sampling. We achieve superior image quality and reduced sampling steps by employing AOT in training. Specifically, we achieve FID scores of 1.88 with just 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional generations, respectively. Furthermore, when applying AOT to train the discriminator for guidance, we establish new state-of-the-art FID scores of 1.68 and 1.58 for unconditional and conditional generations, respectively, each with 29 NFEs. This outcome demonstrates the effectiveness of AOT in enhancing the performance of <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=7077--179270-primecomposer-faster-progressively-combined-diffusion-for-image-composition-with-attention-steering-yibin-wang-et-al-2024>(70/77 | 179/270) PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering (Yibin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin. (2024)<br><strong>PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering</strong><br><button class=copy-to-clipboard title="PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05053v1.pdf filename=2403.05053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image composition involves seamlessly integrating given objects into a specific visual context. The current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion in synthesis and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only slows down inference but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster training-free diffuser that composites the images by well-designed attention steering across different noise levels. This steering is predominantly achieved by our Correlation Diffuser, utilizing its <b>self-attention</b> layers at each step. Within these layers, the synthesized subject interacts with both the referenced object and background, capturing intricate details and coherent relationships. This prior information is encoded into the attention weights, which are then integrated into the <b>self-attention</b> layers of the generator to guide the synthesis process. Besides, we introduce a Region-constrained Cross-Attention to confine the impact of specific subject-related words to desired regions, addressing the unwanted artifacts shown in the prior method thereby further improving the coherence in the transition area. Our method exhibits the fastest inference efficiency and extensive experiments demonstrate our superiority both qualitatively and quantitatively.</p></p class="citation"></blockquote><h3 id=7177--180270-ditto-dual-and-integrated-latent-topologies-for-implicit-3d-reconstruction-jaehyeok-shim-et-al-2024>(71/77 | 180/270) DITTO: Dual and Integrated Latent Topologies for Implicit 3D Reconstruction (Jaehyeok Shim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaehyeok Shim, Kyungdon Joo. (2024)<br><strong>DITTO: Dual and Integrated Latent Topologies for Implicit 3D Reconstruction</strong><br><button class=copy-to-clipboard title="DITTO: Dual and Integrated Latent Topologies for Implicit 3D Reconstruction" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05005v1.pdf filename=2403.05005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel concept of dual and integrated latent topologies (DITTO in short) for implicit 3D reconstruction from noisy and sparse point clouds. Most existing methods predominantly focus on single latent type, such as point or grid latents. In contrast, the proposed DITTO leverages both point and grid latents (i.e., dual latent) to enhance their strengths, the stability of grid latents and the detail-rich capability of point latents. Concretely, DITTO consists of dual latent encoder and integrated implicit decoder. In the dual latent encoder, a dual latent layer, which is the key module block composing the encoder, refines both latents in parallel, maintaining their distinct shapes and enabling recursive interaction. Notably, a newly proposed dynamic sparse point <b>transformer</b> within the dual latent layer effectively refines point latents. Then, the integrated implicit decoder systematically combines these refined latents, achieving high-fidelity 3D reconstruction and surpassing previous state-of-the-art methods on object- and scene-level datasets, especially in thin and detailed structures.</p></p class="citation"></blockquote><h3 id=7277--181270-evd4uav-an-altitude-sensitive-benchmark-to-evade-vehicle-detection-in-uav-huiming-sun-et-al-2024>(72/77 | 181/270) EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in UAV (Huiming Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huiming Sun, Jiacheng Guo, Zibo Meng, Tianyun Zhang, Jianwu Fang, Yuewei Lin, Hongkai Yu. (2024)<br><strong>EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in UAV</strong><br><button class=copy-to-clipboard title="EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in UAV" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Benchmarking, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05422v1.pdf filename=2403.05422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vehicle detection in Unmanned Aerial Vehicle (UAV) captured images has wide applications in aerial photography and remote sensing. There are many public <b>benchmark</b> datasets proposed for the vehicle detection and tracking in UAV images. Recent studies show that adding an adversarial patch on objects can fool the well-trained deep neural networks based object detectors, posing security concerns to the downstream tasks. However, the current public UAV datasets might ignore the diverse altitudes, vehicle attributes, fine-grained instance-level annotation in mostly side view with blurred vehicle roof, so none of them is good to study the adversarial patch based vehicle detection attack problem. In this paper, we propose a new dataset named EVD4UAV as an altitude-sensitive <b>benchmark</b> to evade vehicle detection in UAV with 6,284 images and 90,886 fine-grained annotated vehicles. The EVD4UAV dataset has diverse altitudes (50m, 70m, 90m), vehicle attributes (color, type), fine-grained annotation (horizontal and rotated bounding boxes, instance-level mask) in top view with clear vehicle roof. One white-box and two <b>black-box</b> <b>patch</b> based attack methods are implemented to attack three classic deep neural networks based object detectors on EVD4UAV. The experimental results show that these representative attack methods could not achieve the robust altitude-insensitive attack performance.</p></p class="citation"></blockquote><h3 id=7377--182270-occfusion-depth-estimation-free-multi-sensor-fusion-for-3d-occupancy-prediction-ji-zhang-et-al-2024>(73/77 | 182/270) OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction (Ji Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ji Zhang, Yiran Ding. (2024)<br><strong>OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction</strong><br><button class=copy-to-clipboard title="OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05329v1.pdf filename=2403.05329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D occupancy prediction based on multi-sensor fusion, crucial for a reliable autonomous driving system, enables fine-grained understanding of 3D scenes. Previous fusion-based 3D occupancy predictions relied on depth estimation for processing 2D image features. However, depth estimation is an ill-posed problem, hindering the accuracy and robustness of these methods. Furthermore, fine-grained occupancy prediction demands extensive computational resources. We introduce OccFusion, a <b>multi-modal</b> fusion method free from depth estimation, and a corresponding point cloud sampling algorithm for dense integration of image features. Building on this, we propose an active training method and an active coarse to fine pipeline, enabling the model to adaptively learn more from complex samples and optimize predictions specifically for challenging areas such as small or overlapping objects. The active methods we propose can be naturally extended to any occupancy prediction model. Experiments on the OpenOccupancy <b>benchmark</b> show our method surpasses existing state-of-the-art (SOTA) <b>multi-modal</b> methods in IoU across all categories. Additionally, our model is more efficient during both the training and inference phases, requiring far fewer computational resources. Comprehensive ablation studies demonstrate the effectiveness of our proposed techniques.</p></p class="citation"></blockquote><h3 id=7477--183270-arbitrary-scale-point-cloud-upsampling-by-voxel-based-network-with-latent-geometric-consistent-learning-hang-du-et-al-2024>(74/77 | 183/270) Arbitrary-Scale Point Cloud Upsampling by Voxel-Based Network with Latent Geometric-Consistent Learning (Hang Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Du, Xuejun Yan, Jingjing Wang, Di Xie, Shiliang Pu. (2024)<br><strong>Arbitrary-Scale Point Cloud Upsampling by Voxel-Based Network with Latent Geometric-Consistent Learning</strong><br><button class=copy-to-clipboard title="Arbitrary-Scale Point Cloud Upsampling by Voxel-Based Network with Latent Geometric-Consistent Learning" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05117v1.pdf filename=2403.05117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, arbitrary-scale point cloud upsampling mechanism became increasingly popular due to its efficiency and convenience for practical applications. To achieve this, most previous approaches formulate it as a problem of surface approximation and employ point-based networks to learn surface representations. However, learning surfaces from sparse point clouds is more challenging, and thus they often suffer from the low-fidelity <b>geometry</b> approximation. To address it, we propose an arbitrary-scale Point cloud Upsampling framework using Voxel-based Network (\textbf{PU-VoxelNet}). Thanks to the completeness and regularity inherited from the voxel representation, voxel-based networks are capable of providing predefined grid space to approximate 3D surface, and an arbitrary number of points can be reconstructed according to the predicted density distribution within each grid cell. However, we investigate the inaccurate grid sampling caused by imprecise density predictions. To address this issue, a density-guided grid resampling method is developed to generate high-fidelity points while effectively avoiding sampling outliers. Further, to improve the fine-grained details, we present an auxiliary training supervision to enforce the latent geometric consistency among local surface patches. Extensive experiments indicate the proposed approach outperforms the state-of-the-art approaches not only in terms of fixed upsampling rates but also for arbitrary-scale upsampling.</p></p class="citation"></blockquote><h3 id=7577--184270-decoupling-degradations-with-recurrent-network-for-video-restoration-in-under-display-camera-chengxu-liu-et-al-2024>(75/77 | 184/270) Decoupling Degradations with Recurrent Network for Video Restoration in Under-Display Camera (Chengxu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengxu Liu, Xuan Wang, Yuanting Fan, Shuai Li, Xueming Qian. (2024)<br><strong>Decoupling Degradations with Recurrent Network for Video Restoration in Under-Display Camera</strong><br><button class=copy-to-clipboard title="Decoupling Degradations with Recurrent Network for Video Restoration in Under-Display Camera" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05660v1.pdf filename=2403.05660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Under-display camera (UDC) systems are the foundation of full-screen display devices in which the lens mounts under the display. The pixel array of light-emitting diodes used for display diffracts and attenuates incident light, causing various degradations as the light intensity changes. Unlike general video restoration which recovers video by treating different degradation factors equally, video restoration for UDC systems is more challenging that concerns removing diverse degradation over time while preserving temporal consistency. In this paper, we introduce a novel video restoration network, called D$^2$RNet, specifically designed for UDC systems. It employs a set of Decoupling Attention Modules (DAM) that effectively separate the various video degradation factors. More specifically, a soft mask generation function is proposed to formulate each frame into flare and haze based on the diffraction arising from incident light of different intensities, followed by the proposed flare and haze removal components that leverage long- and short-term feature learning to handle the respective degradations. Such a design offers an targeted and effective solution to eliminating various types of degradation in UDC systems. We further extend our design into multi-scale to overcome the scale-changing of degradation that often occur in long-range videos. To demonstrate the superiority of D$^2$RNet, we propose a large-scale UDC video <b>benchmark</b> by gathering HDR videos and generating realistically degraded videos using the point spread function measured by a commercial UDC system. Extensive quantitative and qualitative evaluations demonstrate the superiority of D$^2$RNet compared to other state-of-the-art video restoration and UDC image restoration methods. Code is available at <a href=https://github.com/ChengxuLiu/DDRNet.git>https://github.com/ChengxuLiu/DDRNet.git</a></p></p class="citation"></blockquote><h3 id=7677--185270-lvic-multi-modality-segmentation-by-lifting-visual-info-as-cue-zichao-dong-et-al-2024>(76/77 | 185/270) LVIC: Multi-modality segmentation by Lifting Visual Info as Cue (Zichao Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zichao Dong, Bowen Pang, Xufeng Huang, Hang Ji, Xin Zhan, Junbo Chen. (2024)<br><strong>LVIC: Multi-modality segmentation by Lifting Visual Info as Cue</strong><br><button class=copy-to-clipboard title="LVIC: Multi-modality segmentation by Lifting Visual Info as Cue" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05159v1.pdf filename=2403.05159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-modality fusion is proven an effective method for 3d perception for autonomous driving. However, most current multi-modality fusion pipelines for LiDAR semantic segmentation have complicated fusion mechanisms. Point painting is a quite straight forward method which directly bind LiDAR points with visual information. Unfortunately, previous point painting like methods suffer from projection error between camera and LiDAR. In our experiments, we find that this projection error is the devil in point painting. As a result of that, we propose a depth aware point painting mechanism, which significantly boosts the multi-modality fusion. Apart from that, we take a deeper look at the desired visual feature for LiDAR to operate semantic segmentation. By Lifting Visual Information as Cue, LVIC ranks 1st on nuScenes LiDAR semantic segmentation <b>benchmark.</b> Our experiments show the robustness and effectiveness. Codes would be make publicly available soon.</p></p class="citation"></blockquote><h3 id=7777--186270-reps-reconstruction-based-point-cloud-sampling-guoqing-zhang-et-al-2024>(77/77 | 186/270) REPS: Reconstruction-based Point Cloud Sampling (Guoqing Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoqing Zhang, Wenbo Zhao, Jian Liu, Xianming Liu. (2024)<br><strong>REPS: Reconstruction-based Point Cloud Sampling</strong><br><button class=copy-to-clipboard title="REPS: Reconstruction-based Point Cloud Sampling" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05047v1.pdf filename=2403.05047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling is widely used in various point cloud tasks as it can effectively reduce resource consumption. Recently, some methods have proposed utilizing neural networks to optimize the sampling process for various task requirements. Currently, deep downsampling methods can be categorized into two main types: generative-based and score-based. Generative-based methods directly generate sampled point clouds using networks, whereas score-based methods assess the importance of points according to specific rules and then select sampled point clouds based on their scores. However, these methods often result in noticeable <b>clustering</b> effects in high-intensity feature areas, compromising their ability to preserve small-scale features and leading to the loss of some structures, thereby affecting the performance of subsequent tasks. In this paper, we propose REPS, a reconstruction-based scoring strategy that evaluates the importance of each vertex by removing and reconstructing them using surrounding vertices. Our reconstruction process comprises point reconstruction and shape reconstruction. The two aforementioned reconstruction methods effectively evaluate the importance of vertices by removing them at different scales for reconstruction. These reconstructions ensure that our method maintains the overall geometric features of the point cloud and avoids disturbing small-scale structures during sampling. Additionally, we propose the Global-Local Fusion Attention (GLFA) module, which aggregates local and global attention features of point clouds, ensuring high-quality reconstruction and sampling effects. Our method outperforms previous approaches in preserving the structural features of the sampled point clouds. Furthermore, abundant experimental results demonstrate the superior performance of our method across various common tasks.</p></p class="citation"></blockquote><h2 id=csmm-2>cs.MM (2)</h2><h3 id=12--187270-multimodal-infusion-tuning-for-large-models-hao-sun-et-al-2024>(1/2 | 187/270) Multimodal Infusion Tuning for Large Models (Hao Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Sun, Yu Song, Jihong Hu, Xinyao Yu, Jiaqing Liu, Yen-Wei Chen, Lanfen Lin. (2024)<br><strong>Multimodal Infusion Tuning for Large Models</strong><br><button class=copy-to-clipboard title="Multimodal Infusion Tuning for Large Models" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-HC, cs-MM, cs.MM<br>Keyword Score: 56<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Reasoning, Sentiment Analysis, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05060v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05060v1.pdf filename=2403.05060v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large-scale</b> <b>models</b> <b>have</b> showcased remarkable generalization capabilities in various tasks. However, integrating <b>multimodal</b> processing into these models presents a significant challenge, as it often comes with a high computational burden. To address this challenge, we introduce a new parameter-efficient <b>multimodal</b> tuning strategy for <b>large</b> <b>models</b> <b>in</b> this paper, referred to as <b>Multimodal</b> Infusion Tuning (MiT). MiT leverages decoupled <b>self-attention</b> mechanisms within <b>large</b> <b>language</b> <b>models</b> to effectively integrate information from diverse modalities such as images and acoustics. In MiT, we also design a novel adaptive rescaling strategy at the head level, which optimizes the representation of infused <b>multimodal</b> features. Notably, all <b>foundation</b> <b>models</b> are kept frozen during the tuning process to reduce the computational burden(only 2.5% parameters are tunable). We conduct experiments across a range of <b>multimodal</b> tasks, including image-related tasks like referring segmentation and non-image tasks such as <b>sentiment</b> <b>analysis.</b> Our results showcase that MiT achieves state-of-the-art performance in <b>multimodal</b> understanding while significantly reducing computational overhead(10% of previous methods). Moreover, our tuned model exhibits robust <b>reasoning</b> abilities even in complex scenarios.</p></p class="citation"></blockquote><h3 id=22--188270-towards-real-world-stickers-use-a-new-dataset-for-multi-tag-sticker-recognition-bingbing-wang-et-al-2024>(2/2 | 188/270) Towards Real-World Stickers Use: A New Dataset for Multi-Tag Sticker Recognition (Bingbing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingbing Wang, Bin Liang, Chun-Mei Feng, Wangmeng Zuo, Zhixin Bai, Shijue Huang, Kam-Fai Wong, Ruifeng Xu. (2024)<br><strong>Towards Real-World Stickers Use: A New Dataset for Multi-Tag Sticker Recognition</strong><br><button class=copy-to-clipboard title="Towards Real-World Stickers Use: A New Dataset for Multi-Tag Sticker Recognition" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM<br>Keyword Score: 20<br>Keywords: Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05428v1.pdf filename=2403.05428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world conversations, the diversity and ambiguity of stickers often lead to varied interpretations based on the context, necessitating the requirement for comprehensively understanding stickers and supporting multi-tagging. To address this challenge, we introduce StickerTAG, the first multi-tag sticker dataset comprising a collected tag set with 461 tags and 13,571 sticker-tag pairs, designed to provide a deeper understanding of stickers. Recognizing multiple tags for stickers becomes particularly challenging due to sticker tags usually are fine-grained attribute aware. Hence, we propose an Attentive Attribute-oriented <b>Prompt</b> <b>Learning</b> method, ie, Att$^2$PL, to capture informative features of stickers in a fine-grained manner to better differentiate tags. Specifically, we first apply an Attribute-oriented Description Generation (ADG) module to obtain the description for stickers from four attributes. Then, a Local Re-attention (LoR) module is designed to perceive the importance of local information. Finally, we use <b>prompt</b> <b>learning</b> to guide the recognition process and adopt confidence penalty optimization to penalize the confident output distribution. Extensive experiments show that our method achieves encouraging results for all commonly used metrics.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--189270-text-to-audio-generation-synchronized-with-videos-shentong-mo-et-al-2024>(1/2 | 189/270) Text-to-Audio Generation Synchronized with Videos (Shentong Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shentong Mo, Jing Shi, Yapeng Tian. (2024)<br><strong>Text-to-Audio Generation Synchronized with Videos</strong><br><button class=copy-to-clipboard title="Text-to-Audio Generation Synchronized with Videos" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-CV, cs-LG, cs-MM, cs-SD, cs.SD, eess-AS<br>Keyword Score: 53<br>Keywords: ControlNet, Diffusion Model, Benchmarking, Contrastive Learning, Transformer, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07938v1.pdf filename=2403.07938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent times, the focus on <b>text-to-audio</b> <b>(TTA)</b> generation has intensified, as researchers strive to synthesize audio from textual descriptions. However, most existing methods, though leveraging latent <b>diffusion</b> <b>models</b> to learn the correlation between audio and <b>text</b> <b>embeddings,</b> fall short when it comes to maintaining a seamless synchronization between the produced audio and its video. This often results in discernible audio-visual mismatches. To bridge this gap, we introduce a groundbreaking <b>benchmark</b> for <b>Text-to-Audio</b> <b>generation</b> that aligns with Videos, named T2AV-Bench. This <b>benchmark</b> distinguishes itself with three novel metrics dedicated to evaluating visual alignment and temporal consistency. To complement this, we also present a simple yet effective video-aligned TTA generation model, namely T2AV. Moving beyond traditional methods, T2AV refines the latent <b>diffusion</b> <b>approach</b> by integrating visual-aligned <b>text</b> <b>embeddings</b> as its conditional foundation. It employs a temporal multi-head attention <b>transformer</b> to extract and understand temporal nuances from video data, a feat amplified by our Audio-Visual <b>ControlNet</b> that adeptly merges temporal visual representations with <b>text</b> <b>embeddings.</b> Further enhancing this integration, we weave in a <b>contrastive</b> <b>learning</b> objective, designed to ensure that the visual-aligned <b>text</b> <b>embeddings</b> resonate closely with the audio features. Extensive evaluations on the AudioCaps and T2AV-Bench demonstrate that our T2AV sets a new standard for video-aligned TTA generation in ensuring visual alignment and temporal consistency.</p></p class="citation"></blockquote><h3 id=22--190270-rfwave-multi-band-rectified-flow-for-audio-waveform-reconstruction-peng-liu-et-al-2024>(2/2 | 190/270) RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction (Peng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Liu, Dongyang Dai. (2024)<br><strong>RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction</strong><br><button class=copy-to-clipboard title="RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05010v1.pdf filename=2403.05010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in generative modeling have led to significant progress in audio waveform reconstruction from diverse representations. Although <b>diffusion</b> <b>models</b> have been used for reconstructing audio waveforms, they tend to exhibit latency issues because they operate at the level of individual sample points and require a relatively large number of sampling steps. In this study, we introduce RFWave, a novel multi-band Rectified Flow approach that reconstructs high-fidelity audio waveforms from Mel-spectrograms. RFWave is distinctive for generating complex spectrograms and operating at the frame level, processing all subbands concurrently to enhance efficiency. Thanks to Rectified Flow, which aims for a flat transport trajectory, RFWave requires only 10 sampling steps. Empirical evaluations demonstrate that RFWave achieves exceptional reconstruction quality and superior computational efficiency, capable of generating audio at a speed 90 times faster than real-time.</p></p class="citation"></blockquote><h2 id=physicsapp-ph-1>physics.app-ph (1)</h2><h3 id=11--191270-inverse-design-of-photonic-crystal-surface-emitting-lasers-is-a-sequence-modeling-problem-ceyao-zhang-et-al-2024>(1/1 | 191/270) Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem (Ceyao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ceyao Zhang, Renjie Li, Cheng Zhang, Zhaoyu Zhang, Feng Yin. (2024)<br><strong>Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem</strong><br><button class=copy-to-clipboard title="Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.app-ph<br>Categories: cs-AI, physics-app-ph, physics.app-ph<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Simulation, Simulator, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05149v1.pdf filename=2403.05149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Photonic Crystal Surface Emitting Lasers (PCSEL)&rsquo;s inverse design demands expert knowledge in physics, materials science, and quantum mechanics which is prohibitively labor-intensive. Advanced AI technologies, especially <b>reinforcement</b> <b>learning</b> (RL), have emerged as a powerful tool to augment and accelerate this inverse design process. By modeling the inverse design of PCSEL as a sequential decision-making problem, RL approaches can construct a satisfactory PCSEL structure from scratch. However, the data inefficiency resulting from online interactions with precise and expensive <b>simulation</b> environments impedes the broader applicability of RL approaches. Recently, sequential models, especially the <b>Transformer</b> architecture, have exhibited compelling performance in sequential decision-making problems due to their simplicity and scalability to <b>large</b> <b>language</b> <b>models.</b> In this paper, we introduce a novel framework named PCSEL Inverse Design <b>Transformer</b> (PiT) that abstracts the inverse design of PCSEL as a sequence modeling problem. The central part of our PiT is a <b>Transformer-based</b> structure that leverages the past trajectories and current states to predict the current actions. Compared with the traditional RL approaches, PiT can output the optimal actions and achieve target PCSEL designs by leveraging offline data and conditioning on the desired return. Results demonstrate that PiT achieves superior performance and data efficiency compared to baselines.</p></p class="citation"></blockquote><h2 id=cspl-3>cs.PL (3)</h2><h3 id=13--192270-llm4decompile-decompiling-binary-code-with-large-language-models-hanzhuo-tan-et-al-2024>(1/3 | 192/270) LLM4Decompile: Decompiling Binary Code with Large Language Models (Hanzhuo Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzhuo Tan, Qi Luo, Jing Li, Yuqun Zhang. (2024)<br><strong>LLM4Decompile: Decompiling Binary Code with Large Language Models</strong><br><button class=copy-to-clipboard title="LLM4Decompile: Decompiling Binary Code with Large Language Models" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-CL, cs-PL, cs.PL<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05286v1.pdf filename=2403.05286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> show promise for programming tasks, motivating their application to decompilation. However, there does not exist any open-source <b>LLM</b> for decompilation. Moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. Therefore, we release the first open-access decompilation <b>LLMs</b> ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code. The open-source <b>LLMs</b> can serve as baselines for further development in the field. To ensure practical program evaluation, we introduce Decompile-Eval, the first dataset that considers re-compilability and re-executability for decompilation. The <b>benchmark</b> emphasizes the importance of evaluating the decompilation model from the perspective of program semantics. Experiments indicate that our LLM4Decompile has demonstrated the capability to accurately decompile 21% of the assembly code, which achieves a 50% improvement over <b>GPT-4.</b> Our code, dataset, and models are released at <a href=https://github.com/albertan017/LLM4Decompile>https://github.com/albertan017/LLM4Decompile</a></p></p class="citation"></blockquote><h3 id=23--193270-we-know-i-know-you-know-choreographic-programming-with-multicast-and-multiply-located-values-mako-bates-et-al-2024>(2/3 | 193/270) We Know I Know You Know; Choreographic Programming With Multicast and Multiply Located Values (Mako Bates et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mako Bates, Joseph P. Near. (2024)<br><strong>We Know I Know You Know; Choreographic Programming With Multicast and Multiply Located Values</strong><br><button class=copy-to-clipboard title="We Know I Know You Know; Choreographic Programming With Multicast and Multiply Located Values" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-DC, cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05417v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05417v2.pdf filename=2403.05417v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Concurrent distributed systems are notoriously difficult to construct and reason about. Choreographic programming is a recent paradigm that describes a distributed system in a single global program called a choreography. Choreographies simplify <b>reasoning</b> about distributed systems and can ensure deadlock freedom by static analysis. In previous choreographic programming languages, each value is located at a single party, and the programmer is expected to insert special untyped &ldquo;select&rdquo; operations to ensure that all parties follow the same communication pattern. We present He-Lambda-Small, a new choreographic programming language with Multiply Located Values. He-Lambda-Small allows multicasting to a set of parties, and the resulting value will be located at all of them. This approach enables a simple and elegant alternative to &ldquo;select&rdquo;: He-Lambda-Small requires that the guard for a conditional be located at all of the relevant parties. In He-Lambda-Small, checking that a choreography is well-typed suffices to show that it is deadlock-free. We present several case studies that demonstrate the use of multiply-located values to concisely encode tricky communication patterns described in previous work without the use of &ldquo;select&rdquo; or redundant communication.</p></p class="citation"></blockquote><h3 id=33--194270-modeling-dynamic-deallocations-of-local-memory-for-translation-validation-abhishek-rose-et-al-2024>(3/3 | 194/270) Modeling Dynamic (De)Allocations of Local Memory for Translation Validation (Abhishek Rose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Rose, Sorav Bansal. (2024)<br><strong>Modeling Dynamic (De)Allocations of Local Memory for Translation Validation</strong><br><button class=copy-to-clipboard title="Modeling Dynamic (De)Allocations of Local Memory for Translation Validation" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05302v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05302v2.pdf filename=2403.05302v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-End Translation Validation is the problem of verifying the executable code generated by a compiler against the corresponding input source code for a single compilation. This becomes particularly hard in the presence of dynamically-allocated local memory where addresses of local memory may be observed by the program. In the context of validating the translation of a C procedure to executable code, a validator needs to tackle constant-length local arrays, address-taken local variables, address-taken formal parameters, variable-length local arrays, procedure-call arguments (including variadic arguments), and the alloca() operator. We provide an execution model, a definition of refinement, and an algorithm to soundly convert a refinement check into first-order logic queries that an off-the-shelf SMT solver can handle efficiently. In our experiments, we perform blackbox translation validation of C procedures (with up to 100+ SLOC), involving these local memory allocation constructs, against their corresponding assembly implementations (with up to 200+ instructions) generated by an optimizing compiler with complex loop and vectorizing transformations.</p></p class="citation"></blockquote><h2 id=csro-15>cs.RO (15)</h2><h3 id=115--195270-are-large-language-models-aligned-with-peoples-social-intuitions-for-human-robot-interactions-lennart-wachowiak-et-al-2024>(1/15 | 195/270) Are Large Language Models Aligned with People&rsquo;s Social Intuitions for Human-Robot Interactions? (Lennart Wachowiak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lennart Wachowiak, Andrew Coles, Oya Celiktutan, Gerard Canal. (2024)<br><strong>Are Large Language Models Aligned with People&rsquo;s Social Intuitions for Human-Robot Interactions?</strong><br><button class=copy-to-clipboard title="Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-HC, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05701v1.pdf filename=2403.05701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are increasingly used in robotics, especially for high-level action planning. Meanwhile, many robotics applications involve human supervisors or collaborators. Hence, it is crucial for <b>LLMs</b> to generate socially acceptable actions that align with people&rsquo;s preferences and values. In this work, we test whether <b>LLMs</b> capture people&rsquo;s intuitions about behavior judgments and communication preferences in human-robot interaction (HRI) scenarios. For evaluation, we reproduce three HRI user studies, comparing the output of <b>LLMs</b> with that of real participants. We find that <b>GPT-4</b> strongly outperforms other models, generating answers that correlate strongly with users&rsquo; answers in two studies $\unicode{x2014}$ the first study dealing with selecting the most appropriate communicative act for a robot in various situations ($r_s$ = 0.82), and the second with judging the desirability, intentionality, and surprisingness of behavior ($r_s$ = 0.83). However, for the last study, testing whether people judge the behavior of robots and humans differently, no model achieves strong correlations. Moreover, we show that vision models fail to capture the essence of video stimuli and that <b>LLMs</b> tend to rate different communicative acts and behavior desirability higher than people.</p></p class="citation"></blockquote><h3 id=215--196270-take-your-best-shot-sampling-based-next-best-view-planning-for-autonomous-photography--inspection-shijie-gao-et-al-2024>(2/15 | 196/270) Take Your Best Shot: Sampling-Based Next-Best-View Planning for Autonomous Photography & Inspection (Shijie Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijie Gao, Lauren Bramblett, Nicola Bezzo. (2024)<br><strong>Take Your Best Shot: Sampling-Based Next-Best-View Planning for Autonomous Photography & Inspection</strong><br><button class=copy-to-clipboard title="Take Your Best Shot: Sampling-Based Next-Best-View Planning for Autonomous Photography & Inspection" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Gaussian Process, Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05477v1.pdf filename=2403.05477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous mobile robots (AMRs) equipped with high-quality cameras have revolutionized the field of inspections by providing efficient and cost-effective means of conducting surveys. The use of autonomous inspection is becoming more widespread in a variety of contexts, yet it is still challenging to acquire the best inspection information autonomously. In situations where objects may block a robot&rsquo;s view, it is necessary to use <b>reasoning</b> to determine the optimal points for collecting data. Although researchers have explored cloud-based applications to store inspection data, these applications may not operate optimally under network constraints, and parsing these datasets can be manually intensive. Instead, there is an emerging requirement for AMRs to autonomously capture the most informative views efficiently. To address this challenge, we present an autonomous Next-Best-View (NBV) framework that maximizes the inspection information while reducing the number of pictures needed during operations. The framework consists of a formalized evaluation metric using ray-tracing and <b>Gaussian</b> <b>process</b> interpolation to estimate information reward based on the current understanding of the partially-known environment. A derivative-free optimization (DFO) method is used to sample candidate views in the environment and identify the NBV point. The proposed approach&rsquo;s effectiveness is shown by comparing it with existing methods and further validated through <b>simulations</b> and experiments with various vehicles.</p></p class="citation"></blockquote><h3 id=315--197270-improving-the-successful-robotic-grasp-detection-using-convolutional-neural-networks-hamed-hosseini-et-al-2024>(3/15 | 197/270) Improving the Successful Robotic Grasp Detection Using Convolutional Neural Networks (Hamed Hosseini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Hosseini, Mehdi Tale Masouleh, Ahmad Kalhor. (2024)<br><strong>Improving the Successful Robotic Grasp Detection Using Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Improving the Successful Robotic Grasp Detection Using Convolutional Neural Networks" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Object Detection, Convolution, Convolutional Neural Network, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05211v1.pdf filename=2403.05211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic grasp should be carried out in a real-time manner by proper accuracy. Perception is the first and significant step in this procedure. This paper proposes an improved pipeline model trying to detect grasp as a rectangle representation for different seen or unseen <b>objects.</b> <b>It</b> helps the robot to start control procedures from nearer to the proper part of the <b>object.</b> <b>The</b> main idea consists in pre-processing, output normalization, and <b>data</b> <b>augmentation</b> to improve accuracy by 4.3 percent without making the system slow. Also, a comparison has been conducted over different pre-trained models like AlexNet, ResNet, Vgg19, which are the most famous feature extractors for image processing in <b>object</b> <b>detection.</b> Although AlexNet has less complexity than other ones, it outperformed them, which helps the real-time property.</p></p class="citation"></blockquote><h3 id=415--198270-prepared-for-the-worst-a-learning-based-adversarial-attack-for-resilience-analysis-of-the-icp-algorithm-ziyu-zhang-et-al-2024>(4/15 | 198/270) Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm (Ziyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Zhang, Johann Laconte, Daniil Lisus, Timothy D. Barfoot. (2024)<br><strong>Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm</strong><br><button class=copy-to-clipboard title="Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05666v1.pdf filename=2403.05666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel method to assess the resilience of the Iterative Closest Point (ICP) algorithm via deep-learning-based attacks on lidar point clouds. For safety-critical applications such as autonomous navigation, ensuring the resilience of algorithms prior to deployments is of utmost importance. The ICP algorithm has become the standard for lidar-based localization. However, the pose estimate it produces can be greatly affected by corruption in the measurements. Corruption can arise from a variety of scenarios such as occlusions, adverse weather, or mechanical issues in the sensor. Unfortunately, the complex and iterative nature of ICP makes assessing its resilience to corruption challenging. While there have been efforts to create challenging datasets and develop <b>simulations</b> to evaluate the resilience of ICP empirically, our method focuses on finding the maximum possible ICP pose error using perturbation-based <b>adversarial</b> <b>attacks.</b> The proposed attack induces significant pose errors on ICP and outperforms baselines more than 88% of the time across a wide range of scenarios. As an example application, we demonstrate that our attack can be used to identify areas on a map where ICP is particularly vulnerable to corruption in the measurements.</p></p class="citation"></blockquote><h3 id=515--199270-federated-joint-learning-of-robot-networks-in-stroke-rehabilitation-xinyu-jiang-et-al-2024>(5/15 | 199/270) Federated Joint Learning of Robot Networks in Stroke Rehabilitation (Xinyu Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Jiang, Yibei Guo, Mengsha Hu, Ruoming Jin, Hai Phan, Jay Alberts, Rui Liu. (2024)<br><strong>Federated Joint Learning of Robot Networks in Stroke Rehabilitation</strong><br><button class=copy-to-clipboard title="Federated Joint Learning of Robot Networks in Stroke Rehabilitation" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05472v1.pdf filename=2403.05472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advanced by rich perception and precise execution, robots possess immense potential to provide professional and customized rehabilitation exercises for patients with mobility impairments caused by strokes. Autonomous robotic rehabilitation significantly reduces human workloads in the <b>long</b> <b>and</b> <b>tedious</b> <b>rehabilitation</b> process. However, training a rehabilitation robot is challenging due to the data scarcity issue. This challenge arises from privacy concerns (e.g., the risk of leaking private disease and identity information of patients) during clinical data access and usage. Data from various patients and hospitals cannot be shared for adequate robot training, further compromising rehabilitation safety and limiting implementation scopes. To address this challenge, this work developed a novel federated joint learning (FJL) method to jointly train robots across hospitals. FJL also adopted a <b>long</b> <b>short-term</b> <b>memory</b> <b>network</b> <b>(LSTM)-Transformer</b> learning mechanism to effectively explore the complex tempo-spatial relations among patient mobility conditions and robotic rehabilitation motions. To validate FJL&rsquo;s effectiveness in training a robot network, a clinic-simulation combined experiment was designed. Real rehabilitation exercise data from 200 patients with stroke diseases (upper limb hemiplegia, Parkinson&rsquo;s syndrome, and back pain syndrome) were adopted. Inversely driven by clinical data, 300,000 robotic rehabilitation guidances were simulated. FJL proved to be effective in joint rehabilitation learning, performing 20% - 30% better than baseline methods.</p></p class="citation"></blockquote><h3 id=615--200270-safe-execution-of-learned-orientation-skills-with-conic-control-barrier-functions-zheng-shen-et-al-2024>(6/15 | 200/270) Safe Execution of Learned Orientation Skills with Conic Control Barrier Functions (Zheng Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Shen, Matteo Saveriano, Fares J. Abu-Dakka, Sami Haddadin. (2024)<br><strong>Safe Execution of Learned Orientation Skills with Conic Control Barrier Functions</strong><br><button class=copy-to-clipboard title="Safe Execution of Learned Orientation Skills with Conic Control Barrier Functions" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Human Intervention, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05447v1.pdf filename=2403.05447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of Learning from Demonstration (LfD), Dynamical Systems (DSs) have gained significant attention due to their ability to generate real-time motions and reach predefined targets. However, the conventional convergence-centric behavior exhibited by DSs may fall short in safety-critical tasks, specifically, those requiring precise replication of demonstrated trajectories or strict adherence to constrained regions even in the presence of perturbations or <b>human</b> <b>intervention.</b> Moreover, existing DS research often assumes demonstrations solely in Euclidean space, overlooking the crucial aspect of orientation in various applications. To alleviate these shortcomings, we present an innovative approach geared toward ensuring the safe execution of learned orientation skills within constrained regions surrounding a reference trajectory. This involves learning a stable DS on SO(3), extracting time-varying conic constraints from the variability observed in expert demonstrations, and bounding the evolution of the DS with Conic Control Barrier Function (CCBF) to fulfill the constraints. We validated our approach through extensive evaluation in <b>simulation</b> and showcased its effectiveness for a cutting skill in the context of assisted teleoperation.</p></p class="citation"></blockquote><h3 id=715--201270-embracing-large-language-and-multimodal-models-for-prosthetic-technologies-sharmita-dey-et-al-2024>(7/15 | 201/270) Embracing Large Language and Multimodal Models for Prosthetic Technologies (Sharmita Dey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sharmita Dey, Arndt F. Schilling. (2024)<br><strong>Embracing Large Language and Multimodal Models for Prosthetic Technologies</strong><br><button class=copy-to-clipboard title="Embracing Large Language and Multimodal Models for Prosthetic Technologies" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04974v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04974v2.pdf filename=2403.04974v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents a vision for the future of prosthetic devices, leveraging the advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>Large</b> <b>Multimodal</b> <b>Models</b> (LMMs) to revolutionize the interaction between humans and assistive technologies. Unlike traditional prostheses, which rely on limited and predefined commands, this approach aims to develop intelligent prostheses that understand and respond to users&rsquo; needs through natural language and <b>multimodal</b> inputs. The realization of this vision involves developing a control system capable of understanding and translating a wide array of natural language and <b>multimodal</b> inputs into actionable commands for prosthetic devices. This includes the creation of models that can extract and interpret features from both textual and <b>multimodal</b> data, ensuring devices not only follow user commands but also respond intelligently to the environment and user intent, thus marking a significant leap forward in prosthetic technology.</p></p class="citation"></blockquote><h3 id=815--202270-ocean-an-openspace-collision-free-trajectory-planner-for-autonomous-parking-based-on-admm-dongxu-wang-et-al-2024>(8/15 | 202/270) OCEAN: An Openspace Collision-free Trajectory Planner for Autonomous Parking Based on ADMM (Dongxu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongxu Wang, Yanbin Lu, Weilong Liu, Hao Zuo, Jiade Xin, Xiang Long, Yuncheng Jiang. (2024)<br><strong>OCEAN: An Openspace Collision-free Trajectory Planner for Autonomous Parking Based on ADMM</strong><br><button class=copy-to-clipboard title="OCEAN: An Openspace Collision-free Trajectory Planner for Autonomous Parking Based on ADMM" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05090v1.pdf filename=2403.05090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose an Openspace Collision-freE trAjectory plaNner (OCEAN) for autonomous parking. OCEAN is an optimization-based trajectory planner accelerated by Alternating Direction Method of Multiplier (ADMM) with enhanced computational efficiency and robustness, and is suitable for all scenes with few dynamic obstacles. Starting from a hierarchical optimization-based collision avoidance framework, the trajectory planning problem is first warm-started by a collision-free Hybrid A* trajectory, then the collision avoidance trajectory planning problem is reformulated as a smooth and convex dual form, and solved by ADMM in parallel. The optimization variables are carefully split into several groups so that ADMM sub-problems are formulated as Quadratic Programming (QP), Sequential Quadratic Programming (SQP),and Second Order Cone Programming (SOCP) problems that can be efficiently and robustly solved. We validate our method both in hundreds of <b>simulation</b> scenarios and hundreds of hours of public parking areas. The results show that the proposed method has better system performance compared with other <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=915--203270-interactive-perception-for-deformable-object-manipulation-zehang-weng-et-al-2024>(9/15 | 203/270) Interactive Perception for Deformable Object Manipulation (Zehang Weng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehang Weng, Peng Zhou, Hang Yin, Alexander Kravberg, Anastasiia Varava, David Navarro-Alarcon, Danica Kragic. (2024)<br><strong>Interactive Perception for Deformable Object Manipulation</strong><br><button class=copy-to-clipboard title="Interactive Perception for Deformable Object Manipulation" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05177v1.pdf filename=2403.05177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interactive perception enables robots to manipulate the environment and objects to bring them into states that benefit the perception process. Deformable objects pose challenges to this due to significant manipulation difficulty and occlusion in vision-based perception. In this work, we address such a problem with a setup involving both an active camera and an object manipulator. Our approach is based on a sequential decision-making framework and explicitly considers the motion regularity and structure in coupling the camera and manipulator. We contribute a method for constructing and computing a subspace, called Dynamic Active Vision Space (DAVS), for effectively utilizing the regularity in motion exploration. The effectiveness of the framework and approach are validated in both a <b>simulation</b> and a real dual-arm robot setup. Our results confirm the necessity of an active camera and coordinative motion in interactive perception for deformable objects.</p></p class="citation"></blockquote><h3 id=1015--204270-efficient-data-collection-for-robotic-manipulation-via-compositional-generalization-jensen-gao-et-al-2024>(10/15 | 204/270) Efficient Data Collection for Robotic Manipulation via Compositional Generalization (Jensen Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, Dorsa Sadigh. (2024)<br><strong>Efficient Data Collection for Robotic Manipulation via Compositional Generalization</strong><br><button class=copy-to-clipboard title="Efficient Data Collection for Robotic Manipulation via Compositional Generalization" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05110v1.pdf filename=2403.05110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data collection has become an increasingly important problem in robotic manipulation, yet there still lacks much understanding of how to effectively collect data to facilitate broad generalization. Recent works on large-scale robotic data collection typically vary a wide range of environmental factors during data collection, such as object types and table textures. While these works attempt to cover a diverse variety of scenarios, they do not explicitly account for the possible compositional abilities of policies trained on the data. If robot policies are able to compose different environmental factors of variation (e.g., object types, table heights) from their training data to succeed when encountering unseen factor combinations, then we can exploit this to avoid collecting data for situations that composition would address. To investigate this possibility, we conduct thorough empirical studies both in <b>simulation</b> and on a real robot that compare data collection strategies and assess whether visual imitation learning policies can compose environmental factors. We find that policies do exhibit composition, although leveraging prior robotic datasets is critical for this on a real robot. We use these insights to provide better practices for in-domain data collection by proposing data collection strategies that exploit composition, which can induce better generalization than naive approaches for the same amount of effort during data collection. We further demonstrate that a real robot policy trained on data from such a strategy achieves a success rate of 77.5% when transferred to entirely new environments that encompass unseen combinations of environmental factors, whereas policies trained using data collected without accounting for environmental variation fail to transfer effectively, with a success rate of only 2.5%. We provide videos at <a href=http://iliad.stanford.edu/robot-data-comp/>http://iliad.stanford.edu/robot-data-comp/</a>.</p></p class="citation"></blockquote><h3 id=1115--205270-spatiotemporal-predictive-pre-training-for-robotic-motor-control-jiange-yang-et-al-2024>(11/15 | 205/270) Spatiotemporal Predictive Pre-training for Robotic Motor Control (Jiange Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiange Yang, Bei Liu, Jianlong Fu, Bocheng Pan, Gangshan Wu, Limin Wang. (2024)<br><strong>Spatiotemporal Predictive Pre-training for Robotic Motor Control</strong><br><button class=copy-to-clipboard title="Spatiotemporal Predictive Pre-training for Robotic Motor Control" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05304v1.pdf filename=2403.05304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic motor control necessitates the ability to predict the dynamics of environments and interaction objects. However, advanced <b>self-supervised</b> pre-trained visual representations (PVRs) in robotic motor control, leveraging large-scale egocentric videos, often focus solely on learning the static content features of sampled image frames. This neglects the crucial temporal motion clues in human video data, which implicitly contain key knowledge about sequential interacting and manipulating with the environments and objects. In this paper, we present a simple yet effective robotic motor control visual pre-training framework that jointly performs spatiotemporal predictive learning utilizing large-scale video data, termed as STP. Our STP samples paired frames from video clips. It adheres to two key designs in a multi-task learning manner. First, we perform spatial prediction on the masked current frame for learning content features. Second, we utilize the future frame with an extremely high masking ratio as a condition, based on the masked current frame, to conduct temporal prediction of future frame for capturing motion features. These efficient designs ensure that our representation focusing on motion information while capturing spatial details. We carry out the largest-scale evaluation of PVRs for robotic motor control to date, which encompasses 21 tasks within a real-world Franka robot arm and 5 simulated environments. Extensive experiments demonstrate the effectiveness of STP as well as unleash its generality and data efficiency by further post-pre-training and hybrid pre-training.</p></p class="citation"></blockquote><h3 id=1215--206270-model-comparison-for-fast-domain-adaptation-in-table-service-scenario-woo-han-yun-et-al-2024>(12/15 | 206/270) Model Comparison for Fast Domain Adaptation in Table Service Scenario (Woo-han Yun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Woo-han Yun, Minsu Jang, Jaehong Kim. (2024)<br><strong>Model Comparison for Fast Domain Adaptation in Table Service Scenario</strong><br><button class=copy-to-clipboard title="Model Comparison for Fast Domain Adaptation in Table Service Scenario" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05092v1.pdf filename=2403.05092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In restaurants, many aspects of customer service, such as greeting customers, taking orders, and processing payments, are automated. Due to the various cuisines, required services, and different standards of each restaurant, one challenging part of making the entire automated process is inspecting and providing appropriate services at the table during a meal. In this paper, we demonstrate an approach for automatically checking and providing services at the table. We initially construct a base model to recognize common information to comprehend the context of the table, such as object category, remaining food quantity, and meal progress status. After that, we add a service recognition classifier and retrain the model using a small amount of local restaurant data. We gathered data capturing the restaurant table during the meal in order to find a suitable service recognition classifier. With different inputs, combinations, time series, and data choices, we carried out a variety of tests. Through these tests, we discovered that the model with few significant data points and trainable parameters is more crucial in the case of sparse and redundant retraining data.</p></p class="citation"></blockquote><h3 id=1315--207270-integrating-predictive-motion-uncertainties-with-distributionally-robust-risk-aware-control-for-safe-robot-navigation-in-crowds-kanghyun-ryu-et-al-2024>(13/15 | 207/270) Integrating Predictive Motion Uncertainties with Distributionally Robust Risk-Aware Control for Safe Robot Navigation in Crowds (Kanghyun Ryu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanghyun Ryu, Negar Mehr. (2024)<br><strong>Integrating Predictive Motion Uncertainties with Distributionally Robust Risk-Aware Control for Safe Robot Navigation in Crowds</strong><br><button class=copy-to-clipboard title="Integrating Predictive Motion Uncertainties with Distributionally Robust Risk-Aware Control for Safe Robot Navigation in Crowds" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05081v1.pdf filename=2403.05081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring safe navigation in human-populated environments is crucial for autonomous mobile robots. Although recent advances in machine learning offer promising methods to predict human trajectories in crowded areas, it remains unclear how one can safely incorporate these learned models into a control loop due to the uncertain nature of human motion, which can make predictions of these models imprecise. In this work, we address this challenge and introduce a distributionally robust chance-constrained model predictive control (DRCC-MPC) which: (i) adopts a probability of collision as a pre-specified, interpretable risk metric, and (ii) offers robustness against discrepancies between actual human trajectories and their predictions. We consider the risk of collision in the form of a chance constraint, providing an interpretable measure of robot safety. To enable real-time evaluation of chance constraints, we consider conservative approximations of chance constraints in the form of distributionally robust Conditional Value at Risk constraints. The resulting formulation offers computational efficiency as well as robustness with respect to <b>out-of-distribution</b> human motion. With the parallelization of a sampling-based optimization technique, our method operates in real-time, demonstrating successful and safe navigation in a number of case studies with real-world pedestrian data.</p></p class="citation"></blockquote><h3 id=1415--208270-robust-surgical-tool-tracking-with-pixel-based-probabilities-for-projected-geometric-primitives-christopher-dambrosia-et-al-2024>(14/15 | 208/270) Robust Surgical Tool Tracking with Pixel-based Probabilities for Projected Geometric Primitives (Christopher D&rsquo;Ambrosia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher D&rsquo;Ambrosia, Florian Richter, Zih-Yun Chiu, Nikhil Shinde, Fei Liu, Henrik I. Christensen, Michael C. Yip. (2024)<br><strong>Robust Surgical Tool Tracking with Pixel-based Probabilities for Projected Geometric Primitives</strong><br><button class=copy-to-clipboard title="Robust Surgical Tool Tracking with Pixel-based Probabilities for Projected Geometric Primitives" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04971v1.pdf filename=2403.04971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Controlling robotic manipulators via visual feedback requires a known coordinate frame transformation between the robot and the camera. Uncertainties in mechanical systems as well as camera calibration create errors in this coordinate frame transformation. These errors result in poor localization of robotic manipulators and create a significant challenge for applications that rely on precise interactions between manipulators and the environment. In this work, we estimate the camera-to-base transform and joint angle measurement errors for surgical robotic tools using an image based insertion-shaft detection algorithm and <b>probabilistic</b> <b>models.</b> We apply our proposed approach in both a structured environment as well as an unstructured environment and measure to demonstrate the efficacy of our methods.</p></p class="citation"></blockquote><h3 id=1515--209270-degradation-resilient-lidar-radar-inertial-odometry-morten-nissov-et-al-2024>(15/15 | 209/270) Degradation Resilient LiDAR-Radar-Inertial Odometry (Morten Nissov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Morten Nissov, Nikhil Khedekar, Kostas Alexis. (2024)<br><strong>Degradation Resilient LiDAR-Radar-Inertial Odometry</strong><br><button class=copy-to-clipboard title="Degradation Resilient LiDAR-Radar-Inertial Odometry" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO, eess-SP<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05332v1.pdf filename=2403.05332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enabling autonomous robots to operate robustly in challenging environments is necessary in a future with increased autonomy. For many autonomous systems, estimation and odometry remains a single point of failure, from which it can often be difficult, if not impossible, to recover. As such robust odometry solutions are of key importance. In this work a method for tightly-coupled LiDAR-Radar-Inertial fusion for odometry is proposed, enabling the mitigation of the effects of LiDAR degeneracy by leveraging a complementary perception modality while preserving the accuracy of LiDAR in well-conditioned environments. The proposed approach combines modalities in a factor <b>graph-based</b> windowed smoother with sensor information-specific factor formulations which enable, in the case of degeneracy, partial information to be conveyed to the <b>graph</b> along the non-degenerate axes. The proposed method is evaluated in real-world tests on a flying robot experiencing degraded conditions including geometric self-similarity as well as obscurant occlusion. For the benefit of the community we release the datasets presented: <a href=https://github.com/ntnu-arl/lidar_degeneracy_datasets>https://github.com/ntnu-arl/lidar_degeneracy_datasets</a>.</p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--210270-speech-robust-bench-a-robustness-benchmark-for-speech-recognition-muhammad-a-shah-et-al-2024>(1/1 | 210/270) Speech Robust Bench: A Robustness Benchmark For Speech Recognition (Muhammad A. Shah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad A. Shah, David Solans Noguero, Mikko A. Heikkila, Nicolas Kourtellis. (2024)<br><strong>Speech Robust Bench: A Robustness Benchmark For Speech Recognition</strong><br><button class=copy-to-clipboard title="Speech Robust Bench: A Robustness Benchmark For Speech Recognition" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 33<br>Keywords: Benchmarking, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07937v1.pdf filename=2403.07937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose <b>Speech</b> <b>Robust</b> Bench (SRB), a comprehensive <b>benchmark</b> for evaluating the robustness of <b>ASR</b> models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that <b>ASR</b> models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art <b>ASR</b> models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of <b>ASR</b> models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model&rsquo;s robustness across subgroups. We believe that SRB will facilitate future research towards robust <b>ASR</b> models, by making it easier to conduct comprehensive and comparable robustness evaluations.</p></p class="citation"></blockquote><h2 id=csse-2>cs.SE (2)</h2><h3 id=12--211270-profile-of-vulnerability-remediations-in-dependencies-using-graph-analysis-fernando-vera-et-al-2024>(1/2 | 211/270) Profile of Vulnerability Remediations in Dependencies Using Graph Analysis (Fernando Vera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando Vera, Palina Pauliuchenka, Ethan Oh, Bai Chien Kao, Louis DiValentin, David A. Bader. (2024)<br><strong>Profile of Vulnerability Remediations in Dependencies Using Graph Analysis</strong><br><button class=copy-to-clipboard title="Profile of Vulnerability Remediations in Dependencies Using Graph Analysis" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Graph Attention Networks, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04989v1.pdf filename=2403.04989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research introduces <b>graph</b> analysis methods and a modified <b>Graph</b> Attention <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(GAT)</b> to the critical challenge of open source package vulnerability remediation by analyzing control flow <b>graphs</b> to profile breaking changes in applications occurring from dependency upgrades intended to remediate vulnerabilities. Our approach uniquely applies node centrality metrics &ndash; degree, norm, and closeness centrality &ndash; to the <b>GAT</b> model, enabling a detailed examination of package code interactions with a focus on identifying and understanding vulnerable nodes, and when dependency package upgrades will interfere with application workflow. The study&rsquo;s application on a varied dataset reveals an unexpected limited inter-connectivity of vulnerabilities in core code, thus challenging established notions in software security. The results demonstrate the effectiveness of the enhanced <b>GAT</b> model in offering nuanced insights into the relational dynamics of code vulnerabilities, proving its potential in advancing cybersecurity measures. This approach not only aids in the strategic mitigation of vulnerabilities but also lays the groundwork for the development of sophisticated, sustainable monitoring systems for the evaluation of work effort for vulnerability remediation resulting from open source software. The insights gained from this study mark a significant advancement in the field of package vulnerability analysis and cybersecurity.</p></p class="citation"></blockquote><h3 id=22--212270-effective-fault-localization-using-probabilistic-and-grouping-approach-saksham-sahai-srivastava-et-al-2024>(2/2 | 212/270) Effective Fault Localization using Probabilistic and Grouping Approach (Saksham Sahai Srivastava et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saksham Sahai Srivastava, Arpita Dutta, Rajib Mall. (2024)<br><strong>Effective Fault Localization using Probabilistic and Grouping Approach</strong><br><button class=copy-to-clipboard title="Effective Fault Localization using Probabilistic and Grouping Approach" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05022v1.pdf filename=2403.05022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: Fault localization (FL) is the key activity while debugging a program. Any improvement to this activity leads to significant improvement in total software development cost. There is an internal linkage between the program spectrum and test execution result. Conditional probability in statistics captures the probability of occurring one event in relationship to one or more other events. Objectives: The aim of this paper is to use the conception of conditional probability to design an effective fault localization technique. Methods: In the paper, we present a fault localization technique that derives the association between statement coverage information and test case execution result using condition probability statistics. This association with the failed test case result shows the fault containing the probability of that specific statement. Subsequently, we use a grouping method to refine the obtained statement ranking sequence for better fault localization. Results: We evaluated the effectiveness of proposed method over eleven open-source data sets. Our obtained results show that on average, the proposed CGFL method is 24.56% more effective than other contemporary fault localization methods such as D*, Tarantula, Ochiai, Crosstab, BPNN, RBFNN, DNN, and <b>CNN.</b> Conclusion: We devised an effective fault localization technique by combining the conditional probabilistic method with failed test case execution-based approach. Our experimental evaluation shows our proposed method outperforms the existing fault localization techniques.</p></p class="citation"></blockquote><h2 id=econem-1>econ.EM (1)</h2><h3 id=11--213270-non-robustness-of-diffusion-estimates-on-networks-with-measurement-error-arun-g-chandrasekhar-et-al-2024>(1/1 | 213/270) Non-robustness of diffusion estimates on networks with measurement error (Arun G. Chandrasekhar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arun G. Chandrasekhar, Paul Goldsmith-Pinkham, Tyler H. McCormick, Samuel Thau, Jerry Wei. (2024)<br><strong>Non-robustness of diffusion estimates on networks with measurement error</strong><br><button class=copy-to-clipboard title="Non-robustness of diffusion estimates on networks with measurement error" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.EM<br>Categories: cs-SI, econ-EM, econ.EM, stat-AP, stat-ME<br>Keyword Score: 30<br>Keywords: Diffusion Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05704v1.pdf filename=2403.05704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Network <b>diffusion</b> <b>models</b> are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of <b>diffusions</b> <b>are</b> highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of <b>diffusion</b> <b>will</b> greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected <b>diffusion</b> <b>path.</b> We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo <b>simulations</b> on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China.</p></p class="citation"></blockquote><h2 id=csar-3>cs.AR (3)</h2><h3 id=13--214270-algorithm-hardware-co-design-of-distribution-aware-logarithmic-posit-encodings-for-efficient-dnn-inference-akshat-ramachandran-et-al-2024>(1/3 | 214/270) Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference (Akshat Ramachandran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshat Ramachandran, Zishen Wan, Geonhwa Jeong, John Gustafson, Tushar Krishna. (2024)<br><strong>Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference</strong><br><button class=copy-to-clipboard title="Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs-LG, cs-NE, cs.AR<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05465v1.pdf filename=2403.05465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional Deep Neural Network (DNN) <b>quantization</b> methods using integer, fixed-point, or floating-point data types struggle to capture diverse DNN parameter distributions at low precision, and often require large silicon overhead and intensive <b>quantization-aware</b> training. In this study, we introduce Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by posits that dynamically adapts to DNN weight/activation distributions by parameterizing LP bit fields. We also develop a novel genetic-algorithm based framework, LP <b>Quantization</b> (LPQ), to find optimal layer-wise LP parameters while reducing representational divergence between <b>quantized</b> and full-precision models through a novel global-local contrastive objective. Additionally, we design a unified mixed-precision LP accelerator (LPA) architecture comprising of processing elements (PEs) incorporating LP in the computational datapath. Our algorithm-hardware co-design demonstrates on average &lt;1% drop in top-1 accuracy across various <b>CNN</b> and ViT models. It also achieves ~ 2x improvements in performance per unit area and 2.2x gains in energy efficiency compared to state-of-the-art <b>quantization</b> accelerators using different data types.</p></p class="citation"></blockquote><h3 id=23--215270-lightator-an-optical-near-sensor-accelerator-with-compressive-acquisition-enabling-versatile-image-processing-mehrdad-morsali-et-al-2024>(2/3 | 215/270) Lightator: An Optical Near-Sensor Accelerator with Compressive Acquisition Enabling Versatile Image Processing (Mehrdad Morsali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehrdad Morsali, Brendan Reidy, Deniz Najafi, Sepehr Tabrizchi, Mohsen Imani, Mahdi Nikdast, Arman Roohi, Ramtin Zand, Shaahin Angizi. (2024)<br><strong>Lightator: An Optical Near-Sensor Accelerator with Compressive Acquisition Enabling Versatile Image Processing</strong><br><button class=copy-to-clipboard title="Lightator: An Optical Near-Sensor Accelerator with Compressive Acquisition Enabling Versatile Image Processing" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR, eess-SP<br>Keyword Score: 30<br>Keywords: Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05037v1.pdf filename=2403.05037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a high-performance and energy-efficient optical near-sensor accelerator for vision applications, called Lightator. Harnessing the promising efficiency offered by photonic devices, Lightator features innovative compressive acquisition of input frames and fine-grained <b>convolution</b> operations for low-power and versatile image processing at the edge for the first time. This will substantially diminish the energy consumption and latency of conversion, transmission, and processing within the established cloud-centric architecture as well as recently designed edge accelerators. Our device-to-architecture <b>simulation</b> results show that with favorable accuracy, Lightator achieves 84.4 Kilo FPS/W and reduces power consumption by a factor of ~24x and 73x on average compared with existing photonic accelerators and GPU baseline.</p></p class="citation"></blockquote><h3 id=33--216270-a-286-mjiter-stable-diffusion-processor-for-text-to-image-generation-with-patch-similarity-based-sparsity-augmentation-and-text-based-mixed-precision-jiwon-choi-et-al-2024>(3/3 | 216/270) A 28.6 mJ/iter Stable Diffusion Processor for Text-to-Image Generation with Patch Similarity-based Sparsity Augmentation and Text-based Mixed-Precision (Jiwon Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiwon Choi, Wooyoung Jo, Seongyon Hong, Beomseok Kwon, Wonhoon Park, Hoi-Jun Yoo. (2024)<br><strong>A 28.6 mJ/iter Stable Diffusion Processor for Text-to-Image Generation with Patch Similarity-based Sparsity Augmentation and Text-based Mixed-Precision</strong><br><button class=copy-to-clipboard title="A 28.6 mJ/iter Stable Diffusion Processor for Text-to-Image Generation with Patch Similarity-based Sparsity Augmentation and Text-based Mixed-Precision" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Text2image, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04982v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04982v2.pdf filename=2403.04982v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an energy-efficient stable diffusion processor for <b>text-to-image</b> generation. While stable diffusion attained attention for high-quality image synthesis results, its inherent characteristics hinder its deployment on mobile platforms. The proposed processor achieves high throughput and energy efficiency with three key features as solutions: 1) Patch similarity-based sparsity augmentation (PSSA) to reduce external memory access (EMA) energy of <b>self-attention</b> score by 60.3 %, leading to 37.8 % total EMA energy reduction. 2) Text-based important pixel spotting (TIPS) to allow 44.8 % of the FFN layer workload to be processed with low-precision activation. 3) Dual-mode bit-slice core (DBSC) architecture to enhance energy efficiency in FFN layers by 43.0 %. The proposed processor is implemented in 28 nm CMOS technology and achieves 3.84 TOPS peak throughput with 225.6 mW average power consumption. In sum, 28.6 mJ/iteration highly energy-efficient <b>text-to-image</b> generation processor can be achieved at MS-COCO dataset.</p></p class="citation"></blockquote><h2 id=cshc-8>cs.HC (8)</h2><h3 id=18--217270-trust-recognition-in-human-robot-cooperation-using-eeg-caiyue-xu-et-al-2024>(1/8 | 217/270) Trust Recognition in Human-Robot Cooperation Using EEG (Caiyue Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Caiyue Xu, Changming Zhang, Yanmin Zhou, Zhipeng Wang, Ping Lu, Bin He. (2024)<br><strong>Trust Recognition in Human-Robot Cooperation Using EEG</strong><br><button class=copy-to-clipboard title="Trust Recognition in Human-Robot Cooperation Using EEG" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05225v1.pdf filename=2403.05225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collaboration between humans and robots is becoming increasingly crucial in our daily life. In order to accomplish efficient cooperation, trust recognition is vital, empowering robots to predict human behaviors and make trust-aware decisions. Consequently, there is an urgent need for a generalized approach to recognize human-robot trust. This study addresses this need by introducing an EEG-based method for trust recognition during human-robot cooperation. A human-robot cooperation game scenario is used to stimulate various human trust levels when working with robots. To enhance recognition performance, the study proposes an EEG <b>Vision</b> <b>Transformer</b> model coupled with a 3-D spatial representation to capture the spatial information of EEG, taking into account the topological relationship among electrodes. To validate this approach, a public EEG-based human trust dataset called EEGTrust is constructed. Experimental results indicate the effectiveness of the proposed approach, achieving an accuracy of 74.99% in slice-wise cross-validation and 62.00% in trial-wise cross-validation. This outperforms baseline models in both recognition accuracy and generalization. Furthermore, an ablation study demonstrates a significant improvement in trust recognition performance of the spatial representation. The source code and EEGTrust dataset are available at <a href=https://github.com/CaiyueXu/EEGTrust>https://github.com/CaiyueXu/EEGTrust</a>.</p></p class="citation"></blockquote><h3 id=28--218270-aqua-automated-question-answering-in-software-tutorial-videos-with-visual-anchors-saelyne-yang-et-al-2024>(2/8 | 218/270) AQuA: Automated Question-Answering in Software Tutorial Videos with Visual Anchors (Saelyne Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saelyne Yang, Jo Vermeulen, George Fitzmaurice, Justin Matejka. (2024)<br><strong>AQuA: Automated Question-Answering in Software Tutorial Videos with Visual Anchors</strong><br><button class=copy-to-clipboard title="AQuA: Automated Question-Answering in Software Tutorial Videos with Visual Anchors" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: GPT, GPT-4, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05213v1.pdf filename=2403.05213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tutorial videos are a popular help source for learning feature-rich software. However, getting quick answers to <b>questions</b> <b>about</b> tutorial videos is difficult. We present an automated approach for responding to tutorial <b>questions.</b> <b>By</b> analyzing 633 <b>questions</b> <b>found</b> in 5,944 video comments, we identified different <b>question</b> <b>types</b> and observed that users frequently described parts of the video in <b>questions.</b> <b>We</b> then asked participants (N=24) to watch tutorial videos and ask <b>questions</b> <b>while</b> annotating the video with relevant visual anchors. Most visual anchors referred to UI elements and the application workspace. Based on these insights, we built AQuA, a pipeline that generates useful answers to <b>questions</b> <b>with</b> visual anchors. We demonstrate this for Fusion 360, showing that we can recognize UI elements in visual anchors and generate answers using <b>GPT-4</b> augmented with that visual information and software documentation. An evaluation study (N=16) demonstrates that our approach provides better answers than baseline methods.</p></p class="citation"></blockquote><h3 id=38--219270-vibopneumo-a-vibratory-pneumatic-finger-worn-haptic-device-for-altering-perceived-texture-roughness-in-mixed-reality-shaoyu-cai-et-al-2024>(3/8 | 219/270) ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering Perceived Texture Roughness in Mixed Reality (Shaoyu Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoyu Cai, Zhenlin Chen, Haichen Gao, Ya Huang, Qi Zhang, Xinge Yu, Kening Zhu. (2024)<br><strong>ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering Perceived Texture Roughness in Mixed Reality</strong><br><button class=copy-to-clipboard title="ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering Perceived Texture Roughness in Mixed Reality" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-GR, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05182v1.pdf filename=2403.05182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extensive research has been done in haptic feedback for texture <b>simulation</b> in virtual reality (VR). However, it is challenging to modify the perceived tactile texture of existing physical objects which usually serve as anchors for virtual objects in mixed reality (MR). In this paper, we present ViboPneumo, a finger-worn haptic device that uses vibratory-pneumatic feedback to modulate (i.e., increase and decrease) the perceived roughness of the material surface contacted by the user&rsquo;s fingerpad while supporting the perceived sensation of other haptic properties (e.g., temperature or stickiness) in MR. Our device includes a silicone-based pneumatic actuator that can lift the user&rsquo;s fingerpad on the physical surface to reduce the contact area for roughness decreasing, and an on-finger vibrator for roughness increasing. Our user-perception experimental results showed that the participants could perceive changes in roughness, both increasing and decreasing, compared to the original material surface. We also observed the overlapping roughness ratings among certain haptic stimuli (i.e., vibrotactile and pneumatic) and the originally perceived roughness of some materials without any haptic feedback. This suggests the potential to alter the perceived texture of one type of material to another in terms of roughness (e.g., modifying the perceived texture of ceramics as glass). Lastly, a user study of MR experience showed that ViboPneumo could significantly improve the MR user experience, particularly for visual-haptic matching, compared to the condition of a bare finger. We also demonstrated a few application scenarios for ViboPneumo.</p></p class="citation"></blockquote><h3 id=48--220270-scoping-out-the-scalability-issues-of-autonomous-vehicle-pedestrian-interaction-tram-thi-minh-tran-et-al-2024>(4/8 | 220/270) Scoping Out the Scalability Issues of Autonomous Vehicle-Pedestrian Interaction (Tram Thi Minh Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tram Thi Minh Tran, Callum Parker, Martin Tomitsch. (2024)<br><strong>Scoping Out the Scalability Issues of Autonomous Vehicle-Pedestrian Interaction</strong><br><button class=copy-to-clipboard title="Scoping Out the Scalability Issues of Autonomous Vehicle-Pedestrian Interaction" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05727v1.pdf filename=2403.05727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous vehicles (AVs) may use external interfaces, such as LED light bands, to communicate with pedestrians safely and intuitively. While previous research has demonstrated the effectiveness of these interfaces in simple traffic scenarios involving one pedestrian and one vehicle, their performance in more complex scenarios with multiple road users remains unclear. The scalability of AV external communication has therefore attracted increasing attention, <b>prompting</b> the need for further investigation. This scoping review synthesises information from 54 papers to identify seven key scalability issues in multi-vehicle and multi-pedestrian environments, with Clarity of Recipients, Information Overload, and Multi-Lane Safety emerging as the most pressing concerns. To guide future research in scalable AV-pedestrian interactions, we propose high-level design directions focused on three communication loci: vehicle, infrastructure, and pedestrian. Our work contributes the groundwork and a roadmap for designing simplified, coordinated, and targeted external AV communication, ultimately improving safety and efficiency in complex traffic scenarios.</p></p class="citation"></blockquote><h3 id=58--221270-digital-wellbeing-redefined-toward-user-centric-approach-for-positive-social-media-engagement-yixue-zhao-et-al-2024>(5/8 | 221/270) Digital Wellbeing Redefined: Toward User-Centric Approach for Positive Social Media Engagement (Yixue Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixue Zhao, Tianyi Li, Michael Sobolev. (2024)<br><strong>Digital Wellbeing Redefined: Toward User-Centric Approach for Positive Social Media Engagement</strong><br><button class=copy-to-clipboard title="Digital Wellbeing Redefined: Toward User-Centric Approach for Positive Social Media Engagement" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-SE, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05723v1.pdf filename=2403.05723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prevalence of social media and its escalating impact on mental health has highlighted the need for effective digital wellbeing strategies. Current digital wellbeing interventions have primarily focused on reducing screen time and social media use, often neglecting the potential benefits of these platforms. This paper introduces a new perspective centered around empowering positive social media experiences, instead of limiting users with restrictive rules. In line with this perspective, we lay out the key requirements that should be considered in future work, aiming to spark a dialogue in this emerging area. We further present our initial effort to address these requirements with PauseNow, an innovative digital wellbeing intervention designed to align users&rsquo; digital behaviors with their intentions. PauseNow leverages digital nudging and intention-aware <b>recommendations</b> to gently guide users back to their original intentions when they &ldquo;get lost&rdquo; during their digital usage, promoting a more mindful use of social media.</p></p class="citation"></blockquote><h3 id=68--222270-enabling-developers-protecting-users-investigating-harassment-and-safety-in-vr-abhinaya-s-b-et-al-2024>(6/8 | 222/270) Enabling Developers, Protecting Users: Investigating Harassment and Safety in VR (Abhinaya S. B. et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhinaya S. B., Aafaq Sabir, Anupam Das. (2024)<br><strong>Enabling Developers, Protecting Users: Investigating Harassment and Safety in VR</strong><br><button class=copy-to-clipboard title="Enabling Developers, Protecting Users: Investigating Harassment and Safety in VR" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CR, cs-CY, cs-ET, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05499v1.pdf filename=2403.05499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Virtual Reality (VR) has witnessed a rising issue of harassment, <b>prompting</b> the integration of safety controls like muting and blocking in VR applications. However, the lack of standardized safety measures across VR applications hinders their universal effectiveness, especially across contexts like socializing, gaming, and streaming. While prior research has studied safety controls in social VR applications, our user study (n = 27) takes a multi-perspective approach, examining both users&rsquo; perceptions of safety control usability and effectiveness as well as the challenges that developers face in designing and deploying VR safety controls. We identify challenges VR users face while employing safety controls, such as finding users in crowded virtual spaces to block them. VR users also find controls ineffective in addressing harassment; for instance, they fail to eliminate the harassers&rsquo; presence from the environment. Further, VR users find the current methods of submitting evidence for reports time-consuming and cumbersome. Improvements desired by users include live moderation and behavior tracking across VR apps; however, developers cite technological, financial, and legal obstacles to implementing such solutions, often due to a lack of awareness and high development costs. We emphasize the importance of establishing technical and legal guidelines to enhance user safety in virtual environments.</p></p class="citation"></blockquote><h3 id=78--223270-comparison-of-spatial-visualization-techniques-for-radiation-in-augmented-reality-fintan-mcgee-et-al-2024>(7/8 | 223/270) Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality (Fintan McGee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fintan McGee, Roderick McCall, Joan Baixauli. (2024)<br><strong>Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality</strong><br><button class=copy-to-clipboard title="Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05403v1.pdf filename=2403.05403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Augmented Reality (AR) provides a safe and low-cost option for hazardous safety training that allows for the visualization of aspects that may be invisible, such as radiation. Effectively visually communicating such threats in the environment around the user is not straightforward. This work describes visually encoding radiation using the spatial awareness mesh of an AR Head Mounted Display. We leverage the AR device&rsquo;s GPUs to develop a real time solution that accumulates multiple dynamic sources and uses stencils to prevent an environment being over saturated with a visualization, as well as supporting the encoding of direction explicitly in the visualization. We perform a user study (25 participants) of different visualizations and obtain user feedback. Results show that there are complex interactions and while no visual representation was statistically superior or inferior, user opinions vary widely. We also discuss the evaluation approaches and provide <b>recommendations.</b></p></p class="citation"></blockquote><h3 id=88--224270-know-your-audience-the-benefits-and-pitfalls-of-generating-plain-language-summaries-beyond-the-general-audience-tal-august-et-al-2024>(8/8 | 224/270) Know Your Audience: The benefits and pitfalls of generating plain language summaries beyond the &lsquo;general&rsquo; audience (Tal August et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tal August, Kyle Lo, Noah A. Smith, Katharina Reinecke. (2024)<br><strong>Know Your Audience: The benefits and pitfalls of generating plain language summaries beyond the &lsquo;general&rsquo; audience</strong><br><button class=copy-to-clipboard title="Know Your Audience: The benefits and pitfalls of generating plain language summaries beyond the 'general' audience" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04979v1.pdf filename=2403.04979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models (LMs) show promise as tools for communicating science to the general public by simplifying and summarizing complex language. Because models can be <b>prompted</b> to generate text for a specific audience (e.g., college-educated adults), LMs might be used to create multiple versions of plain language summaries for people with different familiarities of scientific topics. However, it is not clear what the benefits and pitfalls of adaptive plain language are. When is simplifying necessary, what are the costs in doing so, and do these costs differ for readers with different background knowledge? Through three within-subjects studies in which we surface summaries for different envisioned audiences to participants of different backgrounds, we found that while simpler text led to the best reading experience for readers with little to no familiarity in a topic, high familiarity readers tended to ignore certain details in overly plain summaries (e.g., study limitations). Our work provides methods and guidance on ways of adapting plain language summaries beyond the single &ldquo;general&rdquo; audience.</p></p class="citation"></blockquote><h2 id=eesssy-9>eess.SY (9)</h2><h3 id=19--225270-device-fault-prediction-model-based-on-lstm-and-random-forest-jing-xu-et-al-2024>(1/9 | 225/270) Device Fault Prediction Model based on LSTM and Random Forest (Jing Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Xu, Yongbo Zhang. (2024)<br><strong>Device Fault Prediction Model based on LSTM and Random Forest</strong><br><button class=copy-to-clipboard title="Device Fault Prediction Model based on LSTM and Random Forest" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05179v1.pdf filename=2403.05179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quality of power grid equipment forms the material foundation for the safety of the large power grid. Ensuring the quality of equipment entering the grid is a core task in material management. Currently, the inspection of incoming materials involves the generation of sampling plans, sampling, sealing, sample delivery, and testing. Due to the lack of a comprehensive control system and effective control measures, it is not possible to trace the execution process of business operations afterward. This inability to trace hampers the investigation of testing issues and risk control, as it lacks effective data support. Additionally, a significant amount of original record information for key parameters in the testing process, which is based on sampling operation standards, has not been effectively utilized. To address these issues, we conduct researches on key monitoring technologies in the typical material inspection process based on the Internet of Things (IoT) and analyze the key parameters in inspection results. For purpose of complete the above tasks, this paper investigates the use of <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> algorithms for quality prediction in material equipment based on key inspection parameters. In summary, this paper aims to provide professional and reliable quality data support for various business processes within the company, including material procurement, engineering construction, and equipment operation.</p></p class="citation"></blockquote><h3 id=29--226270-sampling-model-for-grid-material-inspection-based-on-analytic-hierarchy-process-with-absolute-measurement-jing-xu-et-al-2024>(2/9 | 226/270) Sampling Model for Grid Material Inspection Based on Analytic Hierarchy Process with Absolute Measurement (Jing Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Xu, Yongbo Zhang. (2024)<br><strong>Sampling Model for Grid Material Inspection Based on Analytic Hierarchy Process with Absolute Measurement</strong><br><button class=copy-to-clipboard title="Sampling Model for Grid Material Inspection Based on Analytic Hierarchy Process with Absolute Measurement" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05079v1.pdf filename=2403.05079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quality of power grid equipment forms the material foundation for the safety of the large power grid. Ensuring the quality of equipment entering the grid is a core task in material management. Currently, the inspection of incoming materials involves the generation of sampling plans, sampling, sealing, sample delivery, and testing. Due to the lack of a comprehensive control system and effective control measures, it is not possible to trace the execution process of business operations afterward. This inability to trace hampers the investigation of testing issues and risk control, as it lacks effective data support. Additionally, a significant amount of original record information for key parameters in the testing process, which is based on sampling operation standards, has not been effectively utilized. To address these issues, we conduct researches on key monitoring technologies in the typical material inspection process based on the Internet of Things (IoT) and analyze the key parameters in inspection results. For purpose of complete the above tasks, this paper investigates the use of <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> algorithms for quality prediction in material equipment based on key inspection parameters. In summary, this paper aims to provide professional and reliable quality data support for various business processes within the company, including material procurement, engineering construction, and equipment operation.</p></p class="citation"></blockquote><h3 id=39--227270-economic-capacity-withholding-bounds-of-competitive-energy-storage-bidders-xin-qin-et-al-2024>(3/9 | 227/270) Economic Capacity Withholding Bounds of Competitive Energy Storage Bidders (Xin Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Qin, Ioannis Lestas, Bolun Xu. (2024)<br><strong>Economic Capacity Withholding Bounds of Competitive Energy Storage Bidders</strong><br><button class=copy-to-clipboard title="Economic Capacity Withholding Bounds of Competitive Energy Storage Bidders" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05705v1.pdf filename=2403.05705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Economic withholding in electricity markets refers to generators bidding higher than their true marginal fuel cost, and is a typical approach to exercising market power. However, existing market designs require storage to design bids strategically based on their own future price predictions, motivating storage to conduct economic withholding without assuming market power. As energy storage takes up more significant roles in wholesale electricity markets, understanding its motivations for economic withholding and the consequent effects on social welfare becomes increasingly vital. This paper derives a theoretical framework to study the economic capacity withholding behavior of storage participating in competitive electricity markets and validate our results in <b>simulations</b> based on the ISO New England system. We demonstrate that storage bids can reach unbounded high levels under conditions where future price predictions show bounded expectations but unbounded deviations. Conversely, in scenarios with peak price limitations, we show the upper bounds of storage bids are grounded in bounded price expectations. Most importantly, we show that storage capacity withholding can potentially lower the overall system cost when price models account for system uncertainties. Our paper reveals energy storage is not a market manipulator but an honest player contributing to the social welfare. It helps electricity market researchers and operators better understand the economic withholding behavior of storage and reform market policies to maximize storage contributing to a cost-efficient decolonization.</p></p class="citation"></blockquote><h3 id=49--228270-stability-certified-on-policy-data-driven-lqr-via-recursive-learning-and-policy-gradient-lorenzo-sforni-et-al-2024>(4/9 | 228/270) Stability-Certified On-Policy Data-Driven LQR via Recursive Learning and Policy Gradient (Lorenzo Sforni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Sforni, Guido Carnevale, Ivano Notarnicola, Giuseppe Notarstefano. (2024)<br><strong>Stability-Certified On-Policy Data-Driven LQR via Recursive Learning and Policy Gradient</strong><br><button class=copy-to-clipboard title="Stability-Certified On-Policy Data-Driven LQR via Recursive Learning and Policy Gradient" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05367v1.pdf filename=2403.05367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate a data-driven framework to solve Linear Quadratic Regulator (LQR) problems when the dynamics is unknown, with the additional challenge of providing stability certificates for the overall learning and control scheme. Specifically, in the proposed on-policy learning framework, the control input is applied to the actual (unknown) linear system while iteratively optimized. We propose a learning and control procedure, termed RELEARN LQR, that combines a recursive least squares method with a direct policy search based on the gradient method. The resulting scheme is analyzed by modeling it as a feedback-interconnected nonlinear dynamical system. A Lyapunov-based approach, exploiting averaging and singular perturbations theory for nonlinear systems, allows us to provide formal stability guarantees for the whole interconnected scheme. The effectiveness of the proposed strategy is corroborated by numerical <b>simulations,</b> where RELEARN LQR is deployed on an aircraft control problem, with both static and drifting parameters.</p></p class="citation"></blockquote><h3 id=59--229270-correlation-analysis-technique-of-key-parameters-for-transformer-material-inspection-based-on-fp-tree-and-knowledge-graph-jing-xu-et-al-2024>(5/9 | 229/270) Correlation analysis technique of key parameters for transformer material inspection based on FP-tree and knowledge graph (Jing Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Xu, Yongbo Zhang. (2024)<br><strong>Correlation analysis technique of key parameters for transformer material inspection based on FP-tree and knowledge graph</strong><br><button class=copy-to-clipboard title="Correlation analysis technique of key parameters for transformer material inspection based on FP-tree and knowledge graph" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05076v1.pdf filename=2403.05076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As one of the key equipment in the distribution system, the distribution <b>transformer</b> directly affects the reliability of the user power supply. The probability of accidents occurring in the operation of <b>transformer</b> equipment is high, so it has become a focus of material inspection in recent years. However, the large amount of raw data from sample testing is not being used effectively. Given the above problems, this paper aims to mine the relationship between the unqualified distribution <b>transformer</b> inspection items by using the association rule algorithm based on the distribution <b>transformer</b> inspection data collected from 2017 to 2021 and sorting out the key inspection items. At the same time, the unqualified judgment basis of the relevant items is given, and the internal relationship between the inspection items is clarified to a certain extent. Furthermore, based on material and equipment inspection reports, correlations between failed inspection items, and expert <b>knowledge,</b> <b>the</b> <b>knowledge</b> <b>graph</b> of material equipment inspection management is constructed in the <b>graph</b> database Neo4j. The experimental results show that the FP-Growth method performs significantly better than the Apriori method and can accurately assess the relationship between failed distribution <b>transformer</b> inspection items. Finally, the <b>knowledge</b> <b>graph</b> network is visualized to provide a systematic <b>knowledge</b> <b>base</b> for material inspection, which is convenient for <b>knowledge</b> <b>query</b> and management. This method can provide a scientific guidance program for operation and maintenance personnel to do equipment maintenance and also offers a reference for the state evaluation of other high-voltage equipment.</p></p class="citation"></blockquote><h3 id=69--230270-a-framework-for-effective-ai-recommendations-in-cyber-physical-human-systems-aditya-dave-et-al-2024>(6/9 | 230/270) A Framework for Effective AI Recommendations in Cyber-Physical-Human Systems (Aditya Dave et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Dave, Heeseung Bang, Andreas A. Malikopoulos. (2024)<br><strong>A Framework for Effective AI Recommendations in Cyber-Physical-Human Systems</strong><br><button class=copy-to-clipboard title="A Framework for Effective AI Recommendations in Cyber-Physical-Human Systems" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-HC, cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05715v1.pdf filename=2403.05715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many cyber-physical-human systems (CPHS) involve a human decision-maker who may receive <b>recommendations</b> from an artificial intelligence (AI) platform while holding the ultimate responsibility of making decisions. In such CPHS applications, the human decision-maker may depart from an optimal recommended decision and instead implement a different one for various reasons. In this letter, we develop a rigorous framework to overcome this challenge. In our framework, we consider that humans may deviate from AI <b>recommendations</b> as they perceive and interpret the system&rsquo;s state in a different way than the AI platform. We establish the structural properties of optimal <b>recommendation</b> strategies and develop an approximate human model (AHM) used by the AI. We provide theoretical bounds on the optimality gap that arises from an AHM and illustrate the efficacy of our results in a numerical example.</p></p class="citation"></blockquote><h3 id=79--231270-control-oriented-identification-for-the-linear-quadratic-regulator-technical-report-sean-anderson-et-al-2024>(7/9 | 231/270) Control-Oriented Identification for the Linear Quadratic Regulator: Technical Report (Sean Anderson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sean Anderson, João Pedro Hespanha. (2024)<br><strong>Control-Oriented Identification for the Linear Quadratic Regulator: Technical Report</strong><br><button class=copy-to-clipboard title="Control-Oriented Identification for the Linear Quadratic Regulator: Technical Report" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05455v1.pdf filename=2403.05455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-driven control benefits from rich datasets, but constructing such datasets becomes challenging when gathering data is limited. We consider an offline experiment design approach to gathering data where we design a control input to collect data that will most improve the performance of a feedback controller. We show how such a control-oriented approach can be used in a setting with linear dynamics and quadratic objective and, through design of a gradient estimator, solve the problem via <b>stochastic</b> <b>gradient</b> <b>descent.</b> We contrast our method with a classical A-optimal experiment design approach and numerically demonstrate that our method outperforms A-optimal design in terms of improving control performance.</p></p class="citation"></blockquote><h3 id=89--232270-exploiting-polar-symmetry-in-designing-equivariant-observers-for-vision-based-motion-estimation-tarek-bouazza-et-al-2024>(8/9 | 232/270) Exploiting polar symmetry in designing equivariant observers for vision-based motion estimation (Tarek Bouazza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tarek Bouazza, Robert Mahony, Tarek Hamel. (2024)<br><strong>Exploiting polar symmetry in designing equivariant observers for vision-based motion estimation</strong><br><button class=copy-to-clipboard title="Exploiting polar symmetry in designing equivariant observers for vision-based motion estimation" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05450v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05450v2.pdf filename=2403.05450v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately estimating camera motion from image sequences poses a significant challenge in computer vision and robotics. Many computer vision methods first compute the essential matrix associated with a motion and then extract orientation and normalized translation as inputs to pose estimation, reconstructing the scene scale (that is unobservable in the epipolar construction) from separate information. In this paper, we design a <b>continuous-time</b> <b>filter</b> that exploits the same perspective by using the epipolar constraint to define pseudo-measurements. We propose a novel polar symmetry on the pose of the camera that makes these measurements equivariant. This allows us to apply recent results from equivariant systems theory to estimating pose. We provide a novel explicit persistence of excitation condition to characterize observability of the full pose, ensuring reconstruction of the scale parameter that is not directly observable in the epipolar construction.</p></p class="citation"></blockquote><h3 id=99--233270-formal-verification-of-unknown-stochastic-systems-via-non-parametric-estimation-zhi-zhang-et-al-2024>(9/9 | 233/270) Formal Verification of Unknown Stochastic Systems via Non-parametric Estimation (Zhi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhi Zhang, Chenyu Ma, Saleh Soudijani, Sadegh Soudjani. (2024)<br><strong>Formal Verification of Unknown Stochastic Systems via Non-parametric Estimation</strong><br><button class=copy-to-clipboard title="Formal Verification of Unknown Stochastic Systems via Non-parametric Estimation" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05350v1.pdf filename=2403.05350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A novel data-driven method for formal verification is proposed to study complex systems operating in safety-critical domains. The proposed approach is able to formally verify <b>discrete-time</b> <b>stochastic</b> dynamical systems against temporal logic specifications only using observation samples and without the knowledge of the model, and provide a probabilistic guarantee on the satisfaction of the specification. We first propose the theoretical results for using non-parametric estimation to estimate an asymptotic upper bound for the \emph{Lipschitz constant} of the stochastic system, which can determine a finite abstraction of the system. Our results prove that the asymptotic convergence rate of the estimation is $O(n^{-\frac{1}{3+d}})$, where $d$ is the dimension of the system and $n$ is the data scale. We then construct interval Markov decision processes using two different data-driven methods, namely non-parametric estimation and empirical estimation of transition probabilities, to perform formal verification against a given temporal logic specification. Multiple case studies are presented to validate the effectiveness of the proposed methods.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--234270-ris-empowered-topology-control-for-distributed-learning-in-urban-air-mobility-kai-xiong-et-al-2024>(1/2 | 234/270) RIS-empowered Topology Control for Distributed Learning in Urban Air Mobility (Kai Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Xiong, Rui Wang, Supeng Leng, Wenyang Che, Chongwen Huang, Chau Yuen. (2024)<br><strong>RIS-empowered Topology Control for Distributed Learning in Urban Air Mobility</strong><br><button class=copy-to-clipboard title="RIS-empowered Topology Control for Distributed Learning in Urban Air Mobility" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs-NI, cs.IT, math-IT<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05133v1.pdf filename=2403.05133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Urban Air Mobility (UAM) expands vehicles from the ground to the near-ground space, envisioned as a revolution for transportation systems. Comprehensive scene perception is the foundation for autonomous aerial driving. However, UAM encounters the intelligent perception challenge: high perception learning requirements conflict with the limited sensors and computing chips of flying cars. To overcome the challenge, <b>federated</b> <b>learning</b> (FL) and other collaborative learning have been proposed to enable resource-limited devices to conduct onboard deep learning (DL) collaboratively. But traditional collaborative learning like FL relies on a central integrator for DL model aggregation, which is difficult to deploy in dynamic environments. The fully decentralized learning schemes may be the intuitive solution while the convergence of distributed learning cannot be guaranteed. Accordingly, this paper explores reconfigurable intelligent surfaces (RIS) empowered distributed learning, taking account of topological attributes to facilitate the learning performance with convergence guarantee. We propose several FL topological criteria for optimizing the transmission delay and convergence rate by exploiting the Laplacian matrix eigenvalues of the communication network. Subsequently, we innovatively leverage the RIS link modification ability to remold the current network according to the proposed topological criteria. This paper rethinks the functions of RIS from the perspective of the network layer. Furthermore, a deep deterministic policy gradient-based RIS phase shift control algorithm is developed to construct or deconstruct the network links simultaneously to reshape the communication network. <b>Simulation</b> experiments are conducted over MobileNet-based multi-view learning to verify the efficiency of the distributed FL framework.</p></p class="citation"></blockquote><h3 id=22--235270-gan-based-massive-mimo-channel-model-trained-on-measured-data-florian-euchner-et-al-2024>(2/2 | 235/270) GAN-based Massive MIMO Channel Model Trained on Measured Data (Florian Euchner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Euchner, Janina Sanzi, Marcus Henninger, Stephan ten Brink. (2024)<br><strong>GAN-based Massive MIMO Channel Model Trained on Measured Data</strong><br><button class=copy-to-clipboard title="GAN-based Massive MIMO Channel Model Trained on Measured Data" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 25<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05321v1.pdf filename=2403.05321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wireless channel models are a commonly used tool for the development of wireless telecommunication systems and standards. The currently prevailing <b>geometry-based</b> stochastic channel models (GSCMs) were manually specified for certain environments in a manual process requiring extensive domain knowledge, on the basis of channel measurement campaigns. By taking into account the stochastic distribution of certain channel properties like Rician k-factor, path loss or delay spread, they model the distribution of channel realizations. Instead of this manual process, a <b>generative</b> <b>machine</b> <b>learning</b> model like a <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> may be used to automatically learn the distribution of channel statistics. Subsequently, the <b>GAN&rsquo;s</b> generator may be viewed as a channel model that can replace conventional stochastic or raytracer-based models. We propose a <b>GAN</b> architecture for a massive MIMO channel model, and train it on measurement data produced by a distributed massive MIMO channel sounder.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--236270-enhancing-non-intrusive-reduced-order-models-with-space-dependent-aggregation-methods-anna-ivagnes-et-al-2024>(1/2 | 236/270) Enhancing non-intrusive Reduced Order Models with space-dependent aggregation methods (Anna Ivagnes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Ivagnes, Niccolò Tonicello, Paola Cinnella, Gianluigi Rozza. (2024)<br><strong>Enhancing non-intrusive Reduced Order Models with space-dependent aggregation methods</strong><br><button class=copy-to-clipboard title="Enhancing non-intrusive Reduced Order Models with space-dependent aggregation methods" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Autoencoder, Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05710v1.pdf filename=2403.05710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this manuscript, we combine non-intrusive reduced order models (ROMs) with space-dependent aggregation techniques to build a mixed-ROM. The prediction of the mixed formulation is given by a convex linear combination of the predictions of some previously-trained ROMs, where we assign to each model a space-dependent weight. The ROMs taken into account to build the mixed model exploit different reduction techniques, such as Proper Orthogonal Decomposition (POD) and <b>AutoEncoders</b> (AE), and/or different approximation techniques, namely a Radial Basis Function Interpolation (RBF), a <b>Gaussian</b> <b>Process</b> Regression (GPR) or a feed-forward Artificial Neural Network (ANN). The contribution of each model is retained with higher weights in the regions where the model performs best, and, vice versa, with smaller weights where the model has a lower accuracy with respect to the other models. Finally, a regression technique, namely a Random Forest, is exploited to evaluate the weights for unseen conditions. The performance of the aggregated model is evaluated on two different test cases: the 2D flow past a NACA 4412 airfoil, with an angle of attack of 5 degrees, having as parameter the Reynolds number varying between 1e5 and 1e6 and a transonic flow over a NACA 0012 airfoil, considering as parameter the angle of attack. In both cases, the mixed-ROM has provided improved accuracy with respect to each individual ROM technique.</p></p class="citation"></blockquote><h3 id=22--237270-multirate-time-integration-based-on-dynamic-ode-partitioning-through-adaptively-refined-meshes-for-compressible-fluid-dynamics-daniel-doehring-et-al-2024>(2/2 | 237/270) Multirate Time-Integration based on Dynamic ODE Partitioning through Adaptively Refined Meshes for Compressible Fluid Dynamics (Daniel Doehring et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Doehring, Michael Schlottke-Lakemper, Gregor J. Gassner, Manuel Torrilhon. (2024)<br><strong>Multirate Time-Integration based on Dynamic ODE Partitioning through Adaptively Refined Meshes for Compressible Fluid Dynamics</strong><br><button class=copy-to-clipboard title="Multirate Time-Integration based on Dynamic ODE Partitioning through Adaptively Refined Meshes for Compressible Fluid Dynamics" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65L06, 65M20, 76Mxx, 76Nxx, cs-NA, math-MP, math-NA, math-ph, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05144v1.pdf filename=2403.05144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we apply the Paired-Explicit Runge-Kutta (P-ERK) schemes by Vermeire et. al. (2019, 2022) to dynamically partitioned systems arising from adaptive mesh refinement. The P-ERK schemes enable multirate time-integration with no changes in the spatial discretization methodology, making them readily implementable in existing codes that employ a method-of-lines approach. We show that speedup compared to a range of state of the art Runge-Kutta methods can be realized, despite additional overhead due to the dynamic re-assignment of flagging variables and restricting nonlinear stability properties. The effectiveness of the approach is demonstrated for a range of <b>simulation</b> setups for viscous and inviscid convection-dominated compressible flows for which we provide a reproducibility repository. In addition, we perform a thorough investigation of the nonlinear stability properties of the Paired-Explicit Runge-Kutta schemes regarding limitations due to the violation of monotonicity properties of the underlying spatial discretization. Furthermore, we present a novel approach for estimating the relevant eigenvalues of large Jacobians required for the optimization of stability polynomials.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--238270-the-r2d2-deep-neural-network-series-paradigm-for-fast-precision-imaging-in-radio-astronomy-amir-aghabiglou-et-al-2024>(1/1 | 238/270) The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy (Amir Aghabiglou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Aghabiglou, Chung San Chu, Arwa Dabbech, Yves Wiaux. (2024)<br><strong>The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy</strong><br><button class=copy-to-clipboard title="The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-CV, cs-LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05452v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05452v2.pdf filename=2403.05452v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radio-interferometric (RI) imaging entails solving high-resolution high-dynamic range inverse problems from large data volumes. Recent image reconstruction techniques grounded in optimization theory have demonstrated remarkable capability for imaging precision, well beyond CLEAN&rsquo;s capability. These range from advanced proximal algorithms propelled by handcrafted regularization operators, such as the SARA family, to hybrid plug-and-play (PnP) algorithms propelled by learned regularization denoisers, such as AIRI. Optimization and PnP structures are however highly iterative, which hinders their ability to handle the extreme data sizes expected from future instruments. To address this scalability challenge, we introduce a novel deep learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic range imaging&rsquo;&rsquo;. R2D2&rsquo;s reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration&rsquo;s image estimate and associated data residual as inputs. It thus takes a hybrid structure between a PnP algorithm and a learned version of the matching pursuit algorithm that underpins CLEAN. We present a comprehensive study of our approach, featuring its multiple incarnations distinguished by their DNN architectures. We provide a detailed description of its training process, targeting a telescope-specific approach. R2D2&rsquo;s capability to deliver high precision is demonstrated in <b>simulation,</b> across a variety of image and observation settings using the Very Large Array (VLA). Its reconstruction speed is also demonstrated: with only few iterations required to clean data residuals at dynamic ranges up to 100000, R2D2 opens the door to fast precision imaging. R2D2 codes are available in the BASPLib library on GitHub.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--239270-numerical-simulations-of-a-stochastic-dynamics-leading-to-cascades-and-loss-of-regularity-applications-to-fluid-turbulence-and-generation-of-fractional-gaussian-fields-geoffrey-beck-et-al-2024>(1/1 | 239/270) Numerical simulations of a stochastic dynamics leading to cascades and loss of regularity: applications to fluid turbulence and generation of fractional Gaussian fields (Geoffrey Beck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geoffrey Beck, Charles-Edouard Bréhier, Laurent Chevillard, Ricardo Grande, Wandrille Ruffenach. (2024)<br><strong>Numerical simulations of a stochastic dynamics leading to cascades and loss of regularity: applications to fluid turbulence and generation of fractional Gaussian fields</strong><br><button class=copy-to-clipboard title="Numerical simulations of a stochastic dynamics leading to cascades and loss of regularity: applications to fluid turbulence and generation of fractional Gaussian fields" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-NA, math-NA, physics-comp-ph, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05401v1.pdf filename=2403.05401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the modeling of the spatial structure of the velocity field of three-dimensional turbulent flows, and the phenomenology of cascade phenomena, a linear dynamics has been recently proposed able to generate high velocity gradients from a smooth-in-space forcing term. It is based on a linear Partial Differential Equation (PDE) stirred by an additive random forcing term which is delta-correlated in time. The underlying proposed deterministic mechanism corresponds to a transport in Fourier space which aims at transferring energy injected at large scales towards small scales. The key role of the random forcing is to realize these transfers in a statistically homogeneous way. Whereas at finite times and positive viscosity the solutions are smooth, a loss of regularity is observed for the statistically stationary state in the inviscid limit. We here present novel <b>simulations,</b> based on finite volume methods in the Fourier domain and a splitting method in time, which are more accurate than the pseudo-spectral <b>simulations.</b> We show that the novel algorithm is able to reproduce accurately the expected local and statistical structure of the predicted solutions. We conduct numerical <b>simulations</b> in one, two and three spatial dimensions, and we display the solutions both in physical and Fourier spaces. We additionally display key statistical quantities such as second-order structure functions and power spectral densities at various viscosities.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--240270-variational-inference-of-parameters-in-opinion-dynamics-models-jacopo-lenti-et-al-2024>(1/2 | 240/270) Variational Inference of Parameters in Opinion Dynamics Models (Jacopo Lenti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacopo Lenti, Fabrizio Silvestri, Gianmarco De Francisci Morales. (2024)<br><strong>Variational Inference of Parameters in Opinion Dynamics Models</strong><br><button class=copy-to-clipboard title="Variational Inference of Parameters in Opinion Dynamics Models" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-LG, cs-SI, cs.CY, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05358v1.pdf filename=2403.05358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the frequent use of agent-based models (ABMs) for studying social phenomena, parameter estimation remains a challenge, often relying on costly <b>simulation-based</b> heuristics. This work uses variational inference to estimate the parameters of an opinion dynamics ABM, by transforming the estimation problem into an optimization task that can be solved directly. Our proposal relies on probabilistic generative ABMs (PGABMs): we start by synthesizing a probabilistic generative model from the ABM rules. Then, we transform the inference process into an optimization problem suitable for automatic differentiation. In particular, we use the Gumbel-Softmax reparameterization for categorical agent attributes and stochastic variational inference for parameter estimation. Furthermore, we explore the trade-offs of using variational distributions with different complexity: normal distributions and normalizing flows. We validate our method on a bounded confidence model with agent roles (leaders and followers). Our approach estimates both macroscopic (bounded confidence intervals and backfire thresholds) and microscopic ($200$ categorical, agent-level roles) more accurately than <b>simulation-based</b> and MCMC methods. Consequently, our technique enables experts to tune and validate their ABMs against real-world observations, thus providing insights into human behavior in social systems via data-driven analysis.</p></p class="citation"></blockquote><h3 id=22--241270-interoperability-of-the-metaverse-a-digital-ecosystem-perspective-review-liang-yang-et-al-2024>(2/2 | 241/270) Interoperability of the Metaverse: A Digital Ecosystem Perspective Review (Liang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Yang, Shi-Ting Ni, Yuyang Wang, Ao Yu, Jyh-An Lee, Pan Hui. (2024)<br><strong>Interoperability of the Metaverse: A Digital Ecosystem Perspective Review</strong><br><button class=copy-to-clipboard title="Interoperability of the Metaverse: A Digital Ecosystem Perspective Review" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05205v1.pdf filename=2403.05205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Metaverse is at the vanguard of the impending digital revolution, with the potential to significantly transform industries and lifestyles. However, in 2023, skepticism surfaced within industrial and academic spheres, raising concerns that excitement may outpace actual technological progress. Interoperability, recognized as a major barrier to the Metaverse&rsquo;s full potential, is central to this debate. CoinMarketCap&rsquo;s report in February 2023 indicated that of over 240 metaverse initiatives, most existed in isolation, underscoring the interoperability challenge. Despite consensus on its critical role, there is a research gap in exploring the impact on the Metaverse, significance, and developmental extent. Our study bridges this gap via a systematic literature review and content analysis of the Web of Science (WoS) and Scopus databases, yielding 74 publications after a rigorous selection process. Interoperability, difficult to define due to varied contexts and lack of standardization, is central to the Metaverse, often seen as a digital ecosystem. Urs Gasser&rsquo;s framework from Harvard Law School, outlining technological, data, human, and institutional dimensions, systematically addresses interoperability complexities. Incorporating this framework, we dissect literature for a comprehensive Metaverse interoperability overview. Our study seeks to establish <b>benchmarks</b> for future inquiries, navigating the complex field of Metaverse interoperability studies and contributing to academic advancement.</p></p class="citation"></blockquote><h2 id=csce-3>cs.CE (3)</h2><h3 id=13--242270-modeling-of-progressive-high-cycle-fatigue-in-composite-laminates-accounting-for-local-stress-ratios-p-hofman-et-al-2024>(1/3 | 242/270) Modeling of progressive high-cycle fatigue in composite laminates accounting for local stress ratios (P. Hofman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>P. Hofman, F. P. van der Meer, L. J. Sluys. (2024)<br><strong>Modeling of progressive high-cycle fatigue in composite laminates accounting for local stress ratios</strong><br><button class=copy-to-clipboard title="Modeling of progressive high-cycle fatigue in composite laminates accounting for local stress ratios" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05356v1.pdf filename=2403.05356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A numerical framework for simulating progressive failure under high-cycle fatigue loading is validated against experiments of composite quasi-isotropic open-hole laminates. Transverse matrix cracking and delamination are modeled with a mixed-mode fatigue cohesive zone model, covering crack initiation and propagation. Furthermore, XFEM is used for simulating transverse matrix cracks and splits at arbitrary locations. An adaptive cycle jump approach is employed for efficiently simulating high-cycle fatigue while accounting for local stress ratio variations in the presence of thermal residual stresses. The cycle jump scheme is integrated in the XFEM framework, where the local stress ratio is used to determine the insertion of cracks and to propagate fatigue damage. The fatigue cohesive zone model is based on S-N curves and requires static material properties and only a few fatigue parameters, calibrated on simple fracture testing specimens. The <b>simulations</b> demonstrate a good correspondence with experiments in terms of fatigue life and damage evolution.</p></p class="citation"></blockquote><h3 id=23--243270-robust-automated-calcification-meshing-for-biomechanical-cardiac-digital-twins-daniel-h-pak-et-al-2024>(2/3 | 243/270) Robust automated calcification meshing for biomechanical cardiac digital twins (Daniel H. Pak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel H. Pak, Minliang Liu, Theodore Kim, Caglar Ozturk, Raymond McKay, Ellen T. Roche, Rudolph Gleason, James S. Duncan. (2024)<br><strong>Robust automated calcification meshing for biomechanical cardiac digital twins</strong><br><button class=copy-to-clipboard title="Robust automated calcification meshing for biomechanical cardiac digital twins" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-CV, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04998v1.pdf filename=2403.04998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Calcification has significant influence over cardiovascular diseases and interventions. Detailed characterization of calcification is thus desired for predictive modeling, but calcified heart meshes for physics-driven <b>simulations</b> are still often reconstructed using manual operations. This poses a major bottleneck for large-scale adoption of computational <b>simulations</b> for research or clinical use. To address this, we propose an end-to-end automated meshing algorithm that enables robust incorporation of patient-specific calcification onto a given heart mesh. The algorithm provides a substantial speed-up from several hours of manual meshing to $\sim$1 minute of automated computation, and it solves an important problem that cannot be addressed with recent template registration-based heart meshing techniques. We validated our final calcified heart meshes with extensive <b>simulations,</b> demonstrating our ability to accurately model patient-specific aortic stenosis and Transcatheter Aortic Valve Replacement. Our method may serve as an important tool for accelerating the development and usage of physics-driven <b>simulations</b> for cardiac digital twins.</p></p class="citation"></blockquote><h3 id=33--244270-rediscovering-the-mullins-effect-with-deep-symbolic-regression-rasul-abdusalamov-et-al-2024>(3/3 | 244/270) Rediscovering the Mullins Effect With Deep Symbolic Regression (Rasul Abdusalamov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rasul Abdusalamov, Jendrik Weise, Mikhail Itskov. (2024)<br><strong>Rediscovering the Mullins Effect With Deep Symbolic Regression</strong><br><button class=copy-to-clipboard title="Rediscovering the Mullins Effect With Deep Symbolic Regression" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05495v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05495v2.pdf filename=2403.05495v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Mullins effect represents a softening phenomenon observed in rubber-like materials and soft biological tissues. It is usually accompanied by many other inelastic effects like for example residual strain and induced anisotropy. In spite of the long term research and many material models proposed in literature, accurate modeling and prediction of this complex phenomenon still remain a challenging task. In this work, we present a novel approach using deep symbolic regression (DSR) to generate material models describing the Mullins effect in the context of nearly incompressible hyperelastic materials. The two step framework first identifies a strain energy function describing the primary loading. Subsequently, a damage function characterizing the softening behavior under cyclic loading is identified. The efficiency of the proposed approach is demonstrated through <b>benchmark</b> tests using the generalized the Mooney-Rivlin and the Ogden-Roxburgh model. The generalizability and robustness of the presented framework are thoroughly studied. In addition, the proposed methodology is extensively validated on a temperature-dependent data set, which demonstrates its versatile and reliable performance.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--245270-wykorzystanie-rekonfigurowalnych-iinteligentnych-matryc-antenowych-w-łączu-dosyłowym-sieci-5g6g-wykorzystującej-bezzałogowe-statki-powietrzne-salim-janji-et-al-2024>(1/3 | 245/270) Wykorzystanie Rekonfigurowalnych Iinteligentnych Matryc Antenowych w Łączu Dosyłowym Sieci 5G/6G Wykorzystującej Bezzałogowe Statki Powietrzne (Salim Janji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salim Janji, Paweł Sroka, Adrian Kliks. (2024)<br><strong>Wykorzystanie Rekonfigurowalnych Iinteligentnych Matryc Antenowych w Łączu Dosyłowym Sieci 5G/6G Wykorzystującej Bezzałogowe Statki Powietrzne</strong><br><button class=copy-to-clipboard title="Wykorzystanie Rekonfigurowalnych Iinteligentnych Matryc Antenowych w Łączu Dosyłowym Sieci 5G/6G Wykorzystującej Bezzałogowe Statki Powietrzne" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05301v1.pdf filename=2403.05301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drony, dzi\k{e}ki mo.zliwo'sci ich szybkiego rozmieszczenia w trudnym terenie, uwa.zane s\k{a} za jeden z kluczowych element'ow system'ow bezprzewodowych 6G. Jednak w celu wykorzystania ich jako punkty dost\k{e}powe sieci konieczne jest zapewnienie {\l}\k{a}cza dosy{\l}owego o odpowiedniej przepustowo'sci. Dlatego w niniejszym artykule rozwa.zane jest zwi\k{e}kszenie zasi\k{e}gu sieci bezprzewodowej przez zapewnienie {\l}\k{a}cza dosy{\l}owego dla ko'ncowego punktu dost\k{e}powego z wykorzystaniem okre'slonej liczby dron'ow-przeka'znik'ow oraz rekonfigurowalnych inteligentnych matryc antenowych (RIS). Zaprezentowane wyniki bada'n symulacyjnych pokazuj\k{a}, .ze u.zycie RIS pozwala na znacz\k{a}ce zwi\k{e}kszenie zasi\k{e}gu sieci bez konieczno'sci stosowania dodatkowych przeka'znik'ow. &ndash; Unmanned Aerial Vehicles, due to the possibility of their fast deployment, are considered an essential element of the future wireless 6G communication systems. However, an essential enabler for their use as access points is to provide a sufficient throughput wireless backhaul link. Thus, in this paper we consider the aspect of extension of network coverage with the use of drone-based relaying and reconfigurable intelligent surfaces (RIS) for backhauling. Presented results of <b>simulation</b> experiments indicate that the use of RIS allows for significant improvement of network coverage without the need to use additional relays.</p></p class="citation"></blockquote><h3 id=23--246270-ris-aided-multi-hop-backhauling-for-5g6g-uav-assisted-access-points-salim-janji-et-al-2024>(2/3 | 246/270) RIS-aided multi-hop backhauling for 5G/6G UAV-assisted access points (Salim Janji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salim Janji, Paweł Sroka. (2024)<br><strong>RIS-aided multi-hop backhauling for 5G/6G UAV-assisted access points</strong><br><button class=copy-to-clipboard title="RIS-aided multi-hop backhauling for 5G/6G UAV-assisted access points" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05292v1.pdf filename=2403.05292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drones are envisaged as an important part of the future 6G systems. With the possibility of their fast deployment they provide additional connectivity options in form of a hotspot. However, typically in such a use case they require provisioning of a wireless backhaul link to facilitate their proper operation, which might be a challenging task in urban environment. One of the possible ways to connect such nodes is to use the integrate access and backhaul (IAB) approach, where part of the spectrum dedicated for user access at the base station is used for wireless backhauling. Thus, in this work we consider the problem of establishing a multi-hop wireless backhaul link following the IAB concept, with the aid of drone relay stations (DRSs) and reconfigurable intelligent surfaces (RISs). We formulate the problem of coverage improvement with fixed number of relays assuming certain throughput requirements on the backhaul. We show with <b>simulations</b> that the use of RISs allows for improvement of coverage in such a scenario or reduction in the number of involved nodes to provide the required backhaul.</p></p class="citation"></blockquote><h3 id=33--247270-adroit6g-dai-driven-open-and-programmable-architecture-for-6g-networks-christophoros-christophorou-et-al-2024>(3/3 | 247/270) ADROIT6G DAI-driven Open and Programmable Architecture for 6G Networks (Christophoros Christophorou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christophoros Christophorou, Iacovos Ioannou, Vasos Vassiliou, Loizos Christofi, John S Vardakas, Erin E Seder, Carla Fabiana Chiasserini, Marius Iordache, Chaouki Ben Issaid, Ioannis Markopoulos, Giulio Franzese, Tanel Järvet, Christos Verikoukis. (2024)<br><strong>ADROIT6G DAI-driven Open and Programmable Architecture for 6G Networks</strong><br><button class=copy-to-clipboard title="ADROIT6G DAI-driven Open and Programmable Architecture for 6G Networks" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05277v1.pdf filename=2403.05277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the upcoming 6G era, mobile networks must deal with more challenging applications (e.g., holographic telepresence and immersive communication) and meet far more stringent application requirements <b>stemming</b> along the edge-cloud continuum. These new applications will create an elevated level of expectations on performance, reliability, ubiquity, trustworthiness, security, openness, and sustainability, pushing the boundaries of innovation and driving transformational change across the architecture of future mobile networks. Towards this end, ADROIT6G proposes a set of disruptive innovations with a clear vision on setting a 6G network architecture that can be tailored to the requirements of innovative applications and match the ambitious KPIs set for 6G networks. More specifically, the key transformations that ADROIT6G considers essential to 6G network evolution are: i) AI/ML-powered optimisations across the network, exploring solutions in the &ldquo;Distributed Artificial Intelligence (DAI)&rdquo; domain for high performance and automation; ii) Transforming to fully cloud-native network software, which can be implemented across various edge-cloud platforms, with security built integrally into the network user plan; and iii) Software driven, zero-touch operations and ultimately automation of every aspect of the network and the services it delivers.</p></p class="citation"></blockquote><h2 id=quant-ph-3>quant-ph (3)</h2><h3 id=13--248270-load-balancing-for-high-performance-computing-using-quantum-annealing-omer-rathore-et-al-2024>(1/3 | 248/270) Load Balancing For High Performance Computing Using Quantum Annealing (Omer Rathore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omer Rathore, Alastair Basden, Nicholas Chancellor, Halim Kusumaatmaja. (2024)<br><strong>Load Balancing For High Performance Computing Using Quantum Annealing</strong><br><button class=copy-to-clipboard title="Load Balancing For High Performance Computing Using Quantum Annealing" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DC, physics-comp-ph, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05278v1.pdf filename=2403.05278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of exascale computing, effective load balancing in massively parallel software applications is critically important for leveraging the full potential of high performance computing systems. Load balancing is the distribution of computational work between available processors. Here, we investigate the application of quantum annealing to load balance two paradigmatic algorithms in high performance computing. Namely, adaptive mesh refinement and smoothed particle hydrodynamics are chosen as representative grid and off-grid target applications. While the methodology for obtaining real <b>simulation</b> data to partition is application specific, the proposed balancing protocol itself remains completely general. In a grid based context, quantum annealing is found to outperform classical methods such as the round robin protocol but lacks a decisive advantage over more advanced methods such as steepest descent or simulated annealing despite remaining competitive. The primary obstacle to scalability is found to be limited coupling on current quantum annealing hardware. However, for the more complex particle formulation, approached as a multi-objective optimization, quantum annealing solutions are demonstrably Pareto dominant to state of the art classical methods across both objectives. This signals a noteworthy advancement in solution quality which can have a large impact on effective CPU usage.</p></p class="citation"></blockquote><h3 id=23--249270-q-chop-quantum-constrained-hamiltonian-optimization-michael-a-perlin-et-al-2024>(2/3 | 249/270) Q-CHOP: Quantum constrained Hamiltonian optimization (Michael A. Perlin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael A. Perlin, Ruslan Shaydulin, Benjamin P. Hall, Pierre Minssen, Changhao Li, Kabir Dubey, Rich Rines, Eric R. Anschuetz, Marco Pistoia, Pranav Gokhale. (2024)<br><strong>Q-CHOP: Quantum constrained Hamiltonian optimization</strong><br><button class=copy-to-clipboard title="Q-CHOP: Quantum constrained Hamiltonian optimization" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, quant-ph, quant-ph<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05653v1.pdf filename=2403.05653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Combinatorial optimization problems that arise in science and industry typically have constraints. Yet the presence of constraints makes them challenging to tackle using both classical and quantum optimization algorithms. We propose a new quantum algorithm for constrained optimization, which we call quantum constrained Hamiltonian optimization (Q-CHOP). Our algorithm leverages the observation that for many problems, while the best solution is difficult to find, the worst feasible (constraint-satisfying) solution is known. The basic idea is to to enforce a Hamiltonian constraint at all times, thereby restricting evolution to the subspace of feasible states, and slowly &ldquo;rotate&rdquo; an objective Hamiltonian to trace an adiabatic path from the worst feasible state to the best feasible state. We additionally propose a version of Q-CHOP that can start in any feasible state. Finally, we <b>benchmark</b> Q-CHOP against the commonly-used adiabatic algorithm with constraints enforced using a penalty term and find that Q-CHOP performs consistently better on a wide range of problems, including textbook problems on <b>graphs,</b> knapsack, combinatorial auction, as well as a real-world financial use case, namely bond exchange-traded fund basket optimization.</p></p class="citation"></blockquote><h3 id=33--250270-maximal-non-kochen-specker-sets-and-a-lower-bound-on-the-size-of-kochen-specker-sets-tom-williams-et-al-2024>(3/3 | 250/270) Maximal Non-Kochen-Specker Sets and a Lower Bound on the Size of Kochen-Specker Sets (Tom Williams et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Williams, Andrei Constantin. (2024)<br><strong>Maximal Non-Kochen-Specker Sets and a Lower Bound on the Size of Kochen-Specker Sets</strong><br><button class=copy-to-clipboard title="Maximal Non-Kochen-Specker Sets and a Lower Bound on the Size of Kochen-Specker Sets" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DM, math-MG, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05230v1.pdf filename=2403.05230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A Kochen-Specker (KS) set is a finite collection of vectors on the two-sphere containing no antipodal pairs for which it is impossible to assign 0s and 1s such that no two orthogonal vectors are assigned 1 and exactly one vector in every triplet of mutually orthogonal vectors is assigned 1. The existence of KS sets lies at the heart of Kochen and Specker&rsquo;s argument against non-contextual hidden variable theories and the Conway-Kochen free will theorem. Identifying small KS sets can simplify these arguments and may contribute to the understanding of the role played by contextuality in quantum protocols. In this paper we derive a weak lower bound of 10 vectors for the size of any KS set by studying the opposite notion of large non-KS sets and using a probability argument that is independent of the <b>graph</b> structure of KS sets. We also point out an interesting connection with a generalisation of the moving sofa problem around a right-angled hallway on the two-sphere.</p></p class="citation"></blockquote><h2 id=cset-2>cs.ET (2)</h2><h3 id=12--251270-user-connection-and-resource-allocation-optimization-in-blockchain-empowered-metaverse-over-6g-wireless-communications-liangxin-qian-et-al-2024>(1/2 | 251/270) User Connection and Resource Allocation Optimization in Blockchain Empowered Metaverse over 6G Wireless Communications (Liangxin Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangxin Qian, Chang Liu, Jun Zhao. (2024)<br><strong>User Connection and Resource Allocation Optimization in Blockchain Empowered Metaverse over 6G Wireless Communications</strong><br><button class=copy-to-clipboard title="User Connection and Resource Allocation Optimization in Blockchain Empowered Metaverse over 6G Wireless Communications" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs-NI, cs.ET, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05116v1.pdf filename=2403.05116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The convergence of blockchain, Metaverse, and non-fungible tokens (NFTs) brings transformative digital opportunities alongside challenges like privacy and resource management. Addressing these, we focus on optimizing user connectivity and resource allocation in an NFT-centric and blockchain-enabled Metaverse in this paper. Through user work-offloading, we optimize data tasks, user connection parameters, and server computing frequency division. In the resource allocation phase, we optimize communication-computation resource distributions, including bandwidth, transmit power, and computing frequency. We introduce the trust-cost ratio (TCR), a pivotal measure combining trust scores from users&rsquo; resources and server history with delay and energy costs. This balance ensures sustained user engagement and trust. The DASHF algorithm, central to our approach, encapsulates the Dinkelbach algorithm, alternating optimization, semidefinite relaxation (SDR), the Hungarian method, and a novel fractional programming technique from a recent IEEE JSAC paper [2]. The most challenging part of DASHF is to rewrite an optimization problem as Quadratically Constrained Quadratic Programming (QCQP) via carefully designed transformations, in order to be solved by SDR and the Hungarian algorithm. Extensive <b>simulations</b> validate the DASHF algorithm&rsquo;s efficacy, revealing critical insights for enhancing blockchain-Metaverse applications, especially with NFTs.</p></p class="citation"></blockquote><h3 id=22--252270-paving-the-way-for-pass-disturb-free-vertical-nand-storage-via-a-dedicated-and-string-compatible-pass-gate-zijian-zhao-et-al-2024>(2/2 | 252/270) Paving the Way for Pass Disturb Free Vertical NAND Storage via A Dedicated and String-Compatible Pass Gate (Zijian Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Zhao, Sola Woo, Khandker Akif Aabrar, Sharadindu Gopal Kirtania, Zhouhang Jiang, Shan Deng, Yi Xiao, Halid Mulaosmanovic, Stefan Duenkel, Dominik Kleimaier, Steven Soss, Sven Beyer, Rajiv Joshi, Scott Meninger, Mohamed Mohamed, Kijoon Kim, Jongho Woo, Suhwan Lim, Kwangsoo Kim, Wanki Kim, Daewon Ha, Vijaykrishnan Narayanan, Suman Datta, Shimeng Yu, Kai Ni. (2024)<br><strong>Paving the Way for Pass Disturb Free Vertical NAND Storage via A Dedicated and String-Compatible Pass Gate</strong><br><button class=copy-to-clipboard title="Paving the Way for Pass Disturb Free Vertical NAND Storage via A Dedicated and String-Compatible Pass Gate" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs.ET<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04981v1.pdf filename=2403.04981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose a dual-port cell design to address the pass disturb in vertical NAND storage, which can pass signals through a dedicated and string-compatible pass gate. We demonstrate that: i) the pass disturb-free feature originates from weakening of the depolarization field by the pass bias at the high-${V}<em>{TH}$ (HVT) state and the screening of the applied field by channel at the low-${V}</em>{TH}$ (LVT) state; ii) combined <b>simulations</b> and experimental demonstrations of dual-port design verify the disturb-free operation in a NAND string, overcoming a key challenge in single-port designs; iii) the proposed design can be incorporated in a highly scaled vertical NAND FeFET string and the pass gate can be incorporated into the existing 3D NAND with the negligible overhead of the pass gate interconnection through a global bottom pass gate contact in the substrate.</p></p class="citation"></blockquote><h2 id=q-biope-1>q-bio.PE (1)</h2><h3 id=11--253270-information-theory-in-a-darwinian-evolution-population-dynamics-model-eddy-kwessi-2024>(1/1 | 253/270) Information Theory in a Darwinian Evolution Population Dynamics Model (Eddy Kwessi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eddy Kwessi. (2024)<br><strong>Information Theory in a Darwinian Evolution Population Dynamics Model</strong><br><button class=copy-to-clipboard title="Information Theory in a Darwinian Evolution Population Dynamics Model" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.PE<br>Categories: cs-IT, math-DS, math-IT, math-ST, q-bio-PE, q-bio.PE, stat-TH<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05044v1.pdf filename=2403.05044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using information theory, we propose an estimation method for traits parameters in a Darwinian evolution model for species with on trait or multiple traits. We use the Fisher&rsquo;s information to obtain the errors on the estimation for one species with one or multiple traits. We perform <b>simulations</b> to illustrate the method.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--254270-extracting-protein-protein-interactions-ppis-from-biomedical-literature-using-attention-based-relational-context-information-gilchan-park-et-al-2024>(1/1 | 254/270) Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information (Gilchan Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gilchan Park, Sean McCorkle, Carlos Soto, Ian Blaby, Shinjae Yoo. (2024)<br><strong>Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information</strong><br><button class=copy-to-clipboard title="Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-CL, cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 20<br>Keywords: Transformer, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05602v1.pdf filename=2403.05602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Because protein-protein interactions (PPIs) are crucial to understand living systems, harvesting these data is essential to probe disease development and discern gene/protein functions and biological processes. Some curated datasets contain PPI data derived from the literature and other sources (e.g., IntAct, BioGrid, DIP, and HPRD). However, they are far from exhaustive, and their maintenance is a labor-intensive process. On the other hand, machine learning methods to automate PPI knowledge extraction from the scientific literature have been limited by a shortage of appropriate annotated data. This work presents a unified, multi-source PPI corpora with vetted interaction definitions augmented by binary interaction type labels and a <b>Transformer-based</b> deep learning method that exploits entities&rsquo; <b>relational</b> <b>context</b> information for <b>relation</b> <b>representation</b> to improve <b>relation</b> <b>classification</b> performance. The model&rsquo;s performance is evaluated on four widely studied biomedical <b>relation</b> <b>extraction</b> datasets, as well as this work&rsquo;s target PPI datasets, to observe the effectiveness of the representation to <b>relation</b> <b>extraction</b> tasks in various data. Results show the model outperforms prior state-of-the-art models. The code and data are available at: <a href=https://github.com/BNLNLP/PPI-Relation-Extraction>https://github.com/BNLNLP/PPI-Relation-Extraction</a></p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--255270-online-contention-resolution-schemes-for-network-revenue-management-and-combinatorial-auctions-will-ma-et-al-2024>(1/1 | 255/270) Online Contention Resolution Schemes for Network Revenue Management and Combinatorial Auctions (Will Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Will Ma, Calum MacRury, Jingwei Zhang. (2024)<br><strong>Online Contention Resolution Schemes for Network Revenue Management and Combinatorial Auctions</strong><br><button class=copy-to-clipboard title="Online Contention Resolution Schemes for Network Revenue Management and Combinatorial Auctions" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT, math-OC<br>Keyword Score: 13<br>Keywords: Optical Character Recognition, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05378v1.pdf filename=2403.05378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the Network Revenue Management (NRM) problem, products composed of up to L resources are sold to stochastically arriving customers. We take a randomized rounding approach to NRM, motivated by developments in Online Contention Resolution Schemes <b>(OCRS).</b> The goal is to take a fractional solution to NRM that satisfies the resource constraints in expectation, and implement it in an online policy that satisfies the resource constraints in any state, while (approximately) preserving all of the sales that were prescribed by the fractional solution. <b>OCRS</b> cannot be naively applied to NRM or revenue management problems in general, because customer substitution induces a negative correlation in products being demanded. We start by deriving an <b>OCRS</b> that achieves a guarantee of 1/(1+L) for NRM with customer substitution, matching a common <b>benchmark</b> in the literature. We then show how to beat this <b>benchmark</b> for all integers L>1 assuming no substitution, i.e., in the standard <b>OCRS</b> setting. By contrast, we show that this <b>benchmark</b> is unbeatable using <b>OCRS</b> or any fractional relaxation if there is customer substitution, for all integers L that are the power of a prime number. Finally, we show how to beat 1/(1+L) even with customer substitution, if the products comprise one item from each of up to L groups. Our results have corresponding implications for Online Combinatorial Auctions, in which buyers bid for bundles of up to L items, and buyers being single-minded is akin to no substitution. Our final result also beats 1/(1+L) for Prophet Inequality on the intersection of L partition matroids. All in all, our paper provides a unifying framework for applying <b>OCRS</b> to these problems, delineating the impact of substitution, and establishing a separation between the guarantees achievable with vs. without substitution under general resource constraints parametrized by L.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--256270-node-centrality-approximation-for-large-networks-based-on-inductive-graph-neural-networks-yiwei-zou-et-al-2024>(1/1 | 256/270) Node Centrality Approximation For Large Networks Based On Inductive Graph Neural Networks (Yiwei Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Zou, Ting Li, Zong-fu Luo. (2024)<br><strong>Node Centrality Approximation For Large Networks Based On Inductive Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Node Centrality Approximation For Large Networks Based On Inductive Graph Neural Networks" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-SI, cs.SI<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04977v1.pdf filename=2403.04977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Closeness Centrality (CC) and Betweenness Centrality (BC) are crucial metrics in network analysis, providing essential reference for discerning the significance of nodes within complex networks. These measures find wide applications in critical tasks, such as community detection and network dismantling. However, their practical implementation on extensive networks remains computationally demanding due to their high time complexity. To mitigate these computational challenges, numerous approximation algorithms have been developed to expedite the computation of CC and BC. Nevertheless, even these approximations still necessitate substantial processing time when applied to large-scale networks. Furthermore, their output proves sensitive to even minor perturbations within the network structure. In this work, We redefine the CC and BC node ranking problem as a machine learning problem and propose the CNCA-IGE model, which is an encoder-decoder model based on inductive <b>graph</b> <b>neural</b> <b>networks</b> designed to rank nodes based on specified CC or BC metrics. We incorporate the MLP-Mixer model as the decoder in the BC ranking prediction task to enhance the model&rsquo;s robustness and capacity. Our approach is evaluated on diverse synthetic and real-world networks of varying scales, and the experimental results demonstrate that the CNCA-IGE model outperforms state-of-the-art baseline models, significantly reducing execution time while improving performance.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--257270-privacy-preserving-sharing-of-data-analytics-runtime-metrics-for-performance-modeling-jonathan-will-et-al-2024>(1/1 | 257/270) Privacy-Preserving Sharing of Data Analytics Runtime Metrics for Performance Modeling (Jonathan Will et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Will, Dominik Scheinert, Jan Bode, Cedric Kring, Seraphin Zunzer, Lauritz Thamsen. (2024)<br><strong>Privacy-Preserving Sharing of Data Analytics Runtime Metrics for Performance Modeling</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Sharing of Data Analytics Runtime Metrics for Performance Modeling" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05692v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05692v2.pdf filename=2403.05692v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Performance modeling for large-scale data analytics workloads can improve the efficiency of cluster resource allocations and job scheduling. However, the performance of these workloads is influenced by numerous factors, such as job inputs and the assigned cluster resources. As a result, performance models require significant amounts of training data. This data can be obtained by exchanging runtime metrics between collaborating organizations. Yet, not all organizations may be inclined to publicly disclose such metadata. We present a privacy-preserving approach for sharing runtime metrics based on <b>differential</b> <b>privacy</b> and data synthesis. Our evaluation on performance data from 736 Spark job executions indicates that fully anonymized training data largely maintains performance prediction accuracy, particularly when there is minimal original data available. With 30 or fewer available original data samples, the use of synthetic training data resulted only in a one percent reduction in performance model accuracy on average.</p></p class="citation"></blockquote><h2 id=statml-2>stat.ML (2)</h2><h3 id=12--258270-follow-the-perturbed-leader-with-fréchet-type-tail-distributions-optimality-in-adversarial-bandits-and-best-of-both-worlds-jongyeong-lee-et-al-2024>(1/2 | 258/270) Follow-the-Perturbed-Leader with Fréchet-type Tail Distributions: Optimality in Adversarial Bandits and Best-of-Both-Worlds (Jongyeong Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongyeong Lee, Junya Honda, Shinji Ito, Min-hwan Oh. (2024)<br><strong>Follow-the-Perturbed-Leader with Fréchet-type Tail Distributions: Optimality in Adversarial Bandits and Best-of-Both-Worlds</strong><br><button class=copy-to-clipboard title="Follow-the-Perturbed-Leader with Fréchet-type Tail Distributions: Optimality in Adversarial Bandits and Best-of-Both-Worlds" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05134v1.pdf filename=2403.05134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the optimality of the Follow-the-Perturbed-Leader (FTPL) policy in both adversarial and stochastic $K$-armed <b>bandits.</b> Despite the widespread use of the Follow-the-Regularized-Leader (FTRL) framework with various choices of regularization, the FTPL framework, which relies on random perturbations, has not received much attention, despite its inherent simplicity. In adversarial <b>bandits,</b> there has been conjecture that FTPL could potentially achieve $\mathcal{O}(\sqrt{KT})$ regrets if perturbations follow a distribution with a Fr'{e}chet-type tail. Recent work by Honda et al. (2023) showed that FTPL with Fr'{e}chet distribution with shape $\alpha=2$ indeed attains this bound and, notably logarithmic regret in stochastic <b>bandits,</b> meaning the Best-of-Both-Worlds (BOBW) capability of FTPL. However, this result only partly resolves the above conjecture because their analysis heavily relies on the specific form of the Fr'{e}chet distribution with this shape. In this paper, we establish a sufficient condition for perturbations to achieve $\mathcal{O}(\sqrt{KT})$ regrets in the adversarial setting, which covers, e.g., Fr'{e}chet, Pareto, and Student-$t$ distributions. We also demonstrate the BOBW achievability of FTPL with certain Fr'{e}chet-type tail distributions. Our results contribute not only to resolving existing conjectures through the lens of extreme value theory but also potentially offer insights into the effect of the regularization functions in FTRL through the mapping from FTPL to FTRL.</p></p class="citation"></blockquote><h3 id=22--259270-spectral-clustering-of-categorical-and-mixed-type-data-via-extra-graph-nodes-dylan-soemitro-et-al-2024>(2/2 | 259/270) Spectral Clustering of Categorical and Mixed-type Data via Extra Graph Nodes (Dylan Soemitro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dylan Soemitro, Jeova Farias Sales Rocha Neto. (2024)<br><strong>Spectral Clustering of Categorical and Mixed-type Data via Extra Graph Nodes</strong><br><button class=copy-to-clipboard title="Spectral Clustering of Categorical and Mixed-type Data via Extra Graph Nodes" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05669v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05669v1.pdf filename=2403.05669v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Clustering</b> data objects into homogeneous groups is one of the most important tasks in data mining. Spectral <b>clustering</b> is arguably one of the most important algorithms for <b>clustering,</b> as it is appealing for its theoretical soundness and is adaptable to many real-world data settings. For example, mixed data, where the data is composed of numerical and categorical features, is typically handled via numerical discretization, dummy coding, or similarity computation that takes into account both data types. This paper explores a more natural way to incorporate both numerical and categorical information into the spectral <b>clustering</b> algorithm, avoiding the need for data preprocessing or the use of sophisticated similarity functions. We propose adding extra nodes corresponding to the different categories the data may belong to and show that it leads to an interpretable <b>clustering</b> objective function. Furthermore, we demonstrate that this simple framework leads to a linear-time spectral <b>clustering</b> algorithm for categorical-only data. Finally, we compare the performance of our algorithms against other related methods and show that it provides a competitive alternative to them in terms of performance and runtime.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--260270-data-dependent-lsh-for-the-earth-movers-distance-rajesh-jayaram-et-al-2024>(1/2 | 260/270) Data-Dependent LSH for the Earth Mover&rsquo;s Distance (Rajesh Jayaram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rajesh Jayaram, Erik Waingarten, Tian Zhang. (2024)<br><strong>Data-Dependent LSH for the Earth Mover&rsquo;s Distance</strong><br><button class=copy-to-clipboard title="Data-Dependent LSH for the Earth Mover's Distance" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 10<br>Keywords: GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05041v1.pdf filename=2403.05041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give new data-dependent locality sensitive hashing schemes (LSH) for the Earth Mover&rsquo;s Distance ($\mathsf{EMD}$), and as a result, improve the best approximation for nearest neighbor search under $\mathsf{EMD}$ by a quadratic factor. Here, the metric $\mathsf{EMD}_s(\mathbb{R}^d,\ell_p)$ consists of sets of $s$ vectors in $\mathbb{R}^d$, and for any two sets $x,y$ of $s$ vectors the distance $\mathsf{EMD}(x,y)$ is the minimum cost of a perfect matching between $x,y$, where the cost of matching two vectors is their $\ell_p$ distance. Previously, Andoni, Indyk, and Krauthgamer gave a (data-independent) locality-sensitive hashing scheme for $\mathsf{EMD}_s(\mathbb{R}^d,\ell_p)$ when $p \in [1,2]$ with approximation $O(\log^2 s)$. By being data-dependent, we improve the approximation to $\tilde{O}(\log s)$. Our main technical contribution is to show that for any distribution $\mu$ supported on the metric $\mathsf{EMD}_s(\mathbb{R}^d, \ell_p)$, there exists a data-dependent LSH for dense regions of $\mu$ which achieves approximation $\tilde{O}(\log s)$, and that the data-independent LSH actually achieves a $\tilde{O}(\log s)$-approximation outside of those dense regions. Finally, we show how to <b>&ldquo;glue&rdquo;</b> together these two hashing schemes without any additional loss in the approximation. Beyond nearest neighbor search, our data-dependent LSH also gives optimal (distributional) sketches for the Earth Mover&rsquo;s Distance. By known sketching lower bounds, this implies that our LSH is optimal (up to $\mathrm{poly}(\log \log s)$ factors) among those that collide close points with constant probability.</p></p class="citation"></blockquote><h3 id=22--261270-efficient-algorithms-for-personalized-pagerank-computation-a-survey-mingji-yang-et-al-2024>(2/2 | 261/270) Efficient Algorithms for Personalized PageRank Computation: A Survey (Mingji Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingji Yang, Hanzhi Wang, Zhewei Wei, Sibo Wang, Ji-Rong Wen. (2024)<br><strong>Efficient Algorithms for Personalized PageRank Computation: A Survey</strong><br><button class=copy-to-clipboard title="Efficient Algorithms for Personalized PageRank Computation: A Survey" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05198v1.pdf filename=2403.05198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalized PageRank (PPR) is a traditional measure for node proximity on large <b>graphs.</b> For a pair of nodes $s$ and $t$, the PPR value $\pi_s(t)$ equals the probability that an $\alpha$-discounted random walk from $s$ terminates at $t$ and reflects the importance between $s$ and $t$ in a bidirectional way. As a generalization of Google&rsquo;s celebrated PageRank centrality, PPR has been extensively studied and has found multifaceted applications in many fields, such as network analysis, <b>graph</b> mining, and <b>graph</b> machine learning. Despite numerous studies devoted to PPR over the decades, efficient computation of PPR remains a challenging problem, and there is a dearth of systematic summaries and comparisons of existing algorithms. In this paper, we recap several frequently used techniques for PPR computation and conduct a comprehensive survey of various recent PPR algorithms from an algorithmic perspective. We classify these approaches based on the types of queries they address and review their methodologies and contributions. We also discuss some representative algorithms for computing PPR on dynamic <b>graphs</b> and in parallel or distributed environments.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--262270-splattingavatar-realistic-real-time-human-avatars-with-mesh-embedded-gaussian-splatting-zhijing-shao-et-al-2024>(1/1 | 262/270) SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting (Zhijing Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang. (2024)<br><strong>SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting</strong><br><button class=copy-to-clipboard title="SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-CV, cs-GR, cs.GR<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05087v1.pdf filename=2403.05087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh <b>geometry</b> and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency <b>geometry</b> and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--263270-geometric-neural-network-based-on-phase-space-for-bci-decoding-igor-carrara-et-al-2024>(1/1 | 263/270) Geometric Neural Network based on Phase Space for BCI decoding (Igor Carrara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Igor Carrara, Bruno Aristimunha, Marie-Constance Corsi, Raphael Y. de Camargo, Sylvain Chevallier, Théodore Papadopoulo. (2024)<br><strong>Geometric Neural Network based on Phase Space for BCI decoding</strong><br><button class=copy-to-clipboard title="Geometric Neural Network based on Phase Space for BCI decoding" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: I-5-1; I-6-3; I-2-6, cs-AI, cs-LG, eess-SP, eess.SP, q-bio-NC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05645v1.pdf filename=2403.05645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of Deep Learning (DL) algorithms on brain signal analysis is still in its nascent stages compared to their success in fields like Computer Vision, especially in Brain-Computer Interface (BCI), where the brain activity is decoded to control external devices without requiring muscle control. Electroencephalography (EEG) is a widely adopted choice for designing BCI systems due to its non-invasive and cost-effective nature and excellent temporal resolution. Still, it comes at the expense of limited training data, poor signal-to-noise, and a large variability across and within-subject recordings. Finally, setting up a BCI system with many electrodes takes a long time, hindering the widespread adoption of reliable DL architectures in BCIs outside research laboratories. To improve adoption, we need to improve user comfort using, for instance, reliable algorithms that operate with few electrodes. \textbf{Approach:} Our research aims to develop a DL algorithm that delivers effective results with a limited number of electrodes. Taking advantage of the Augmented Covariance Method with SPDNet, we propose the SPDNet$<em>{\psi}$ architecture and analyze its performance and computational impact, as well as the interpretability of the results. The evaluation is conducted on 5-fold cross-validation, using only three electrodes positioned above the Motor Cortex. The methodology was tested on nearly 100 subjects from several open-source datasets using the Mother Of All BCI <b>Benchmark</b> (MOABB) framework. \textbf{Main results:} The results of our SPDNet$</em>{\psi}$ demonstrate that the augmented approach combined with the SPDNet significantly outperforms all the current state-of-the-art DL architecture in MI decoding. \textbf{Significance:} This new architecture is explainable, with a low number of trainable parameters and a reduced carbon footprint.</p></p class="citation"></blockquote><h2 id=mathco-4>math.CO (4)</h2><h3 id=14--264270-the-metric-menger-problem-júlia-baligács-et-al-2024>(1/4 | 264/270) The metric Menger problem (Júlia Baligács et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Júlia Baligács, Joseph MacManus. (2024)<br><strong>The metric Menger problem</strong><br><button class=copy-to-clipboard title="The metric Menger problem" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 68R10, cs-CC, math-CO, math-MG, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05630v1.pdf filename=2403.05630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a generalization of the well-known disjoint paths problem which we call the metric Menger problem, denoted MM(r,k), where one is given two subsets of a <b>graph</b> and must decide whether they can be connected by $k$ paths of pairwise distance at least $r$. We prove that this problem is NP-complete for every $r\geq 3$ and $k\geq 2$ by giving a reduction from 3SAT. This resolves a conjecture recently stated by Georgakopoulos and Papasoglu. On the other hand, we show that the problem is in XP when parameterised by treewidth and maximum degree by observing that it is `locally checkable&rsquo;. In the case $r\leq 3$, we prove that it suffices to parameterise by treewidth. We also state some open questions relating to this work.</p></p class="citation"></blockquote><h3 id=24--265270-on-balanceable-and-simply-balanceable-regular-graphs-milad-ahanjideh-et-al-2024>(2/4 | 265/270) On balanceable and simply balanceable regular graphs (Milad Ahanjideh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Milad Ahanjideh, Martin Milanič, Mary Servatius. (2024)<br><strong>On balanceable and simply balanceable regular graphs</strong><br><button class=copy-to-clipboard title="On balanceable and simply balanceable regular graphs" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C55 (Primary), 05C75, 68Q25 (Secondary), cs-CC, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05418v1.pdf filename=2403.05418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We continue the study of balanceable <b>graphs,</b> defined by Caro, Hansberg, and Montejano in 2021 as <b>graphs</b> $G$ such that any $2$-coloring of the edges of a sufficiently large complete <b>graph</b> containing sufficiently many edges of each color contains a balanced copy of $G$. While the problem of recognizing balanceable <b>graphs</b> was conjectured to be NP-complete by Dailly, Hansberg, and Ventura in 2021, balanceable <b>graphs</b> admit an elegant combinatorial characterization: a <b>graph</b> is balanceable if and only there exist two vertex subsets, one containing half of all the <b>graph&rsquo;s</b> edges and another one such that the corresponding cut contains half of all the <b>graph&rsquo;s</b> edges. We consider a special case of this property, namely when one of the two sets is a vertex cover, and call the corresponding <b>graphs</b> simply balanceable. We prove a number of results on balanceable and simply balanceable regular <b>graphs.</b> First, we characterize simply balanceable regular <b>graphs</b> via a condition involving the independence number of the <b>graph.</b> Second, we address a question of Dailly, Hansberg, and Ventura from 2021 and show that every cubic <b>graph</b> is balanceable. Third, using Brooks&rsquo; theorem, we show that every $4$-regular <b>graph</b> with order divisible by $4$ is balanceable. Finally, we show that it is NP-complete to determine if a $9$-regular <b>graph</b> is simply balanceable.</p></p class="citation"></blockquote><h3 id=34--266270-path-eccentricity-of-k-at-free-graphs-and-application-on-graphs-with-the-consecutive-ones-property-paul-bastide-et-al-2024>(3/4 | 266/270) Path eccentricity of $k$-AT-free graphs and application on graphs with the consecutive ones property (Paul Bastide et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Bastide, Claire Hilaire, Eileen Robinson. (2024)<br><strong>Path eccentricity of $k$-AT-free graphs and application on graphs with the consecutive ones property</strong><br><button class=copy-to-clipboard title="Path eccentricity of $k$-AT-free graphs and application on graphs with the consecutive ones property" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05360v1.pdf filename=2403.05360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The central path problem is a variation on the single facility location problem. The aim is to find, in a given connected <b>graph</b> $G$, a path $P$ minimizing its eccentricity, which is the maximal distance from $P$ to any vertex of the <b>graph</b> $G$. The path eccentricity of $G$ is the minimal eccentricity achievable over all paths in $G$. In this article we consider the path eccentricity of the class of the $k$-AT-free <b>graphs.</b> They are <b>graphs</b> in which any set of three vertices contains a pair for which every path between them uses at least one vertex of the closed neighborhood at distance $k$ of the third. We prove that they have path eccentricity bounded by $k$. Moreover, we answer a question of G'omez and Guti'errez asking if there is a relation between path eccentricity and the consecutive ones property. The latter is the property for a binary matrix to admit a permutation of the rows placing the 1&rsquo;s consecutively on the columns. It was already known that <b>graphs</b> whose adjacency matrices have the consecutive ones property have path eccentricity at most 1, and that the same remains true when the augmented adjacency matrices (with ones on the diagonal) has the consecutive ones property. We generalize these results as follow. We study <b>graphs</b> whose adjacency matrices can be made to satisfy the consecutive ones property after changing some values on the diagonal, and show that those <b>graphs</b> have path eccentricity at most 2, by showing that they are 2-AT-free.</p></p class="citation"></blockquote><h3 id=44--267270-extremal-chemical-graphs-for-the-arithmetic-geometric-index-alain-hertz-et-al-2024>(4/4 | 267/270) Extremal Chemical Graphs for the Arithmetic-Geometric Index (Alain Hertz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alain Hertz, Sébastien Bonte, Gauvain Devillez, Valentin Dusollier, Hadrien Mélot, David Schindl. (2024)<br><strong>Extremal Chemical Graphs for the Arithmetic-Geometric Index</strong><br><button class=copy-to-clipboard title="Extremal Chemical Graphs for the Arithmetic-Geometric Index" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05226v1.pdf filename=2403.05226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The arithmetic-geometric index is a newly proposed degree-based <b>graph</b> invariant in mathematical chemistry. We give a sharp upper bound on the value of this invariant for connected chemical <b>graphs</b> of given order and size and characterize the connected chemical <b>graphs</b> that reach the bound. We also prove that the removal of the constraint that extremal chemical <b>graphs</b> must be connected does not allow to increase the upper bound.</p></p class="citation"></blockquote><h2 id=statap-1>stat.AP (1)</h2><h3 id=11--268270-bayesian-hierarchical-probabilistic-forecasting-of-intraday-electricity-prices-daniel-nickelsen-et-al-2024>(1/1 | 268/270) Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices (Daniel Nickelsen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Nickelsen, Gernot Müller. (2024)<br><strong>Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices</strong><br><button class=copy-to-clipboard title="Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.AP<br>Categories: G-3; I-2-6; I-6-3; I-6-5, cs-LG, stat-AP, stat.AP<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05441v1.pdf filename=2403.05441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a first study of Bayesian forecasting of electricity prices traded on the German continuous intraday market which fully incorporates parameter uncertainty. Our target variable is the IDFull price index, forecasts are given in terms of posterior predictive distributions. For validation we use the exceedingly volatile electricity prices of 2022, which have hardly been the subject of forecasting studies before. As a <b>benchmark</b> model, we use all available intraday transactions at the time of forecast creation to compute a current value for the IDFull. According to the weak-form efficiency hypothesis, it would not be possible to significantly improve this <b>benchmark</b> built from last price information. We do, however, observe statistically significant improvement in terms of both point measures and probability scores. Finally, we challenge the declared gold standard of using LASSO for feature selection in electricity price forecasting by presenting strong statistical evidence that Orthogonal Matching Pursuit (OMP) leads to better forecasting performance.</p></p class="citation"></blockquote><h2 id=mathpr-1>math.PR (1)</h2><h3 id=11--269270-limit-laws-for-critical-dispersion-on-complete-graphs-umberto-de-ambroggio-et-al-2024>(1/1 | 269/270) Limit Laws for Critical Dispersion on Complete Graphs (Umberto De Ambroggio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Umberto De Ambroggio, Tamás Makai, Konstantinos Panagiotou, Annika Steibel. (2024)<br><strong>Limit Laws for Critical Dispersion on Complete Graphs</strong><br><button class=copy-to-clipboard title="Limit Laws for Critical Dispersion on Complete Graphs" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.PR<br>Categories: cs-DM, math-CO, math-PR, math.PR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05372v1.pdf filename=2403.05372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a synchronous process of particles moving on the vertices of a <b>graph</b> $G$, introduced by Cooper, McDowell, Radzik, Rivera and Shiraga (2018). Initially, $M$ particles are placed on a vertex of $G$. In subsequent time steps, all particles that are located on a vertex inhabited by at least two particles jump independently to a neighbour chosen uniformly at random. The process ends at the first step when no vertex is inhabited by more than one particle; we call this (random) time step the dispersion time. In this work we study the case where $G$ is the complete <b>graph</b> on $n$ vertices and the number of particles is $M=n/2+\alpha n^{1/2} + o(n^{1/2})$, $\alpha\in \mathbb{R}$. This choice of $M$ corresponds to the critical window of the process, with respect to the dispersion time. We show that the dispersion time, if rescaled by $n^{-1/2}$, converges in $p$-th mean, as $n\rightarrow \infty$ and for any $p \in \mathbb{R}$, to a continuous and almost surely positive random variable $T_\alpha$. We find that $T_\alpha$ is the absorption time of a standard logistic branching process, thoroughly investigated by Lambert (2005), and we determine its expectation. In particular, in the middle of the critical window we show that $\mathbb{E}[T_0] = \pi^{3/2}/\sqrt{7}$, and furthermore we formulate explicit asymptotics when $|\alpha|$ gets large that quantify the transition into and out of the critical window. We also study the random variable counting the total number of jumps that are performed by the particles until the dispersion time is reached and prove that, if rescaled by $n\ln n$, it converges to $2/7$ in probability.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--270270-estimation-of-electronic-band-gap-energy-from-material-properties-using-machine-learning-sagar-prakash-barad-et-al-2024>(1/1 | 270/270) Estimation of Electronic Band Gap Energy From Material Properties Using Machine Learning (Sagar Prakash Barad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sagar Prakash Barad, Sajag Kumar, Subhankar Mishra. (2024)<br><strong>Estimation of Electronic Band Gap Energy From Material Properties Using Machine Learning</strong><br><button class=copy-to-clipboard title="Estimation of Electronic Band Gap Energy From Material Properties Using Machine Learning" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05119v1.pdf filename=2403.05119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning techniques are utilized to estimate the electronic band gap energy and forecast the band gap category of materials based on experimentally quantifiable properties. The determination of band gap energy is critical for discerning various material properties, such as its metallic nature, and potential applications in electronic and optoelectronic devices. While numerical methods exist for computing band gap energy, they often entail high computational costs and have limitations in accuracy and scalability. A machine learning-driven model capable of swiftly predicting material band gap energy using easily obtainable experimental properties would offer a superior alternative to conventional density functional theory (DFT) methods. Our model does not require any preliminary DFT-based calculation or knowledge of the structure of the material. We present a scheme for improving the performance of simple regression and classification models by partitioning the dataset into multiple clusters. A new evaluation scheme for comparing the performance of ML-based models in material sciences involving both regression and classification tasks is introduced based on traditional evaluation metrics. It is shown that on this new evaluation metric, our method of <b>clustering</b> the dataset results in better performance.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.09</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.11</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-35>cs.CL (35)</a><ul><li><a href=#135--1270-a-benchmark-of-domain-adapted-large-language-models-for-generating-brief-hospital-course-summaries-asad-aali-et-al-2024>(1/35 | 1/270) A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries (Asad Aali et al., 2024)</a></li><li><a href=#235--2270-to-err-is-human-but-llamas-can-learn-it-too-agnes-luhtaru-et-al-2024>(2/35 | 2/270) To Err Is Human, but Llamas Can Learn It Too (Agnes Luhtaru et al., 2024)</a></li><li><a href=#335--3270-rat-retrieval-augmented-thoughts-elicit-context-aware-reasoning-in-long-horizon-generation-zihao-wang-et-al-2024>(3/35 | 3/270) RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation (Zihao Wang et al., 2024)</a></li><li><a href=#435--4270-gemini-15-unlocking-multimodal-understanding-across-millions-of-tokens-of-context-machel-reid-et-al-2024>(4/35 | 4/270) Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context (Machel Reid et al., 2024)</a></li><li><a href=#535--5270-bias-augmented-consistency-training-reduces-biased-reasoning-in-chain-of-thought-james-chua-et-al-2024>(5/35 | 5/270) Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought (James Chua et al., 2024)</a></li><li><a href=#635--6270-cross-lingual-transfer-or-machine-translation-on-data-augmentation-for-monolingual-semantic-textual-similarity-sho-hoshino-et-al-2024>(6/35 | 6/270) Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity (Sho Hoshino et al., 2024)</a></li><li><a href=#735--7270-cant-remember-details-in-long-documents-you-need-some-rr-devanshu-agrawal-et-al-2024>(7/35 | 7/270) Can&rsquo;t Remember Details in Long Documents? You Need Some R&amp;R (Devanshu Agrawal et al., 2024)</a></li><li><a href=#835--8270-will-gpt-4-run-doom-adrian-de-wynter-2024>(8/35 | 8/270) Will GPT-4 Run DOOM? (Adrian de Wynter, 2024)</a></li><li><a href=#935--9270-diffchat-learning-to-chat-with-text-to-image-synthesis-models-for-interactive-image-creation-jiapeng-wang-et-al-2024>(9/35 | 9/270) DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation (Jiapeng Wang et al., 2024)</a></li><li><a href=#1035--10270-harnessing-multi-role-capabilities-of-large-language-models-for-open-domain-question-answering-hongda-sun-et-al-2024>(10/35 | 10/270) Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering (Hongda Sun et al., 2024)</a></li><li><a href=#1135--11270-can-we-obtain-significant-success-in-rst-discourse-parsing-by-using-large-language-models-aru-maekawa-et-al-2024>(11/35 | 11/270) Can we obtain significant success in RST discourse parsing by using Large Language Models? (Aru Maekawa et al., 2024)</a></li><li><a href=#1235--12270-cost-performance-optimization-for-processing-low-resource-language-tasks-using-commercial-llms-arijit-nag-et-al-2024>(12/35 | 12/270) Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs (Arijit Nag et al., 2024)</a></li><li><a href=#1335--13270-the-impact-of-quantization-on-the-robustness-of-transformer-based-text-classifiers-seyed-parsa-neshaei-et-al-2024>(13/35 | 13/270) The Impact of Quantization on the Robustness of Transformer-based Text Classifiers (Seyed Parsa Neshaei et al., 2024)</a></li><li><a href=#1435--14270-explaining-pre-trained-language-models-with-attribution-scores-an-analysis-in-low-resource-settings-wei-zhou-et-al-2024>(14/35 | 14/270) Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings (Wei Zhou et al., 2024)</a></li><li><a href=#1535--15270-erbench-an-entity-relationship-based-automatically-verifiable-hallucination-benchmark-for-large-language-models-jio-oh-et-al-2024>(15/35 | 15/270) ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models (Jio Oh et al., 2024)</a></li><li><a href=#1635--16270-piperag-fast-retrieval-augmented-generation-via-algorithm-system-co-design-wenqi-jiang-et-al-2024>(16/35 | 16/270) PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design (Wenqi Jiang et al., 2024)</a></li><li><a href=#1735--17270-authorship-attribution-in-bangla-literature-aabl-via-transfer-learning-using-ulmfit-aisha-khatun-et-al-2024>(17/35 | 17/270) Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT (Aisha Khatun et al., 2024)</a></li><li><a href=#1835--18270-chatasu-evoking-llms-reflexion-to-truly-understand-aspect-sentiment-in-dialogues-yiding-liu-et-al-2024>(18/35 | 18/270) ChatASU: Evoking LLM&rsquo;s Reflexion to Truly Understand Aspect Sentiment in Dialogues (Yiding Liu et al., 2024)</a></li><li><a href=#1935--19270-aclsum-a-new-dataset-for-aspect-based-summarization-of-scientific-publications-sotaro-takeshita-et-al-2024>(19/35 | 19/270) ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications (Sotaro Takeshita et al., 2024)</a></li><li><a href=#2035--20270-deep-prompt-multi-task-network-for-abuse-language-detection-jian-zhu-et-al-2024>(20/35 | 20/270) Deep Prompt Multi-task Network for Abuse Language Detection (Jian Zhu et al., 2024)</a></li><li><a href=#2135--21270-towards-a-psychology-of-machines-large-language-models-predict-human-memory-markus-huff-et-al-2024>(21/35 | 21/270) Towards a Psychology of Machines: Large Language Models Predict Human Memory (Markus Huff et al., 2024)</a></li><li><a href=#2235--22270-chatuie-exploring-chat-based-unified-information-extraction-using-large-language-models-jun-xu-et-al-2024>(22/35 | 22/270) ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models (Jun Xu et al., 2024)</a></li><li><a href=#2335--23270-is-this-the-real-life-is-this-just-fantasy-the-misleading-success-of-simulating-social-interactions-with-llms-xuhui-zhou-et-al-2024>(23/35 | 23/270) Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs (Xuhui Zhou et al., 2024)</a></li><li><a href=#2435--24270-an-in-depth-evaluation-of-gpt-4-in-sentence-simplification-with-error-based-human-assessment-xuanxin-wu-et-al-2024>(24/35 | 24/270) An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment (Xuanxin Wu et al., 2024)</a></li><li><a href=#2535--25270-towards-multimodal-sentiment-analysis-debiasing-via-bias-purification-dingkang-yang-et-al-2024>(25/35 | 25/270) Towards Multimodal Sentiment Analysis Debiasing via Bias Purification (Dingkang Yang et al., 2024)</a></li><li><a href=#2635--26270-rouge-k-do-your-summaries-have-keywords-sotaro-takeshita-et-al-2024>(26/35 | 26/270) ROUGE-K: Do Your Summaries Have Keywords? (Sotaro Takeshita et al., 2024)</a></li><li><a href=#2735--27270-tracing-the-roots-of-facts-in-multilingual-language-models-independent-shared-and-transferred-knowledge-xin-zhao-et-al-2024>(27/35 | 27/270) Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge (Xin Zhao et al., 2024)</a></li><li><a href=#2835--28270-seegull-multilingual-a-dataset-of-geo-culturally-situated-stereotypes-mukul-bhutani-et-al-2024>(28/35 | 28/270) SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes (Mukul Bhutani et al., 2024)</a></li><li><a href=#2935--29270-socialpet-socially-informed-pattern-exploiting-training-for-few-shot-stance-detection-in-social-media-parisa-jamadi-khiabani-et-al-2024>(29/35 | 29/270) SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot Stance Detection in Social Media (Parisa Jamadi Khiabani et al., 2024)</a></li><li><a href=#3035--30270-are-human-conversations-special-a-large-language-model-perspective-toshish-jawale-et-al-2024>(30/35 | 30/270) Are Human Conversations Special? A Large Language Model Perspective (Toshish Jawale et al., 2024)</a></li><li><a href=#3135--31270-commitbench-a-benchmark-for-commit-message-generation-maximilian-schall-et-al-2024>(31/35 | 31/270) CommitBench: A Benchmark for Commit Message Generation (Maximilian Schall et al., 2024)</a></li><li><a href=#3235--32270-generating-hard-negative-out-of-scope-data-with-chatgpt-for-intent-classification-zhijian-li-et-al-2024>(32/35 | 32/270) Generating Hard-Negative Out-of-Scope Data with ChatGPT for Intent Classification (Zhijian Li et al., 2024)</a></li><li><a href=#3335--33270-bayesian-preference-elicitation-with-language-models-kunal-handa-et-al-2024>(33/35 | 33/270) Bayesian Preference Elicitation with Language Models (Kunal Handa et al., 2024)</a></li><li><a href=#3435--34270-ffstc-fongbe-to-french-speech-translation-corpus-d-fortune-kponou-et-al-2024>(34/35 | 34/270) FFSTC: Fongbe to French Speech Translation Corpus (D. Fortune Kponou et al., 2024)</a></li><li><a href=#3535--35270-rule-driven-news-captioning-ning-xu-et-al-2024>(35/35 | 35/270) Rule-driven News Captioning (Ning Xu et al., 2024)</a></li></ul></li><li><a href=#csai-18>cs.AI (18)</a><ul><li><a href=#118--36270-deepseek-vl-towards-real-world-vision-language-understanding-haoyu-lu-et-al-2024>(1/18 | 36/270) DeepSeek-VL: Towards Real-World Vision-Language Understanding (Haoyu Lu et al., 2024)</a></li><li><a href=#218--37270-bjtt-a-large-scale-multimodal-dataset-for-traffic-prediction-chengyang-zhang-et-al-2024>(2/18 | 37/270) BjTT: A Large-scale Multimodal Dataset for Traffic Prediction (Chengyang Zhang et al., 2024)</a></li><li><a href=#318--38270-can-large-language-models-play-games-a-case-study-of-a-self-play-approach-hongyi-guo-et-al-2024>(3/18 | 38/270) Can Large Language Models Play Games? A Case Study of A Self-Play Approach (Hongyi Guo et al., 2024)</a></li><li><a href=#418--39270-decomposing-vision-based-llm-predictions-for-auto-evaluation-with-gpt-4-qingqing-zhu-et-al-2024>(4/18 | 39/270) Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4 (Qingqing Zhu et al., 2024)</a></li><li><a href=#518--40270-tuning-free-accountable-intervention-for-llm-deployment----a-metacognitive-approach-zhen-tan-et-al-2024>(5/18 | 40/270) Tuning-Free Accountable Intervention for LLM Deployment &ndash; A Metacognitive Approach (Zhen Tan et al., 2024)</a></li><li><a href=#618--41270-tell-me-the-truth-a-system-to-measure-the-trustworthiness-of-large-language-models-carlo-lipizzi-2024>(6/18 | 41/270) Tell me the truth: A system to measure the trustworthiness of Large Language Models (Carlo Lipizzi, 2024)</a></li><li><a href=#718--42270-sora-as-an-agi-world-model-a-complete-survey-on-text-to-video-generation-joseph-cho-et-al-2024>(7/18 | 42/270) Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation (Joseph Cho et al., 2024)</a></li><li><a href=#818--43270-from-chain-to-tree-refining-chain-like-rules-into-tree-like-rules-on-knowledge-graphs-wangtao-sun-et-al-2024>(8/18 | 43/270) From Chain to Tree: Refining Chain-like Rules into Tree-like Rules on Knowledge Graphs (Wangtao Sun et al., 2024)</a></li><li><a href=#918--44270-tapilot-crossing-benchmarking-and-evolving-llms-towards-interactive-data-analysis-agents-jinyang-li-et-al-2024>(9/18 | 44/270) Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents (Jinyang Li et al., 2024)</a></li><li><a href=#1018--45270-towards-multimodal-human-intention-understanding-debiasing-via-subject-deconfounding-dingkang-yang-et-al-2024>(10/18 | 45/270) Towards Multimodal Human Intention Understanding Debiasing via Subject-Deconfounding (Dingkang Yang et al., 2024)</a></li><li><a href=#1118--46270-algorithmic-identification-of-essential-exogenous-nodes-for-causal-sufficiency-in-brain-networks-abdolmahdi-bagheri-et-al-2024>(11/18 | 46/270) Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks (Abdolmahdi Bagheri et al., 2024)</a></li><li><a href=#1218--47270-predicting-single-cell-drug-sensitivity-by-adaptive-weighted-feature-for-adversarial-multi-source-domain-adaptation-wei-duan-et-al-2024>(12/18 | 47/270) Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for Adversarial Multi-source Domain Adaptation (Wei Duan et al., 2024)</a></li><li><a href=#1318--48270-rlperi-accelerating-visual-perimetry-test-with-reinforcement-learning-and-convolutional-feature-extraction-tanvi-verma-et-al-2024>(13/18 | 48/270) RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning and Convolutional Feature Extraction (Tanvi Verma et al., 2024)</a></li><li><a href=#1418--49270-efficient-public-health-intervention-planning-using-decomposition-based-decision-focused-learning-sanket-shah-et-al-2024>(14/18 | 49/270) Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning (Sanket Shah et al., 2024)</a></li><li><a href=#1518--50270-a-feature-based-generalizable-prediction-model-for-both-perceptual-and-abstract-reasoning-quan-do-et-al-2024>(15/18 | 50/270) A Feature-based Generalizable Prediction Model for Both Perceptual and Abstract Reasoning (Quan Do et al., 2024)</a></li><li><a href=#1618--51270-multi-agent-reinforcement-learning-with-a-hierarchy-of-reward-machines-xuejing-zheng-et-al-2024>(16/18 | 51/270) Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines (Xuejing Zheng et al., 2024)</a></li><li><a href=#1718--52270-mmoe-robust-spoiler-detection-with-multi-modal-information-and-domain-aware-mixture-of-experts-zinan-zeng-et-al-2024>(17/18 | 52/270) MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts (Zinan Zeng et al., 2024)</a></li><li><a href=#1818--53270-looking-ahead-to-avoid-being-late-solving-hard-constrained-traveling-salesman-problem-jingxiao-chen-et-al-2024>(18/18 | 53/270) Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling Salesman Problem (Jingxiao Chen et al., 2024)</a></li></ul></li><li><a href=#cscr-9>cs.CR (9)</a><ul><li><a href=#19--54270-dp-tabicl-in-context-learning-with-differentially-private-tabular-data-alycia-n-carey-et-al-2024>(1/9 | 54/270) DP-TabICL: In-Context Learning with Differentially Private Tabular Data (Alycia N. Carey et al., 2024)</a></li><li><a href=#29--55270-defending-against-unforeseen-failure-modes-with-latent-adversarial-training-stephen-casper-et-al-2024>(2/9 | 55/270) Defending Against Unforeseen Failure Modes with Latent Adversarial Training (Stephen Casper et al., 2024)</a></li><li><a href=#39--56270-secgpt-an-execution-isolation-architecture-for-llm-based-systems-yuhao-wu-et-al-2024>(3/9 | 56/270) SecGPT: An Execution Isolation Architecture for LLM-Based Systems (Yuhao Wu et al., 2024)</a></li><li><a href=#49--57270-on-protecting-the-data-privacy-of-large-language-models-llms-a-survey-biwei-yan-et-al-2024>(4/9 | 57/270) On Protecting the Data Privacy of Large Language Models (LLMs): A Survey (Biwei Yan et al., 2024)</a></li><li><a href=#59--58270-exploring-the-adversarial-frontier-quantifying-robustness-via-adversarial-hypervolume-ping-guo-et-al-2024>(5/9 | 58/270) Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume (Ping Guo et al., 2024)</a></li><li><a href=#69--59270-inception-attacks-immersive-hijacking-in-virtual-reality-systems-zhuolin-yang-et-al-2024>(6/9 | 59/270) Inception Attacks: Immersive Hijacking in Virtual Reality Systems (Zhuolin Yang et al., 2024)</a></li><li><a href=#79--60270-vspace-voting-in-a-scalable-privacy-aware-and-confidential-election-se-elnour-et-al-2024>(7/9 | 60/270) vSPACE: Voting in a Scalable, Privacy-Aware and Confidential Election (Se Elnour et al., 2024)</a></li><li><a href=#89--61270-elections-in-the-post-quantum-era-is-the-complexity-shield-strong-enough-šimon-schierreich-2024>(8/9 | 61/270) Elections in the Post-Quantum Era: Is the Complexity Shield Strong Enough? (Šimon Schierreich, 2024)</a></li><li><a href=#99--62270-private-count-release-a-simple-and-scalable-approach-for-private-data-analytics-ryan-rogers-2024>(9/9 | 62/270) Private Count Release: A Simple and Scalable Approach for Private Data Analytics (Ryan Rogers, 2024)</a></li></ul></li><li><a href=#csir-4>cs.IR (4)</a><ul><li><a href=#14--63270-cfairllm-consumer-fairness-evaluation-in-large-language-model-recommender-system-yashar-deldjoo-et-al-2024>(1/4 | 63/270) CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System (Yashar Deldjoo et al., 2024)</a></li><li><a href=#24--64270-aligning-large-language-models-for-controllable-recommendations-wensheng-lu-et-al-2024>(2/4 | 64/270) Aligning Large Language Models for Controllable Recommendations (Wensheng Lu et al., 2024)</a></li><li><a href=#34--65270-personalized-audiobook-recommendations-at-spotify-through-graph-neural-networks-marco-de-nadai-et-al-2024>(3/4 | 65/270) Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks (Marco De Nadai et al., 2024)</a></li><li><a href=#44--66270-multi-tower-multi-interest-recommendation-with-user-representation-repel-tianyu-xiong-et-al-2024>(4/4 | 66/270) Multi-Tower Multi-Interest Recommendation with User Representation Repel (Tianyu Xiong et al., 2024)</a></li></ul></li><li><a href=#hep-ph-2>hep-ph (2)</a><ul><li><a href=#12--67270-omnijet-α-the-first-cross-task-foundation-model-for-particle-physics-joschka-birk-et-al-2024>(1/2 | 67/270) OmniJet-$α$: The first cross-task foundation model for particle physics (Joschka Birk et al., 2024)</a></li><li><a href=#22--68270-jet-discrimination-with-quantum-complete-graph-neural-network-yi-an-chen-et-al-2024>(2/2 | 68/270) Jet Discrimination with Quantum Complete Graph Neural Network (Yi-An Chen et al., 2024)</a></li></ul></li><li><a href=#cslg-32>cs.LG (32)</a><ul><li><a href=#132--69270-unfamiliar-finetuning-examples-control-how-language-models-hallucinate-katie-kang-et-al-2024>(1/32 | 69/270) Unfamiliar Finetuning Examples Control How Language Models Hallucinate (Katie Kang et al., 2024)</a></li><li><a href=#232--70270-overcoming-reward-overoptimization-via-adversarial-policy-optimization-with-lightweight-uncertainty-estimation-xiaoying-zhang-et-al-2024>(2/32 | 70/270) Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation (Xiaoying Zhang et al., 2024)</a></li><li><a href=#332--71270-simple-multigraph-convolution-networks-danyang-wu-et-al-2024>(3/32 | 71/270) Simple Multigraph Convolution Networks (Danyang Wu et al., 2024)</a></li><li><a href=#432--72270-simulating-battery-powered-tinyml-systems-optimised-using-reinforcement-learning-in-image-based-anomaly-detection-jared-m-ping-et-al-2024>(4/32 | 72/270) Simulating Battery-Powered TinyML Systems Optimised using Reinforcement Learning in Image-Based Anomaly Detection (Jared M. Ping et al., 2024)</a></li><li><a href=#532--73270-spectral-invariant-learning-for-dynamic-graphs-under-distribution-shifts-zeyang-zhang-et-al-2024>(5/32 | 73/270) Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (Zeyang Zhang et al., 2024)</a></li><li><a href=#632--74270-provable-multi-party-reinforcement-learning-with-diverse-human-feedback-huiying-zhong-et-al-2024>(6/32 | 74/270) Provable Multi-Party Reinforcement Learning with Diverse Human Feedback (Huiying Zhong et al., 2024)</a></li><li><a href=#732--75270-benchmarking-large-language-models-for-molecule-prediction-tasks-zhiqiang-zhong-et-al-2024>(7/32 | 75/270) Benchmarking Large Language Models for Molecule Prediction Tasks (Zhiqiang Zhong et al., 2024)</a></li><li><a href=#832--76270-denoising-autoregressive-representation-learning-yazhe-li-et-al-2024>(8/32 | 76/270) Denoising Autoregressive Representation Learning (Yazhe Li et al., 2024)</a></li><li><a href=#932--77270-unity-by-diversity-improved-representation-learning-in-multimodal-vaes-thomas-m-sutter-et-al-2024>(9/32 | 77/270) Unity by Diversity: Improved Representation Learning in Multimodal VAEs (Thomas M. Sutter et al., 2024)</a></li><li><a href=#1032--78270-unsupervised-graph-neural-architecture-search-with-disentangled-self-supervision-zeyang-zhang-et-al-2024>(10/32 | 78/270) Unsupervised Graph Neural Architecture Search with Disentangled Self-supervision (Zeyang Zhang et al., 2024)</a></li><li><a href=#1132--79270-augmentations-vs-algorithms-what-works-in-self-supervised-learning-warren-morningstar-et-al-2024>(11/32 | 79/270) Augmentations vs Algorithms: What Works in Self-Supervised Learning (Warren Morningstar et al., 2024)</a></li><li><a href=#1232--80270-tune-without-validation-searching-for-learning-rate-and-weight-decay-on-training-sets-lorenzo-brigato-et-al-2024>(12/32 | 80/270) Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets (Lorenzo Brigato et al., 2024)</a></li><li><a href=#1332--81270-gear-an-efficient-kv-cache-compression-recipe-for-near-lossless-generative-inference-of-llm-hao-kang-et-al-2024>(13/32 | 81/270) GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM (Hao Kang et al., 2024)</a></li><li><a href=#1432--82270-considering-nonstationary-within-multivariate-time-series-with-variational-hierarchical-transformer-for-forecasting-muyao-wang-et-al-2024>(14/32 | 82/270) Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting (Muyao Wang et al., 2024)</a></li><li><a href=#1532--83270-adversarial-sparse-teacher-defense-against-distillation-based-model-stealing-attacks-using-adversarial-examples-eda-yilmaz-et-al-2024>(15/32 | 83/270) Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples (Eda Yilmaz et al., 2024)</a></li><li><a href=#1632--84270-adaptive-split-learning-over-energy-constrained-wireless-edge-networks-zuguang-li-et-al-2024>(16/32 | 84/270) Adaptive Split Learning over Energy-Constrained Wireless Edge Networks (Zuguang Li et al., 2024)</a></li><li><a href=#1732--85270-ectonas-evolutionary-cross-topology-neural-architecture-search-elisabeth-j-schiessler-et-al-2024>(17/32 | 85/270) ECToNAS: Evolutionary Cross-Topology Neural Architecture Search (Elisabeth J. Schiessler et al., 2024)</a></li><li><a href=#1832--86270-reset--distill-a-recipe-for-overcoming-negative-transfer-in-continual-reinforcement-learning-hongjoon-ahn-et-al-2024>(18/32 | 86/270) Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning (Hongjoon Ahn et al., 2024)</a></li><li><a href=#1932--87270-quantifying-manifolds-do-the-manifolds-learned-by-generative-adversarial-networks-converge-to-the-real-data-manifold-anupam-chaudhuri-et-al-2024>(19/32 | 87/270) Quantifying Manifolds: Do the manifolds learned by Generative Adversarial Networks converge to the real data manifold (Anupam Chaudhuri et al., 2024)</a></li><li><a href=#2032--88270-poly-view-contrastive-learning-amitis-shidani-et-al-2024>(20/32 | 88/270) Poly-View Contrastive Learning (Amitis Shidani et al., 2024)</a></li><li><a href=#2132--89270-overcoming-data-inequality-across-domains-with-semi-supervised-domain-generalization-jinha-park-et-al-2024>(21/32 | 89/270) Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization (Jinha Park et al., 2024)</a></li><li><a href=#2232--90270-a-concept-based-interpretable-model-for-the-diagnosis-of-choroid-neoplasias-using-multimodal-data-yifan-wu-et-al-2024>(22/32 | 90/270) A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data (Yifan Wu et al., 2024)</a></li><li><a href=#2332--91270-mathtttsgt-stochastic-time-series-modeling-with-transformer-łukasz-kuciński-et-al-2024>(23/32 | 91/270) $\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer (Łukasz Kuciński et al., 2024)</a></li><li><a href=#2432--92270-shielded-deep-reinforcement-learning-for-complex-spacecraft-tasking-robert-reed-et-al-2024>(24/32 | 92/270) Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking (Robert Reed et al., 2024)</a></li><li><a href=#2532--93270-what-is-different-between-these-datasets-varun-babbar-et-al-2024>(25/32 | 93/270) What is different between these datasets? (Varun Babbar et al., 2024)</a></li><li><a href=#2632--94270-recovery-guarantees-of-unsupervised-neural-networks-for-inverse-problems-trained-with-gradient-descent-nathan-buskulic-et-al-2024>(26/32 | 94/270) Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent (Nathan Buskulic et al., 2024)</a></li><li><a href=#2732--95270-switching-the-loss-reduces-the-cost-in-batch-reinforcement-learning-alex-ayoub-et-al-2024>(27/32 | 95/270) Switching the Loss Reduces the Cost in Batch Reinforcement Learning (Alex Ayoub et al., 2024)</a></li><li><a href=#2832--96270-leveraging-continuous-time-to-understand-momentum-when-training-diagonal-linear-networks-hristo-papazov-et-al-2024>(28/32 | 96/270) Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks (Hristo Papazov et al., 2024)</a></li><li><a href=#2932--97270-fairness-aware-interpretable-modeling-faim-for-trustworthy-machine-learning-in-healthcare-mingxuan-liu-et-al-2024>(29/32 | 97/270) Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare (Mingxuan Liu et al., 2024)</a></li><li><a href=#3032--98270-continual-learning-and-catastrophic-forgetting-gido-m-van-de-ven-et-al-2024>(30/32 | 98/270) Continual Learning and Catastrophic Forgetting (Gido M. van de Ven et al., 2024)</a></li><li><a href=#3132--99270-vtrust-controllable-value-function-based-subset-selection-for-data-centric-trustworthy-ai-soumi-das-et-al-2024>(31/32 | 99/270) VTruST: Controllable value function based subset selection for Data-Centric Trustworthy AI (Soumi Das et al., 2024)</a></li><li><a href=#3232--100270-synthetic-data-generation-for-system-identification-leveraging-knowledge-transfer-from-similar-systems-dario-piga-et-al-2024>(32/32 | 100/270) Synthetic data generation for system identification: leveraging knowledge transfer from similar systems (Dario Piga et al., 2024)</a></li></ul></li><li><a href=#eessiv-9>eess.IV (9)</a><ul><li><a href=#19--101270-hybridized-convolutional-neural-networks-and-long-short-term-memory-for-improved-alzheimers-disease-diagnosis-from-mri-scans-maleka-khatun-et-al-2024>(1/9 | 101/270) Hybridized Convolutional Neural Networks and Long Short-Term Memory for Improved Alzheimer&rsquo;s Disease Diagnosis from MRI Scans (Maleka Khatun et al., 2024)</a></li><li><a href=#29--102270-c2p-gcn-cell-to-patch-graph-convolutional-network-for-colorectal-cancer-grading-sudipta-paul-et-al-2024>(2/9 | 102/270) C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer Grading (Sudipta Paul et al., 2024)</a></li><li><a href=#39--103270-spatial-aware-transformer-gru-framework-for-enhanced-glaucoma-diagnosis-from-3d-oct-imaging-mona-ashtari-majlan-et-al-2024>(3/9 | 103/270) Spatial-aware Transformer-GRU Framework for Enhanced Glaucoma Diagnosis from 3D OCT Imaging (Mona Ashtari-Majlan et al., 2024)</a></li><li><a href=#49--104270-a-data-augmentation-pipeline-to-generate-synthetic-labeled-datasets-of-3d-echocardiography-images-using-a-gan-cristiana-tiago-et-al-2024>(4/9 | 104/270) A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of 3D Echocardiography Images using a GAN (Cristiana Tiago et al., 2024)</a></li><li><a href=#59--105270-lightm-unet-mamba-assists-in-lightweight-unet-for-medical-image-segmentation-weibin-liao-et-al-2024>(5/9 | 105/270) LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation (Weibin Liao et al., 2024)</a></li><li><a href=#69--106270-fedfms-exploring-federated-foundation-models-for-medical-image-segmentation-yuxi-liu-et-al-2024>(6/9 | 106/270) FedFMS: Exploring Federated Foundation Models for Medical Image Segmentation (Yuxi Liu et al., 2024)</a></li><li><a href=#79--107270-a-probabilistic-hadamard-u-net-for-mri-bias-field-correction-xin-zhu-et-al-2024>(7/9 | 107/270) A Probabilistic Hadamard U-Net for MRI Bias Field Correction (Xin Zhu et al., 2024)</a></li><li><a href=#89--108270-dudouninext-dual-domain-unified-hybrid-model-for-single-and-multi-contrast-undersampled-mri-reconstruction-ziqi-gao-et-al-2024>(8/9 | 108/270) DuDoUniNeXt: Dual-domain unified hybrid model for single and multi-contrast undersampled MRI reconstruction (Ziqi Gao et al., 2024)</a></li><li><a href=#99--109270-noise-level-adaptive-diffusion-model-for-robust-reconstruction-of-accelerated-mri-shoujin-huang-et-al-2024>(9/9 | 109/270) Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI (Shoujin Huang et al., 2024)</a></li></ul></li><li><a href=#cscv-77>cs.CV (77)</a><ul><li><a href=#177--110270-tracking-meets-lora-faster-training-larger-model-stronger-performance-liting-lin-et-al-2024>(1/77 | 110/270) Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance (Liting Lin et al., 2024)</a></li><li><a href=#277--111270-beyond-finite-data-towards-data-free-out-of-distribution-generalization-via-extrapolation-yijiang-li-et-al-2024>(2/77 | 111/270) Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation (Yijiang Li et al., 2024)</a></li><li><a href=#377--112270-sirst-5k-exploring-massive-negatives-synthesis-with-self-supervised-learning-for-robust-infrared-small-target-detection-yahao-lu-et-al-2024>(3/77 | 112/270) SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised Learning for Robust Infrared Small Target Detection (Yahao Lu et al., 2024)</a></li><li><a href=#477--113270-exploring-robust-features-for-few-shot-object-detection-in-satellite-imagery-xavier-bou-et-al-2024>(4/77 | 113/270) Exploring Robust Features for Few-Shot Object Detection in Satellite Imagery (Xavier Bou et al., 2024)</a></li><li><a href=#577--114270-debiasing-large-visual-language-models-yi-fan-zhang-et-al-2024>(5/77 | 114/270) Debiasing Large Visual Language Models (Yi-Fan Zhang et al., 2024)</a></li><li><a href=#677--115270-ella-equip-diffusion-models-with-llm-for-enhanced-semantic-alignment-xiwei-hu-et-al-2024>(6/77 | 115/270) ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment (Xiwei Hu et al., 2024)</a></li><li><a href=#777--116270-federated-learning-method-for-preserving-privacy-in-face-recognition-system-enoch-solomon-et-al-2024>(7/77 | 116/270) Federated Learning Method for Preserving Privacy in Face Recognition System (Enoch Solomon et al., 2024)</a></li><li><a href=#877--117270-fine-tuning-a-multiple-instance-learning-feature-extractor-with-masked-context-modelling-and-knowledge-distillation-juan-i-pisula-et-al-2024>(8/77 | 117/270) Fine-tuning a Multiple Instance Learning Feature Extractor with Masked Context Modelling and Knowledge Distillation (Juan I. Pisula et al., 2024)</a></li><li><a href=#977--118270-radardistill-boosting-radar-based-object-detection-performance-via-knowledge-distillation-from-lidar-features-geonho-bang-et-al-2024>(9/77 | 118/270) RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features (Geonho Bang et al., 2024)</a></li><li><a href=#1077--119270-histgen-histopathology-report-generation-via-local-global-feature-encoding-and-cross-modal-context-interaction-zhengrui-guo-et-al-2024>(10/77 | 119/270) HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction (Zhengrui Guo et al., 2024)</a></li><li><a href=#1177--120270-jointmotion-joint-self-supervision-for-joint-motion-prediction-royden-wagner-et-al-2024>(11/77 | 120/270) JointMotion: Joint Self-supervision for Joint Motion Prediction (Royden Wagner et al., 2024)</a></li><li><a href=#1277--121270-rethinking-transformers-pre-training-for-multi-spectral-satellite-imagery-mubashir-noman-et-al-2024>(12/77 | 121/270) Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery (Mubashir Noman et al., 2024)</a></li><li><a href=#1377--122270-self-supervised-multiple-instance-learning-for-acute-myeloid-leukemia-classification-salome-kazeminia-et-al-2024>(13/77 | 122/270) Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification (Salome Kazeminia et al., 2024)</a></li><li><a href=#1477--123270-peeb-part-based-image-classifiers-with-an-explainable-and-editable-language-bottleneck-thang-m-pham-et-al-2024>(14/77 | 123/270) PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck (Thang M. Pham et al., 2024)</a></li><li><a href=#1577--124270-towards-effective-usage-of-human-centric-priors-in-diffusion-models-for-text-based-human-image-generation-junyan-wang-et-al-2024>(15/77 | 124/270) Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation (Junyan Wang et al., 2024)</a></li><li><a href=#1677--125270-improving-diffusion-models-for-virtual-try-on-yisol-choi-et-al-2024>(16/77 | 125/270) Improving Diffusion Models for Virtual Try-on (Yisol Choi et al., 2024)</a></li><li><a href=#1777--126270-spectrum-translation-for-refinement-of-image-generation-stig-based-on-contrastive-learning-and-spectral-filter-profile-seokjun-lee-et-al-2024>(17/77 | 126/270) Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile (Seokjun Lee et al., 2024)</a></li><li><a href=#1877--127270-instructgie-towards-generalizable-image-editing-zichong-meng-et-al-2024>(18/77 | 127/270) InstructGIE: Towards Generalizable Image Editing (Zichong Meng et al., 2024)</a></li><li><a href=#1977--128270-xpsr-cross-modal-priors-for-diffusion-based-image-super-resolution-yunpeng-qu-et-al-2024>(19/77 | 128/270) XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution (Yunpeng Qu et al., 2024)</a></li><li><a href=#2077--129270-med3dinsight-enhancing-3d-medical-image-understanding-with-2d-multi-modal-large-language-models-qiuhui-chen-et-al-2024>(20/77 | 129/270) Med3DInsight: Enhancing 3D Medical Image Understanding with 2D Multi-Modal Large Language Models (Qiuhui Chen et al., 2024)</a></li><li><a href=#2177--130270-attention-guided-feature-distillation-for-semantic-segmentation-amir-m-mansourian-et-al-2024>(21/77 | 130/270) Attention-guided Feature Distillation for Semantic Segmentation (Amir M. Mansourian et al., 2024)</a></li><li><a href=#2277--131270-videoelevator-elevating-video-generation-quality-with-versatile-text-to-image-diffusion-models-yabo-zhang-et-al-2024>(22/77 | 131/270) VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models (Yabo Zhang et al., 2024)</a></li><li><a href=#2377--132270-vlm-pl-advanced-pseudo-labeling-approach-class-incremental-object-detection-with-vision-language-model-junsu-kim-et-al-2024>(23/77 | 132/270) VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model (Junsu Kim et al., 2024)</a></li><li><a href=#2477--133270-mammil-multiple-instance-learning-for-whole-slide-images-with-state-space-models-zijie-fang-et-al-2024>(24/77 | 133/270) MamMIL: Multiple Instance Learning for Whole Slide Images with State Space Models (Zijie Fang et al., 2024)</a></li><li><a href=#2577--134270-clip-gaze-towards-general-gaze-estimation-via-visual-linguistic-model-pengwei-yin-et-al-2024>(25/77 | 134/270) CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model (Pengwei Yin et al., 2024)</a></li><li><a href=#2677--135270-cogview3-finer-and-faster-text-to-image-generation-via-relay-diffusion-wendi-zheng-et-al-2024>(26/77 | 135/270) CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion (Wendi Zheng et al., 2024)</a></li><li><a href=#2777--136270-face2diffusion-for-fast-and-editable-face-personalization-kaede-shiohara-et-al-2024>(27/77 | 136/270) Face2Diffusion for Fast and Editable Face Personalization (Kaede Shiohara et al., 2024)</a></li><li><a href=#2877--137270-promptiqa-boosting-the-performance-and-generalization-for-no-reference-image-quality-assessment-via-prompts-zewen-chen-et-al-2024>(28/77 | 137/270) PromptIQA: Boosting the Performance and Generalization for No-Reference Image Quality Assessment via Prompts (Zewen Chen et al., 2024)</a></li><li><a href=#2977--138270-pipsus-self-supervised-dense-point-tracking-in-ultrasound-wanwen-chen-et-al-2024>(29/77 | 138/270) PIPsUS: Self-Supervised Dense Point Tracking in Ultrasound (Wanwen Chen et al., 2024)</a></li><li><a href=#3077--139270-stereodiffusion-training-free-stereo-image-generation-using-latent-diffusion-models-lezhong-wang-et-al-2024>(30/77 | 139/270) StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models (Lezhong Wang et al., 2024)</a></li><li><a href=#3177--140270-contrastdiagnosis-enhancing-interpretability-in-lung-nodule-diagnosis-using-contrastive-learning-chenglong-wang-et-al-2024>(31/77 | 140/270) ContrastDiagnosis: Enhancing Interpretability in Lung Nodule Diagnosis Using Contrastive Learning (Chenglong Wang et al., 2024)</a></li><li><a href=#3277--141270-scene-graph-aided-radiology-report-generation-jun-wang-et-al-2024>(32/77 | 141/270) Scene Graph Aided Radiology Report Generation (Jun Wang et al., 2024)</a></li><li><a href=#3377--142270-omnicount-multi-label-object-counting-with-semantic-geometric-priors-anindya-mondal-et-al-2024>(33/77 | 142/270) OmniCount: Multi-label Object Counting with Semantic-Geometric Priors (Anindya Mondal et al., 2024)</a></li><li><a href=#3477--143270-part-aware-personalized-segment-anything-model-for-patient-specific-segmentation-chenhui-zhao-et-al-2024>(34/77 | 143/270) Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation (Chenhui Zhao et al., 2024)</a></li><li><a href=#3577--144270-diffsf-diffusion-models-for-scene-flow-estimation-yushan-zhang-et-al-2024>(35/77 | 144/270) DiffSF: Diffusion Models for Scene Flow Estimation (Yushan Zhang et al., 2024)</a></li><li><a href=#3677--145270-synthetic-privileged-information-enhances-medical-image-representation-learning-lucas-farndale-et-al-2024>(36/77 | 145/270) Synthetic Privileged Information Enhances Medical Image Representation Learning (Lucas Farndale et al., 2024)</a></li><li><a href=#3777--146270-micro-fracture-detection-in-photovoltaic-cells-with-hardware-constrained-devices-and-computer-vision-booy-vitas-faassen-et-al-2024>(37/77 | 146/270) Micro-Fracture Detection in Photovoltaic Cells with Hardware-Constrained Devices and Computer Vision (Booy Vitas Faassen et al., 2024)</a></li><li><a href=#3877--147270-semantic-feature-learning-for-universal-unsupervised-cross-domain-retrieval-lixu-wang-et-al-2024>(38/77 | 147/270) Semantic Feature Learning for Universal Unsupervised Cross-Domain Retrieval (Lixu Wang et al., 2024)</a></li><li><a href=#3977--148270-dualbev-cnn-is-all-you-need-in-view-transformation-peidong-li-et-al-2024>(39/77 | 148/270) DualBEV: CNN is All You Need in View Transformation (Peidong Li et al., 2024)</a></li><li><a href=#4077--149270-frequency-adaptive-dilated-convolution-for-semantic-segmentation-linwei-chen-et-al-2024>(40/77 | 149/270) Frequency-Adaptive Dilated Convolution for Semantic Segmentation (Linwei Chen et al., 2024)</a></li><li><a href=#4177--150270-evaluating-text-to-image-generative-models-an-empirical-study-on-human-image-synthesis-muxi-chen-et-al-2024>(41/77 | 150/270) Evaluating Text-to-Image Generative Models: An Empirical Study on Human Image Synthesis (Muxi Chen et al., 2024)</a></li><li><a href=#4277--151270-apple-adversarial-privacy-aware-perturbations-on-latent-embedding-for-unfairness-mitigation-zikang-xu-et-al-2024>(42/77 | 151/270) APPLE: Adversarial Privacy-aware Perturbations on Latent Embedding for Unfairness Mitigation (Zikang Xu et al., 2024)</a></li><li><a href=#4377--152270-crm-single-image-to-3d-textured-mesh-with-convolutional-reconstruction-model-zhengyi-wang-et-al-2024>(43/77 | 152/270) CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model (Zhengyi Wang et al., 2024)</a></li><li><a href=#4477--153270-actformer-scalable-collaborative-perception-via-active-queries-suozhi-huang-et-al-2024>(44/77 | 153/270) ActFormer: Scalable Collaborative Perception via Active Queries (Suozhi Huang et al., 2024)</a></li><li><a href=#4577--154270-learning-to-rematch-mismatched-pairs-for-robust-cross-modal-retrieval-haochen-han-et-al-2024>(45/77 | 154/270) Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval (Haochen Han et al., 2024)</a></li><li><a href=#4677--155270-probabilistic-image-driven-traffic-modeling-via-remote-sensing-scott-workman-et-al-2024>(46/77 | 155/270) Probabilistic Image-Driven Traffic Modeling via Remote Sensing (Scott Workman et al., 2024)</a></li><li><a href=#4777--156270-benchmarking-micro-action-recognition-dataset-methods-and-applications-dan-guo-et-al-2024>(47/77 | 156/270) Benchmarking Micro-action Recognition: Dataset, Methods, and Applications (Dan Guo et al., 2024)</a></li><li><a href=#4877--157270-3d-face-reconstruction-using-a-spectral-based-graph-convolution-encoder-haoxin-xu-et-al-2024>(48/77 | 157/270) 3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder (Haoxin Xu et al., 2024)</a></li><li><a href=#4977--158270-feature-cam-interpretable-ai-in-image-classification-frincy-clement-et-al-2024>(49/77 | 158/270) Feature CAM: Interpretable AI in Image Classification (Frincy Clement et al., 2024)</a></li><li><a href=#5077--159270-uforecon-generalizable-sparse-view-surface-reconstruction-from-arbitrary-and-unfavorable-sets-youngju-na-et-al-2024>(50/77 | 159/270) UFORecon: Generalizable Sparse-View Surface Reconstruction from Arbitrary and UnFavOrable Sets (Youngju Na et al., 2024)</a></li><li><a href=#5177--160270-audio-synchronized-visual-animation-lin-zhang-et-al-2024>(51/77 | 160/270) Audio-Synchronized Visual Animation (Lin Zhang et al., 2024)</a></li><li><a href=#5277--161270-tell-dont-show-language-guidance-eases-transfer-across-domains-in-images-and-videos-tarun-kalluri-et-al-2024>(52/77 | 161/270) Tell, Don&rsquo;t Show!: Language Guidance Eases Transfer Across Domains in Images and Videos (Tarun Kalluri et al., 2024)</a></li><li><a href=#5377--162270-multiple-instance-learning-with-random-sampling-for-whole-slide-image-classification-h-keshvarikhojasteh-et-al-2024>(53/77 | 162/270) Multiple Instance Learning with random sampling for Whole Slide Image Classification (H. Keshvarikhojasteh et al., 2024)</a></li><li><a href=#5477--163270-laneptrnet-revisiting-lane-detection-as-point-voting-and-grouping-on-curves-jiayan-cao-et-al-2024>(54/77 | 163/270) LanePtrNet: Revisiting Lane Detection as Point Voting and Grouping on Curves (Jiayan Cao et al., 2024)</a></li><li><a href=#5577--164270-agile-multi-source-free-domain-adaptation-xinyao-li-et-al-2024>(55/77 | 164/270) Agile Multi-Source-Free Domain Adaptation (Xinyao Li et al., 2024)</a></li><li><a href=#5677--165270-dyronet-a-low-rank-adapter-enhanced-dynamic-routing-network-for-streaming-perception-xiang-huang-et-al-2024>(56/77 | 165/270) DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception (Xiang Huang et al., 2024)</a></li><li><a href=#5777--166270-beyond-mot-semantic-multi-object-tracking-yunhao-li-et-al-2024>(57/77 | 166/270) Beyond MOT: Semantic Multi-Object Tracking (Yunhao Li et al., 2024)</a></li><li><a href=#5877--167270-diffclass-diffusion-based-class-incremental-learning-zichong-meng-et-al-2024>(58/77 | 167/270) DiffClass: Diffusion-Based Class Incremental Learning (Zichong Meng et al., 2024)</a></li><li><a href=#5977--168270-unlocking-the-potential-of-multimodal-unified-discrete-representation-through-training-free-codebook-optimization-and-hierarchical-alignment-hai-huang-et-al-2024>(59/77 | 168/270) Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment (Hai Huang et al., 2024)</a></li><li><a href=#6077--169270-not-just-birds-and-cars-generic-scalable-and-explainable-models-for-professional-visual-recognition-junde-wu-et-al-2024>(60/77 | 169/270) Not just Birds and Cars: Generic, Scalable and Explainable Models for Professional Visual Recognition (Junde Wu et al., 2024)</a></li><li><a href=#6177--170270-generalized-correspondence-matching-via-flexible-hierarchical-refinement-and-patch-descriptor-distillation-yu-han-et-al-2024>(61/77 | 170/270) Generalized Correspondence Matching via Flexible Hierarchical Refinement and Patch Descriptor Distillation (Yu Han et al., 2024)</a></li><li><a href=#6277--171270-enhancing-plausibility-evaluation-for-generated-designs-with-denoising-autoencoder-jiajie-fan-et-al-2024>(62/77 | 171/270) Enhancing Plausibility Evaluation for Generated Designs with Denoising Autoencoder (Jiajie Fan et al., 2024)</a></li><li><a href=#6377--172270-cross-modal-and-uni-modal-soft-label-alignment-for-image-text-retrieval-hailang-huang-et-al-2024>(63/77 | 172/270) Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval (Hailang Huang et al., 2024)</a></li><li><a href=#6477--173270-hide-in-thicket-generating-imperceptible-and-rational-adversarial-perturbations-on-3d-point-clouds-tianrui-lou-et-al-2024>(64/77 | 173/270) Hide in Thicket: Generating Imperceptible and Rational Adversarial Perturbations on 3D Point Clouds (Tianrui Lou et al., 2024)</a></li><li><a href=#6577--174270-learning-expressive-and-generalizable-motion-features-for-face-forgery-detection-jingyi-zhang-et-al-2024>(65/77 | 174/270) Learning Expressive And Generalizable Motion Features For Face Forgery Detection (Jingyi Zhang et al., 2024)</a></li><li><a href=#6677--175270-diffult-how-to-make-diffusion-model-useful-for-long-tail-recognition-jie-shao-et-al-2024>(66/77 | 175/270) DiffuLT: How to Make Diffusion Model Useful for Long-tail Recognition (Jie Shao et al., 2024)</a></li><li><a href=#6777--176270-gsedit-efficient-text-guided-editing-of-3d-objects-via-gaussian-splatting-francesco-palandra-et-al-2024>(67/77 | 176/270) GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting (Francesco Palandra et al., 2024)</a></li><li><a href=#6877--177270-enhancing-texture-generation-with-high-fidelity-using-advanced-texture-priors-kuo-xu-et-al-2024>(68/77 | 177/270) Enhancing Texture Generation with High-Fidelity Using Advanced Texture Priors (Kuo Xu et al., 2024)</a></li><li><a href=#6977--178270-improving-diffusion-based-generative-models-via-approximated-optimal-transport-daegyu-kim-et-al-2024>(69/77 | 178/270) Improving Diffusion-Based Generative Models via Approximated Optimal Transport (Daegyu Kim et al., 2024)</a></li><li><a href=#7077--179270-primecomposer-faster-progressively-combined-diffusion-for-image-composition-with-attention-steering-yibin-wang-et-al-2024>(70/77 | 179/270) PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering (Yibin Wang et al., 2024)</a></li><li><a href=#7177--180270-ditto-dual-and-integrated-latent-topologies-for-implicit-3d-reconstruction-jaehyeok-shim-et-al-2024>(71/77 | 180/270) DITTO: Dual and Integrated Latent Topologies for Implicit 3D Reconstruction (Jaehyeok Shim et al., 2024)</a></li><li><a href=#7277--181270-evd4uav-an-altitude-sensitive-benchmark-to-evade-vehicle-detection-in-uav-huiming-sun-et-al-2024>(72/77 | 181/270) EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in UAV (Huiming Sun et al., 2024)</a></li><li><a href=#7377--182270-occfusion-depth-estimation-free-multi-sensor-fusion-for-3d-occupancy-prediction-ji-zhang-et-al-2024>(73/77 | 182/270) OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction (Ji Zhang et al., 2024)</a></li><li><a href=#7477--183270-arbitrary-scale-point-cloud-upsampling-by-voxel-based-network-with-latent-geometric-consistent-learning-hang-du-et-al-2024>(74/77 | 183/270) Arbitrary-Scale Point Cloud Upsampling by Voxel-Based Network with Latent Geometric-Consistent Learning (Hang Du et al., 2024)</a></li><li><a href=#7577--184270-decoupling-degradations-with-recurrent-network-for-video-restoration-in-under-display-camera-chengxu-liu-et-al-2024>(75/77 | 184/270) Decoupling Degradations with Recurrent Network for Video Restoration in Under-Display Camera (Chengxu Liu et al., 2024)</a></li><li><a href=#7677--185270-lvic-multi-modality-segmentation-by-lifting-visual-info-as-cue-zichao-dong-et-al-2024>(76/77 | 185/270) LVIC: Multi-modality segmentation by Lifting Visual Info as Cue (Zichao Dong et al., 2024)</a></li><li><a href=#7777--186270-reps-reconstruction-based-point-cloud-sampling-guoqing-zhang-et-al-2024>(77/77 | 186/270) REPS: Reconstruction-based Point Cloud Sampling (Guoqing Zhang et al., 2024)</a></li></ul></li><li><a href=#csmm-2>cs.MM (2)</a><ul><li><a href=#12--187270-multimodal-infusion-tuning-for-large-models-hao-sun-et-al-2024>(1/2 | 187/270) Multimodal Infusion Tuning for Large Models (Hao Sun et al., 2024)</a></li><li><a href=#22--188270-towards-real-world-stickers-use-a-new-dataset-for-multi-tag-sticker-recognition-bingbing-wang-et-al-2024>(2/2 | 188/270) Towards Real-World Stickers Use: A New Dataset for Multi-Tag Sticker Recognition (Bingbing Wang et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--189270-text-to-audio-generation-synchronized-with-videos-shentong-mo-et-al-2024>(1/2 | 189/270) Text-to-Audio Generation Synchronized with Videos (Shentong Mo et al., 2024)</a></li><li><a href=#22--190270-rfwave-multi-band-rectified-flow-for-audio-waveform-reconstruction-peng-liu-et-al-2024>(2/2 | 190/270) RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction (Peng Liu et al., 2024)</a></li></ul></li><li><a href=#physicsapp-ph-1>physics.app-ph (1)</a><ul><li><a href=#11--191270-inverse-design-of-photonic-crystal-surface-emitting-lasers-is-a-sequence-modeling-problem-ceyao-zhang-et-al-2024>(1/1 | 191/270) Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem (Ceyao Zhang et al., 2024)</a></li></ul></li><li><a href=#cspl-3>cs.PL (3)</a><ul><li><a href=#13--192270-llm4decompile-decompiling-binary-code-with-large-language-models-hanzhuo-tan-et-al-2024>(1/3 | 192/270) LLM4Decompile: Decompiling Binary Code with Large Language Models (Hanzhuo Tan et al., 2024)</a></li><li><a href=#23--193270-we-know-i-know-you-know-choreographic-programming-with-multicast-and-multiply-located-values-mako-bates-et-al-2024>(2/3 | 193/270) We Know I Know You Know; Choreographic Programming With Multicast and Multiply Located Values (Mako Bates et al., 2024)</a></li><li><a href=#33--194270-modeling-dynamic-deallocations-of-local-memory-for-translation-validation-abhishek-rose-et-al-2024>(3/3 | 194/270) Modeling Dynamic (De)Allocations of Local Memory for Translation Validation (Abhishek Rose et al., 2024)</a></li></ul></li><li><a href=#csro-15>cs.RO (15)</a><ul><li><a href=#115--195270-are-large-language-models-aligned-with-peoples-social-intuitions-for-human-robot-interactions-lennart-wachowiak-et-al-2024>(1/15 | 195/270) Are Large Language Models Aligned with People&rsquo;s Social Intuitions for Human-Robot Interactions? (Lennart Wachowiak et al., 2024)</a></li><li><a href=#215--196270-take-your-best-shot-sampling-based-next-best-view-planning-for-autonomous-photography--inspection-shijie-gao-et-al-2024>(2/15 | 196/270) Take Your Best Shot: Sampling-Based Next-Best-View Planning for Autonomous Photography & Inspection (Shijie Gao et al., 2024)</a></li><li><a href=#315--197270-improving-the-successful-robotic-grasp-detection-using-convolutional-neural-networks-hamed-hosseini-et-al-2024>(3/15 | 197/270) Improving the Successful Robotic Grasp Detection Using Convolutional Neural Networks (Hamed Hosseini et al., 2024)</a></li><li><a href=#415--198270-prepared-for-the-worst-a-learning-based-adversarial-attack-for-resilience-analysis-of-the-icp-algorithm-ziyu-zhang-et-al-2024>(4/15 | 198/270) Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm (Ziyu Zhang et al., 2024)</a></li><li><a href=#515--199270-federated-joint-learning-of-robot-networks-in-stroke-rehabilitation-xinyu-jiang-et-al-2024>(5/15 | 199/270) Federated Joint Learning of Robot Networks in Stroke Rehabilitation (Xinyu Jiang et al., 2024)</a></li><li><a href=#615--200270-safe-execution-of-learned-orientation-skills-with-conic-control-barrier-functions-zheng-shen-et-al-2024>(6/15 | 200/270) Safe Execution of Learned Orientation Skills with Conic Control Barrier Functions (Zheng Shen et al., 2024)</a></li><li><a href=#715--201270-embracing-large-language-and-multimodal-models-for-prosthetic-technologies-sharmita-dey-et-al-2024>(7/15 | 201/270) Embracing Large Language and Multimodal Models for Prosthetic Technologies (Sharmita Dey et al., 2024)</a></li><li><a href=#815--202270-ocean-an-openspace-collision-free-trajectory-planner-for-autonomous-parking-based-on-admm-dongxu-wang-et-al-2024>(8/15 | 202/270) OCEAN: An Openspace Collision-free Trajectory Planner for Autonomous Parking Based on ADMM (Dongxu Wang et al., 2024)</a></li><li><a href=#915--203270-interactive-perception-for-deformable-object-manipulation-zehang-weng-et-al-2024>(9/15 | 203/270) Interactive Perception for Deformable Object Manipulation (Zehang Weng et al., 2024)</a></li><li><a href=#1015--204270-efficient-data-collection-for-robotic-manipulation-via-compositional-generalization-jensen-gao-et-al-2024>(10/15 | 204/270) Efficient Data Collection for Robotic Manipulation via Compositional Generalization (Jensen Gao et al., 2024)</a></li><li><a href=#1115--205270-spatiotemporal-predictive-pre-training-for-robotic-motor-control-jiange-yang-et-al-2024>(11/15 | 205/270) Spatiotemporal Predictive Pre-training for Robotic Motor Control (Jiange Yang et al., 2024)</a></li><li><a href=#1215--206270-model-comparison-for-fast-domain-adaptation-in-table-service-scenario-woo-han-yun-et-al-2024>(12/15 | 206/270) Model Comparison for Fast Domain Adaptation in Table Service Scenario (Woo-han Yun et al., 2024)</a></li><li><a href=#1315--207270-integrating-predictive-motion-uncertainties-with-distributionally-robust-risk-aware-control-for-safe-robot-navigation-in-crowds-kanghyun-ryu-et-al-2024>(13/15 | 207/270) Integrating Predictive Motion Uncertainties with Distributionally Robust Risk-Aware Control for Safe Robot Navigation in Crowds (Kanghyun Ryu et al., 2024)</a></li><li><a href=#1415--208270-robust-surgical-tool-tracking-with-pixel-based-probabilities-for-projected-geometric-primitives-christopher-dambrosia-et-al-2024>(14/15 | 208/270) Robust Surgical Tool Tracking with Pixel-based Probabilities for Projected Geometric Primitives (Christopher D&rsquo;Ambrosia et al., 2024)</a></li><li><a href=#1515--209270-degradation-resilient-lidar-radar-inertial-odometry-morten-nissov-et-al-2024>(15/15 | 209/270) Degradation Resilient LiDAR-Radar-Inertial Odometry (Morten Nissov et al., 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--210270-speech-robust-bench-a-robustness-benchmark-for-speech-recognition-muhammad-a-shah-et-al-2024>(1/1 | 210/270) Speech Robust Bench: A Robustness Benchmark For Speech Recognition (Muhammad A. Shah et al., 2024)</a></li></ul></li><li><a href=#csse-2>cs.SE (2)</a><ul><li><a href=#12--211270-profile-of-vulnerability-remediations-in-dependencies-using-graph-analysis-fernando-vera-et-al-2024>(1/2 | 211/270) Profile of Vulnerability Remediations in Dependencies Using Graph Analysis (Fernando Vera et al., 2024)</a></li><li><a href=#22--212270-effective-fault-localization-using-probabilistic-and-grouping-approach-saksham-sahai-srivastava-et-al-2024>(2/2 | 212/270) Effective Fault Localization using Probabilistic and Grouping Approach (Saksham Sahai Srivastava et al., 2024)</a></li></ul></li><li><a href=#econem-1>econ.EM (1)</a><ul><li><a href=#11--213270-non-robustness-of-diffusion-estimates-on-networks-with-measurement-error-arun-g-chandrasekhar-et-al-2024>(1/1 | 213/270) Non-robustness of diffusion estimates on networks with measurement error (Arun G. Chandrasekhar et al., 2024)</a></li></ul></li><li><a href=#csar-3>cs.AR (3)</a><ul><li><a href=#13--214270-algorithm-hardware-co-design-of-distribution-aware-logarithmic-posit-encodings-for-efficient-dnn-inference-akshat-ramachandran-et-al-2024>(1/3 | 214/270) Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference (Akshat Ramachandran et al., 2024)</a></li><li><a href=#23--215270-lightator-an-optical-near-sensor-accelerator-with-compressive-acquisition-enabling-versatile-image-processing-mehrdad-morsali-et-al-2024>(2/3 | 215/270) Lightator: An Optical Near-Sensor Accelerator with Compressive Acquisition Enabling Versatile Image Processing (Mehrdad Morsali et al., 2024)</a></li><li><a href=#33--216270-a-286-mjiter-stable-diffusion-processor-for-text-to-image-generation-with-patch-similarity-based-sparsity-augmentation-and-text-based-mixed-precision-jiwon-choi-et-al-2024>(3/3 | 216/270) A 28.6 mJ/iter Stable Diffusion Processor for Text-to-Image Generation with Patch Similarity-based Sparsity Augmentation and Text-based Mixed-Precision (Jiwon Choi et al., 2024)</a></li></ul></li><li><a href=#cshc-8>cs.HC (8)</a><ul><li><a href=#18--217270-trust-recognition-in-human-robot-cooperation-using-eeg-caiyue-xu-et-al-2024>(1/8 | 217/270) Trust Recognition in Human-Robot Cooperation Using EEG (Caiyue Xu et al., 2024)</a></li><li><a href=#28--218270-aqua-automated-question-answering-in-software-tutorial-videos-with-visual-anchors-saelyne-yang-et-al-2024>(2/8 | 218/270) AQuA: Automated Question-Answering in Software Tutorial Videos with Visual Anchors (Saelyne Yang et al., 2024)</a></li><li><a href=#38--219270-vibopneumo-a-vibratory-pneumatic-finger-worn-haptic-device-for-altering-perceived-texture-roughness-in-mixed-reality-shaoyu-cai-et-al-2024>(3/8 | 219/270) ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering Perceived Texture Roughness in Mixed Reality (Shaoyu Cai et al., 2024)</a></li><li><a href=#48--220270-scoping-out-the-scalability-issues-of-autonomous-vehicle-pedestrian-interaction-tram-thi-minh-tran-et-al-2024>(4/8 | 220/270) Scoping Out the Scalability Issues of Autonomous Vehicle-Pedestrian Interaction (Tram Thi Minh Tran et al., 2024)</a></li><li><a href=#58--221270-digital-wellbeing-redefined-toward-user-centric-approach-for-positive-social-media-engagement-yixue-zhao-et-al-2024>(5/8 | 221/270) Digital Wellbeing Redefined: Toward User-Centric Approach for Positive Social Media Engagement (Yixue Zhao et al., 2024)</a></li><li><a href=#68--222270-enabling-developers-protecting-users-investigating-harassment-and-safety-in-vr-abhinaya-s-b-et-al-2024>(6/8 | 222/270) Enabling Developers, Protecting Users: Investigating Harassment and Safety in VR (Abhinaya S. B. et al., 2024)</a></li><li><a href=#78--223270-comparison-of-spatial-visualization-techniques-for-radiation-in-augmented-reality-fintan-mcgee-et-al-2024>(7/8 | 223/270) Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality (Fintan McGee et al., 2024)</a></li><li><a href=#88--224270-know-your-audience-the-benefits-and-pitfalls-of-generating-plain-language-summaries-beyond-the-general-audience-tal-august-et-al-2024>(8/8 | 224/270) Know Your Audience: The benefits and pitfalls of generating plain language summaries beyond the &lsquo;general&rsquo; audience (Tal August et al., 2024)</a></li></ul></li><li><a href=#eesssy-9>eess.SY (9)</a><ul><li><a href=#19--225270-device-fault-prediction-model-based-on-lstm-and-random-forest-jing-xu-et-al-2024>(1/9 | 225/270) Device Fault Prediction Model based on LSTM and Random Forest (Jing Xu et al., 2024)</a></li><li><a href=#29--226270-sampling-model-for-grid-material-inspection-based-on-analytic-hierarchy-process-with-absolute-measurement-jing-xu-et-al-2024>(2/9 | 226/270) Sampling Model for Grid Material Inspection Based on Analytic Hierarchy Process with Absolute Measurement (Jing Xu et al., 2024)</a></li><li><a href=#39--227270-economic-capacity-withholding-bounds-of-competitive-energy-storage-bidders-xin-qin-et-al-2024>(3/9 | 227/270) Economic Capacity Withholding Bounds of Competitive Energy Storage Bidders (Xin Qin et al., 2024)</a></li><li><a href=#49--228270-stability-certified-on-policy-data-driven-lqr-via-recursive-learning-and-policy-gradient-lorenzo-sforni-et-al-2024>(4/9 | 228/270) Stability-Certified On-Policy Data-Driven LQR via Recursive Learning and Policy Gradient (Lorenzo Sforni et al., 2024)</a></li><li><a href=#59--229270-correlation-analysis-technique-of-key-parameters-for-transformer-material-inspection-based-on-fp-tree-and-knowledge-graph-jing-xu-et-al-2024>(5/9 | 229/270) Correlation analysis technique of key parameters for transformer material inspection based on FP-tree and knowledge graph (Jing Xu et al., 2024)</a></li><li><a href=#69--230270-a-framework-for-effective-ai-recommendations-in-cyber-physical-human-systems-aditya-dave-et-al-2024>(6/9 | 230/270) A Framework for Effective AI Recommendations in Cyber-Physical-Human Systems (Aditya Dave et al., 2024)</a></li><li><a href=#79--231270-control-oriented-identification-for-the-linear-quadratic-regulator-technical-report-sean-anderson-et-al-2024>(7/9 | 231/270) Control-Oriented Identification for the Linear Quadratic Regulator: Technical Report (Sean Anderson et al., 2024)</a></li><li><a href=#89--232270-exploiting-polar-symmetry-in-designing-equivariant-observers-for-vision-based-motion-estimation-tarek-bouazza-et-al-2024>(8/9 | 232/270) Exploiting polar symmetry in designing equivariant observers for vision-based motion estimation (Tarek Bouazza et al., 2024)</a></li><li><a href=#99--233270-formal-verification-of-unknown-stochastic-systems-via-non-parametric-estimation-zhi-zhang-et-al-2024>(9/9 | 233/270) Formal Verification of Unknown Stochastic Systems via Non-parametric Estimation (Zhi Zhang et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--234270-ris-empowered-topology-control-for-distributed-learning-in-urban-air-mobility-kai-xiong-et-al-2024>(1/2 | 234/270) RIS-empowered Topology Control for Distributed Learning in Urban Air Mobility (Kai Xiong et al., 2024)</a></li><li><a href=#22--235270-gan-based-massive-mimo-channel-model-trained-on-measured-data-florian-euchner-et-al-2024>(2/2 | 235/270) GAN-based Massive MIMO Channel Model Trained on Measured Data (Florian Euchner et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--236270-enhancing-non-intrusive-reduced-order-models-with-space-dependent-aggregation-methods-anna-ivagnes-et-al-2024>(1/2 | 236/270) Enhancing non-intrusive Reduced Order Models with space-dependent aggregation methods (Anna Ivagnes et al., 2024)</a></li><li><a href=#22--237270-multirate-time-integration-based-on-dynamic-ode-partitioning-through-adaptively-refined-meshes-for-compressible-fluid-dynamics-daniel-doehring-et-al-2024>(2/2 | 237/270) Multirate Time-Integration based on Dynamic ODE Partitioning through Adaptively Refined Meshes for Compressible Fluid Dynamics (Daniel Doehring et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--238270-the-r2d2-deep-neural-network-series-paradigm-for-fast-precision-imaging-in-radio-astronomy-amir-aghabiglou-et-al-2024>(1/1 | 238/270) The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy (Amir Aghabiglou et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--239270-numerical-simulations-of-a-stochastic-dynamics-leading-to-cascades-and-loss-of-regularity-applications-to-fluid-turbulence-and-generation-of-fractional-gaussian-fields-geoffrey-beck-et-al-2024>(1/1 | 239/270) Numerical simulations of a stochastic dynamics leading to cascades and loss of regularity: applications to fluid turbulence and generation of fractional Gaussian fields (Geoffrey Beck et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--240270-variational-inference-of-parameters-in-opinion-dynamics-models-jacopo-lenti-et-al-2024>(1/2 | 240/270) Variational Inference of Parameters in Opinion Dynamics Models (Jacopo Lenti et al., 2024)</a></li><li><a href=#22--241270-interoperability-of-the-metaverse-a-digital-ecosystem-perspective-review-liang-yang-et-al-2024>(2/2 | 241/270) Interoperability of the Metaverse: A Digital Ecosystem Perspective Review (Liang Yang et al., 2024)</a></li></ul></li><li><a href=#csce-3>cs.CE (3)</a><ul><li><a href=#13--242270-modeling-of-progressive-high-cycle-fatigue-in-composite-laminates-accounting-for-local-stress-ratios-p-hofman-et-al-2024>(1/3 | 242/270) Modeling of progressive high-cycle fatigue in composite laminates accounting for local stress ratios (P. Hofman et al., 2024)</a></li><li><a href=#23--243270-robust-automated-calcification-meshing-for-biomechanical-cardiac-digital-twins-daniel-h-pak-et-al-2024>(2/3 | 243/270) Robust automated calcification meshing for biomechanical cardiac digital twins (Daniel H. Pak et al., 2024)</a></li><li><a href=#33--244270-rediscovering-the-mullins-effect-with-deep-symbolic-regression-rasul-abdusalamov-et-al-2024>(3/3 | 244/270) Rediscovering the Mullins Effect With Deep Symbolic Regression (Rasul Abdusalamov et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--245270-wykorzystanie-rekonfigurowalnych-iinteligentnych-matryc-antenowych-w-łączu-dosyłowym-sieci-5g6g-wykorzystującej-bezzałogowe-statki-powietrzne-salim-janji-et-al-2024>(1/3 | 245/270) Wykorzystanie Rekonfigurowalnych Iinteligentnych Matryc Antenowych w Łączu Dosyłowym Sieci 5G/6G Wykorzystującej Bezzałogowe Statki Powietrzne (Salim Janji et al., 2024)</a></li><li><a href=#23--246270-ris-aided-multi-hop-backhauling-for-5g6g-uav-assisted-access-points-salim-janji-et-al-2024>(2/3 | 246/270) RIS-aided multi-hop backhauling for 5G/6G UAV-assisted access points (Salim Janji et al., 2024)</a></li><li><a href=#33--247270-adroit6g-dai-driven-open-and-programmable-architecture-for-6g-networks-christophoros-christophorou-et-al-2024>(3/3 | 247/270) ADROIT6G DAI-driven Open and Programmable Architecture for 6G Networks (Christophoros Christophorou et al., 2024)</a></li></ul></li><li><a href=#quant-ph-3>quant-ph (3)</a><ul><li><a href=#13--248270-load-balancing-for-high-performance-computing-using-quantum-annealing-omer-rathore-et-al-2024>(1/3 | 248/270) Load Balancing For High Performance Computing Using Quantum Annealing (Omer Rathore et al., 2024)</a></li><li><a href=#23--249270-q-chop-quantum-constrained-hamiltonian-optimization-michael-a-perlin-et-al-2024>(2/3 | 249/270) Q-CHOP: Quantum constrained Hamiltonian optimization (Michael A. Perlin et al., 2024)</a></li><li><a href=#33--250270-maximal-non-kochen-specker-sets-and-a-lower-bound-on-the-size-of-kochen-specker-sets-tom-williams-et-al-2024>(3/3 | 250/270) Maximal Non-Kochen-Specker Sets and a Lower Bound on the Size of Kochen-Specker Sets (Tom Williams et al., 2024)</a></li></ul></li><li><a href=#cset-2>cs.ET (2)</a><ul><li><a href=#12--251270-user-connection-and-resource-allocation-optimization-in-blockchain-empowered-metaverse-over-6g-wireless-communications-liangxin-qian-et-al-2024>(1/2 | 251/270) User Connection and Resource Allocation Optimization in Blockchain Empowered Metaverse over 6G Wireless Communications (Liangxin Qian et al., 2024)</a></li><li><a href=#22--252270-paving-the-way-for-pass-disturb-free-vertical-nand-storage-via-a-dedicated-and-string-compatible-pass-gate-zijian-zhao-et-al-2024>(2/2 | 252/270) Paving the Way for Pass Disturb Free Vertical NAND Storage via A Dedicated and String-Compatible Pass Gate (Zijian Zhao et al., 2024)</a></li></ul></li><li><a href=#q-biope-1>q-bio.PE (1)</a><ul><li><a href=#11--253270-information-theory-in-a-darwinian-evolution-population-dynamics-model-eddy-kwessi-2024>(1/1 | 253/270) Information Theory in a Darwinian Evolution Population Dynamics Model (Eddy Kwessi, 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--254270-extracting-protein-protein-interactions-ppis-from-biomedical-literature-using-attention-based-relational-context-information-gilchan-park-et-al-2024>(1/1 | 254/270) Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information (Gilchan Park et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--255270-online-contention-resolution-schemes-for-network-revenue-management-and-combinatorial-auctions-will-ma-et-al-2024>(1/1 | 255/270) Online Contention Resolution Schemes for Network Revenue Management and Combinatorial Auctions (Will Ma et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--256270-node-centrality-approximation-for-large-networks-based-on-inductive-graph-neural-networks-yiwei-zou-et-al-2024>(1/1 | 256/270) Node Centrality Approximation For Large Networks Based On Inductive Graph Neural Networks (Yiwei Zou et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--257270-privacy-preserving-sharing-of-data-analytics-runtime-metrics-for-performance-modeling-jonathan-will-et-al-2024>(1/1 | 257/270) Privacy-Preserving Sharing of Data Analytics Runtime Metrics for Performance Modeling (Jonathan Will et al., 2024)</a></li></ul></li><li><a href=#statml-2>stat.ML (2)</a><ul><li><a href=#12--258270-follow-the-perturbed-leader-with-fréchet-type-tail-distributions-optimality-in-adversarial-bandits-and-best-of-both-worlds-jongyeong-lee-et-al-2024>(1/2 | 258/270) Follow-the-Perturbed-Leader with Fréchet-type Tail Distributions: Optimality in Adversarial Bandits and Best-of-Both-Worlds (Jongyeong Lee et al., 2024)</a></li><li><a href=#22--259270-spectral-clustering-of-categorical-and-mixed-type-data-via-extra-graph-nodes-dylan-soemitro-et-al-2024>(2/2 | 259/270) Spectral Clustering of Categorical and Mixed-type Data via Extra Graph Nodes (Dylan Soemitro et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--260270-data-dependent-lsh-for-the-earth-movers-distance-rajesh-jayaram-et-al-2024>(1/2 | 260/270) Data-Dependent LSH for the Earth Mover&rsquo;s Distance (Rajesh Jayaram et al., 2024)</a></li><li><a href=#22--261270-efficient-algorithms-for-personalized-pagerank-computation-a-survey-mingji-yang-et-al-2024>(2/2 | 261/270) Efficient Algorithms for Personalized PageRank Computation: A Survey (Mingji Yang et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--262270-splattingavatar-realistic-real-time-human-avatars-with-mesh-embedded-gaussian-splatting-zhijing-shao-et-al-2024>(1/1 | 262/270) SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting (Zhijing Shao et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--263270-geometric-neural-network-based-on-phase-space-for-bci-decoding-igor-carrara-et-al-2024>(1/1 | 263/270) Geometric Neural Network based on Phase Space for BCI decoding (Igor Carrara et al., 2024)</a></li></ul></li><li><a href=#mathco-4>math.CO (4)</a><ul><li><a href=#14--264270-the-metric-menger-problem-júlia-baligács-et-al-2024>(1/4 | 264/270) The metric Menger problem (Júlia Baligács et al., 2024)</a></li><li><a href=#24--265270-on-balanceable-and-simply-balanceable-regular-graphs-milad-ahanjideh-et-al-2024>(2/4 | 265/270) On balanceable and simply balanceable regular graphs (Milad Ahanjideh et al., 2024)</a></li><li><a href=#34--266270-path-eccentricity-of-k-at-free-graphs-and-application-on-graphs-with-the-consecutive-ones-property-paul-bastide-et-al-2024>(3/4 | 266/270) Path eccentricity of $k$-AT-free graphs and application on graphs with the consecutive ones property (Paul Bastide et al., 2024)</a></li><li><a href=#44--267270-extremal-chemical-graphs-for-the-arithmetic-geometric-index-alain-hertz-et-al-2024>(4/4 | 267/270) Extremal Chemical Graphs for the Arithmetic-Geometric Index (Alain Hertz et al., 2024)</a></li></ul></li><li><a href=#statap-1>stat.AP (1)</a><ul><li><a href=#11--268270-bayesian-hierarchical-probabilistic-forecasting-of-intraday-electricity-prices-daniel-nickelsen-et-al-2024>(1/1 | 268/270) Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices (Daniel Nickelsen et al., 2024)</a></li></ul></li><li><a href=#mathpr-1>math.PR (1)</a><ul><li><a href=#11--269270-limit-laws-for-critical-dispersion-on-complete-graphs-umberto-de-ambroggio-et-al-2024>(1/1 | 269/270) Limit Laws for Critical Dispersion on Complete Graphs (Umberto De Ambroggio et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--270270-estimation-of-electronic-band-gap-energy-from-material-properties-using-machine-learning-sagar-prakash-barad-et-al-2024>(1/1 | 270/270) Estimation of Electronic Band Gap Energy From Material Properties Using Machine Learning (Sagar Prakash Barad et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>