<!doctype html><html><head><title>arXiv @ 2024.03.28</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.28"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cs.AI (26) cs.CG (2) cs.CL (64) cs.CR (9) cs.CV (81) cs.CY (1) cs.DB (2) cs.DC (1) cs.DS (2) cs.GR (1) cs.GT (2) cs.HC (5) cs.IR (16) cs.IT (3) cs.LG (49) cs.MM (1) cs.NE (1) cs.NI (1) cs.PL (1) cs.RO (16) cs.SD (3) cs.SE (5) cs.SI (2) econ.TH (1) eess.AS (2) eess.IV (18) eess.SP (1) eess.SY (11) hep-ex (1) math.CO (2) math.NA (2) math.OC (5) math.ST (2) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240328000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-28T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-28T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.28"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240328000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Mar 28, 2024</p></div><div class=title><h1>arXiv @ 2024.03.28</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csai-26>cs.AI (26)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cscg-2>cs.CG (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cscl-64>cs.CL (64)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cscr-9>cs.CR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cscv-81>cs.CV (81)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csdb-2>cs.DB (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csgt-2>cs.GT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csir-16>cs.IR (16)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csit-3>cs.IT (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cslg-49>cs.LG (49)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csro-16>cs.RO (16)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cssd-3>cs.SD (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#eessiv-18>eess.IV (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#eesssy-11>eess.SY (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#hep-ex-1>hep-ex (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#mathco-2>math.CO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#mathoc-5>math.OC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#mathst-2>math.ST (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#physicsapp-ph-1>physics.app-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#physicssoc-ph-2>physics.soc-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/#statml-2>stat.ML (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.IR</th><th>cs.LG</th><th>cs.RO</th><th>eess.IV</th><th>eess.SY</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td>1</td><td></td><td>2</td><td></td><td>2</td><td>1</td><td>2</td><td></td></tr><tr><td>Automatic Evaluation</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>6</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td>1</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>3</td><td>19</td><td>24</td><td>4</td><td>8</td><td>2</td><td></td><td>1</td></tr><tr><td>Black Box</td><td></td><td>1</td><td></td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Causal Intervention</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td>5</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td></td><td>2</td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Code Generation</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td></td><td></td><td></td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>2</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>5</td><td></td><td>4</td><td></td><td>6</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>2</td><td></td><td>5</td><td></td><td>8</td><td></td></tr><tr><td>Counter-factual</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td>4</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Dense Retrieval</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Dialogue State Tracking</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Dialogue System</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>14</td><td></td><td>4</td><td></td><td>3</td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Distribution Shift</td><td>2</td><td></td><td>8</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Document Embedding</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Event Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td>3</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td></td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>Few-shot</td><td></td><td>7</td><td>3</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>2</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>1</td><td>14</td><td>3</td><td>1</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Foundation Model</td><td></td><td></td><td>4</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>GPT</td><td>1</td><td>10</td><td>2</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-2</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td></td><td>4</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td></td><td>10</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4 turbo</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td>2</td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>7</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Grammatical Error Correction</td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Graph</td><td>6</td><td>4</td><td>3</td><td>2</td><td>7</td><td>6</td><td>2</td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Graph Neural Network</td><td>2</td><td>1</td><td></td><td>2</td><td>11</td><td></td><td></td><td></td></tr><tr><td>Grounding</td><td>2</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>3</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td></td><td>9</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>2</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Karush-Kuhn-Tucker</td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td>3</td><td>3</td><td>6</td><td></td><td></td><td></td></tr><tr><td>Knowledge Graph</td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>1</td><td>3</td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Language Generation</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>14</td><td>54</td><td>8</td><td>4</td><td>7</td><td>1</td><td>1</td><td></td></tr><tr><td>Logistic Regression</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Low-Resource</td><td></td><td>1</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Masked Language Model</td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td>1</td><td></td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>6</td><td>13</td><td>14</td><td>2</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>5</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Explanation</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>5</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>8</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Online Reinforcement Learning</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Open-Domain Question Answering</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td>1</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-domain</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>PaLM</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Parameter Sharing</td><td></td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td></td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Prompt</td><td>5</td><td>13</td><td>7</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Pruning</td><td>1</td><td>1</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Question Answering</td><td>1</td><td>15</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>2</td><td>4</td><td>2</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Recommendation</td><td>1</td><td>2</td><td>1</td><td>9</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td></td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>4</td><td>2</td><td></td><td>2</td><td>7</td><td>2</td><td></td><td>1</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>2</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Rerank</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Scaling Law</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>4</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Distillation</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td>1</td><td>2</td><td>7</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sentence Embedding</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>2</td><td></td><td>1</td><td>1</td><td>4</td><td>7</td><td>1</td><td>8</td></tr><tr><td>Simulator</td><td>2</td><td></td><td>1</td><td>1</td><td>4</td><td>7</td><td>1</td><td>8</td></tr><tr><td>Slot Filling</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Summarization</td><td>2</td><td>3</td><td>2</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td>6</td><td>6</td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>T5</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>3</td><td>1</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text Mining</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Transformation</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>2</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Topic Model</td><td></td><td>3</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Topic Modeling</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>3</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transformer</td><td>2</td><td>5</td><td>12</td><td>2</td><td>6</td><td>2</td><td>5</td><td></td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>4</td><td>2</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Variational Autoencoder</td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td>2</td><td></td><td>2</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>2</td><td>4</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td>2</td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Yolo</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>3</td><td>5</td><td>2</td><td></td><td>3</td><td></td><td></td></tr><tr><td>falcon</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-64>cs.CL (64)</h2><h3 id=164--1347-ellen-extremely-lightly-supervised-learning-for-efficient-named-entity-recognition-haris-riaz-et-al-2024>(1/64 | 1/347) ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition (Haris Riaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haris Riaz, Razvan-Gabriel Dumitru, Mihai Surdeanu. (2024)<br><strong>ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition</strong><br><button class=copy-to-clipboard title="ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 120<br>Keywords: Fine-tuning, Supervised Learning, Supervised Learning, Unsupervised Learning, Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, Named Entity Recognition, Named Entity Recognition, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17385v1.pdf filename=2403.17385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we revisit the problem of semi-supervised <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class. We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends <b>fine-tuned</b> language models with linguistic rules. These rules include insights such as &lsquo;&lsquo;One Sense Per Discourse&rsquo;&rsquo;, using a <b>Masked</b> <b>Language</b> <b>Model</b> as an <b>unsupervised</b> <b>NER,</b> leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised <b>NER</b> methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a <b>zero-shot</b> scenario on WNUT-17 where we find that it outperforms <b>GPT-3.5</b> and achieves comparable performance to <b>GPT-4.</b> In a <b>zero-shot</b> setting, ELLEN also achieves over 75% of the performance of a strong, fully <b>supervised</b> <b>model</b> trained on gold data. Our code is available at: <a href=https://github.com/hriaz17/ELLEN>https://github.com/hriaz17/ELLEN</a>.</p></p class="citation"></blockquote><h3 id=264--2347-internlm2-technical-report-zheng-cai-et-al-2024>(2/64 | 2/347) InternLM2 Technical Report (Zheng Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Dahua Lin. (2024)<br><strong>InternLM2 Technical Report</strong><br><button class=copy-to-clipboard title="InternLM2 Technical Report" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 113<br>Keywords: Benchmarking, Fine-tuning, Online Reinforcement Learning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Supervised Learning, ChatGPT, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17297v1.pdf filename=2403.17297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT</b> and <b>GPT-4</b> has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source <b>LLM</b> that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 <b>benchmarks,</b> long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and <b>fine-tuning</b> stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack" test. InternLM2 is further aligned using <b>Supervised</b> <b>Fine-Tuning</b> (SFT) and a novel Conditional <b>Online</b> <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> (COOL <b>RLHF)</b> strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model&rsquo;s evolution.</p></p class="citation"></blockquote><h3 id=364--3347-large-language-models-are-state-of-the-art-evaluator-for-grammatical-error-correction-masamune-kobayashi-et-al-2024>(3/64 | 3/347) Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction (Masamune Kobayashi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masamune Kobayashi, Masato Mita, Mamoru Komachi. (2024)<br><strong>Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction</strong><br><button class=copy-to-clipboard title="Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Automatic Evaluation, GPT, GPT-4, Grammatical Error Correction, Grammatical Error Correction, Neural Machine Translation, Text Summarization, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17540v1.pdf filename=2403.17540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been reported to outperform existing <b>automatic</b> <b>evaluation</b> metrics in some tasks, such as <b>text</b> <b>summarization</b> and <b>machine</b> <b>translation.</b> However, there has been a lack of research on <b>LLMs</b> as evaluators in <b>grammatical</b> <b>error</b> <b>correction</b> <b>(GEC).</b> In this study, we investigate the performance of <b>LLMs</b> in <b>GEC</b> evaluation by employing <b>prompts</b> designed to incorporate various evaluation criteria inspired by previous research. Our extensive experimental results demonstrate that <b>GPT-4</b> achieved Kendall&rsquo;s rank correlation of 0.662 with human judgments, surpassing all existing methods. Furthermore, in recent <b>GEC</b> evaluations, we have underscored the significance of the <b>LLMs</b> scale and particularly emphasized the importance of fluency among evaluation criteria.</p></p class="citation"></blockquote><h3 id=464--4347-illuminer-instruction-tuned-large-language-models-as-few-shot-intent-classifier-and-slot-filler-paramita-mirza-et-al-2024>(4/64 | 4/347) ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler (Paramita Mirza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paramita Mirza, Viju Sudhi, Soumya Ranjan Sahoo, Sinchana Ramakanth Bhat. (2024)<br><strong>ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler</strong><br><button class=copy-to-clipboard title="ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 103<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Zero-shot, GPT-3, GPT-3.5, Language Generation, Slot Filling, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17536v1.pdf filename=2403.17536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art intent classification (IC) and <b>slot</b> <b>filling</b> (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications. <b>Large</b> <b>language</b> <b>models</b> on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable <b>zero-shot</b> performance across various natural <b>language</b> <b>tasks.</b> This study evaluates Instruct-LLMs on popular <b>benchmark</b> datasets for IC and SF, emphasizing their capacity to learn from fewer examples. We introduce ILLUMINER, an approach framing IC and SF as <b>language</b> <b>generation</b> tasks for Instruct-LLMs, with a more efficient SF-prompting method compared to prior work. A comprehensive comparison with multiple baselines shows that our approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint IC+SF method and <b>in-context</b> <b>learning</b> with <b>GPT3.5</b> (175B), particularly in <b>slot</b> <b>filling</b> by 11.1&ndash;32.2 percentage points. Additionally, our in-depth ablation study demonstrates that parameter-efficient <b>fine-tuning</b> requires less than 6% of training data to yield comparable performance with traditional full-weight <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=564--5347-verbing-weirds-language-models-evaluation-of-english-zero-derivation-in-five-llms-david-r-mortensen-et-al-2024>(5/64 | 5/347) Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs (David R. Mortensen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich Schütze, Leonie Weissweiler. (2024)<br><strong>Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs</strong><br><button class=copy-to-clipboard title="Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Mistral, falcon, Natural Language Inference, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17856v1.pdf filename=2403.17856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a <b>large</b> <b>part</b> <b>of</b> the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of <b>large</b> <b>language</b> <b>models</b> with reference to conversion. We design a task for testing lexical-syntactic flexibility &ndash; the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a <b>natural</b> <b>language</b> <b>inference</b> paradigm. We test the abilities of five language models &ndash; two proprietary models <b>(GPT-3.5</b> and <b>GPT-4),</b> three open-source models <b>(Mistral</b> 7B, <b>Falcon</b> 40B, and <b>Llama</b> 2 70B). We find that <b>GPT-4</b> performs best on the task, followed by <b>GPT-3.5,</b> but that the open source language models are also able to perform it and that the 7B parameter <b>Mistral</b> displays as little difference between its baseline performance on the <b>natural</b> <b>language</b> <b>inference</b> task and the non-prototypical syntactic category task, as the massive <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=664--6347-enhancing-legal-document-retrieval-a-multi-phase-approach-with-large-language-models-hai-long-nguyen-et-al-2024>(6/64 | 6/347) Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models (Hai-Long Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hai-Long Nguyen, Duc-Minh Nguyen, Tan-Minh Nguyen, Ha-Thanh Nguyen, Thi-Hai-Yen Vuong, Ken Satoh. (2024)<br><strong>Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models</strong><br><button class=copy-to-clipboard title="Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: BERT, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18093v1.pdf filename=2403.18093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> with billions of parameters, such as <b>GPT-3.5,</b> <b>GPT-4,</b> and <b>LLaMA,</b> are increasingly prevalent. Numerous studies have explored effective <b>prompting</b> techniques to harness the power of these <b>LLMs</b> for various research problems. Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of <b>Prompting</b> techniques due to the <b>large</b> <b>number</b> <b>and</b> substantial length of legal articles. This research focuses on maximizing the potential of <b>prompting</b> by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and <b>BERT-based</b> Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating <b>prompting</b> techniques on <b>LLMs</b> into the retrieval system significantly improves retrieval accuracy. However, error analysis reveals several existing issues in the retrieval system that still need resolution.</p></p class="citation"></blockquote><h3 id=764--7347-language-models-for-text-classification-is-in-context-learning-enough-aleksandra-edwards-et-al-2024>(7/64 | 7/347) Language Models for Text Classification: Is In-Context Learning Enough? (Aleksandra Edwards et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksandra Edwards, Jose Camacho-Collados. (2024)<br><strong>Language Models for Text Classification: Is In-Context Learning Enough?</strong><br><button class=copy-to-clipboard title="Language Models for Text Classification: Is In-Context Learning Enough?" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Few-shot, Fine-tuning, Text Classification, Text Generation, In-context Learning, In-context Learning, Large Language Model, Masked Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17661v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17661v1.pdf filename=2403.17661v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and <b>few-shot</b> settings. An advantage of these models over more standard approaches based on <b>fine-tuning</b> is the ability to understand instructions written in natural language <b>(prompts),</b> which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing <b>text</b> <b>classification</b> problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how <b>text</b> <b>generation</b> models combined with <b>prompting</b> techniques compare to more established methods for <b>text</b> <b>classification</b> such as <b>fine-tuning</b> <b>masked</b> <b>language</b> <b>models.</b> In this paper, we address this research gap by performing a <b>large-scale</b> <b>evaluation</b> <b>study</b> for 16 <b>text</b> <b>classification</b> datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and <b>few-shot</b> approaches of <b>large</b> <b>language</b> <b>models</b> to <b>fine-tuning</b> smaller language models. We also analyse the results by <b>prompt,</b> classification type, domain, and number of labels. In general, the results show how <b>fine-tuning</b> smaller and more efficient language models can still outperform <b>few-shot</b> approaches of larger language models, which have room for improvement when it comes to <b>text</b> <b>classification.</b></p></p class="citation"></blockquote><h3 id=864--8347-extracting-biomedical-entities-from-noisy-audio-transcripts-nima-ebadi-et-al-2024>(8/64 | 8/347) Extracting Biomedical Entities from Noisy Audio Transcripts (Nima Ebadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nima Ebadi, Kellen Morgan, Adrian Tan, Billy Linares, Sheri Osborn, Emma Majors, Jeremy Davis, Anthony Rios. (2024)<br><strong>Extracting Biomedical Entities from Noisy Audio Transcripts</strong><br><button class=copy-to-clipboard title="Extracting Biomedical Entities from Noisy Audio Transcripts" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Zero-shot, GPT-4, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17363v1.pdf filename=2403.17363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> technology is fundamental in transcribing spoken language into text, with considerable applications in the clinical realm, including streamlining medical transcription and integrating with Electronic Health Record (EHR) systems. Nevertheless, challenges persist, especially when transcriptions contain noise, leading to significant drops in performance when Natural Language Processing (NLP) models are applied. <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER),</b> an essential clinical task, is particularly affected by such noise, often termed the <b>ASR-NLP</b> gap. Prior works have primarily studied <b>ASR&rsquo;s</b> efficiency in clean recordings, leaving a research gap concerning the performance in noisy environments. This paper introduces a novel dataset, BioASR-NER, designed to bridge the <b>ASR-NLP</b> gap in the biomedical domain, focusing on extracting adverse drug reactions and mentions of entities from the Brief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a comprehensive collection of almost 2,000 clean and noisy recordings. In addressing the noise challenge, we present an innovative transcript-cleaning method using <b>GPT4,</b> investigating both <b>zero-shot</b> and <b>few-shot</b> methodologies. Our study further delves into an error analysis, shedding light on the types of errors in transcription software, corrections by <b>GPT4,</b> and the challenges <b>GPT4</b> faces. This paper aims to foster improved understanding and potential solutions for the <b>ASR-NLP</b> gap, ultimately supporting enhanced healthcare documentation practices.</p></p class="citation"></blockquote><h3 id=964--9347-large-language-models-as-financial-data-annotators-a-study-on-effectiveness-and-efficiency-toyin-aguda-et-al-2024>(9/64 | 9/347) Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency (Toyin Aguda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toyin Aguda, Suchetha Siddagangappa, Elena Kochkina, Simerjot Kaur, Dongsheng Wang, Charese Smiley, Sameena Shah. (2024)<br><strong>Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency</strong><br><button class=copy-to-clipboard title="Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Recommendation, GPT, GPT-4, PaLM, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18152v1.pdf filename=2403.18152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them. While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains underexplored. To address this gap, we investigate the potential of <b>LLMs</b> as efficient data annotators for extracting relations in financial documents. We compare the annotations produced by three <b>LLMs</b> <b>(GPT-4,</b> <b>PaLM</b> 2, and MPT Instruct) against expert annotators and crowdworkers. We demonstrate that the current state-of-the-art <b>LLMs</b> can be sufficient alternatives to non-expert crowdworkers. We analyze models using various <b>prompts</b> and parameter settings and find that customizing the <b>prompts</b> for each relation group by providing specific examples belonging to those groups is paramount. Furthermore, we introduce a reliability index <b>(LLM-RelIndex)</b> used to identify outputs that may require expert attention. Finally, we perform an extensive time, cost and error analysis and provide <b>recommendations</b> for the collection and usage of automated annotations in domain-specific settings.</p></p class="citation"></blockquote><h3 id=1064--10347-naive-bayes-based-context-extension-for-large-language-models-jianlin-su-et-al-2024>(10/64 | 10/347) Naive Bayes-based Context Extension for Large Language Models (Jianlin Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianlin Su, Murtadha Ahmed, Wenbo, Luo Ao, Mingren Zhu, Yunfeng Liu. (2024)<br><strong>Naive Bayes-based Context Extension for Large Language Models</strong><br><button class=copy-to-clipboard title="Naive Bayes-based Context Extension for Large Language Models" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Transformer, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17552v1.pdf filename=2403.17552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown promising <b>in-context</b> <b>learning</b> abilities. However, conventional <b>In-Context</b> <b>Learning</b> <b>(ICL)</b> approaches are often impeded by length limitations of <b>transformer</b> architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing <b>LLMs</b> to perform <b>ICL</b> with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require <b>fine-tuning</b> or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target <b>LLM&rsquo;s</b> maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes&rsquo; theorem to generate the test task. Our experimental results demonstrate that NBCE substantially enhances performance, particularly as the number of demonstration examples increases, consistently outperforming alternative methods. The NBCE code will be made publicly accessible. The code NBCE is available at: <a href=https://github.com/amurtadha/NBCE-master>https://github.com/amurtadha/NBCE-master</a></p></p class="citation"></blockquote><h3 id=1164--11347-pctoolkit-a-unified-plug-and-play-prompt-compression-toolkit-of-large-language-models-jinyi-li-et-al-2024>(11/64 | 11/347) PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models (Jinyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyi Li, Yihuai Lan, Lei Wang, Hao Wang. (2024)<br><strong>PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models</strong><br><button class=copy-to-clipboard title="PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, Question Answering, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17411v1.pdf filename=2403.17411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> compression is an innovative method for efficiently condensing input <b>prompts</b> while preserving essential information. To facilitate quick-start services, user-friendly interfaces, and compatibility with common datasets and metrics, we present the <b>Prompt</b> Compression Toolkit (PCToolkit). This toolkit is a unified plug-and-play solution for compressing <b>prompts</b> in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> featuring cutting-edge <b>prompt</b> compressors, diverse datasets, and metrics for comprehensive performance evaluation. PCToolkit boasts a modular design, allowing for easy integration of new datasets and metrics through portable and user-friendly interfaces. In this paper, we outline the key components and functionalities of PCToolkit. We conducted evaluations of the compressors within PCToolkit across various natural language tasks, including reconstruction, <b>summarization,</b> mathematical problem-solving, <b>question</b> <b>answering,</b> <b>few-shot</b> <b>learning,</b> synthetic tasks, code completion, boolean expressions, multiple choice <b>questions,</b> <b>and</b> lies recognition.</p></p class="citation"></blockquote><h3 id=1264--12347-supervisory-prompt-training-jean-ghislain-billa-et-al-2024>(12/64 | 12/347) Supervisory Prompt Training (Jean Ghislain Billa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean Ghislain Billa, Min Oh, Liang Du. (2024)<br><strong>Supervisory Prompt Training</strong><br><button class=copy-to-clipboard title="Supervisory Prompt Training" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18051v1.pdf filename=2403.18051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> relies heavily on the quality of <b>prompts,</b> which are often manually engineered and task-specific, making them costly and non-scalable. We propose a novel approach, Supervisory <b>Prompt</b> Training (SPT). SPT automates the generation of highly effective <b>prompts</b> using a dual <b>LLM</b> system. In this system, one <b>LLM,</b> the generator, performs a task while the other, the corrector, provides feedback and generates improved <b>prompts.</b> In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their <b>prompts</b> over time. We also introduce the concept of \textit{impact scores} to measure the sentence-level effectiveness of the <b>prompts.</b> Our method was tested on four <b>benchmarks,</b> testing the level of hallucinations in <b>LLMs.</b> Notably, we were able to increase the accuracy of <b>GPT-4</b> on GSM8K from 65.8% to 94.1% (28.3% increase). SPT advances <b>LLMs</b> by refining <b>prompts</b> to enhance performance and reduce hallucinations, offering an efficient and scalable alternative to traditional model <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=1364--13347-large-language-models-produce-responses-perceived-to-be-empathic-yoon-kyung-lee-et-al-2024>(13/64 | 13/347) Large Language Models Produce Responses Perceived to be Empathic (Yoon Kyung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong. (2024)<br><strong>Large Language Models Produce Responses Perceived to be Empathic</strong><br><button class=copy-to-clipboard title="Large Language Models Produce Responses Perceived to be Empathic" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT-4, GPT-4 turbo, Mistral, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18148v1.pdf filename=2403.18148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy. Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations. Across two studies (N=192, 202), we showed human raters a variety of responses written by several models <b>(GPT4</b> <b>Turbo,</b> Llama2, and <b>Mistral),</b> and had people rate these responses on how empathic they seemed to be. We found that <b>LLM-generated</b> responses were consistently rated as more empathic than human-written responses. Linguistic analyses also show that these models write in distinct, predictable ``styles", in terms of their use of punctuation, emojis, and certain words. These results highlight the potential of using <b>LLMs</b> to enhance human peer support in contexts where empathy is important.</p></p class="citation"></blockquote><h3 id=1464--14347-constructions-are-so-difficult-that-even-large-language-models-get-them-right-for-the-wrong-reasons-shijia-zhou-et-al-2024>(14/64 | 14/347) Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons (Shijia Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijia Zhou, Leonie Weissweiler, Taiqi He, Hinrich Schütze, David R. Mortensen, Lori Levin. (2024)<br><strong>Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons</strong><br><button class=copy-to-clipboard title="Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, LLaMA, Natural Language Inference, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17760v1.pdf filename=2403.17760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for <b>NLI</b> with <b>large</b> <b>lexical</b> <b>overlap,</b> which minimises the possibility of models discerning entailment solely based on token distinctions, and show that <b>GPT-4</b> and <b>Llama</b> 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for <b>LLM&rsquo;s</b> understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don&rsquo;t adequately represent their meaning or capture the lexical properties of phrasal heads.</p></p class="citation"></blockquote><h3 id=1564--15347-dancer-entity-description-augmented-named-entity-corrector-for-automatic-speech-recognition-yi-cheng-wang-et-al-2024>(15/64 | 15/347) DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition (Yi-Cheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-Cheng Wang, Hsin-Wei Wang, Bi-Cheng Yan, Chi-Han Lin, Berlin Chen. (2024)<br><strong>DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition</strong><br><button class=copy-to-clipboard title="DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Dense Retrieval, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Masked Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17645v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17645v2.pdf filename=2403.17645v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-end <b>automatic</b> <b>speech</b> <b>recognition</b> (E2E <b>ASR)</b> systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for <b>ASR</b> have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on <b>ASR</b> transcription. To this end, an efficient entity description augmented <b>masked</b> <b>language</b> <b>model</b> (EDA-MLM) comprised of a <b>dense</b> <b>retrieval</b> model is introduced, enabling <b>MLM</b> to adapt swiftly to domain-specific entities for the NEC task. A series of experiments conducted on the AISHELL-1 and Homophone datasets confirm the effectiveness of our modeling approach. DANCER outperforms a strong baseline, the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate (CER) reduction of about 7% relatively on AISHELL-1 for named entities. More notably, when tested on Homophone that contain named entities of high phonetic confusion, DANCER offers a more pronounced CER reduction of 46% relatively over PED-NEC for named entities.</p></p class="citation"></blockquote><h3 id=1664--16347-kdmcse-knowledge-distillation-multimodal-sentence-embeddings-with-adaptive-angular-margin-contrastive-learning-cong-duy-nguyen-et-al-2024>(16/64 | 16/347) KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning (Cong-Duy Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cong-Duy Nguyen, Thong Nguyen, Xiaobao Wu, Anh Tuan Luu. (2024)<br><strong>KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning</strong><br><button class=copy-to-clipboard title="KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Contrastive Learning, Knowledge Distillation, Knowledge Distillation, Multi-modal, Multi-modal, Supervised Learning, Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17486v1.pdf filename=2403.17486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous work on <b>multimodal</b> <b>sentence</b> <b>embedding</b> has proposed <b>multimodal</b> <b>contrastive</b> <b>learning</b> and achieved promising results. However, by taking the rest of the batch as negative samples without reviewing when forming <b>contrastive</b> <b>pairs,</b> those studies encountered many suspicious and noisy negative examples, significantly affecting the methods&rsquo; overall performance. In this work, we propose KDMCSE <b>(Knowledge</b> <b>Distillation</b> <b>Multimodal</b> <b>contrastive</b> <b>learning</b> of <b>Sentence</b> <b>Embeddings),</b> a novel approach that enhances the discrimination and generalizability of <b>multimodal</b> representation and inherits the <b>knowledge</b> <b>from</b> the teacher model to learn the difference between positive and negative instances and via that, can detect noisy and wrong negative samples effectively before they are calculated in the <b>contrastive</b> <b>objective.</b> Furthermore, to overcome the limitation of modeling the variation within negative pairs, we introduce a new <b>contrastive</b> <b>objective,</b> AdapACSE (Adaptive Angular Margin <b>Supervised</b> <b>Contrastive</b> <b>Learning</b> for <b>Multimodal</b> <b>sentence</b> <b>embeddings),</b> that enhances the discriminative representation by strengthening the margin within the angular space while capturing varying semantics within the negative. Experimental results on widely used Semantic Textual Similarity (STS) <b>benchmarks</b> demonstrate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=1764--17347-chain-of-action-faithful-and-multimodal-question-answering-through-large-language-models-zhenyu-pan-et-al-2024>(17/64 | 17/347) Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models (Zhenyu Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Pan, Haozheng Luo, Manling Li, Han Liu. (2024)<br><strong>Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models</strong><br><button class=copy-to-clipboard title="Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Question Answering, Question Answering, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17359v1.pdf filename=2403.17359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a Chain-of-Action (CoA) framework for <b>multimodal</b> and retrieval-augmented <b>Question-Answering</b> <b>(QA).</b> Compared to the literature, CoA overcomes two major challenges of current <b>QA</b> applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak <b>reasoning</b> performance over compositional information. Our key contribution is a novel <b>reasoning-retrieval</b> mechanism that decomposes a complex <b>question</b> <b>into</b> a <b>reasoning</b> chain via systematic <b>prompting</b> and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play&rsquo; actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public <b>benchmarks</b> and a Web3 case study to demonstrate the capability of CoA over other methods.</p></p class="citation"></blockquote><h3 id=1864--18347-gpts-and-language-barrier-a-cross-lingual-legal-qa-examination-ha-thanh-nguyen-et-al-2024>(18/64 | 18/347) GPTs and Language Barrier: A Cross-Lingual Legal QA Examination (Ha-Thanh Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ha-Thanh Nguyen, Hiroaki Yamada, Ken Satoh. (2024)<br><strong>GPTs and Language Barrier: A Cross-Lingual Legal QA Examination</strong><br><button class=copy-to-clipboard title="GPTs and Language Barrier: A Cross-Lingual Legal QA Examination" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Benchmarking, Benchmarking, GPT, Transformer, Question Answering, Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18098v1.pdf filename=2403.18098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the application of Generative Pre-trained <b>Transformers</b> <b>(GPTs)</b> in cross-lingual legal <b>Question-Answering</b> <b>(QA)</b> systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By <b>benchmarking</b> four different combinations of English and Japanese <b>prompts</b> and data, we provide valuable insights into <b>GPTs&rsquo;</b> performance in multilingual legal <b>QA</b> scenarios, contributing to the development of more efficient and accurate cross-lingual <b>QA</b> solutions in the legal domain.</p></p class="citation"></blockquote><h3 id=1964--19347-m3p-towards-multimodal-multilingual-translation-with-multimodal-prompt-jian-yang-et-al-2024>(19/64 | 19/347) m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt (Jian Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Yang, Hongcheng Guo, Yuwei Yin, Jiaqi Bai, Bing Wang, Jiaheng Liu, Xinnian Liang, Linzheng Cahi, Liqun Yang, Zhoujun Li. (2024)<br><strong>m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt</strong><br><button class=copy-to-clipboard title="m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Low-Resource, Multi-modal, Multi-modal, Neural Machine Translation, Neural Machine Translation, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17556v1.pdf filename=2403.17556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large. To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation. In this paper, we propose a framework to leverage the <b>multimodal</b> <b>prompt</b> to guide the <b>Multimodal</b> Multilingual <b>neural</b> <b>Machine</b> <b>Translation</b> (m3P), which aligns the representations of different languages with the same meaning and generates the conditional <b>vision-language</b> memory for translation. We construct a multilingual <b>multimodal</b> instruction dataset (InstrMulti102) to support 102 languages. Our method aims to minimize the representation distance of different languages by regarding the image as a central language. Experimental results show that m3P outperforms previous text-only baselines and multilingual <b>multimodal</b> methods by a large margin. Furthermore, the probing experiments validate the effectiveness of our method in enhancing translation under the <b>low-resource</b> and massively multilingual scenario.</p></p class="citation"></blockquote><h3 id=2064--20347-the-unreasonable-ineffectiveness-of-the-deeper-layers-andrey-gromov-et-al-2024>(20/64 | 20/347) The Unreasonable Ineffectiveness of the Deeper Layers (Andrey Gromov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts. (2024)<br><strong>The Unreasonable Ineffectiveness of the Deeper Layers</strong><br><button class=copy-to-clipboard title="The Unreasonable Ineffectiveness of the Deeper Layers" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, stat-ML<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Pruning, Quantization, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17887v1.pdf filename=2403.17887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained <b>LLMs,</b> finding minimal degradation of performance on different <b>question-answering</b> <b>benchmarks</b> until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to &ldquo;heal&rdquo; the damage, we perform a small amount of <b>finetuning.</b> In particular, we use parameter-efficient <b>finetuning</b> (PEFT) methods, specifically <b>quantization</b> and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer <b>pruning</b> methods can complement other PEFT strategies to further reduce computational resources of <b>finetuning</b> on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these <b>LLMs</b> to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.</p></p class="citation"></blockquote><h3 id=2164--21347-decoding-probing-revealing-internal-linguistic-structures-in-neural-language-models-using-minimal-pairs-linyang-he-et-al-2024>(21/64 | 21/347) Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models using Minimal Pairs (Linyang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linyang He, Peili Chen, Ercong Nie, Yuanning Li, Jonathan R. Brennan. (2024)<br><strong>Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models using Minimal Pairs</strong><br><button class=copy-to-clipboard title="Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models using Minimal Pairs" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, q-bio-NC<br>Keyword Score: 53<br>Keywords: Benchmarking, Self-supervised Learning, GPT, GPT-2, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17299v1.pdf filename=2403.17299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by cognitive neuroscience studies, we introduce a novel <code>decoding probing' method that uses minimal pairs &lt;b>benchmark&lt;/b> (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the </code>brain&rsquo; and its representations as `neural activations&rsquo;, we decode grammaticality labels of minimal pairs from the intermediate layers&rsquo; representations. This approach reveals: 1) <b>Self-supervised</b> language models capture abstract linguistic structures in intermediate layers that GloVe and <b>RNN</b> language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of <b>GPT-2</b> and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For <b>Transformer-based</b> models, both embeddings and attentions capture grammatical features but show distinct patterns. Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions.</p></p class="citation"></blockquote><h3 id=2264--22347-denoising-table-text-retrieval-for-open-domain-question-answering-deokhyung-kang-et-al-2024>(22/64 | 22/347) Denoising Table-Text Retrieval for Open-Domain Question Answering (Deokhyung Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deokhyung Kang, Baikjin Jung, Yunsu Kim, Gary Geunbae Lee. (2024)<br><strong>Denoising Table-Text Retrieval for Open-Domain Question Answering</strong><br><button class=copy-to-clipboard title="Denoising Table-Text Retrieval for Open-Domain Question Answering" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Open-Domain Question Answering, Question Answering, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17611v1.pdf filename=2403.17611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In table-text <b>open-domain</b> <b>question</b> <b>answering,</b> a retriever system retrieves relevant evidence from tables and text to answer <b>questions.</b> <b>Previous</b> studies in table-text <b>open-domain</b> <b>question</b> <b>answering</b> have two common challenges: firstly, their retrievers can be affected by false-positive labels in training datasets; secondly, they may struggle to provide appropriate evidence for <b>questions</b> <b>that</b> require <b>reasoning</b> across the table. To address these issues, we propose Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a denoised training dataset with fewer false positive labels by discarding instances with lower <b>question-relevance</b> <b>scores</b> measured through a false positive detection model. Subsequently, we integrate table-level ranking information into the retriever to assist in finding evidence for <b>questions</b> <b>that</b> demand <b>reasoning</b> across the table. To encode this ranking information, we <b>fine-tune</b> a rank-aware column encoder to identify minimum and maximum values within a column. Experimental results demonstrate that DoTTeR significantly outperforms strong baselines on both retrieval recall and downstream <b>QA</b> tasks. Our code is available at <a href=https://github.com/deokhk/DoTTeR>https://github.com/deokhk/DoTTeR</a>.</p></p class="citation"></blockquote><h3 id=2364--23347-chatgpt-rates-natural-language-explanation-quality-like-humans-but-on-which-scales-fan-huang-et-al-2024>(23/64 | 23/347) ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales? (Fan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Huang, Haewoon Kwak, Kunwoo Park, Jisun An. (2024)<br><strong>ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?</strong><br><button class=copy-to-clipboard title="ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: ChatGPT, Natural Language Explanation, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17368v1.pdf filename=2403.17368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As AI becomes more integral in our lives, the need for transparency and responsibility grows. While <b>natural</b> <b>language</b> <b>explanations</b> (NLEs) are vital for clarifying the <b>reasoning</b> behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings. This study explores the alignment between <b>ChatGPT</b> and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale). We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement. We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations. Our results show that <b>ChatGPT</b> aligns better with humans in more coarse-grained scales. Also, paired comparisons and dynamic <b>prompting</b> (i.e., providing semantically similar examples in the <b>prompt)</b> improve the alignment. This research advances our understanding of <b>large</b> <b>language</b> <b>models&rsquo;</b> capabilities to assess the text explanation quality in different configurations for responsible AI development.</p></p class="citation"></blockquote><h3 id=2464--24347-arabicaqa-a-comprehensive-dataset-for-arabic-question-answering-abdelrahman-abdallah-et-al-2024>(24/64 | 24/347) ArabicaQA: A Comprehensive Dataset for Arabic Question Answering (Abdelrahman Abdallah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt. (2024)<br><strong>ArabicaQA: A Comprehensive Dataset for Arabic Question Answering</strong><br><button class=copy-to-clipboard title="ArabicaQA: A Comprehensive Dataset for Arabic Question Answering" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 46<br>Keywords: Benchmarking, Benchmarking, Open-Domain Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17848v1.pdf filename=2403.17848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the significant gap in Arabic natural language processing (NLP) resources by introducing ArabicaQA, the first <b>large-scale</b> <b>dataset</b> <b>for</b> machine reading comprehension and <b>open-domain</b> <b>question</b> <b>answering</b> in Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701 unanswerable <b>questions</b> <b>created</b> by crowdworkers to look similar to answerable ones, along with additional labels of <b>open-domain</b> <b>questions</b> <b>marks</b> a crucial advancement in Arabic NLP resources. We also present AraDPR, the first dense passage retrieval model trained on the Arabic Wikipedia corpus, specifically designed to tackle the unique challenges of Arabic text retrieval. Furthermore, our study includes extensive <b>benchmarking</b> of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for Arabic <b>question</b> <b>answering,</b> critically evaluating their performance in the Arabic language context. In conclusion, ArabicaQA, AraDPR, and the <b>benchmarking</b> of <b>LLMs</b> in Arabic <b>question</b> <b>answering</b> offer significant advancements in the field of Arabic NLP. The dataset and code are publicly accessible for further research <a href=https://github.com/DataScienceUIBK/ArabicaQA>https://github.com/DataScienceUIBK/ArabicaQA</a>.</p></p class="citation"></blockquote><h3 id=2564--25347-hill-hierarchy-aware-information-lossless-contrastive-learning-for-hierarchical-text-classification-he-zhu-et-al-2024>(25/64 | 25/347) HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification (He Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He Zhu, Junran Wu, Ruomei Liu, Yue Hou, Ze Yuan, Shangzhe Li, Yicheng Pan, Ke Xu. (2024)<br><strong>HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification</strong><br><button class=copy-to-clipboard title="HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IT, cs.CL, math-IT<br>Keyword Score: 45<br>Keywords: Contrastive Learning, Representation Learning, Self-supervised Learning, Text Classification, Document Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17307v1.pdf filename=2403.17307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>self-supervised</b> methods in natural language processing (NLP), especially hierarchical <b>text</b> <b>classification</b> (HTC), mainly focus on <b>self-supervised</b> <b>contrastive</b> <b>learning,</b> extremely relying on human-designed augmentation rules to generate <b>contrastive</b> <b>samples,</b> which can potentially corrupt or distort the original information. In this paper, we tend to investigate the feasibility of a <b>contrastive</b> <b>learning</b> scheme in which the semantic and syntactic information inherent in the input sample is adequately reserved in the <b>contrastive</b> <b>samples</b> and fused during the learning process. Specifically, we propose an information lossless <b>contrastive</b> <b>learning</b> strategy for HTC, namely \textbf{H}ierarchy-aware \textbf{I}nformation \textbf{L}ossless <b>contrastive</b> <b>\textbf{L}earning</b> (HILL), which consists of a <b>text</b> <b>encoder</b> representing the input <b>document,</b> <b>and</b> a structure encoder directly generating the positive sample. The structure encoder takes the <b>document</b> <b>embedding</b> as input, extracts the essential syntactic information inherent in the label hierarchy with the principle of structural entropy minimization, and injects the syntactic information into the <b>text</b> <b>representation</b> <b>via</b> hierarchical <b>representation</b> <b>learning.</b> Experiments on three common datasets are conducted to verify the superiority of HILL.</p></p class="citation"></blockquote><h3 id=2664--26347-coig-cqia-quality-is-all-you-need-for-chinese-instruction-fine-tuning-yuelin-bai-et-al-2024>(26/64 | 26/347) COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning (Yuelin Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, Ruibin Yuan, Haihong Wu, Hongquan Lin, Wenhao Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Min Yang, Shiwen Ni, Ge Zhang. (2024)<br><strong>COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning</strong><br><button class=copy-to-clipboard title="COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18058v1.pdf filename=2403.18058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there have been significant advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> particularly focused on the English language. These advancements have enabled these <b>LLMs</b> to understand and execute complex <b>instructions</b> <b>with</b> unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese <b>instruction</b> <b>tuning.</b> The unique linguistic features and cultural depth of the Chinese language pose challenges for <b>instruction</b> <b>tuning</b> tasks. Existing datasets are either derived from English-centric <b>LLMs</b> or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese <b>instruction</b> <b>tuning</b> dataset. Our aim is to build a diverse, wide-ranging <b>instruction-tuning</b> <b>dataset</b> to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various sources on the Chinese Internet, including Q&amp;A communities, Wikis, examinations, and existing NLP datasets. This corpus was rigorously filtered and carefully processed to form the COIG-CQIA dataset. Furthermore, we train models of various scales on different subsets of CQIA, following in-depth evaluation and analyses. The findings from our experiments offer valuable insights for selecting and developing Chinese <b>instruction-tuning</b> <b>datasets.</b> We also find that models trained on CQIA-Subset achieve competitive results in human assessment as well as knowledge and security <b>benchmarks.</b> Data are available at <a href=https://huggingface.co/datasets/m-a-p/COIG-CQIA>https://huggingface.co/datasets/m-a-p/COIG-CQIA</a></p></p class="citation"></blockquote><h3 id=2764--27347-chroniclingamericaqa-a-large-scale-question-answering-dataset-based-on-historical-american-newspaper-pages-bhawna-piryani-et-al-2024>(27/64 | 27/347) ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages (Bhawna Piryani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhawna Piryani, Jamshid Mozafari, Adam Jatowt. (2024)<br><strong>ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages</strong><br><button class=copy-to-clipboard title="ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Optical Character Recognition, Benchmarking, Question Answering, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17859v1.pdf filename=2403.17859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Question</b> <b>answering</b> <b>(QA)</b> and Machine Reading Comprehension (MRC) tasks have significantly advanced in recent years due to the rapid development of deep learning techniques and, more recently, <b>large</b> <b>language</b> <b>models.</b> At the same time, many <b>benchmark</b> datasets have become available for <b>QA</b> and MRC tasks. However, most existing <b>large-scale</b> <b>benchmark</b> <b>datasets</b> have been created predominantly using synchronous document collections like Wikipedia or the Web. Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train <b>large</b> <b>language</b> <b>models.</b> To further contribute to advancing <b>QA</b> and MRC tasks and to overcome the limitation of previous datasets, we introduce ChroniclingAmericaQA, a <b>large-scale</b> <b>dataset</b> <b>with</b> 485K <b>question-answer</b> <b>pairs</b> created based on the historical newspaper collection Chronicling America. Our dataset is constructed from a subset of the Chronicling America newspaper collection spanning 120 years. One of the significant challenges for utilizing digitized historical newspaper collections is the low quality of <b>OCR</b> text. Therefore, to enable realistic testing of <b>QA</b> models, our dataset can be used in three different ways: answering <b>questions</b> <b>from</b> raw and noisy content, answering <b>questions</b> <b>from</b> cleaner, corrected version of the content, as well as answering <b>questions</b> <b>from</b> scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA spans the longest time period among available <b>QA</b> datasets make it quite a unique and useful resource.</p></p class="citation"></blockquote><h3 id=2864--28347-can-multiple-choice-questions-really-be-useful-in-detecting-the-abilities-of-llms-wangyue-li-et-al-2024>(28/64 | 28/347) Can multiple-choice questions really be useful in detecting the abilities of LLMs? (Wangyue Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu, Wei Deng, Noa Garcia. (2024)<br><strong>Can multiple-choice questions really be useful in detecting the abilities of LLMs?</strong><br><button class=copy-to-clipboard title="Can multiple-choice questions really be useful in detecting the abilities of LLMs?" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17752v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17752v2.pdf filename=2403.17752v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiple-choice <b>questions</b> <b>(MCQs)</b> are widely used in the evaluation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure <b>LLM&rsquo;s</b> capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ&rsquo;s efficacy, which we undertake in this paper by evaluating nine <b>LLMs</b> on four <b>question-answering</b> <b>(QA)</b> datasets in two languages: Chinese and English. We identify a significant issue: <b>LLMs</b> exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation <b>questions</b> <b>(LFGQs)</b> by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical <b>questions.</b> <b>Additionally,</b> we propose two methods to quantify the consistency and confidence of <b>LLMs&rsquo;</b> output, which can be generalized to other <b>QA</b> evaluation <b>benchmarks.</b> Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms of expected calibration error. Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space. Our code and models can be accessed at <a href=https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs>https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs</a>.</p></p class="citation"></blockquote><h3 id=2964--29347-intrinsic-subgraph-generation-for-interpretable-graph-based-visual-question-answering-pascal-tilli-et-al-2024>(29/64 | 29/347) Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering (Pascal Tilli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pascal Tilli, Ngoc Thang Vu. (2024)<br><strong>Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering</strong><br><button class=copy-to-clipboard title="Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Question Answering, Visual Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17647v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17647v2.pdf filename=2403.17647v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The large success of deep learning based methods in <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> has concurrently increased the demand for explainable methods. Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model. In this work, we introduce an interpretable approach for <b>graph-based</b> <b>VQA</b> <b>and</b> demonstrate competitive performance on the GQA dataset. This approach bridges the gap between interpretability and performance. Our model is designed to intrinsically produce a subgraph during the <b>question-answering</b> <b>process</b> as its explanation, providing insight into the decision making. To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for <b>graph</b> <b>neural</b> <b>networks,</b> and perform a human evaluation. Moreover, we present quantitative metrics that correlate with the evaluations of human assessors, acting as automatic metrics for the generated explanatory subgraphs. Our implementation is available at <a href=https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA>https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA</a>.</p></p class="citation"></blockquote><h3 id=3064--30347-dgot-dynamic-graph-of-thoughts-for-scientific-abstract-generation-xinyu-ning-et-al-2024>(30/64 | 30/347) DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation (Xinyu Ning et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Ning, Yutong Zhao, Yitong Liu, Hongwen Yang. (2024)<br><strong>DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation</strong><br><button class=copy-to-clipboard title="DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Graph, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17491v1.pdf filename=2403.17491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The method of training language models based on domain datasets has obtained significant achievements in the task of generating scientific paper abstracts. However, such models face problems of generalization and expensive training costs. The use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to solve the task of generating paper abstracts saves the cost of model training. However, due to the hallucination problem of <b>LLM,</b> it is often necessary to improve the reliability of the results through multi-round query <b>prompt</b> approach such as <b>Graph</b> of Thoughts (GoT), which also brings additional <b>reasoning</b> costs. In this paper, we propose a Dynamic <b>Graph</b> of Thought (DGoT). It not only inherits the advantages of the existing GoT <b>prompt</b> approach, but also dynamically adjust the <b>graph</b> structure according to data characteristics while reducing model <b>reasoning</b> cost. Experimental results show that our method&rsquo;s cost-effectiveness in abstract generation tasks is only 43.7% to 56.4% of other multi-round query <b>prompt</b> approaches. Our code is available at <a href=https://github.com/JayceNing/DGoT>https://github.com/JayceNing/DGoT</a>.</p></p class="citation"></blockquote><h3 id=3164--31347-jmultiwoz-a-large-scale-japanese-multi-domain-task-oriented-dialogue-dataset-atsumoto-ohashi-et-al-2024>(31/64 | 31/347) JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset (Atsumoto Ohashi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atsumoto Ohashi, Ryu Hirai, Shinya Iizuka, Ryuichiro Higashinaka. (2024)<br><strong>JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset</strong><br><button class=copy-to-clipboard title="JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Dialogue State Tracking, Dialogue System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17319v1.pdf filename=2403.17319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Dialogue</b> <b>datasets</b> <b>are</b> crucial for deep learning-based task-oriented <b>dialogue</b> <b>system</b> <b>research.</b> While numerous English language multi-domain task-oriented <b>dialogue</b> <b>datasets</b> <b>have</b> been developed and contributed to significant advancements in task-oriented <b>dialogue</b> <b>systems,</b> <b>such</b> a dataset does not exist in Japanese, and research in this area is limited compared to that in English. In this study, towards the advancement of research and development of task-oriented <b>dialogue</b> <b>systems</b> <b>in</b> Japanese, we constructed JMultiWOZ, the first Japanese language <b>large-scale</b> <b>multi-domain</b> <b>task-oriented</b> <b>dialogue</b> <b>dataset.</b> <b>Using</b> JMultiWOZ, we evaluated the <b>dialogue</b> <b>state</b> <b>tracking</b> and response generation capabilities of the state-of-the-art methods on the existing major English <b>benchmark</b> dataset MultiWOZ2.2 and the latest <b>large</b> <b>language</b> <b>model</b> <b>(LLM)-based</b> methods. Our evaluation results demonstrated that JMultiWOZ provides a <b>benchmark</b> that is on par with MultiWOZ2.2. In addition, through evaluation experiments of interactive <b>dialogues</b> <b>with</b> <b>the</b> models and human participants, we identified limitations in the task completion capabilities of <b>LLMs</b> in Japanese.</p></p class="citation"></blockquote><h3 id=3264--32347-improving-pre-trained-language-model-sensitivity-via-mask-specific-losses-a-case-study-on-biomedical-ner-micheal-abaho-et-al-2024>(32/64 | 32/347) Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER (Micheal Abaho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Micheal Abaho, Danushka Bollegala, Gary Leeming, Dan Joyce, Iain E Buchan. (2024)<br><strong>Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER</strong><br><button class=copy-to-clipboard title="Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Named Entity Recognition, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18025v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18025v2.pdf filename=2403.18025v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting language models (LMs) to novel domains is often achieved through <b>fine-tuning</b> a <b>pre-trained</b> <b>LM</b> <b>(PLM)</b> on domain-specific data. <b>Fine-tuning</b> introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. <b>Fine-tuning</b> can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive <b>fine-tuning,</b> we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during <b>fine-tuning.</b> MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately predicting DS-terms compared to generic words. Results of our analysis show that MSLM improves LMs sensitivity and detection of DS-terms. We empirically show that an optimal masking rate not only depends on the LM, but also on the dataset and the length of sequences. Our proposed masking strategy outperforms advanced masking strategies such as span- and PMI-based masking.</p></p class="citation"></blockquote><h3 id=3364--33347-dore-a-dataset-for-portuguese-definition-generation-anna-beatriz-dimas-furtado-et-al-2024>(33/64 | 33/347) DORE: A Dataset For Portuguese Definition Generation (Anna Beatriz Dimas Furtado et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Beatriz Dimas Furtado, Tharindu Ranasinghe, Frédéric Blain, Ruslan Mitkov. (2024)<br><strong>DORE: A Dataset For Portuguese Definition Generation</strong><br><button class=copy-to-clipboard title="DORE: A Dataset For Portuguese Definition Generation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: High-Resource, Supervised Learning, Language Generation, Natural Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18018v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18018v2.pdf filename=2403.18018v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Definition modelling (DM) is the task of automatically generating a dictionary definition for a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a <b>supervised</b> <b>natural</b> <b>language</b> <b>generation</b> problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other <b>high-resource</b> <b>languages.</b> <b>While</b> Portuguese is considered a mid/high-resource <b>language</b> <b>in</b> most <b>natural</b> <b>language</b> <b>processing</b> tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings of this paper will facilitate research and study of Portuguese in wider contexts.</p></p class="citation"></blockquote><h3 id=3464--34347-exploring-llms-as-a-source-of-targeted-synthetic-textual-data-to-minimize-high-confidence-misclassifications-philip-lippmann-et-al-2024>(34/64 | 34/347) Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications (Philip Lippmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Lippmann, Matthijs Spaan, Jie Yang. (2024)<br><strong>Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications</strong><br><button class=copy-to-clipboard title="Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Data Augmentation, Out-of-distribution, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17860v1.pdf filename=2403.17860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and <b>out-of-distribution</b> <b>data.</b> <b>Existing</b> work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for <b>data</b> <b>augmentation</b> as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic <b>data</b> <b>generated</b> by <b>LLMs</b> with that of human <b>data</b> <b>obtained</b> via the same procedure. For mitigation, humans or <b>LLMs</b> provide natural language characterizations of high confidence misclassifications to generate synthetic <b>data,</b> <b>which</b> are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and <b>LLMs</b> surpasses an order of magnitude, as <b>LLMs</b> attain human-like performance while being more scalable.</p></p class="citation"></blockquote><h3 id=3564--35347-using-domain-knowledge-to-guide-dialog-structure-induction-via-neural-probabilistic-soft-logic-connor-pryor-et-al-2024>(35/64 | 35/347) Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic (Connor Pryor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor. (2024)<br><strong>Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic</strong><br><button class=copy-to-clipboard title="Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Few-shot, Few-shot Learning, Out-of-domain, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17853v1.pdf filename=2403.17853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, <b>few-shot</b> <b>learning,</b> and <b>out-of-domain</b> generalization performance. Over three dialog structure induction datasets and across <b>unsupervised</b> and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.</p></p class="citation"></blockquote><h3 id=3664--36347-enhanced-short-text-modeling-leveraging-large-language-models-for-topic-refinement-shuyu-chang-et-al-2024>(36/64 | 36/347) Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement (Shuyu Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuyu Chang, Rui Wang, Peng Ren, Haiping Huang. (2024)<br><strong>Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement</strong><br><button class=copy-to-clipboard title="Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Topic Model, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17706v1.pdf filename=2403.17706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crafting effective <b>topic</b> <b>models</b> for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics. Traditional <b>topic</b> <b>models,</b> however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data. In our study, we harness the advanced capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to introduce a novel approach termed <b>&ldquo;Topic</b> <b>Refinement&rdquo;.</b> This approach does not directly involve itself in the initial modeling of <b>topics</b> <b>but</b> focuses on improving <b>topics</b> <b>after</b> they have been mined. By employing <b>prompt</b> engineering, we direct <b>LLMs</b> to eliminate off-topic words within a given <b>topic,</b> <b>ensuring</b> that only contextually relevant words are preserved or substituted with ones that fit better semantically. This method emulates human-like scrutiny and improvement of <b>topics,</b> <b>thereby</b> elevating the semantic quality of the <b>topics</b> <b>generated</b> by various models. Our comprehensive evaluation across three unique datasets has shown that our <b>topic</b> <b>refinement</b> approach significantly enhances the semantic coherence of topics.</p></p class="citation"></blockquote><h3 id=3764--37347-multilingual-sentence-t5-scalable-sentence-encoders-for-multilingual-applications-chihiro-yano-et-al-2024>(37/64 | 37/347) Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications (Chihiro Yano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chihiro Yano, Akihiko Fukuchi, Shoko Fukasawa, Hideyuki Tachibana, Yotaro Watanabe. (2024)<br><strong>Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications</strong><br><button class=copy-to-clipboard title="Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: T5, Natural Language Inference, Natural Language Inference, Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17528v1.pdf filename=2403.17528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior work on multilingual <b>sentence</b> <b>embedding</b> has demonstrated that the efficient use of <b>natural</b> <b>language</b> <b>inference</b> <b>(NLI)</b> data to build high-performance models can outperform conventional methods. However, the potential benefits from the recent ``exponential&rsquo;&rsquo; growth of language models with billions of parameters have not yet been fully explored. In this paper, we introduce Multilingual <b>Sentence</b> <b>T5</b> (m-ST5), as a larger model of <b>NLI-based</b> multilingual <b>sentence</b> <b>embedding,</b> by extending <b>Sentence</b> <b>T5,</b> an existing monolingual model. By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model&rsquo;s size to 5.7 billion parameters. We conducted experiments to evaluate the performance of <b>sentence</b> <b>embedding</b> and verified that the method outperforms the <b>NLI-based</b> prior approach. Furthermore, we also have confirmed a positive correlation between the size of the model and its performance. It was particularly noteworthy that languages with fewer resources or those with less linguistic similarity to English benefited more from the parameter increase. Our model is available at <a href=https://huggingface.co/pkshatech/m-ST5>https://huggingface.co/pkshatech/m-ST5</a>.</p></p class="citation"></blockquote><h3 id=3864--38347-robust-and-scalable-model-editing-for-large-language-models-yingfa-chen-et-al-2024>(38/64 | 38/347) Robust and Scalable Model Editing for Large Language Models (Yingfa Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun. (2024)<br><strong>Robust and Scalable Model Editing for Large Language Models</strong><br><button class=copy-to-clipboard title="Robust and Scalable Model Editing for Large Language Models" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17431v1.pdf filename=2403.17431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can make predictions using parametric knowledge&ndash;knowledge encoded in the model weights&ndash;or contextual knowledge&ndash;knowledge presented in the context. In many scenarios, a desirable behavior is that <b>LLMs</b> give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model&rsquo;s knowledge by <b>in-context</b> editing instead of retraining. Previous works have shown that <b>LLMs</b> are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper <b>prompting</b> methods, instruction-finetuned <b>LLMs</b> can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of <b>LLM</b> editing. To better evaluate the robustness of model editors, we collect a new dataset, that contains irrelevant questions that are more challenging than the ones in existing datasets. Empirical results show that our method outperforms current state-of-the-art methods by a <b>large</b> <b>margin.</b> <b>Unlike</b> existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa). The source code can be found at <a href=https://github.com/thunlp/EREN>https://github.com/thunlp/EREN</a>.</p></p class="citation"></blockquote><h3 id=3964--39347-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction-yixuan-wang-et-al-2024>(39/64 | 39/347) LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction (Yixuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Wang, Baoxin Wang, Yijun Liu, Dayong Wu, Wanxiang Che. (2024)<br><strong>LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction</strong><br><button class=copy-to-clipboard title="LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 35<br>Keywords: Black Box, ChatGPT, Grammatical Error Correction, Grammatical Error Correction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17413v1.pdf filename=2403.17413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over-correction is a critical problem in Chinese <b>grammatical</b> <b>error</b> <b>correction</b> (CGEC) task. Recent work using model ensemble methods based on voting can effectively mitigate over-correction and improve the precision of the <b>GEC</b> system. However, these methods still require the output of several <b>GEC</b> systems and inevitably lead to reduced error recall. In this light, we propose the LM-Combiner, a rewriting model that can directly modify the over-correction of <b>GEC</b> system outputs without a model ensemble. Specifically, we train the model on an over-correction dataset constructed through the proposed K-fold cross inference method, which allows it to directly generate filtered sentences by combining the original and the over-corrected text. In the inference stage, we directly take the original sentences and the output results of other systems as input and then obtain the filtered sentences through LM-Combiner. Experiments on the FCGEC dataset show that our proposed method effectively alleviates the over-correction of the original system (+18.2 Precision) while ensuring the error recall remains unchanged. Besides, we find that LM-Combiner still has a good rewriting performance even with small parameters and few training data, and thus can cost-effectively mitigate the over-correction of <b>black-box</b> <b>GEC</b> systems (e.g., <b>ChatGPT).</b></p></p class="citation"></blockquote><h3 id=4064--40347-large-language-models-for-education-a-survey-and-outlook-shen-wang-et-al-2024>(40/64 | 40/347) Large Language Models for Education: A Survey and Outlook (Shen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, Qingsong Wen. (2024)<br><strong>Large Language Models for Education: A Survey and Outlook</strong><br><button class=copy-to-clipboard title="Large Language Models for Education: A Survey and Outlook" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18105v1.pdf filename=2403.18105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has brought in a new era of possibilities in the realm of education. This survey paper <b>summarizes</b> the various technologies of <b>LLMs</b> in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and <b>benchmarks,</b> and identify the risks and challenges associated with deploying <b>LLMs</b> in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of <b>LLMs</b> to revolutionize educational practices and foster a more effective personalized learning environment.</p></p class="citation"></blockquote><h3 id=4164--41347-the-impact-of-syntactic-and-semantic-proximity-on-machine-translation-with-back-translation-nicolas-guerin-et-al-2024>(41/64 | 41/347) The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation (Nicolas Guerin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Guerin, Shane Steinert-Threlkeld, Emmanuel Chemla. (2024)<br><strong>The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation</strong><br><button class=copy-to-clipboard title="The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Unsupervised Learning, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18031v1.pdf filename=2403.18031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> on-the-fly back-translation, in conjunction with multilingual pretraining, is the dominant method for <b>unsupervised</b> <b>neural</b> <b>machine</b> <b>translation.</b> Theoretically, however, the method should not work in general. We therefore conduct controlled experiments with artificial languages to determine what properties of languages make back-translation an effective training method, covering lexical, syntactic, and semantic properties. We find, contrary to popular belief, that (i) parallel word frequency distributions, (ii) partially shared vocabulary, and (iii) similar syntactic structure across languages are not sufficient to explain the success of back-translation. We show however that even crude semantic signal (similar lexical fields across languages) does improve alignment of two languages through back-translation. We conjecture that rich semantic dependencies, parallel across languages, are at the root of the success of <b>unsupervised</b> methods based on back-translation. Overall, the success of <b>unsupervised</b> <b>machine</b> <b>translation</b> was far from being analytically guaranteed. Instead, it is another proof that languages of the world share deep similarities, and we hope to show how to identify which of these similarities can serve the development of <b>unsupervised,</b> cross-linguistic tools.</p></p class="citation"></blockquote><h3 id=4264--42347-continual-few-shot-event-detection-via-hierarchical-augmentation-networks-chenlong-zhang-et-al-2024>(42/64 | 42/347) Continual Few-shot Event Detection via Hierarchical Augmentation Networks (Chenlong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenlong Zhang, Pengfei Cao, Yubo Chen, Kang Liu, Zhiqiang Zhang, Mengshu Sun, Jun Zhao. (2024)<br><strong>Continual Few-shot Event Detection via Hierarchical Augmentation Networks</strong><br><button class=copy-to-clipboard title="Continual Few-shot Event Detection via Hierarchical Augmentation Networks" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Few-shot, ChatGPT, Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17733v1.pdf filename=2403.17733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional continual <b>event</b> <b>detection</b> relies on abundant labeled data for training, which is often impractical to obtain in real-world applications. In this paper, we introduce continual <b>few-shot</b> <b>event</b> <b>detection</b> (CFED), a more commonly encountered scenario when a substantial number of labeled samples are not accessible. The CFED task is challenging as it involves memorizing previous <b>event</b> <b>types</b> and learning new <b>event</b> <b>types</b> with <b>few-shot</b> samples. To mitigate these challenges, we propose a memory-based framework: Hierarchical Augmentation Networks (HANet). To memorize previous <b>event</b> <b>types</b> with limited memory, we incorporate prototypical augmentation into the memory set. For the issue of learning new <b>event</b> <b>types</b> in <b>few-shot</b> scenarios, we propose a contrastive augmentation module for token representations. Despite comparing with previous state-of-the-art methods, we also conduct comparisons with <b>ChatGPT.</b> Experiment results demonstrate that our method significantly outperforms all of these methods in multiple continual <b>few-shot</b> <b>event</b> <b>detection</b> tasks.</p></p class="citation"></blockquote><h3 id=4364--43347-mix-initiative-response-generation-with-dynamic-prefix-tuning-yuxiang-nie-et-al-2024>(43/64 | 43/347) Mix-Initiative Response Generation with Dynamic Prefix Tuning (Yuxiang Nie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiang Nie, Heyan Huang, Xian-Ling Mao, Lizi Liao. (2024)<br><strong>Mix-Initiative Response Generation with Dynamic Prefix Tuning</strong><br><button class=copy-to-clipboard title="Mix-Initiative Response Generation with Dynamic Prefix Tuning" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Supervised Learning, Unsupervised Learning, Dialogue System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17636v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17636v2.pdf filename=2403.17636v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mixed initiative serves as one of the key factors in controlling conversation directions. For a speaker, responding passively or leading proactively would result in rather different responses. However, most <b>dialogue</b> <b>systems</b> focus on training a holistic response generation model without any distinction among different initiatives. It leads to the cross-contamination problem, where the model confuses different initiatives and generates inappropriate responses. Moreover, obtaining plenty of human annotations for initiative labels can be expensive. To address this issue, we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to decouple different initiatives from the generation model, which learns initiative-aware prefixes in both <b>supervised</b> and <b>unsupervised</b> settings. Specifically, IDPT decouples initiative factors into different prefix parameters and uses the attention mechanism to adjust the selection of initiatives in guiding generation dynamically. The prefix parameters can be tuned towards accurate initiative prediction as well as mix-initiative response generation. Extensive experiments on two public <b>dialogue</b> <b>datasets</b> show that the proposed IDPT outperforms previous baselines on both automatic metrics and human evaluations. It also manages to generate appropriate responses with manipulated initiatives.</p></p class="citation"></blockquote><h3 id=4464--44347-you-are-an-expert-annotator-automatic-best-worst-scaling-annotations-for-emotion-intensity-modeling-christopher-bagdon-et-al-2024>(44/64 | 44/347) &lsquo;You are an expert annotator&rsquo;: Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling (Christopher Bagdon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher Bagdon, Prathamesh Karmalker, Harsha Gurulingappa, Roman Klinger. (2024)<br><strong>&lsquo;You are an expert annotator&rsquo;: Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling</strong><br><button class=copy-to-clipboard title="'You are an expert annotator': Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17612v1.pdf filename=2403.17612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Labeling corpora constitutes a bottleneck to create models for new tasks or domains. <b>Large</b> <b>language</b> <b>models</b> mitigate the issue with automatic corpus labeling methods, particularly for categorical annotations. Some NLP tasks such as emotion intensity prediction, however, require text regression, but there is no work on automating annotations for continuous label assignments. Regression is considered more challenging than classification: The fact that humans perform worse when tasked to choose values from a rating scale lead to comparative annotation methods, including best-worst scaling. This raises the question if <b>large</b> <b>language</b> <b>model-based</b> annotation methods show similar patterns, namely that they perform worse on rating scale annotation tasks than on comparative annotation tasks. To study this, we automate emotion intensity predictions and compare direct rating scale predictions, pairwise comparisons and best-worst scaling. We find that the latter shows the highest reliability. A <b>transformer</b> regressor <b>fine-tuned</b> on these data performs nearly on par with a model trained on the original manual annotations.</p></p class="citation"></blockquote><h3 id=4564--45347-decoding-excellence-mapping-the-demand-for-psychological-traits-of-operations-and-supply-chain-professionals-through-text-mining-s-di-luozzo-et-al-2024>(45/64 | 45/347) Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining (S. Di Luozzo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Di Luozzo, A. Fronzetti Colladon, M. M. Schiraldi. (2024)<br><strong>Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining</strong><br><button class=copy-to-clipboard title="Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; J-4; H-4-0, cs-CL, cs-SI, cs.CL, econ-GN, physics-soc-ph, q-fin-EC<br>Keyword Score: 30<br>Keywords: Topic Model, Text Mining, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17546v1.pdf filename=2403.17546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The current study proposes an innovative methodology for the profiling of psychological traits of Operations Management (OM) and Supply Chain Management (SCM) professionals. We use innovative methods and tools of <b>text</b> <b>mining</b> and social network analysis to map the demand for relevant skills from a set of job descriptions, with a focus on psychological characteristics. The proposed approach aims to evaluate the market demand for specific traits by combining relevant psychological constructs, <b>text</b> <b>mining</b> techniques, and an innovative measure, namely, the Semantic Brand Score. We apply the proposed methodology to a dataset of job descriptions for OM and SCM professionals, with the objective of providing a mapping of their relevant required skills, including psychological characteristics. In addition, the analysis is then detailed by considering the region of the organization that issues the job description, its organizational size, and the seniority level of the open position in order to understand their nuances. Finally, <b>topic</b> <b>modeling</b> is used to examine key components and their relative significance in job descriptions. By employing a novel methodology and considering contextual factors, we provide an innovative understanding of the attitudinal traits that differentiate professionals. This research contributes to talent management, recruitment practices, and professional development initiatives, since it provides new figures and perspectives to improve the effectiveness and success of Operations Management and Supply Chain Management professionals.</p></p class="citation"></blockquote><h3 id=4664--46347-a-gaze-grounded-visual-question-answering-dataset-for-clarifying-ambiguous-japanese-questions-shun-inadumi-et-al-2024>(46/64 | 46/347) A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions (Shun Inadumi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shun Inadumi, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi, Koichiro Yoshino. (2024)<br><strong>A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions</strong><br><button class=copy-to-clipboard title="A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Visual Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17545v1.pdf filename=2403.17545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Situated conversations, which refer to <b>visual</b> <b>information</b> <b>as</b> <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA),</b> often contain ambiguities caused by reliance on directive information. This problem is exacerbated because some languages, such as Japanese, often omit subjective or objective terms. Such ambiguities in <b>questions</b> <b>are</b> often clarified by the contexts in conversational situations, such as joint attention with a user or user gaze information. In this study, we propose the Gaze-grounded <b>VQA</b> dataset (GazeVQA) that clarifies ambiguous <b>questions</b> <b>using</b> gaze information by focusing on a clarification process complemented by gaze information. We also propose a method that utilizes gaze target estimation results to improve the accuracy of GazeVQA tasks. Our experimental results showed that the proposed method improved the performance in some cases of a <b>VQA</b> system on GazeVQA and identified some typical problems of GazeVQA tasks that need to be improved.</p></p class="citation"></blockquote><h3 id=4764--47347-mapguide-a-simple-yet-effective-method-to-reconstruct-continuous-language-from-brain-activities-xinpei-zhao-et-al-2024>(47/64 | 47/347) MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities (Xinpei Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinpei Zhao, Jingyuan Sun, Shaonan Wang, Jing Ye, Xiaohan Zhang, Chengqing Zong. (2024)<br><strong>MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities</strong><br><button class=copy-to-clipboard title="MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Text Generation, BLEU, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17516v1.pdf filename=2403.17516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to <b>text.</b> <b>The</b> previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from <b>text</b> <b>and</b> then guided <b>text</b> <b>generation</b> by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides <b>text</b> <b>reconstruction</b> by directly comparing them with the predicted <b>text</b> <b>embeddings</b> mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on <b>BLEU</b> and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and highlight a critical correlation: the more precisely we map brain activities to <b>text</b> <b>embeddings,</b> the better the <b>text</b> <b>reconstruction</b> results. Such insight can simplify the task of reconstructing language from brain activities for future work, emphasizing the importance of improving brain-to-text-embedding mapping techniques.</p></p class="citation"></blockquote><h3 id=4864--48347-automate-knowledge-concept-tagging-on-math-questions-with-llms-hang-li-et-al-2024>(48/64 | 48/347) Automate Knowledge Concept Tagging on Math Questions with LLMs (Hang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Li, Tianlong Xu, Jiliang Tang, Qingsong Wen. (2024)<br><strong>Automate Knowledge Concept Tagging on Math Questions with LLMs</strong><br><button class=copy-to-clipboard title="Automate Knowledge Concept Tagging on Math Questions with LLMs" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17281v1.pdf filename=2403.17281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge concept tagging for questions plays a crucial role in contemporary intelligent educational applications, including learning progress diagnosis, practice question <b>recommendations,</b> and course content organization. Traditionally, these annotations have been conducted manually with help from pedagogical experts, as the task requires not only a strong semantic understanding of both question stems and knowledge definitions but also deep insights into connecting question-solving logic with corresponding knowledge concepts. In this paper, we explore automating the tagging task using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> in response to the inability of prior manual methods to meet the rapidly growing demand for concept tagging in questions posed by advanced educational applications. Moreover, the zero/few-shot learning capability of <b>LLMs</b> makes them well-suited for application in educational scenarios, which often face challenges in collecting <b>large-scale,</b> <b>expertise-annotated</b> <b>datasets.</b> By conducting extensive experiments with a variety of representative <b>LLMs,</b> we demonstrate that <b>LLMs</b> are a promising tool for concept tagging in math questions. Furthermore, through case studies examining the results from different <b>LLMs,</b> we draw some empirical conclusions about the key factors for success in applying <b>LLMs</b> to the automatic concept tagging task.</p></p class="citation"></blockquote><h3 id=4964--49347-neural-multimodal-topic-modeling-a-comprehensive-evaluation-felipe-gonzález-pizarro-et-al-2024>(49/64 | 49/347) Neural Multimodal Topic Modeling: A Comprehensive Evaluation (Felipe González-Pizarro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felipe González-Pizarro, Giuseppe Carenini. (2024)<br><strong>Neural Multimodal Topic Modeling: A Comprehensive Evaluation</strong><br><button class=copy-to-clipboard title="Neural Multimodal Topic Modeling: A Comprehensive Evaluation" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Topic Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17308v1.pdf filename=2403.17308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural <b>topic</b> <b>models</b> can successfully find coherent and diverse <b>topics</b> <b>in</b> textual data. However, they are limited in dealing with <b>multimodal</b> datasets (e.g., images and text). This paper presents the first systematic and comprehensive evaluation of <b>multimodal</b> <b>topic</b> <b>modeling</b> of documents containing both text and images. In the process, we propose two novel <b>topic</b> <b>modeling</b> solutions and two novel evaluation metrics. Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse <b>topics.</b> <b>Nevertheless,</b> the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics. This alignment not only reinforces the credibility of our metrics but also highlights the potential for their application in guiding future <b>multimodal</b> <b>topic</b> <b>modeling</b> endeavors.</p></p class="citation"></blockquote><h3 id=5064--50347-juru-legal-brazilian-large-language-model-from-reputable-sources-roseval-malaquias-junior-et-al-2024>(50/64 | 50/347) Juru: Legal Brazilian Large Language Model from Reputable Sources (Roseval Malaquias Junior et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira. (2024)<br><strong>Juru: Legal Brazilian Large Language Model from Reputable Sources</strong><br><button class=copy-to-clipboard title="Juru: Legal Brazilian Large Language Model from Reputable Sources" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Few-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18140v1.pdf filename=2403.18140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The high computational cost associated with pretraining <b>large</b> <b>language</b> <b>models</b> limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted <b>few-shot</b> evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of <b>large</b> <b>language</b> <b>models,</b> enabling the exploration of these models at a lower cost.</p></p class="citation"></blockquote><h3 id=5164--51347-for-those-who-dont-know-how-to-ask-building-a-dataset-of-technology-questions-for-digital-newcomers-evan-lucas-et-al-2024>(51/64 | 51/347) For those who don&rsquo;t know (how) to ask: Building a dataset of technology questions for digital newcomers (Evan Lucas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evan Lucas, Kelly S. Steelman, Leo C. Ureel, Charles Wallace. (2024)<br><strong>For those who don&rsquo;t know (how) to ask: Building a dataset of technology questions for digital newcomers</strong><br><button class=copy-to-clipboard title="For those who don't know (how) to ask: Building a dataset of technology questions for digital newcomers" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18125v1.pdf filename=2403.18125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the rise of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has created rich new opportunities to learn about digital technology, many on the margins of this technology struggle to gain and maintain competency due to lexical or conceptual barriers that prevent them from asking appropriate questions. Although there have been many efforts to understand factuality of <b>LLM-created</b> content and ability of <b>LLMs</b> to answer questions, it is not well understood how unclear or nonstandard language queries affect the model outputs. We propose the creation of a dataset that captures questions of digital newcomers and outsiders, utilizing data we have compiled from a decade&rsquo;s worth of one-on-one tutoring. In this paper we lay out our planned efforts and some potential uses of this dataset.</p></p class="citation"></blockquote><h3 id=5264--52347-chatgpt-role-play-dataset-analysis-of-user-motives-and-model-naturalness-yufei-tao-et-al-2024>(52/64 | 52/347) ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness (Yufei Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Tao, Ameeta Agrawal, Judit Dombi, Tetyana Sydorenko, Jung In Lee. (2024)<br><strong>ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness</strong><br><button class=copy-to-clipboard title="ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 20<br>Keywords: ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18121v1.pdf filename=2403.18121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in interactive <b>large</b> <b>language</b> <b>models</b> like <b>ChatGPT</b> have revolutionized various domains; however, their behavior in natural and role-play conversation settings remains underexplored. In our study, we address this gap by deeply investigating how <b>ChatGPT</b> behaves during conversations in different settings by analyzing its interactions in both a normal way and a role-play setting. We introduce a novel dataset of broad range of human-AI conversations annotated with user motives and model naturalness to examine (i) how humans engage with the conversational AI model, and (ii) how natural are AI model responses. Our study highlights the diversity of user motives when interacting with <b>ChatGPT</b> and variable AI naturalness, showing not only the nuanced dynamics of natural conversations between humans and AI, but also providing new avenues for improving the effectiveness of human-AI communication.</p></p class="citation"></blockquote><h3 id=5364--53347-towards-a-zero-data-controllable-adaptive-dialog-system-dirk-väth-et-al-2024>(53/64 | 53/347) Towards a Zero-Data, Controllable, Adaptive Dialog System (Dirk Väth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dirk Väth, Lindsey Vanderlyn, Ngoc Thang Vu. (2024)<br><strong>Towards a Zero-Data, Controllable, Adaptive Dialog System</strong><br><button class=copy-to-clipboard title="Towards a Zero-Data, Controllable, Adaptive Dialog System" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17582v1.pdf filename=2403.17582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational Tree Search (V"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a <b>Reinforcement</b> <b>Learning</b> agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial <b>Large</b> <b>Language</b> <b>Model</b> for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms. Finally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data.</p></p class="citation"></blockquote><h3 id=5464--54347-rubia-a-russian-language-bias-detection-dataset-veronika-grigoreva-et-al-2024>(54/64 | 54/347) RuBia: A Russian Language Bias Detection Dataset (Veronika Grigoreva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Veronika Grigoreva, Anastasiia Ivanova, Ilseyar Alimova, Ekaterina Artemova. (2024)<br><strong>RuBia: A Russian Language Bias Detection Dataset</strong><br><button class=copy-to-clipboard title="RuBia: A Russian Language Bias Detection Dataset" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17553v1.pdf filename=2403.17553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Warning: this work contains upsetting or disturbing content. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> tend to learn the social and cultural biases present in the raw pre-training data. To test if an <b>LLM&rsquo;s</b> behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific. In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia. The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it. These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourcing workers. Overall, there are nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To illustrate the dataset&rsquo;s purpose, we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art <b>LLMs</b> and discuss the <b>LLMs&rsquo;</b> predisposition to social biases.</p></p class="citation"></blockquote><h3 id=5564--55347-transcribing-bengali-text-with-regional-dialects-to-ipa-using-district-guided-tokens-s-m-jishanul-islam-et-al-2024>(55/64 | 55/347) Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens (S M Jishanul Islam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S M Jishanul Islam, Sadia Ahmmed, Sahid Hossain Mustakim. (2024)<br><strong>Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens</strong><br><button class=copy-to-clipboard title="Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: F-2-2; I-2-7, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17407v1.pdf filename=2403.17407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or &ldquo;district&rdquo; of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each district. The DGT technique is applied to <b>fine-tune</b> several <b>transformer-based</b> models, on this new dataset. Experimental results demonstrate the effectiveness of DGT, with the ByT5 model achieving superior performance over word-based models like mT5, BanglaT5, and umT5. This is attributed to ByT5&rsquo;s ability to handle a high percentage of out-of-vocabulary words in the test set. The proposed approach highlights the importance of incorporating regional dialect information into ubiquitous natural language processing systems for languages with diverse phonological variations. The following work was a result of the &ldquo;Bhashamul&rdquo; challenge, which is dedicated to solving the problem of Bengali text with regional dialects to IPA transcription <a href=https://www.kaggle.com/competitions/regipa/>https://www.kaggle.com/competitions/regipa/</a>. The training and inference notebooks are available through the competition link.</p></p class="citation"></blockquote><h3 id=5664--56347-bridging-textual-and-tabular-worlds-for-fact-verification-a-lightweight-attention-based-model-shirin-dabbaghi-varnosfaderani-et-al-2024>(56/64 | 56/347) Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model (Shirin Dabbaghi Varnosfaderani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shirin Dabbaghi Varnosfaderani, Canasai Kruengkrai, Ramin Yahyapour, Junichi Yamagishi. (2024)<br><strong>Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model</strong><br><button class=copy-to-clipboard title="Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 16<br>Keywords: Benchmarking, Multi-modal, Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17361v1.pdf filename=2403.17361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>FEVEROUS is a <b>benchmark</b> and research initiative focused on <b>fact</b> <b>extraction</b> and verification tasks involving unstructured text and structured tabular data. In FEVEROUS, existing works often rely on extensive preprocessing and utilize rule-based transformations of data, leading to potential context loss or misleading encodings. This paper introduces a simple yet powerful model that nullifies the need for modality conversion, thereby preserving the original evidence&rsquo;s context. By leveraging pre-trained models on diverse text and tabular datasets and by incorporating a lightweight attention-based mechanism, our approach efficiently exploits latent connections between different data types, thereby yielding comprehensive and reliable verdict predictions. The model&rsquo;s modular structure adeptly manages <b>multi-modal</b> information, ensuring the integrity and authenticity of the original evidence are uncompromised. Comparative analyses reveal that our approach exhibits competitive performance, aligning itself closely with top-tier models on the FEVEROUS <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=5764--57347-project-mosla-recording-every-moment-of-second-language-acquisition-masato-hagiwara-et-al-2024>(57/64 | 57/347) Project MOSLA: Recording Every Moment of Second Language Acquisition (Masato Hagiwara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masato Hagiwara, Joshua Tanner. (2024)<br><strong>Project MOSLA: Recording Every Moment of Second Language Acquisition</strong><br><button class=copy-to-clipboard title="Project MOSLA: Recording Every Moment of Second Language Acquisition" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 16<br>Keywords: Fine-tuning, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17314v1.pdf filename=2403.17314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Second language acquisition (SLA) is a complex and dynamic process. Many SLA studies that have attempted to record and analyze this process have typically focused on a single modality (e.g., textual output of learners), covered only a short period of time, and/or lacked control (e.g., failed to capture every aspect of the learning process). In Project MOSLA (Moments of Second Language Acquisition), we have created a longitudinal, <b>multimodal,</b> multilingual, and controlled dataset by inviting participants to learn one of three target languages (Arabic, Spanish, and Chinese) from scratch over a span of two years, exclusively through online instruction, and recording every lesson using Zoom. The dataset is semi-automatically annotated with speaker/language IDs and transcripts by both human annotators and <b>fine-tuned</b> state-of-the-art speech models. Our experiments reveal linguistic insights into learners&rsquo; proficiency development over time, as well as the potential for automatically detecting the areas of focus on the screen purely from the unannotated <b>multimodal</b> data. Our dataset is freely available for research purposes and can serve as a valuable resource for a wide range of applications, including but not limited to SLA, proficiency assessment, language and speech processing, pedagogy, and <b>multimodal</b> learning analytics.</p></p class="citation"></blockquote><h3 id=5864--58347-enriching-word-usage-graphs-with-cluster-definitions-mariia-fedorova-et-al-2024>(58/64 | 58/347) Enriching Word Usage Graphs with Cluster Definitions (Mariia Fedorova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mariia Fedorova, Andrey Kutuzov, Nikolay Arefyev, Dominik Schlechtweg. (2024)<br><strong>Enriching Word Usage Graphs with Cluster Definitions</strong><br><button class=copy-to-clipboard title="Enriching Word Usage Graphs with Cluster Definitions" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Graph, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18024v1.pdf filename=2403.18024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a dataset of word usage <b>graphs</b> (WUGs), where the existing WUGs for multiple languages are enriched with cluster labels functioning as sense definitions. They are generated from scratch by <b>fine-tuned</b> encoder-decoder language models. The conducted human evaluation has shown that these definitions match the existing clusters in WUGs better than the definitions chosen from WordNet by two baseline systems. At the same time, the method is straightforward to use and easy to extend to new languages. The resulting enriched datasets can be extremely helpful for moving on to explainable semantic change modeling.</p></p class="citation"></blockquote><h3 id=5964--59347-scinews-from-scholarly-complexities-to-public-narratives----a-dataset-for-scientific-news-report-generation-dongqi-pu-et-al-2024>(59/64 | 59/347) SciNews: From Scholarly Complexities to Public Narratives &ndash; A Dataset for Scientific News Report Generation (Dongqi Pu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg. (2024)<br><strong>SciNews: From Scholarly Complexities to Public Narratives &ndash; A Dataset for Scientific News Report Generation</strong><br><button class=copy-to-clipboard title="SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17768v1.pdf filename=2403.17768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We <b>benchmark</b> our dataset employing state-of-the-art <b>text</b> <b>generation</b> models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related to this work are available at <a href=https://dongqi.me/projects/SciNews>https://dongqi.me/projects/SciNews</a>.</p></p class="citation"></blockquote><h3 id=6064--60347-task-oriented-paraphrase-analytics-marcel-gohsen-et-al-2024>(60/64 | 60/347) Task-Oriented Paraphrase Analytics (Marcel Gohsen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcel Gohsen, Matthias Hagen, Martin Potthast, Benno Stein. (2024)<br><strong>Task-Oriented Paraphrase Analytics</strong><br><button class=copy-to-clipboard title="Task-Oriented Paraphrase Analytics" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Text Transformation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17564v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17564v1.pdf filename=2403.17564v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since paraphrasing is an ill-defined task, the term &ldquo;paraphrasing&rdquo; covers <b>text</b> <b>transformation</b> tasks with different characteristics. Consequently, existing paraphrasing studies have applied quite different (explicit and implicit) criteria as to when a pair of <b>texts</b> <b>is</b> to be considered a paraphrase, all of which amount to postulating a certain level of semantic or lexical similarity. In this paper, we conduct a literature review and propose a taxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using classifiers trained to identify the tasks that a given paraphrasing instance fits, we find that the distributions of task-specific instances in the known paraphrase corpora vary substantially. This means that the use of these corpora, without the respective paraphrase conditions being clearly defined (which is the normal case), must lead to incomparable and misleading results.</p></p class="citation"></blockquote><h3 id=6164--61347-sparse-logistic-regression-with-high-order-features-for-automatic-grammar-rule-extraction-from-treebanks-santiago-herrera-et-al-2024>(61/64 | 61/347) Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks (Santiago Herrera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santiago Herrera, Caio Corro, Sylvain Kahane. (2024)<br><strong>Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks</strong><br><button class=copy-to-clipboard title="Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17534v1.pdf filename=2403.17534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Descriptive grammars are highly valuable, but writing them is time-consuming and difficult. Furthermore, while linguists typically use corpora to create them, grammar descriptions often lack quantitative data. As for formal grammars, they can be challenging to interpret. In this paper, we propose a new method to extract and explore significant fine-grained grammar patterns and potential syntactic grammar rules from treebanks, in order to create an easy-to-understand corpus-based grammar. More specifically, we extract descriptions and rules across different languages for two linguistic phenomena, agreement and word order, using a large search space and paying special attention to the ranking order of the extracted rules. For that, we use a linear classifier to extract the most salient features that predict the linguistic phenomena under study. We associate statistical information to each rule, and we compare the ranking of the model&rsquo;s results to those of other quantitative and statistical measures. Our method captures both well-known and less well-known significant grammar rules in Spanish, French, and Wolof.</p></p class="citation"></blockquote><h3 id=6264--62347-sharing-the-cost-of-success-a-game-for-evaluating-and-learning-collaborative-multi-agent-instruction-giving-and-following-policies-philipp-sadler-et-al-2024>(62/64 | 62/347) Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies (Philipp Sadler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Sadler, Sherzod Hakimov, David Schlangen. (2024)<br><strong>Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies</strong><br><button class=copy-to-clipboard title="Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17497v1.pdf filename=2403.17497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other). In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations. The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players&rsquo; assumed efforts during the interaction. We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions. And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly. However, we observe that in comparison to a reasonable heuristic pairing there is still room for improvement &ndash; which invites further research in the direction of cost-sharing in collaborative interactions.</p></p class="citation"></blockquote><h3 id=6364--63347-common-ground-tracking-in-multimodal-dialogue-ibrahim-khebour-et-al-2024>(63/64 | 63/347) Common Ground Tracking in Multimodal Dialogue (Ibrahim Khebour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ibrahim Khebour, Kenneth Lai, Mariah Bradford, Yifan Zhu, Richard Brutti, Christopher Tam, Jingxuan Tu, Benjamin Ibarra, Nathaniel Blanchard, Nikhil Krishnaswamy, James Pustejovsky. (2024)<br><strong>Common Ground Tracking in Multimodal Dialogue</strong><br><button class=copy-to-clipboard title="Common Ground Tracking in Multimodal Dialogue" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 9<br>Keywords: Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17284v1.pdf filename=2403.17284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Within Dialogue Modeling research in AI and NLP, considerable attention has been spent on <code>dialogue state tracking'' (DST), which is the ability to update the representations of the speaker's needs at each turn in the dialogue by taking into account the past dialogue moves and history. Less studied but just as important to dialogue modeling, however, is </code>common ground tracking&rsquo;&rsquo; (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue: the task-relevant propositions all participants accept as true. In this paper we present a method for automatically identifying the current set of shared beliefs and ``questions under discussion&rsquo;&rsquo; (QUDs) of a group with a shared goal. We annotate a dataset of <b>multimodal</b> interactions in a shared physical space with speech transcriptions, prosodic features, gestures, actions, and facets of collaboration, and operationalize these features for use in a deep neural model to predict moves toward construction of common ground. Model outputs cascade into a set of formal closure rules derived from situated evidence and belief axioms and update operations. We empirically assess the contribution of each feature type toward successful construction of common ground relative to ground truth, establishing a <b>benchmark</b> in this novel, challenging task.</p></p class="citation"></blockquote><h3 id=6464--64347-graph-language-model-glm-a-new-graph-based-approach-to-detect-social-instabilities-wallyson-lemes-de-oliveira-et-al-2024>(64/64 | 64/347) Graph Language Model (GLM): A new graph-based approach to detect social instabilities (Wallyson Lemes de Oliveira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul Singh Inda, Jithendra Sai Veeramaneni, Étienne Voutaz. (2024)<br><strong>Graph Language Model (GLM): A new graph-based approach to detect social instabilities</strong><br><button class=copy-to-clipboard title="Graph Language Model (GLM): A new graph-based approach to detect social instabilities" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17816v1.pdf filename=2403.17816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This scientific report presents a novel methodology for the early prediction of important political events using News datasets. The methodology leverages natural language processing, <b>graph</b> theory, clique analysis, and semantic relationships to uncover hidden predictive signals within the data. Initially, we designed a preliminary version of the method and tested it on a few events. This analysis revealed limitations in the initial research phase. We then enhanced the model in two key ways: first, we added a filtration step to only consider politically relevant news before further processing; second, we adjusted the input features to make the alert system more sensitive to significant spikes in the data. After finalizing the improved methodology, we tested it on eleven events including US protests, the Ukraine war, and French protests. Results demonstrate the superiority of our approach compared to baseline methods. Through targeted refinements, our model can now provide earlier and more accurate predictions of major political events based on subtle patterns in news data.</p></p class="citation"></blockquote><h2 id=cslg-49>cs.LG (49)</h2><h3 id=149--65347-oh-we-freeze-improving-quantized-knowledge-distillation-via-signal-propagation-analysis-for-large-language-models-kartikeya-bhardwaj-et-al-2024>(1/49 | 65/347) Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models (Kartikeya Bhardwaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague. (2024)<br><strong>Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models</strong><br><button class=copy-to-clipboard title="Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 113<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Quantization, Quantization, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18159v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18159v2.pdf filename=2403.18159v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>generative</b> <b>models</b> such as <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>diffusion</b> <b>models</b> have revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight <b>quantization</b> aware fine tuning technique using <b>knowledge</b> <b>distillation</b> <b>(KD-QAT)</b> to improve the performance of 4-bit weight <b>quantized</b> <b>LLMs</b> using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of <b>finetuning,</b> as main contributions, we provide insights into stability of <b>KD-QAT</b> by empirically studying the gradient propagation during training to better understand the vulnerabilities of <b>KD-QAT</b> based approaches to low-bit <b>quantization</b> errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the <b>KD-QAT</b> process. Finally, we experiment with the popular 7B LLaMAv2-Chat model at 4-bit <b>quantization</b> level and demonstrate that ov-freeze results in near floating point precision performance, i.e., less than 0.7% loss of accuracy on <b>Commonsense</b> <b>Reasoning</b> <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=249--66347-variational-graph-auto-encoder-based-inductive-learning-method-for-semi-supervised-classification-hanxuan-yang-et-al-2024>(2/49 | 66/347) Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification (Hanxuan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxuan Yang, Zhaoxin Yu, Qingchao Kong, Wei Liu, Wenji Mao. (2024)<br><strong>Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification</strong><br><button class=copy-to-clipboard title="Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 81<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Representation Learning, Semi-Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17500v1.pdf filename=2403.17500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>representation</b> <b>learning</b> is a fundamental research issue in various domains of applications, of which the inductive learning problem is particularly challenging as it requires models to generalize to unseen <b>graph</b> <b>structures</b> <b>during</b> inference. In recent years, <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have emerged as powerful <b>graph</b> <b>models</b> <b>for</b> inductive learning tasks such as <b>node</b> <b>classification,</b> whereas they typically heavily rely on the annotated <b>nodes</b> <b>under</b> a fully <b>supervised</b> training setting. Compared with the <b>GNN-based</b> methods, variational <b>graph</b> <b>auto-encoders</b> <b>(VGAEs)</b> are known to be more generalizable to capture the internal structural information of <b>graphs</b> <b>independent</b> <b>of</b> <b>node</b> <b>labels</b> and have achieved prominent performance on multiple <b>unsupervised</b> <b>learning</b> tasks. However, so far there is still a lack of work focusing on leveraging the VGAE framework for inductive learning, due to the difficulties in training the model in a <b>supervised</b> manner and avoiding over-fitting the proximity information of <b>graphs.</b> <b>To</b> <b>solve</b> these problems and improve the model performance of VGAEs for inductive <b>graph</b> <b>representation</b> <b>learning,</b> in this work, we propose the Self-Label Augmented VGAE model. To leverage the label information for training, our model takes <b>node</b> <b>labels</b> as one-hot encoded inputs and then performs label reconstruction in model training. To overcome the scarcity problem of <b>node</b> <b>labels</b> for <b>semi-supervised</b> <b>settings,</b> we further propose the Self-Label Augmentation Method (SLAM), which uses pseudo labels generated by our model with a <b>node-wise</b> <b>masking</b> approach to enhance the label information. Experiments on <b>benchmark</b> inductive learning <b>graph</b> <b>datasets</b> <b>verify</b> that our proposed model archives promising results on <b>node</b> <b>classification</b> with particular superiority under <b>semi-supervised</b> <b>learning</b> settings.</p></p class="citation"></blockquote><h3 id=349--67347-chain-of-compression-a-systematic-approach-to-combinationally-compress-convolutional-neural-networks-yingtao-shen-et-al-2024>(3/49 | 67/347) Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks (Yingtao Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingtao Shen, Minqing Sun, Jie Zhao, An Zou. (2024)<br><strong>Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 80<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Knowledge Distillation, Knowledge Distillation, Model Compression, Pruning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17447v1.pdf filename=2403.17447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> have achieved significant popularity, but their computational and memory intensity poses challenges for resource-constrained computing systems, particularly with the prerequisite of real-time performance. To release this burden, <b>model</b> <b>compression</b> has become an important research focus. Many approaches like <b>quantization,</b> <b>pruning,</b> early exit, and <b>knowledge</b> <b>distillation</b> have demonstrated the effect of reducing redundancy in neural networks. Upon closer examination, it becomes apparent that each approach capitalizes on its unique features to compress the neural network, and they can also exhibit complementary behavior when combined. To explore the interactions and reap the benefits from the complementary features, we propose the Chain of Compression, which works on the combinational sequence to apply these common techniques to compress the neural network. Validated on the image-based regression and classification networks across different data sets, our proposed Chain of Compression can significantly compress the computation cost by 100-1000 times with ignorable accuracy loss compared with the baseline model.</p></p class="citation"></blockquote><h3 id=449--68347-targeted-visualization-of-the-backbone-of-encoder-llms-isaac-roberts-et-al-2024>(4/49 | 68/347) Targeted Visualization of the Backbone of Encoder LLMs (Isaac Roberts et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isaac Roberts, Alexander Schulz, Luca Hermes, Barbara Hammer. (2024)<br><strong>Targeted Visualization of the Backbone of Encoder LLMs</strong><br><button class=copy-to-clipboard title="Targeted Visualization of the Backbone of Encoder LLMs" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Explainable AI, Fine-tuning, BERT, GPT, Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18872v1.pdf filename=2403.18872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Attention based <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are the state-of-the-art in natural language processing (NLP). The two most common architectures are encoders such as <b>BERT,</b> and decoders like the <b>GPT</b> models. Despite the success of encoder models, on which we focus in this work, they also bear several risks, including issues with bias or their susceptibility for <b>adversarial</b> <b>attacks,</b> signifying the necessity for <b>explainable</b> <b>AI</b> to detect such issues. While there does exist various local explainability methods focusing on the prediction of single inputs, global methods based on dimensionality reduction for classification inspection, which have emerged in other domains and that go further than just using t-SNE in the embedding space, are not widely spread in NLP. To reduce this gap, we investigate the application of DeepView, a method for visualizing a part of the decision function together with a data set in two dimensions, to the NLP domain. While in previous work, DeepView has been used to inspect deep image classification models, we demonstrate how to apply it to <b>BERT-based</b> NLP classifiers and investigate its usability in this domain, including settings with adversarially perturbed input samples and pre-trained, <b>fine-tuned,</b> and multi-task models.</p></p class="citation"></blockquote><h3 id=549--69347-the-need-for-speed-pruning-transformers-with-one-recipe-samir-khaki-et-al-2024>(5/49 | 69/347) The Need for Speed: Pruning Transformers with One Recipe (Samir Khaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samir Khaki, Konstantinos N. Plataniotis. (2024)<br><strong>The Need for Speed: Pruning Transformers with One Recipe</strong><br><button class=copy-to-clipboard title="The Need for Speed: Pruning Transformers with One Recipe" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Convolutional Neural Network, Knowledge Distillation, Pruning, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17921v1.pdf filename=2403.17921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained <b>transformer</b> architectures $\textit{without requiring re-training}$. Recent works have explored improving <b>transformer</b> efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption. To address these shortcomings, the OPTIN framework leverages intermediate feature <b>distillation,</b> capturing the long-range dependencies of model parameters (coined $\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, <b>transfer</b> <b>learning,</b> and semantic segmentation tasks $\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitive accuracy performance and improved throughput. Particularly, we show a $\leq 2$% accuracy degradation from NLP baselines and a $0.5$% improvement from state-of-the-art methods on image classification at competitive FLOPs reductions. We further demonstrate the generalization of tasks and architecture with comparative performance using Mask2Former for semantic segmentation and <b>cnn-style</b> networks. OPTIN presents one of the first one-shot efficient frameworks for compressing <b>transformer</b> architectures that generalizes well across different class domains, in particular: natural language and image-related tasks, without $\textit{re-training}$.</p></p class="citation"></blockquote><h3 id=649--70347-lisa-layerwise-importance-sampling-for-memory-efficient-large-language-model-fine-tuning-rui-pan-et-al-2024>(6/49 | 70/347) LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning (Rui Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang. (2024)<br><strong>LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</strong><br><button class=copy-to-clipboard title="LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, math-OC<br>Keyword Score: 50<br>Keywords: Fine-tuning, LLaMA, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17919v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17919v2.pdf filename=2403.17919v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The machine learning community has witnessed impressive advancements since the first appearance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> yet their huge memory consumption has become a major roadblock to <b>large-scale</b> <b>training.</b> <b>Parameter</b> Efficient <b>Fine-Tuning</b> techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most <b>large-scale</b> <b>fine-tuning</b> <b>settings.</b> Attempting to complement this deficiency, we investigate layerwise properties of LoRA on <b>fine-tuning</b> tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in <b>LLMs</b> and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream <b>fine-tuning</b> tasks, where LISA consistently outperforms LoRA by over $11%$-$37%$ in terms of <b>MT-Bench</b> scores. On <b>large</b> <b>models,</b> <b>specifically</b> <b>LLaMA-2-70B,</b> LISA achieves on-par or better performance than LoRA on <b>MT-Bench,</b> GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.</p></p class="citation"></blockquote><h3 id=749--71347-mechanistic-design-and-scaling-of-hybrid-architectures-michael-poli-et-al-2024>(7/49 | 71/347) Mechanistic Design and Scaling of Hybrid Architectures (Michael Poli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, Ce Zhang, Stefano Massaroli. (2024)<br><strong>Mechanistic Design and Scaling of Hybrid Architectures</strong><br><button class=copy-to-clipboard title="Mechanistic Design and Scaling of Hybrid Architectures" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Convolution, Transformer, Grounding, Perplexity, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17844v1.pdf filename=2403.17844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by <b>grounding</b> it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of <b>scaling</b> <b>laws.</b> Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal <b>scaling</b> <b>law</b> analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal <b>perplexity,</b> enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art <b>Transformer,</b> <b>convolutional,</b> and recurrent architectures (Transformer++, Hyena, Mamba) in <b>scaling,</b> <b>both</b> at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of <b>scaling</b> <b>laws,</b> and that an optimal architecture should leverage specialized layers via a hybrid topology.</p></p class="citation"></blockquote><h3 id=849--72347-leave-no-patient-behind-enhancing-medication-recommendation-for-rare-disease-patients-zihao-zhao-et-al-2024>(8/49 | 72/347) Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients (Zihao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Zhao, Yi Jing, Fuli Feng, Jiancan Wu, Chongming Gao, Xiangnan He. (2024)<br><strong>Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients</strong><br><button class=copy-to-clipboard title="Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fairness, Recommendation, Self-supervised Learning, Self-supervised Pre-training, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17745v1.pdf filename=2403.17745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medication <b>recommendation</b> systems have gained significant attention in healthcare as a means of providing tailored and effective drug combinations based on patients&rsquo; clinical information. However, existing approaches often suffer from <b>fairness</b> issues, as <b>recommendations</b> tend to be more accurate for patients with common diseases compared to those with rare conditions. In this paper, we propose a novel model called Robust and Accurate <b>REcommendations</b> for Medication (RAREMed), which leverages the pretrain-finetune learning paradigm to enhance accuracy for rare diseases. RAREMed employs a <b>transformer</b> encoder with a unified input sequence approach to capture complex relationships among disease and procedure codes. Additionally, it introduces two <b>self-supervised</b> <b>pre-training</b> tasks, namely Sequence Matching Prediction (SMP) and Self Reconstruction (SR), to learn specialized medication needs and interrelations among clinical codes. Experimental results on two real-world datasets demonstrate that RAREMed provides accurate drug sets for both rare and common disease patients, thereby mitigating unfairness in medication <b>recommendation</b> systems.</p></p class="citation"></blockquote><h3 id=949--73347-el-mlffs-ensemble-learning-of-machine-leaning-force-fields-bangchen-yin-et-al-2024>(9/49 | 73/347) EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields (Bangchen Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bangchen Yin, Yue Yin, Yuda W. Tang, Hai Xiao. (2024)<br><strong>EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields</strong><br><button class=copy-to-clipboard title="EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17507v1.pdf filename=2403.17507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning force fields (MLFFs) have emerged as a promising approach to bridge the accuracy of quantum mechanical methods and the efficiency of classical force fields. However, the abundance of MLFF models and the challenge of accurately predicting atomic forces pose significant obstacles in their practical application. In this paper, we propose a novel ensemble learning framework, EL-MLFFs, which leverages the stacking method to integrate predictions from diverse MLFFs and enhance force prediction accuracy. By constructing a <b>graph</b> <b>representation</b> <b>of</b> molecular structures and employing a <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> as the meta-model, EL-MLFFs effectively captures atomic interactions and refines force predictions. We evaluate our approach on two distinct datasets: methane molecules and methanol adsorbed on a Cu(100) surface. The results demonstrate that EL-MLFFs significantly improves force prediction accuracy compared to individual MLFFs, with the ensemble of all eight models yielding the best performance. Moreover, our ablation study highlights the crucial roles of the residual network and <b>graph</b> <b>attention</b> <b>layers</b> in the model&rsquo;s architecture. The EL-MLFFs framework offers a promising solution to the challenges of model selection and force prediction accuracy in MLFFs, paving the way for more reliable and efficient molecular <b>simulations.</b></p></p class="citation"></blockquote><h3 id=1049--74347-learn-from-heterophily-heterophilous-information-enhanced-graph-neural-network-yilun-zheng-et-al-2024>(10/49 | 74/347) Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network (Yilun Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yilun Zheng, Jiahao Xu, Lihui Chen. (2024)<br><strong>Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network</strong><br><button class=copy-to-clipboard title="Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 36<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17351v1.pdf filename=2403.17351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Under circumstances of heterophily, where <b>nodes</b> <b>with</b> different labels tend to be connected based on semantic meanings, <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> often exhibit suboptimal performance. Current studies on <b>graph</b> <b>heterophily</b> <b>mainly</b> focus on aggregation calibration or neighbor extension and address the heterophily issue by utilizing <b>node</b> <b>features</b> or structural information to improve <b>GNN</b> representations. In this paper, we propose and demonstrate that the valuable semantic information inherent in heterophily can be utilized effectively in <b>graph</b> <b>learning</b> <b>by</b> investigating the distribution of neighbors for each individual <b>node</b> <b>within</b> the <b>graph.</b> <b>The</b> <b>theoretical</b> analysis is carried out to demonstrate the efficacy of the idea in enhancing <b>graph</b> <b>learning.</b> <b>Based</b> on this analysis, we propose HiGNN, an innovative approach that constructs an additional new <b>graph</b> <b>structure,</b> <b>that</b> integrates heterophilous information by leveraging <b>node</b> <b>distribution</b> to enhance connectivity between <b>nodes</b> <b>that</b> share similar semantic characteristics. We conduct empirical assessments on <b>node</b> <b>classification</b> tasks using both homophilous and heterophilous <b>benchmark</b> datasets and compare HiGNN to popular <b>GNN</b> baselines and SoTA methods, confirming the effectiveness in improving <b>graph</b> <b>representations.</b> <b>In</b> addition, by incorporating heterophilous information, we demonstrate a notable enhancement in existing <b>GNN-based</b> approaches, and the homophily degree across real-world datasets, thus affirming the efficacy of our approach.</p></p class="citation"></blockquote><h3 id=1149--75347-are-compressed-language-models-less-subgroup-robust-leonidas-gee-et-al-2024>(11/49 | 75/347) Are Compressed Language Models Less Subgroup Robust? (Leonidas Gee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonidas Gee, Andrea Zugarini, Novi Quadrianto. (2024)<br><strong>Are Compressed Language Models Less Subgroup Robust?</strong><br><button class=copy-to-clipboard title="Are Compressed Language Models Less Subgroup Robust?" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Model Compression, BERT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17811v1.pdf filename=2403.17811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To reduce the inference cost of <b>large</b> <b>language</b> <b>models,</b> <b>model</b> <b>compression</b> is increasingly used to create smaller scalable <b>models.</b> <b>However,</b> little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of <b>BERT</b> language <b>models.</b> <b>We</b> show that worst-group performance does not depend on <b>model</b> <b>size</b> alone, but also on the compression method used. Additionally, we find that <b>model</b> <b>compression</b> does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of <b>model</b> <b>compression.</b></p></p class="citation"></blockquote><h3 id=1249--76347-masked-autoencoders-are-pde-learners-anthony-zhou-et-al-2024>(12/49 | 76/347) Masked Autoencoders are PDE Learners (Anthony Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anthony Zhou, Amir Barati Farimani. (2024)<br><strong>Masked Autoencoders are PDE Learners</strong><br><button class=copy-to-clipboard title="Masked Autoencoders are PDE Learners" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Autoencoder, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17728v1.pdf filename=2403.17728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through <b>self-supervised</b> <b>learning</b> across PDEs, masked <b>autoencoders</b> can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.</p></p class="citation"></blockquote><h3 id=1349--77347-peersimgym-an-environment-for-solving-the-task-offloading-problem-with-reinforcement-learning-frederico-metelo-et-al-2024>(13/49 | 77/347) PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning (Frederico Metelo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frederico Metelo, Stevo Racković, Pedro Ákos, Cláudia Soares. (2024)<br><strong>PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17637v1.pdf filename=2403.17637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, <b>Reinforcement</b> <b>Learning</b> (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable <b>simulation</b> environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \textit{PettingZoo}-based interface for RL agent deployment in both solo and multi-agent setups. Furthermore, we demonstrate the utility of the environment through experiments with Deep <b>Reinforcement</b> <b>Learning</b> agents, showcasing the potential of RL-based approaches to significantly enhance offloading strategies in distributed computing settings. PeersimGym thus bridges the gap between theoretical RL models and their practical applications, paving the way for advancements in efficient task offloading methodologies.</p></p class="citation"></blockquote><h3 id=1449--78347-on-the-benefits-of-over-parameterization-for-out-of-distribution-generalization-yifan-hao-et-al-2024>(14/49 | 78/347) On the Benefits of Over-parameterization for Out-of-Distribution Generalization (Yifan Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Hao, Yong Lin, Difan Zou, Tong Zhang. (2024)<br><strong>On the Benefits of Over-parameterization for Out-of-Distribution Generalization</strong><br><button class=copy-to-clipboard title="On the Benefits of Over-parameterization for Out-of-Distribution Generalization" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17592v1.pdf filename=2403.17592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, machine learning models have achieved success based on the independently and identically distributed assumption. However, this assumption can be easily violated in real-world applications, leading to the <b>Out-of-Distribution</b> (OOD) problem. Understanding how modern over-parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current theoretical understanding is insufficient. Existing theoretical works often provide meaningless results for over-parameterized models in OOD scenarios or even contradict empirical findings. To this end, we are investigating the performance of the over-parameterized model in terms of OOD generalization under the general benign overfitting conditions. Our analysis focuses on a random feature model and examines non-trivial natural distributional shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achieving zero excess in-distribution (ID) loss. We demonstrate that in this scenario, further increasing the model&rsquo;s parameterization can significantly reduce the OOD loss. Intuitively, the variance term of ID loss remains low due to orthogonality of long-tail features, meaning overfitting noise during training generally doesn&rsquo;t raise testing loss. However, in OOD cases, distributional shift increases the variance term. Thankfully, the inherent shift is unrelated to individual x, maintaining the orthogonality of long-tail features. Expanding the hidden dimension can additionally improve this orthogonality by mapping the features into higher-dimensional spaces, thereby reducing the variance term. We further show that model ensembles also improve OOD loss, akin to increasing model capacity. These insights explain the empirical phenomenon of enhanced OOD generalization through model ensembles, supported by consistent <b>simulations</b> with theoretical results.</p></p class="citation"></blockquote><h3 id=1549--79347-bvr-gym-a-reinforcement-learning-environment-for-beyond-visual-range-air-combat-edvards-scukins-et-al-2024>(15/49 | 79/347) BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat (Edvards Scukins et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edvards Scukins, Markus Klein, Lars Kroon, Petter Ögren. (2024)<br><strong>BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat</strong><br><button class=copy-to-clipboard title="BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17533v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17533v1.pdf filename=2403.17533v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating new air combat tactics and discovering novel maneuvers can require numerous hours of expert pilots&rsquo; time. Additionally, for each different combat scenario, the same strategies may not work since small changes in equipment performance may drastically change the air combat outcome. For this reason, we created a <b>reinforcement</b> <b>learning</b> environment to help investigate potential air combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR Gym. This type of air combat is important since long-range missiles are often the first weapon to be used in aerial combat. Some existing environments provide high-fidelity <b>simulations</b> but are either not open source or are not adapted to the BVR air combat domain. Other environments are open source but use less accurate <b>simulation</b> models. Our work provides a high-fidelity environment based on the open-source flight dynamics simulator JSBSim and is adapted to the BVR air combat domain. This article describes the building blocks of the environment and some use cases.</p></p class="citation"></blockquote><h3 id=1649--80347-securing-gnns-explanation-based-identification-of-backdoored-training-graphs-jane-downer-et-al-2024>(16/49 | 80/347) Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs (Jane Downer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jane Downer, Ren Wang, Binghui Wang. (2024)<br><strong>Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs</strong><br><button class=copy-to-clipboard title="Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18136v1.pdf filename=2403.18136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of <b>GNN</b> classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while <b>graph-level</b> <b>explanations</b> <b>can</b> offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of <b>GNN</b> explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple <b>benchmark</b> datasets and examine its efficacy against various attack models. Our results show that our method can achieve high detection performance, marking a significant advancement in safeguarding <b>GNNs</b> against backdoor attacks.</p></p class="citation"></blockquote><h3 id=1749--81347-bidirectional-consistency-models-liangchen-li-et-al-2024>(17/49 | 81/347) Bidirectional Consistency Models (Liangchen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangchen Li, Jiajun He. (2024)<br><strong>Bidirectional Consistency Models</strong><br><button class=copy-to-clipboard title="Bidirectional Consistency Models" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Diffusion Model, Black Box, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18035v1.pdf filename=2403.18035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation and inversion tasks within one framework. Notably, our proposed method enables one-step generation and inversion while also allowing the use of additional steps to enhance generation quality or reduce reconstruction error. Furthermore, by leveraging our model&rsquo;s bidirectional consistency, we introduce a sampling strategy that can enhance FID while preserving the generated image content. We further showcase our model&rsquo;s capabilities in several downstream tasks, such as interpolation and inpainting, and present demonstrations of potential applications, including blind restoration of compressed images and defending <b>black-box</b> <b>adversarial</b> <b>attacks.</b></p></p class="citation"></blockquote><h3 id=1849--82347-herta-a-high-efficiency-and-rigorous-training-algorithm-for-unfolded-graph-neural-networks-yongyi-yang-et-al-2024>(18/49 | 82/347) HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks (Yongyi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongyi Yang, Jiaming Yang, Wei Hu, Michał Dereziński. (2024)<br><strong>HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks</strong><br><button class=copy-to-clipboard title="HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18142v1.pdf filename=2403.18142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a variant of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> Unfolded <b>GNNs</b> offer enhanced interpretability and flexibility over traditional designs. Nevertheless, they still suffer from scalability challenges when it comes to the training cost. Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees. Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded <b>GNNs.</b> In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded <b>GNNs</b> that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded <b>GNNs.</b> Additionally, as a byproduct of HERTA, we propose a new spectral sparsification method applicable to normalized and regularized <b>graph</b> <b>Laplacians</b> <b>that</b> ensures tighter bounds for our algorithm than existing spectral sparsifiers do. Experiments on real-world datasets verify the superiority of HERTA as well as its adaptability to various loss functions and optimizers.</p></p class="citation"></blockquote><h3 id=1949--83347-healthgat-node-classifications-in-electronic-health-records-using-graph-attention-networks-fahmida-liza-piya-et-al-2024>(19/49 | 83/347) HealthGAT: Node Classifications in Electronic Health Records using Graph Attention Networks (Fahmida Liza Piya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fahmida Liza Piya, Mehak Gupta, Rahmatollah Beheshti. (2024)<br><strong>HealthGAT: Node Classifications in Electronic Health Records using Graph Attention Networks</strong><br><button class=copy-to-clipboard title="HealthGAT: Node Classifications in Electronic Health Records using Graph Attention Networks" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph Attention Networks, Node Classification, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18128v1.pdf filename=2403.18128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While electronic health records (EHRs) are widely used across various applications in healthcare, most applications use the EHRs in their raw (tabular) format. Relying on raw or simple data pre-processing can greatly limit the performance or even applicability of downstream tasks using EHRs. To address this challenge, we present HealthGAT, a novel <b>graph</b> <b>attention</b> <b>network</b> framework that utilizes a hierarchical approach to generate embeddings from EHR, surpassing traditional <b>graph-based</b> <b>methods.</b> <b>Our</b> model iteratively refines the embeddings for medical codes, resulting in improved EHR data analysis. We also introduce customized EHR-centric auxiliary pre-training tasks to leverage the rich medical knowledge embedded within the data. This approach provides a comprehensive analysis of complex medical relationships and offers significant advancement over standard data representation techniques. HealthGAT has demonstrated its effectiveness in various healthcare scenarios through comprehensive evaluations against established methodologies. Specifically, our model shows outstanding performance in <b>node</b> <b>classification</b> and downstream tasks such as predicting readmissions and diagnosis classifications. Our code is available at <a href=https://github.com/healthylaife/HealthGAT>https://github.com/healthylaife/HealthGAT</a></p></p class="citation"></blockquote><h3 id=2049--84347-learning-the-optimal-power-flow-environment-design-matters-thomas-wolgast-et-al-2024>(20/49 | 84/347) Learning the Optimal Power Flow: Environment Design Matters (Thomas Wolgast et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Wolgast, Astrid Nieße. (2024)<br><strong>Learning the Optimal Power Flow: Environment Design Matters</strong><br><button class=copy-to-clipboard title="Learning the Optimal Power Flow: Environment Design Matters" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 23<br>Keywords: Benchmarking, Recommendation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17831v1.pdf filename=2403.17831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To solve the optimal power flow (OPF) problem, <b>reinforcement</b> <b>learning</b> (RL) emerges as a promising new approach. However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment. In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice. In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance. Further, we derive some first <b>recommendations</b> regarding the choice of these design decisions. The created environment framework is fully open-source and can serve as a <b>benchmark</b> for future research in the RL-OPF field.</p></p class="citation"></blockquote><h3 id=2149--85347-uncertainty-aware-distributional-offline-reinforcement-learning-xiaocong-chen-et-al-2024>(21/49 | 85/347) Uncertainty-aware Distributional Offline Reinforcement Learning (Xiaocong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaocong Chen, Siyu Wang, Tong Yu, Lina Yao. (2024)<br><strong>Uncertainty-aware Distributional Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Uncertainty-aware Distributional Offline Reinforcement Learning" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17646v1.pdf filename=2403.17646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>reinforcement</b> <b>learning</b> (RL) presents distinct challenges as it relies solely on observational data. A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity. Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity. In this study, we propose an uncertainty-aware distributional <b>offline</b> <b>RL</b> <b>method</b> to simultaneously address both epistemic uncertainty and environmental stochasticity. We propose a model-free <b>offline</b> <b>RL</b> <b>algorithm</b> capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns. Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and risk-neutral <b>benchmarks,</b> demonstrating its superior performance.</p></p class="citation"></blockquote><h3 id=2249--86347-tutorial-on-diffusion-models-for-imaging-and-vision-stanley-h-chan-2024>(22/49 | 86/347) Tutorial on Diffusion Models for Imaging and Vision (Stanley H. Chan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stanley H. Chan. (2024)<br><strong>Tutorial on Diffusion Models for Imaging and Vision</strong><br><button class=copy-to-clipboard title="Tutorial on Diffusion Models for Imaging and Vision" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18103v1.pdf filename=2403.18103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The astonishing growth of generative tools in recent years has empowered many exciting applications in <b>text-to-image</b> generation and text-to-video generation. The underlying principle behind these generative tools is the concept of <b>diffusion,</b> <b>a</b> particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the <b>diffusion</b> <b>models.</b> The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on <b>diffusion</b> <b>models</b> or applying these models to solve other problems.</p></p class="citation"></blockquote><h3 id=2349--87347-counterfactual-fairness-through-transforming-data-orthogonal-to-bias-shuyi-chen-et-al-2024>(23/49 | 87/347) Counterfactual Fairness through Transforming Data Orthogonal to Bias (Shuyi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuyi Chen, Shixiang Zhu. (2024)<br><strong>Counterfactual Fairness through Transforming Data Orthogonal to Bias</strong><br><button class=copy-to-clipboard title="Counterfactual Fairness through Transforming Data Orthogonal to Bias" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Counter-factual, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17852v1.pdf filename=2403.17852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on <b>fairness,</b> the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating <b>counterfactual</b> <b>fairness</b> in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that <b>counterfactual</b> <b>fairness</b> can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine learning models and tasks, and includes a sparse variant to enhance numerical stability through regularization. Through empirical evaluation on simulated and real-world datasets - including the adult income and the COMPAS recidivism datasets - our methodology demonstrates its capacity to enable fairer outcomes without compromising accuracy.</p></p class="citation"></blockquote><h3 id=2449--88347-climate-downscaling-a-deep-learning-based-super-resolution-model-of-precipitation-data-with-attention-block-and-skip-connections-chia-hao-chiang-et-al-2024>(24/49 | 88/347) Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections (Chia-Hao Chiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chia-Hao Chiang, Zheng-Han Huang, Liwen Liu, Hsin-Chien Liang, Yi-Chi Wang, Wan-Ling Tseng, Chao Wang, Che-Ta Chen, Ko-Chih Wang. (2024)<br><strong>Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections</strong><br><button class=copy-to-clipboard title="Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17847v1.pdf filename=2403.17847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change. These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties. To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes. Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness. One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands. Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics. In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to drastically geographical elevation changes and uneven distribution through the year. Thus, it is crucial to track and predict the rainfall to make the most use of it and to prevent the floods. However, climate models have limited resolution and require intensive computational power for local-scale use. Therefore, we proposed a deep <b>convolutional</b> <b>neural</b> <b>network</b> with skip connections, attention blocks, and auxiliary data concatenation, in order to downscale the low-resolution precipitation data into high-resolution one. Eventually, we compare with other climate downscaling methods and show better performance in metrics of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation, structural similarity index (SSIM), and forecast indicators.</p></p class="citation"></blockquote><h3 id=2549--89347-secure-aggregation-is-not-private-against-membership-inference-attacks-khac-hoang-ngo-et-al-2024>(25/49 | 89/347) Secure Aggregation is Not Private Against Membership Inference Attacks (Khac-Hoang Ngo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khac-Hoang Ngo, Johan Östman, Giuseppe Durisi, Alexandre Graell i Amat. (2024)<br><strong>Secure Aggregation is Not Private Against Membership Inference Attacks</strong><br><button class=copy-to-clipboard title="Secure Aggregation is Not Private Against Membership Inference Attacks" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17775v1.pdf filename=2403.17775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in <b>federated</b> <b>learning,</b> affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg&rsquo;s privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local <b>differential</b> <b>privacy</b> (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of <b>federated</b> <b>learning</b> under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offers weak privacy against membership inference attacks even in a single training round. Indeed, it is difficult to hide a local update by adding other independent local updates when the updates are of high dimension. Our findings underscore the imperative for additional privacy-enhancing mechanisms, such as noise injection, in <b>federated</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=2649--90347-ccdsreformer-traffic-flow-prediction-with-a-criss-crossed-dual-stream-enhanced-rectified-transformer-model-zhiqi-shao-et-al-2024>(26/49 | 90/347) CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model (Zhiqi Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqi Shao, Michael G. H. Bell, Ze Wang, D. Glenn Geers, Xusheng Yao, Junbin Gao. (2024)<br><strong>CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model</strong><br><button class=copy-to-clipboard title="CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-0, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17753v1.pdf filename=2403.17753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate, and effective traffic forecasting is vital for smart traffic systems, crucial in urban traffic planning and management. Current Spatio-Temporal <b>Transformer</b> models, despite their prediction capabilities, struggle with balancing computational efficiency and accuracy, favoring global over local information, and handling spatial and temporal data separately, limiting insight into complex interactions. We introduce the Criss-Crossed Dual-Stream Enhanced Rectified <b>Transformer</b> model (CCDSReFormer), which includes three innovative modules: Enhanced Rectified Spatial <b>Self-attention</b> (ReSSA), Enhanced Rectified Delay Aware <b>Self-attention</b> (ReDASA), and Enhanced Rectified Temporal <b>Self-attention</b> (ReTSA). These modules aim to lower computational needs via sparse attention, focus on local information for better traffic dynamics understanding, and merge spatial and temporal insights through a unique learning method. Extensive tests on six real-world datasets highlight CCDSReFormer&rsquo;s superior performance. An ablation study also confirms the significant impact of each component on the model&rsquo;s predictive accuracy, showcasing our model&rsquo;s ability to forecast traffic flow effectively.</p></p class="citation"></blockquote><h3 id=2749--91347-enhancing-privacy-in-federated-learning-through-local-training-nicola-bastianello-et-al-2024>(27/49 | 91/347) Enhancing Privacy in Federated Learning through Local Training (Nicola Bastianello et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicola Bastianello, Changxin Liu, Karl H. Johansson. (2024)<br><strong>Enhancing Privacy in Federated Learning through Local Training</strong><br><button class=copy-to-clipboard title="Enhancing Privacy in Federated Learning through Local Training" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 20<br>Keywords: Federated Learning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17572v1.pdf filename=2403.17572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we propose the <b>federated</b> <b>private</b> local training algorithm (Fed-PLT) for <b>federated</b> <b>learning,</b> to overcome the challenges of (i) expensive communications and (ii) privacy preservation. We address (i) by allowing for both partial participation and local training, which significantly reduce the number of communication rounds between the central coordinator and computing agents. The algorithm matches the state of the art in the sense that the use of local training demonstrably does not impact accuracy. Additionally, agents have the flexibility to choose from various local training solvers, such as (stochastic) gradient descent and accelerated gradient descent. Further, we investigate how employing local training can enhance privacy, addressing point (ii). In particular, we derive <b>differential</b> <b>privacy</b> bounds and highlight their dependence on the number of local training epochs. We assess the effectiveness of the proposed algorithm by comparing it to alternative techniques, considering both theoretical analysis and numerical results from a classification task.</p></p class="citation"></blockquote><h3 id=2849--92347-a-survey-on-deep-learning-and-state-of-the-arts-applications-mohd-halim-mohd-noor-et-al-2024>(28/49 | 92/347) A Survey on Deep Learning and State-of-the-arts Applications (Mohd Halim Mohd Noor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohd Halim Mohd Noor, Ayokunle Olalekan Ige. (2024)<br><strong>A Survey on Deep Learning and State-of-the-arts Applications</strong><br><button class=copy-to-clipboard title="A Survey on Deep Learning and State-of-the-arts Applications" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17561v1.pdf filename=2403.17561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and <b>convolutional</b> <b>neural</b> <b>network</b> architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review the state-of-the-art deep learning models in computer vision, natural language processing, time series analysis and pervasive computing. We highlight the key features of the models and their effectiveness in solving the problems within each domain. Furthermore, this study presents the fundamentals of deep learning, various deep learning model types and prominent <b>convolutional</b> <b>neural</b> <b>network</b> architectures. Finally, challenges and future directions in deep learning research are discussed to offer a broader perspective for future researchers.</p></p class="citation"></blockquote><h3 id=2949--93347-deep-support-vectors-junhoo-lee-et-al-2024>(29/49 | 93/347) Deep Support Vectors (Junhoo Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhoo Lee, Hyunho Lee, Kyomin Hwang, Nojun Kwak. (2024)<br><strong>Deep Support Vectors</strong><br><button class=copy-to-clipboard title="Deep Support Vectors" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Karush-Kuhn-Tucker, Karush-Kuhn-Tucker<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17329v1.pdf filename=2403.17329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional <b>Karush-Kuhn-Tucker</b> <b>(KKT)</b> conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.</p></p class="citation"></blockquote><h3 id=3049--94347-canos-a-fast-and-scalable-neural-ac-opf-solver-robust-to-n-1-perturbations-luis-piloto-et-al-2024>(30/49 | 94/347) CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1 Perturbations (Luis Piloto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luis Piloto, Sofia Liguori, Sephora Madjiheurem, Miha Zgubic, Sean Lovett, Hamish Tomlinson, Sophie Elster, Chris Apps, Sims Witherspoon. (2024)<br><strong>CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1 Perturbations</strong><br><button class=copy-to-clipboard title="CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1 Perturbations" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17660v1.pdf filename=2403.17660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimal Power Flow (OPF) refers to a wide range of related optimization problems with the goal of operating power systems efficiently and securely. In the simplest setting, OPF determines how much power to generate in order to minimize costs while meeting demand for power and satisfying physical and operational constraints. In even the simplest case, power grid operators use approximations of the AC-OPF problem because solving the exact problem is prohibitively slow with state-of-the-art solvers. These approximations sacrifice accuracy and operational feasibility in favor of speed. This trade-off leads to costly &ldquo;uplift payments&rdquo; and increased carbon emissions, especially for large power grids. In the present work, we train a deep learning system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF cost) without compromising speed (running in as little as 33&ndash;65 ms). Importantly, CANOS scales to realistic grid sizes with promising empirical results on grids containing as many as 10,000 buses. Finally, because CANOS is a <b>Graph</b> <b>Neural</b> <b>Network,</b> it is robust to changes in topology. We show that CANOS is accurate across N-1 topological perturbations of a base grid typically used in security-constrained analysis. This paves the way for more efficient optimization of more complex OPF problems which alter grid connectivity such as unit commitment, topology optimization and security-constrained OPF.</p></p class="citation"></blockquote><h3 id=3149--95347-ae-semrl-learning-semantic-association-rules-with-autoencoders-erkan-karabulut-et-al-2024>(31/49 | 95/347) AE SemRL: Learning Semantic Association Rules with Autoencoders (Erkan Karabulut et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erkan Karabulut, Victoria Degeler, Paul Groth. (2024)<br><strong>AE SemRL: Learning Semantic Association Rules with Autoencoders</strong><br><button class=copy-to-clipboard title="AE SemRL: Learning Semantic Association Rules with Autoencoders" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18133v1.pdf filename=2403.18133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Association Rule Mining (ARM) is the task of learning associations among data features in the form of logical rules. Mining association rules from high-dimensional numerical data, for example, time series data from a large number of sensors in a smart environment, is a computationally intensive task. In this study, we propose an <b>Autoencoder-based</b> approach to learn and extract association rules from time series data (AE SemRL). Moreover, we argue that in the presence of semantic information related to time series data sources, semantics can facilitate learning generalizable and explainable association rules. Despite enriching time series data with additional semantic features, AE SemRL makes learning association rules from high-dimensional data feasible. Our experiments show that semantic association rules can be extracted from a latent representation created by an <b>Autoencoder</b> and this method has in the order of hundreds of times faster execution time than state-of-the-art ARM approaches in many scenarios. We believe that this study advances a new way of extracting associations from representations and has the potential to inspire more research in this field.</p></p class="citation"></blockquote><h3 id=3249--96347-recommendation-of-data-free-class-incremental-learning-algorithms-by-simulating-future-data-eva-feillet-et-al-2024>(32/49 | 96/347) Recommendation of data-free class-incremental learning algorithms by simulating future data (Eva Feillet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eva Feillet, Adrian Popescu, Céline Hudelot. (2024)<br><strong>Recommendation of data-free class-incremental learning algorithms by simulating future data</strong><br><button class=copy-to-clipboard title="Recommendation of data-free class-incremental learning algorithms by simulating future data" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18132v1.pdf filename=2403.18132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class-incremental learning deals with sequential data streams composed of batches of classes. Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored. However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings. To solve this problem, we introduce an algorithm <b>recommendation</b> method that simulates the future data stream. Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain. We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting. We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings. Our method outperforms competitive baselines, and performance is close to that of an oracle choosing the best algorithm in each setting. This work contributes to facilitate the practical deployment of incremental learning.</p></p class="citation"></blockquote><h3 id=3349--97347-a-correction-of-pseudo-log-likelihood-method-shi-feng-et-al-2024>(33/49 | 97/347) A Correction of Pseudo Log-Likelihood Method (Shi Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shi Feng, Nuoya Xiong, Zhijie Zhang, Wei Chen. (2024)<br><strong>A Correction of Pseudo Log-Likelihood Method</strong><br><button class=copy-to-clipboard title="A Correction of Pseudo Log-Likelihood Method" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18127v1.pdf filename=2403.18127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pseudo log-likelihood is a type of maximum likelihood estimation (MLE) method used in various fields including contextual <b>bandits,</b> influence maximization of social networks, and causal <b>bandits.</b> However, in previous literature \citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}, the log-likelihood function may not be bounded, which may result in the algorithm they proposed not well-defined. In this paper, we give a counterexample that the maximum pseudo log-likelihood estimation fails and then provide a solution to correct the algorithms in \citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}.</p></p class="citation"></blockquote><h3 id=3449--98347-compressed-multi-task-embeddings-for-data-efficient-downstream-training-and-inference-in-earth-observation-carlos-gomes-et-al-2024>(34/49 | 98/347) Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation (Carlos Gomes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Gomes, Thomas Brunschwiler. (2024)<br><strong>Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation</strong><br><button class=copy-to-clipboard title="Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17886v1.pdf filename=2403.17886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt <b>foundation</b> <b>models</b> (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient yet performant approach for multi-task EO modelling.</p></p class="citation"></blockquote><h3 id=3549--99347-empowering-data-mesh-with-federated-learning-haoyuan-li-et-al-2024>(35/49 | 99/347) Empowering Data Mesh with Federated Learning (Haoyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyuan Li, Salman Toor. (2024)<br><strong>Empowering Data Mesh with Federated Learning</strong><br><button class=copy-to-clipboard title="Empowering Data Mesh with Federated Learning" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17878v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17878v2.pdf filename=2403.17878v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the <b>federated</b> <b>governance</b> to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations. To this end, we introduce a pioneering approach that incorporates <b>Federated</b> <b>Learning</b> into Data Mesh. To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of <b>federated</b> <b>learning</b> methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture.</p></p class="citation"></blockquote><h3 id=3649--100347-tractoracle-towards-an-anatomically-informed-reward-function-for-rl-based-tractography-antoine-théberge-et-al-2024>(36/49 | 100/347) TractOracle: towards an anatomically-informed reward function for RL-based tractography (Antoine Théberge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antoine Théberge, Maxime Descoteaux, Pierre-Marc Jodoin. (2024)<br><strong>TractOracle: towards an anatomically-informed reward function for RL-based tractography</strong><br><button class=copy-to-clipboard title="TractOracle: towards an anatomically-informed reward function for RL-based tractography" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17845v1.pdf filename=2403.17845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL)-based tractography is a competitive alternative to machine learning and classical tractography algorithms due to its high anatomical accuracy obtained without the need for any annotated data. However, the reward functions so far used to train RL agents do not encapsulate anatomical knowledge which causes agents to generate spurious false positives tracts. In this paper, we propose a new RL tractography system, TractOracle, which relies on a reward network trained for streamline classification. This network is used both as a reward function during training as well as a mean for stopping the tracking process early and thus reduce the number of false positive streamlines. This makes our system a unique method that evaluates and reconstructs WM streamlines at the same time. We report an improvement of true positive ratios by almost 20% and a reduction of 3x of false positive ratios on one dataset and an increase between 2x and 7x in the number true positive streamlines on another dataset.</p></p class="citation"></blockquote><h3 id=3749--101347-gpfl-a-gradient-projection-based-client-selection-framework-for-efficient-federated-learning-shijie-na-et-al-2024>(37/49 | 101/347) GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning (Shijie Na et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijie Na, Yuzhi Liang, Siu-Ming Yiu. (2024)<br><strong>GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning</strong><br><button class=copy-to-clipboard title="GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17833v1.pdf filename=2403.17833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> client selection is crucial for determining participant clients while balancing model accuracy and communication efficiency. Existing methods have limitations in handling data heterogeneity, computational burdens, and independent client treatment. To address these challenges, we propose GPFL, which measures client value by comparing local and global descent directions. We also employ an Exploit-Explore mechanism to enhance performance. Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL outperforms baselines in Non-IID scenarios, achieving over 9% improvement in FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times through pre-selection and parameter reuse in <b>federated</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=3849--102347-have-faith-in-faithfulness-going-beyond-circuit-overlap-when-finding-model-mechanisms-michael-hanna-et-al-2024>(38/49 | 102/347) Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms (Michael Hanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Hanna, Sandro Pezzelle, Yonatan Belinkov. (2024)<br><strong>Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms</strong><br><button class=copy-to-clipboard title="Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Causal Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17806v1.pdf filename=2403.17806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM&rsquo;s circuit by performing <b>causal</b> <b>interventions</b> on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model&rsquo;s performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faithful than those found using EAP-IG, even though both have high node overlap with circuits found previously using <b>causal</b> <b>interventions.</b> We conclude more generally that when using circuits to compare the mechanisms models use to solve tasks, faithfulness, not overlap, is what should be measured.</p></p class="citation"></blockquote><h3 id=3949--103347-mep-multiple-kernel-learning-enhancing-relative-positional-encoding-length-extrapolation-weiguo-gao-2024>(39/49 | 103/347) MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation (Weiguo Gao, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiguo Gao. (2024)<br><strong>MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation</strong><br><button class=copy-to-clipboard title="MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17698v1.pdf filename=2403.17698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When the predicted sequence length exceeds the length seen during training, the <b>transformer&rsquo;s</b> inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kernel function adheres to a consistent mean weight coefficient, harnessing the synergistic advantages of different kernels to formulate an innovative bias function. Subsequently, specific slopes are tailored for each kernel function, applying penalties at varying rates, to enhance the model&rsquo;s extrapolation capabilities. Finally, this bias is seamlessly incorporated as a penalty to the post-softmax scores. We present two distinct versions of our method: a parameter-free variant that requires no new learnable parameters, which enhances length extrapolation capabilities without compromising training efficiency, and a parameterized variant capable of integrating state-of-the-art techniques. Empirical evaluations across diverse datasets have demonstrated that both variants of our method achieve state-of-the-art performance, outperforming traditional parameter-free and parameterized approaches.</p></p class="citation"></blockquote><h3 id=4049--104347-how-private-is-dp-sgd-lynn-chua-et-al-2024>(40/49 | 104/347) How Private is DP-SGD? (Lynn Chua et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang. (2024)<br><strong>How Private is DP-SGD?</strong><br><button class=copy-to-clipboard title="How Private is DP-SGD?" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DS, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17673v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17673v1.pdf filename=2403.17673v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD.</p></p class="citation"></blockquote><h3 id=4149--105347-mixing-artificial-and-natural-intelligence-from-statistical-mechanics-to-ai-and-back-to-turbulence-michael-et-al-2024>(41/49 | 105/347) Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence (Michael et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael, Chertkov. (2024)<br><strong>Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence</strong><br><button class=copy-to-clipboard title="Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-stat-mech, cs-AI, cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17993v1.pdf filename=2403.17993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through <b>Diffusion</b> <b>Models</b> rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.</p></p class="citation"></blockquote><h3 id=4249--106347-vdsc-enhancing-exploration-timing-with-value-discrepancy-and-state-counts-marius-captari-et-al-2024>(42/49 | 106/347) VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts (Marius Captari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marius Captari, Remo Sasso, Matthia Sabatelli. (2024)<br><strong>VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts</strong><br><button class=copy-to-clipboard title="VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17542v1.pdf filename=2403.17542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the considerable attention given to the questions of \textit{how much} and \textit{how to} explore in deep <b>reinforcement</b> <b>learning,</b> the investigation into \textit{when} to explore remains relatively less researched. While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\epsilon$-greedy, persist in outperforming them across a broader spectrum of domains. The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains. The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent&rsquo;s internal state. In this paper, we propose to leverage the agent&rsquo;s internal state to decide \textit{when} to explore, addressing the shortcomings of blind switching mechanisms. We present Value Discrepancy and State Counts through homeostasis (VDSC), a novel approach for efficient exploration timing. Experimental results on the Atari suite demonstrate the superiority of our strategy over traditional methods such as $\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like Noisy Nets.</p></p class="citation"></blockquote><h3 id=4349--107347-boosting-adversarial-training-via-fisher-rao-norm-based-regularization-xiangyu-yin-et-al-2024>(43/49 | 107/347) Boosting Adversarial Training via Fisher-Rao Norm-based Regularization (Xiangyu Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Yin, Wenjie Ruan. (2024)<br><strong>Boosting Adversarial Training via Fisher-Rao Norm-based Regularization</strong><br><button class=copy-to-clipboard title="Boosting Adversarial Training via Fisher-Rao Norm-based Regularization" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17520v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17520v1.pdf filename=2403.17520v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>training</b> is extensively utilized to improve the <b>adversarial</b> <b>robustness</b> of deep neural networks. Yet, mitigating the degradation of standard generalization performance in <b>adversarial-trained</b> <b>models</b> remains an open problem. This paper attempts to resolve this issue through the lens of model complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant metric for model complexity, to establish the non-trivial bounds of the Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer Perceptron. Then we generalize a complexity-related variable, which is sensitive to the changes in model width and the trade-off factors in <b>adversarial</b> <b>training.</b> Moreover, intensive empirical evidence validates that this variable highly correlates with the generalization gap of Cross-Entropy loss between <b>adversarial-trained</b> <b>and</b> standard-trained models, especially during the initial and final phases of the training process. Building upon this observation, we propose a novel regularization framework, called Logit-Oriented <b>Adversarial</b> <b>Training</b> (LOAT), which can mitigate the trade-off between robustness and accuracy while imposing only a negligible increase in computational overhead. Our extensive experiments demonstrate that the proposed regularization strategy can boost the performance of the prevalent <b>adversarial</b> <b>training</b> algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT, across various network architectures. Our code will be available at <a href=https://github.com/TrustAI/LOAT>https://github.com/TrustAI/LOAT</a>.</p></p class="citation"></blockquote><h3 id=4449--108347-a-unified-kernel-for-neural-network-learning-shao-qun-zhang-et-al-2024>(44/49 | 108/347) A Unified Kernel for Neural Network Learning (Shao-Qun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shao-Qun Zhang, Zong-Yi Chen, Yong-Ming Tian, Xun Lu. (2024)<br><strong>A Unified Kernel for Neural Network Learning</strong><br><button class=copy-to-clipboard title="A Unified Kernel for Neural Network Learning" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17467v1.pdf filename=2403.17467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and <b>Gaussian</b> <b>processes.</b> Two predominant approaches have emerged: the Neural Network <b>Gaussian</b> <b>Process</b> (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which characterizes the learning dynamics of neural networks with gradient descents and parameter initialization. The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoretically characterize the uniform tightness and learning convergence of the UNK kernel, providing comprehensive insights into this unified kernel. Experimental results underscore the effectiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=4549--109347-imitating-cost-constrained-behaviors-in-reinforcement-learning-qian-shao-et-al-2024>(45/49 | 109/347) Imitating Cost-Constrained Behaviors in Reinforcement Learning (Qian Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Shao, Pradeep Varakantham, Shih-Fen Cheng. (2024)<br><strong>Imitating Cost-Constrained Behaviors in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Imitating Cost-Constrained Behaviors in Reinforcement Learning" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17456v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17456v2.pdf filename=2403.17456v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse <b>reinforcement</b> <b>learning</b> has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the time available. In such problems, imitation learning is challenging as decisions are not only dictated by the reward model but are also dependent on a cost-constrained model. In this paper, we provide multiple methods that match expert distributions in the presence of trajectory cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to find a good trade-off between expected return and minimizing constraint violation; and (c) Cost-violation-based alternating gradient. We empirically show that leading imitation learning approaches imitate cost-constrained behaviors poorly and our meta-gradient-based approach achieves the best performance.</p></p class="citation"></blockquote><h3 id=4649--110347-on-permutation-invariant-neural-networks-masanari-kimura-et-al-2024>(46/49 | 110/347) On permutation-invariant neural networks (Masanari Kimura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masanari Kimura, Ryotaro Shimizu, Yuki Hirakawa, Ryosuke Goto, Yuki Saito. (2024)<br><strong>On permutation-invariant neural networks</strong><br><button class=copy-to-clipboard title="On permutation-invariant neural networks" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17410v1.pdf filename=2403.17410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and <b>Transformers</b> has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of the diverse problem settings and ongoing research efforts pertaining to neural networks that approximate set functions. By delving into the intricacies of these approaches and elucidating the associated challenges, the survey aims to equip readers with a comprehensive understanding of the field. Through this comprehensive perspective, we hope that researchers can gain valuable insights into the potential applications, inherent limitations, and future directions of set-based neural networks. Indeed, from this survey we gain two insights: i) Deep Sets and its variants can be generalized by differences in the aggregation function, and ii) the behavior of Deep Sets is sensitive to the choice of the aggregation function. From these observations, we show that Deep Sets, one of the well-known permutation-invariant neural networks, can be generalized in the sense of a quasi-arithmetic mean.</p></p class="citation"></blockquote><h3 id=4749--111347-not-all-federated-learning-algorithms-are-created-equal-a-performance-evaluation-study-gustav-a-baumgart-et-al-2024>(47/49 | 111/347) Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study (Gustav A. Baumgart et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gustav A. Baumgart, Jaemin Shin, Ali Payani, Myungjin Lee, Ramana Rao Kompella. (2024)<br><strong>Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study</strong><br><button class=copy-to-clipboard title="Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17287v1.pdf filename=2403.17287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) emerged as a practical approach to training a model from decentralized data. The proliferation of FL led to the development of numerous FL algorithms and mechanisms. Many prior efforts have given their primary focus on accuracy of those approaches, but there exists little understanding of other aspects such as computational overheads, performance and training stability, etc. To bridge this gap, we conduct extensive performance evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi, FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source <b>federated</b> <b>learning</b> framework called Flame. Our comprehensive measurement study reveals that no single algorithm works best across different performance metrics. A few key observations are: (1) While some state-of-the-art algorithms achieve higher accuracy than others, they incur either higher computation overheads (FedDyn) or communication overheads (SCAFFOLD). (2) Recent algorithms present smaller standard deviation in accuracy across clients than FedAvg, indicating that the advanced algorithms&rsquo; performances are stable. (3) However, algorithms such as FedDyn and SCAFFOLD are more prone to catastrophic failures without the support of additional techniques such as gradient clipping. We hope that our empirical study can help the community to build best practices in evaluating FL algorithms.</p></p class="citation"></blockquote><h3 id=4849--112347-forest-ore-mining-optimal-rule-ensemble-to-interpret-random-forest-models-haddouchi-maissae-et-al-2024>(48/49 | 112/347) Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest models (Haddouchi Maissae et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haddouchi Maissae, Berrado Abdelaziz. (2024)<br><strong>Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest models</strong><br><button class=copy-to-clipboard title="Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest models" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 8<br>Keywords: Benchmarking, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17588v1.pdf filename=2403.17588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random Forest (RF) is well-known as an efficient ensemble learning method in terms of predictive performance. It is also considered a <b>Black</b> <b>Box</b> because of its hundreds of deep decision trees. This lack of interpretability can be a real drawback for acceptance of RF models in several real-world applications, especially those affecting one&rsquo;s lives, such as in healthcare, security, and law. In this work, we present Forest-ORE, a method that makes RF interpretable via an optimized rule ensemble (ORE) for local and global interpretation. Unlike other rule-based approaches aiming at interpreting the RF model, this method simultaneously considers several parameters that influence the choice of an interpretable rule ensemble. Existing methods often prioritize predictive performance over interpretability coverage and do not provide information about existing overlaps or interactions between rules. Forest-ORE uses a mixed-integer optimization program to build an ORE that considers the trade-off between predictive performance, interpretability coverage, and model size (size of the rule ensemble, rule lengths, and rule overlaps). In addition to providing an ORE competitive in predictive performance with RF, this method enriches the ORE through other rules that afford complementary information. It also enables monitoring of the rule selection process and delivers various metrics that can be used to generate a graphical representation of the final model. This framework is illustrated through an example, and its robustness is assessed through 36 <b>benchmark</b> datasets. A comparative analysis of well-known methods shows that Forest-ORE provides an excellent trade-off between predictive performance, interpretability coverage, and model size.</p></p class="citation"></blockquote><h3 id=4949--113347-incorporating-exponential-smoothing-into-mlp-a-simple-but-effective-sequence-model-jiqun-chu-et-al-2024>(49/49 | 113/347) Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model (Jiqun Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiqun Chu, Zuoquan Lin. (2024)<br><strong>Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model</strong><br><button class=copy-to-clipboard title="Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17445v1.pdf filename=2403.17445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA <b>benchmark.</b></p></p class="citation"></blockquote><h2 id=csir-16>cs.IR (16)</h2><h3 id=116--114347-large-language-models-enhanced-collaborative-filtering-zhongxiang-sun-et-al-2024>(1/16 | 114/347) Large Language Models Enhanced Collaborative Filtering (Zhongxiang Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, Jun Xu. (2024)<br><strong>Large Language Models Enhanced Collaborative Filtering</strong><br><button class=copy-to-clipboard title="Large Language Models Enhanced Collaborative Filtering" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 93<br>Keywords: Benchmarking, Knowledge Distillation, Recommendation, Recommender System, Reasoning, In-context Learning, In-context Learning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17688v1.pdf filename=2403.17688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have attracted considerable interest among researchers to leverage these models to enhance <b>Recommender</b> <b>Systems</b> (RSs). Existing work predominantly utilizes <b>LLMs</b> to generate knowledge-rich texts or utilizes <b>LLM-derived</b> embeddings as features to improve RSs. Although the extensive world knowledge embedded in <b>LLMs</b> generally benefits RSs, the application can only take limited number of users and items as inputs, without adequately exploiting collaborative filtering information. Considering its crucial role in RSs, one key challenge in enhancing RSs with <b>LLMs</b> lies in providing better collaborative filtering information through <b>LLMs.</b> In this paper, drawing inspiration from the <b>in-context</b> <b>learning</b> and chain of thought <b>reasoning</b> in <b>LLMs,</b> we propose the <b>Large</b> <b>Language</b> <b>Models</b> enhanced Collaborative Filtering <b>(LLM-CF)</b> framework, which distils the world knowledge and <b>reasoning</b> capabilities of <b>LLMs</b> into collaborative filtering. We also explored a concise and efficient <b>instruction-tuning</b> <b>method,</b> which improves the <b>recommendation</b> capabilities of <b>LLMs</b> while preserving their general functionalities (e.g., not decreasing on the <b>LLM</b> <b>benchmark).</b> Comprehensive experiments on three real-world datasets demonstrate that <b>LLM-CF</b> significantly enhances several backbone <b>recommendation</b> models and consistently outperforms competitive baselines, showcasing its effectiveness in <b>distilling</b> the world knowledge and <b>reasoning</b> capabilities of <b>LLM</b> into collaborative filtering.</p></p class="citation"></blockquote><h3 id=216--115347-twolar-a-two-step-llm-augmented-distillation-method-for-passage-reranking-davide-baldelli-et-al-2024>(2/16 | 115/347) TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking (Davide Baldelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Baldelli, Junfeng Jiang, Akiko Aizawa, Paolo Torroni. (2024)<br><strong>TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking</strong><br><button class=copy-to-clipboard title="TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Rerank, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17759v1.pdf filename=2403.17759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present TWOLAR: a two-stage pipeline for passage <b>reranking</b> based on the <b>distillation</b> of knowledge from <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM).</b> TWOLAR introduces a new scoring strategy and a <b>distillation</b> process consisting in the creation of a novel and diverse training dataset. The dataset consists of 20K queries, each associated with a set of documents retrieved via four distinct retrieval methods to ensure diversity, and then <b>reranked</b> by exploiting the <b>zero-shot</b> <b>reranking</b> capabilities of an <b>LLM.</b> Our ablation studies demonstrate the contribution of each new component we introduced. Our experimental results show that TWOLAR significantly enhances the document <b>reranking</b> ability of the underlying model, matching and in some cases even outperforming state-of-the-art models with three orders of magnitude more parameters on the TREC-DL test sets and the <b>zero-shot</b> evaluation <b>benchmark</b> BEIR. To facilitate future work we release our data set, <b>finetuned</b> models, and code.</p></p class="citation"></blockquote><h3 id=316--116347-mind-your-language-a-multilingual-dataset-for-cross-lingual-news-recommendation-andreea-iana-et-al-2024>(3/16 | 116/347) MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation (Andreea Iana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreea Iana, Goran Glavaš, Heiko Paulheim. (2024)<br><strong>MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation</strong><br><button class=copy-to-clipboard title="MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: H-3-3, cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, Low-Resource, Recommendation, Zero-shot, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17876v1.pdf filename=2403.17876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital news platforms use news recommenders as the main instrument to cater to the individual information needs of readers. Despite an increasingly language-diverse online community, in which many Internet users consume news in multiple languages, the majority of news <b>recommendation</b> focuses on major, resource-rich languages, and English in particular. Moreover, nearly all news <b>recommendation</b> efforts assume monolingual news consumption, whereas more and more users tend to consume information in at least two languages. Accordingly, the existing body of work on news <b>recommendation</b> suffers from a lack of publicly available multilingual <b>benchmarks</b> that would catalyze development of news recommenders effective in multilingual settings and for <b>low-resource</b> languages. Aiming to fill this gap, we introduce xMIND, an open, multilingual news <b>recommendation</b> dataset derived from the English MIND dataset using <b>machine</b> <b>translation,</b> covering a set of 14 linguistically and geographically diverse languages, with digital footprints of varying sizes. Using xMIND, we systematically <b>benchmark</b> several state-of-the-art content-based neural news recommenders (NNRs) in both <b>zero-shot</b> (ZS-XLT) and <b>few-shot</b> (FS-XLT) cross-lingual transfer scenarios, considering both monolingual and bilingual news consumption patterns. Our findings reveal that (i) current NNRs, even when based on a multilingual language model, suffer from substantial performance losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT training has limited benefits, particularly when combined with a bilingual news consumption. Our findings thus warrant a broader research effort in multilingual and cross-lingual news <b>recommendation.</b> The xMIND dataset is available at <a href=https://github.com/andreeaiana/xMIND>https://github.com/andreeaiana/xMIND</a>.</p></p class="citation"></blockquote><h3 id=416--117347-afdgcf-adaptive-feature-de-correlation-graph-collaborative-filtering-for-recommendations-wei-wu-et-al-2024>(4/16 | 117/347) AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations (Wei Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Wu, Chao Wang, Dazhong Shen, Chuan Qin, Liyi Chen, Hui Xiong. (2024)<br><strong>AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations</strong><br><button class=copy-to-clipboard title="AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17416v1.pdf filename=2403.17416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collaborative filtering methods based on <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have witnessed significant success in <b>recommender</b> <b>systems</b> (RS), capitalizing on their ability to capture collaborative signals within intricate user-item relationships via <b>message-passing</b> mechanisms. However, these <b>GNN-based</b> RS inadvertently introduce excess linear correlation between user and item embeddings, contradicting the goal of providing personalized <b>recommendations.</b> While existing research predominantly ascribes this flaw to the over-smoothing problem, this paper underscores the critical, often overlooked role of the over-correlation issue in diminishing the effectiveness of <b>GNN</b> representations and subsequent <b>recommendation</b> performance. Up to now, the over-correlation issue remains unexplored in RS. Meanwhile, how to mitigate the impact of over-correlation while preserving collaborative filtering signals is a significant challenge. To this end, this paper aims to address the aforementioned gap by undertaking a comprehensive study of the over-correlation issue in <b>graph</b> <b>collaborative</b> <b>filtering</b> models. Firstly, we present empirical evidence to demonstrate the widespread prevalence of over-correlation in these models. Subsequently, we dive into a theoretical analysis which establishes a pivotal connection between the over-correlation and over-smoothing issues. Leveraging these insights, we introduce the Adaptive Feature De-correlation <b>Graph</b> <b>Collaborative</b> <b>Filtering</b> (AFDGCF) framework, which dynamically applies correlation penalties to the feature dimensions of the representation matrix, effectively alleviating both over-correlation and over-smoothing issues. The efficacy of the proposed framework is corroborated through extensive experiments conducted with four representative <b>graph</b> <b>collaborative</b> <b>filtering</b> models across four publicly available datasets.</p></p class="citation"></blockquote><h3 id=516--118347-retentive-decision-transformer-with-adaptive-masking-for-reinforcement-learning-based-recommendation-systems-siyu-wang-et-al-2024>(5/16 | 118/347) Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems (Siyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyu Wang, Xiaocong Chen, Lina Yao. (2024)<br><strong>Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems</strong><br><button class=copy-to-clipboard title="Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, Recommender System, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17634v1.pdf filename=2403.17634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning-based</b> <b>Recommender</b> <b>Systems</b> (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the <b>transformer</b> architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurations. This adaptive approach selectively masks input tokens, transforming the <b>recommendation</b> task into an inference challenge based on varying token subsets, thereby enhancing the agent&rsquo;s ability to infer across diverse trajectory lengths. Furthermore, we incorporate a multi-scale segmented retention mechanism that facilitates efficient modeling of long sequences, significantly enhancing computational efficiency. Our experimental analysis, conducted on both online simulator and offline datasets, clearly demonstrates the advantages of our proposed method.</p></p class="citation"></blockquote><h3 id=616--119347-an-empirical-study-of-training-id-agnostic-multi-modal-sequential-recommenders-youhua-li-et-al-2024>(6/16 | 119/347) An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders (Youhua Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youhua Li, Hanwen Du, Yongxin Ni, Yuanqi He, Junchen Fu, Xiangyan Liu, Qi Guo. (2024)<br><strong>An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders</strong><br><button class=copy-to-clipboard title="An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 36<br>Keywords: Knowledge Distillation, Multi-modal, Multi-modal, Recommendation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17372v1.pdf filename=2403.17372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential <b>Recommendation</b> (SR) aims to predict future user-item interactions based on historical interactions. While many SR approaches concentrate on user IDs and item IDs, the human perception of the world through <b>multi-modal</b> signals, like text and images, has inspired researchers to delve into constructing SR from <b>multi-modal</b> information without using IDs. However, the complexity of <b>multi-modal</b> learning manifests in diverse feature extractors, fusion methods, and pre-trained models. Consequently, designing a simple and universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential \textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable challenge. We systematically <b>summarize</b> the existing <b>multi-modal</b> related SR methods and <b>distill</b> the essence into four core components: visual encoder, text encoder, <b>multimodal</b> fusion module, and sequential architecture. Along these dimensions, we dissect the model designs, and answer the following sub-questions: First, we explore how to construct MMSR from scratch, ensuring its performance either on par with or exceeds existing SR methods without complex techniques. Second, we examine if MMSR can benefit from existing <b>multi-modal</b> pre-training paradigms. Third, we assess MMSR&rsquo;s capability in tackling common challenges like cold start and domain transferring. Our experiment results across four real-world <b>recommendation</b> scenarios demonstrate the great potential ID-agnostic <b>multi-modal</b> sequential <b>recommendation.</b> Our framework can be found at: <a href=https://github.com/MMSR23/MMSR>https://github.com/MMSR23/MMSR</a>.</p></p class="citation"></blockquote><h3 id=716--120347-search-and-society-reimagining-information-access-for-radical-futures-bhaskar-mitra-2024>(7/16 | 120/347) Search and Society: Reimagining Information Access for Radical Futures (Bhaskar Mitra, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhaskar Mitra. (2024)<br><strong>Search and Society: Reimagining Information Access for Radical Futures</strong><br><button class=copy-to-clipboard title="Search and Society: Reimagining Information Access for Radical Futures" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Fairness, Information Retrieval, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17901v1.pdf filename=2403.17901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>retrieval</b> (IR) technologies and research are undergoing transformative changes. It is our perspective that the community should accept this opportunity to re-center our research agendas on societal needs while dismantling the artificial separation between the work on <b>fairness,</b> accountability, transparency, and ethics in IR and the rest of IR research. Instead of adopting a reactionary strategy of trying to mitigate potential social harms from emerging technologies, the community should aim to proactively set the research agenda for the kinds of systems we should build inspired by diverse explicitly stated sociotechnical imaginaries. The sociotechnical imaginaries that underpin the design and development of <b>information</b> <b>access</b> technologies needs to be explicitly articulated, and we need to develop theories of change in context of these diverse perspectives. Our guiding future imaginaries must be informed by other academic fields, such as democratic theory and critical theory, and should be co-developed with social science scholars, legal scholars, civil rights and social justice activists, and artists, among others. In this perspective paper, we motivate why the community must consider this radical shift in how we do research and what we work on, and sketch a path forward towards this transformation.</p></p class="citation"></blockquote><h3 id=816--121347-eulerformer-sequential-user-behavior-modeling-with-complex-vector-attention-zhen-tian-et-al-2024>(8/16 | 121/347) EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention (Zhen Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, Ji-Rong Wen. (2024)<br><strong>EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention</strong><br><button class=copy-to-clipboard title="EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17729v1.pdf filename=2403.17729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To capture user preference, <b>transformer</b> models have been widely applied to model sequential user behavior data. The core of <b>transformer</b> architecture lies in the <b>self-attention</b> mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel <b>transformer</b> variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler&rsquo;s formula, enabling the unified modeling of both semantic and positional information in a complex rotation form.Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts.Furthermore, a phase <b>contrastive</b> <b>learning</b> task is proposed to improve the anisotropy of contextual representations in EulerFormer. Our theoretical framework possesses a high degree of completeness and generality. It is more robust to semantic variations and possesses moresuperior theoretical properties in principle. Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach.</p></p class="citation"></blockquote><h3 id=916--122347-cognitively-biased-users-interacting-with-algorithmically-biased-results-in-whole-session-search-on-controversial-topics-ben-wang-et-al-2024>(9/16 | 122/347) Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Controversial Topics (Ben Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Wang, Jiqun Liu. (2024)<br><strong>Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Controversial Topics</strong><br><button class=copy-to-clipboard title="Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Controversial Topics" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-HC, cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17286v1.pdf filename=2403.17286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When interacting with <b>information</b> <b>retrieval</b> (IR) systems, users, affected by confirmation biases, tend to select search results that confirm their existing beliefs on socially significant contentious issues. To understand the judgments and attitude changes of users searching online, our study examined how cognitively biased users interact with algorithmically biased search engine result pages (SERPs). We designed three-query search sessions on debated topics under various bias conditions. We recruited 1,321 crowdsourcing participants and explored their attitude changes, search interactions, and the effects of confirmation bias. Three key findings emerged: 1) most attitude changes occur in the initial query of a search session; 2) confirmation bias and result presentation on SERPs affect search behaviors in the current query and perceived familiarity with clicked results in subsequent queries. The bias position also affect attitude changes of users with lower perceived openness to conflicting opinions; 3) Interactions in the first query and and dwell time throughout the session are associated with users&rsquo; attitude changes in different forms. Our study goes beyond traditional <b>simulation-based</b> evaluation settings and simulated rational users, sheds light on the mixed effects of human biases and algorithmic biases in controversial <b>information</b> <b>retrieval</b> tasks, and can inform the design of bias-aware user models, human-centered bias mitigation techniques, and socially responsible intelligent IR systems.</p></p class="citation"></blockquote><h3 id=1016--123347-all-in-one-heterogeneous-interaction-modeling-for-cold-start-rating-prediction-shuheng-fang-et-al-2024>(10/16 | 123/347) All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction (Shuheng Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuheng Fang, Kangfei Zhao, Yu Rong, Zhixun Li, Jeffrey Xu Yu. (2024)<br><strong>All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction</strong><br><button class=copy-to-clipboard title="All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17740v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17740v2.pdf filename=2403.17740v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cold-start rating prediction is a fundamental problem in <b>recommender</b> <b>systems</b> that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social <b>recommendations</b> and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific <b>recommendation</b> task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important interactions via the observed data. In the experiments, we evaluate our model under three cold-start settings on three real-world datasets. The experimental results show that HIRE outperforms other baselines by a large margin. Furthermore, we visualize the inferred interactions of HIRE to confirm the contribution of our model.</p></p class="citation"></blockquote><h3 id=1116--124347-touch-the-core-exploring-task-dependence-among-hybrid-targets-for-recommendation-xing-tang-et-al-2024>(11/16 | 124/347) Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation (Xing Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xing Tang, Yang Qiao, Fuyuan Lyu, Dugang Liu, Xiuqiang He. (2024)<br><strong>Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation</strong><br><button class=copy-to-clipboard title="Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17442v1.pdf filename=2403.17442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As user behaviors become complicated on business platforms, online <b>recommendations</b> focus more on how to touch the core conversions, which are highly related to the interests of platforms. These core conversions are usually continuous targets, such as \textit{watch time}, \textit{revenue}, and so on, whose predictions can be enhanced by previous discrete conversion actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to learn these hybrid targets. However, existing works mainly emphasize investigating the sequential dependence among discrete conversion actions, which neglects the complexity of dependence between discrete conversions and the final continuous conversion. Moreover, simultaneously optimizing hybrid tasks with stronger task dependence will suffer from volatile issues where the core regression task might have a larger influence on other tasks. In this paper, we study the MTL problem with hybrid targets for the first time and propose the model named Hybrid Targets Learning Network (HTLNet) to explore task dependence and enhance optimization. Specifically, we introduce label embedding for each task to explicitly transfer the label information among these tasks, which can effectively explore logical task dependence. We also further design the gradient adjustment regime between the final regression task and other classification tasks to enhance the optimization. Extensive experiments on two offline public datasets and one real-world industrial dataset are conducted to validate the effectiveness of HTLNet. Moreover, online A/B tests on the financial <b>recommender</b> <b>system</b> also show our model has superior improvement.</p></p class="citation"></blockquote><h3 id=1216--125347-end4rec-efficient-noise-decoupling-for-multi-behavior-sequential-recommendation-yongqiang-han-et-al-2024>(12/16 | 125/347) END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation (Yongqiang Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, Enhong Chen. (2024)<br><strong>END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation</strong><br><button class=copy-to-clipboard title="END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17603v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17603v1.pdf filename=2403.17603v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>recommendation</b> systems, users frequently engage in multiple types of behaviors, such as clicking, adding to a cart, and purchasing. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence <b>recommendation</b> model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to compare the valid information in the data with the noisy signal, and seamlessly integrate the two denoising processes to achieve a high degree of decoupling of the noisy signal. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our approach in dealing with multi-behavior sequential <b>recommendation.</b></p></p class="citation"></blockquote><h3 id=1316--126347-masked-multi-domain-network-multi-type-and-multi-scenario-conversion-rate-prediction-with-a-single-model-wentao-ouyang-et-al-2024>(13/16 | 126/347) Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model (Wentao Ouyang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Ouyang, Xiuwu Zhang, Chaofeng Guo, Shukui Ren, Yupei Sui, Kun Zhang, Jinmei Luo, Yunfeng Chen, Dongbo Xu, Xiangzheng Liu, Yanlong Du. (2024)<br><strong>Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model</strong><br><button class=copy-to-clipboard title="Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 10<br>Keywords: Parameter Sharing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17425v1.pdf filename=2403.17425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world advertising systems, conversions have different types in nature and ads can be shown in different display scenarios, both of which highly impact the actual conversion rate (CVR). This results in the multi-type and multi-scenario CVR prediction problem. A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario. 2) Scalability: the model <b>parameter</b> <b>size</b> should be affordable. 3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage. Existing approaches cannot simultaneously satisfy these requirements. For example, building a separate model for each (conversion type, display scenario) pair is neither scalable nor convenient. Building a unified model trained on all the data with conversion type and display scenario included as two features is not accurate enough. In this paper, we propose the Masked Multi-domain Network (MMN) to solve this problem. To achieve the accuracy requirement, we model domain-specific <b>parameters</b> <b>and</b> propose a dynamically weighted loss to account for the loss scale imbalance issue within each mini-batch. To achieve the scalability requirement, we propose a <b>parameter</b> <b>sharing</b> and composition strategy to reduce model <b>parameters</b> <b>from</b> a product space to a sum space. To achieve the convenience requirement, we propose an auto-masking strategy which can take mixed data from all the domains as input. It avoids the overhead caused by data partitioning, individual processing and separate storage. Both offline and online experimental results validate the superiority of MMN for multi-type and multi-scenario CVR prediction. MMN is now the serving model for real-time CVR prediction in UC Toutiao.</p></p class="citation"></blockquote><h3 id=1416--127347-ma4div-multi-agent-reinforcement-learning-for-search-result-diversification-yiqun-chen-et-al-2024>(14/16 | 127/347) MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification (Yiqun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong Ma, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Simiu Gu, Dawei Yin. (2024)<br><strong>MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification</strong><br><button class=copy-to-clipboard title="MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17421v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17421v2.pdf filename=2403.17421v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of &ldquo;greedy selection&rdquo;, i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent <b>reinforcement</b> <b>learning</b> (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experiments on public TREC datasets to demonstrate the effectiveness and potential of MA4DIV. Considering the limited number of queries in public TREC datasets, we construct a large-scale dataset from industry sources and show that MA4DIV achieves substantial improvements in both effectiveness and efficiency than existing baselines on a industrial scale dataset.</p></p class="citation"></blockquote><h3 id=1516--128347-multi-domain-recommendation-to-attract-users-via-domain-preference-modeling-hyuunjun-ju-et-al-2024>(15/16 | 128/347) Multi-Domain Recommendation to Attract Users via Domain Preference Modeling (Hyuunjun Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyuunjun Ju, SeongKu Kang, Dongha Lee, Junyoung Hwang, Sanghwan Jang, Hwanjo Yu. (2024)<br><strong>Multi-Domain Recommendation to Attract Users via Domain Preference Modeling</strong><br><button class=copy-to-clipboard title="Multi-Domain Recommendation to Attract Users via Domain Preference Modeling" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17374v1.pdf filename=2403.17374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, web platforms have been operating various service domains simultaneously. Targeting a platform that operates multiple service domains, we introduce a new task, Multi-Domain <b>Recommendation</b> to Attract Users (MDRAU), which recommends items from multiple <code>unseen'' domains with which each user has not interacted yet, by using knowledge from the user's </code>seen&rsquo;&rsquo; domains. In this paper, we point out two challenges of MDRAU task. First, there are numerous possible combinations of mappings from seen to unseen domains because users have usually interacted with a different subset of service domains. Second, a user might have different preferences for each of the target unseen domains, which requires that <b>recommendations</b> reflect the user&rsquo;s preferences on domains as well as items. To tackle these challenges, we propose DRIP framework that models users&rsquo; preferences at two levels (i.e., domain and item) and learns various seen-unseen domain mappings in a unified way with masked domain modeling. Our extensive experiments demonstrate the effectiveness of DRIP in MDRAU task and its ability to capture users&rsquo; domain-level preferences.</p></p class="citation"></blockquote><h3 id=1616--129347-caselink-inductive-graph-learning-for-legal-case-retrieval-yanran-tang-et-al-2024>(16/16 | 129/347) CaseLink: Inductive Graph Learning for Legal Case Retrieval (Yanran Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanran Tang, Ruihong Qiu, Hongzhi Yin, Xue Li, Zi Huang. (2024)<br><strong>CaseLink: Inductive Graph Learning for Legal Case Retrieval</strong><br><button class=copy-to-clipboard title="CaseLink: Inductive Graph Learning for Legal Case Retrieval" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17780v1.pdf filename=2403.17780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In case law, the precedents are the relevant cases that are used to support the decisions made by the judges and the opinions of lawyers towards a given case. This relevance is referred to as the case-to-case reference relation. To efficiently find relevant cases from a large case pool, retrieval tools are widely used by legal practitioners. Existing legal case retrieval models mainly work by comparing the text representations of individual cases. Although they obtain a decent retrieval accuracy, the intrinsic case connectivity relationships among cases have not been well exploited for case encoding, therefore limiting the further improvement of retrieval performance. In a case pool, there are three types of case connectivity relationships: the case reference relationship, the case semantic relationship, and the case legal charge relationship. Due to the inductive manner in the task of legal case retrieval, using case reference as input is not applicable for testing. Thus, in this paper, a CaseLink model based on inductive <b>graph</b> learning is proposed to utilise the intrinsic case connectivity for legal case retrieval, a novel Global Case <b>Graph</b> is incorporated to represent both the case semantic relationship and the case legal charge relationship. A novel contrastive objective with a regularisation on the degree of case nodes is proposed to leverage the information carried by the case reference relationship to optimise the model. Extensive experiments have been conducted on two <b>benchmark</b> datasets, which demonstrate the state-of-the-art performance of CaseLink. The code has been released on <a href=https://github.com/yanran-tang/CaseLink>https://github.com/yanran-tang/CaseLink</a>.</p></p class="citation"></blockquote><h2 id=csai-26>cs.AI (26)</h2><h3 id=126--130347-evaluating-the-efficacy-of-prompt-engineered-large-multimodal-models-versus-fine-tuned-vision-transformers-in-image-based-security-applications-fouad-trad-et-al-2024>(1/26 | 130/347) Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications (Fouad Trad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fouad Trad, Ali Chehab. (2024)<br><strong>Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications</strong><br><button class=copy-to-clipboard title="Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CR, cs-CV, cs.AI<br>Keyword Score: 86<br>Keywords: Vision Transformer, Fine-tuning, Multi-modal, Multi-modal, Gemini, Transformer, Large Language Model, Large Language Model, Prompt, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17787v1.pdf filename=2403.17787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has led to a parallel rise in the development of <b>Large</b> <b>Multimodal</b> <b>Models</b> (LMMs), such as <b>Gemini-pro,</b> which have begun to transform a variety of applications. These sophisticated <b>multimodal</b> models are designed to interpret and analyze complex data, integrating both textual and visual information on a scale previously unattainable, opening new avenues for a range of applications. This paper investigates the applicability and effectiveness of <b>prompt-engineered</b> <b>Gemini-pro</b> LMMs versus <b>fine-tuned</b> <b>Vision</b> <b>Transformer</b> (ViT) models in addressing critical security challenges. We focus on two distinct tasks: a visually evident task of detecting simple triggers, such as small squares in images, indicative of potential backdoors, and a non-visually evident task of malware classification through visual representations. Our results highlight a significant divergence in performance, with <b>Gemini-pro</b> falling short in accuracy and reliability when compared to <b>fine-tuned</b> ViT models. The ViT models, on the other hand, demonstrate exceptional accuracy, achieving near-perfect performance on both tasks. This study not only showcases the strengths and limitations of <b>prompt-engineered</b> LMMs in cybersecurity applications but also emphasizes the unmatched efficacy of <b>fine-tuned</b> ViT models for precise and dependable tasks.</p></p class="citation"></blockquote><h3 id=226--131347-dont-trust-verify----grounding-llm-quantitative-reasoning-with-autoformalization-jin-peng-zhou-et-al-2024>(2/26 | 131/347) Don&rsquo;t Trust: Verify &ndash; Grounding LLM Quantitative Reasoning with Autoformalization (Jin Peng Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu. (2024)<br><strong>Don&rsquo;t Trust: Verify &ndash; Grounding LLM Quantitative Reasoning with Autoformalization</strong><br><button class=copy-to-clipboard title="Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 60<br>Keywords: GPT, Grounding, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18120v1.pdf filename=2403.18120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLM),</b> such as Google&rsquo;s Minerva and OpenAI&rsquo;s <b>GPT</b> families, are becoming increasingly capable of solving mathematical quantitative <b>reasoning</b> problems. However, they still make unjustified logical and computational errors in their <b>reasoning</b> steps and answers. In this paper, we leverage the fact that if the training corpus of <b>LLMs</b> contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be <b>prompted</b> to translate i.e. autoformalize informal mathematical statements into formal Isabelle code &ndash; which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting &ndash; the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and <b>LLM</b> model sizes. The code can be found at <a href=https://github.com/jinpz/dtv>https://github.com/jinpz/dtv</a>.</p></p class="citation"></blockquote><h3 id=326--132347-visual-hallucination-definition-quantification-and-prescriptive-remediations-vipula-rawte-et-al-2024>(3/26 | 132/347) Visual Hallucination: Definition, Quantification, and Prescriptive Remediations (Vipula Rawte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vipula Rawte, Anku Rani, Harshad Sharma, Neeraj Anand, Krishnav Rajbangshi, Amit Sheth, Amitava Das. (2024)<br><strong>Visual Hallucination: Definition, Quantification, and Prescriptive Remediations</strong><br><button class=copy-to-clipboard title="Visual Hallucination: Definition, Quantification, and Prescriptive Remediations" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 60<br>Keywords: Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17306v1.pdf filename=2403.17306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> However, it&rsquo;s worth noting that hallucination is also quite prevalent in <b>Vision-Language</b> models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA).</b> We delineate eight fine-grained orientations of <b>visual</b> <b>hallucination:</b> <b>i)</b> Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) <b>Visual</b> <b>Illusion,</b> <b>v)</b> Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate <b>Visual</b> <b>HallucInation</b> <b>eLiciTation</b> (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and <b>VQA</b> along with human annotations for the categories as mentioned earlier.</p></p class="citation"></blockquote><h3 id=426--133347-kc-genre-a-knowledge-constrained-generative-re-ranking-method-based-on-large-language-models-for-knowledge-graph-completion-yilin-wang-et-al-2024>(4/26 | 133/347) KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion (Yilin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yilin Wang, Minghao Hu, Zhen Huang, Dongsheng Li, Dong Yang, Xicheng Lu. (2024)<br><strong>KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion</strong><br><button class=copy-to-clipboard title="KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 53<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Information Retrieval, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17532v1.pdf filename=2403.17532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The goal of <b>knowledge</b> <b>graph</b> completion (KGC) is to predict missing facts among entities. Previous methods for KGC re-ranking are mostly built on non-generative language models to obtain the probability of each candidate. Recently, generative <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown outstanding performance on several tasks such as <b>information</b> <b>extraction</b> and dialog systems. Leveraging them for KGC re-ranking is beneficial for leveraging the extensive pre-trained <b>knowledge</b> <b>and</b> powerful generative capabilities. However, it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission. To this end, we introduce KC-GenRe, a <b>knowledge-constrained</b> <b>generative</b> re-ranking method based on <b>LLMs</b> for KGC. To overcome the mismatch issue, we formulate the KGC re-ranking task as a candidate identifier sorting generation problem implemented by generative <b>LLMs.</b> To tackle the misordering issue, we develop a <b>knowledge-guided</b> <b>interactive</b> training method that enhances the identification and ranking of candidates. To address the omission issue, we design a <b>knowledge-augmented</b> <b>constrained</b> inference method that enables contextual <b>prompting</b> and controlled generation, so as to obtain valid rankings. Experimental results show that <b>KG-GenRe</b> achieves state-of-the-art performance on four datasets, with gains of up to 6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and 9.0% and 11.1% compared to that without re-ranking. Extensive analysis demonstrates the effectiveness of components in <b>KG-GenRe.</b></p></p class="citation"></blockquote><h3 id=526--134347-lasil-learner-aware-supervised-imitation-learning-for-long-term-microscopic-traffic-simulation-ke-guo-et-al-2024>(5/26 | 134/347) LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation (Ke Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ke Guo, Zhenwei Miao, Wei Jing, Weiwei Liu, Weizi Li, Dayang Hao, Jia Pan. (2024)<br><strong>LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation</strong><br><button class=copy-to-clipboard title="LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 50<br>Keywords: Autoencoder, Simulation, Simulator, Supervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17601v1.pdf filename=2403.17601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microscopic traffic <b>simulation</b> plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate <b>simulations</b> due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term <b>simulations.</b> In this paper, we propose a novel approach called learner-aware <b>supervised</b> imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a <b>variational</b> <b>autoencoder</b> simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware of learner state distribution. Our method, applied to urban traffic <b>simulation,</b> demonstrates significant improvements over existing state-of-the-art baselines in both short-term microscopic and long-term macroscopic realism when evaluated on the real-world dataset pNEUMA.</p></p class="citation"></blockquote><h3 id=626--135347-aligning-large-language-models-for-enhancing-psychiatric-interviews-through-symptom-delineation-and-summarization-jae-hee-so-et-al-2024>(6/26 | 135/347) Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization (Jae-hee So et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jae-hee So, Joonhwan Chang, Eunji Kim, Junho Na, JiYeon Choi, Jy-yong Sohn, Byung-Hoon Kim, Sang Hui Chu. (2024)<br><strong>Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization</strong><br><button class=copy-to-clipboard title="Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 50<br>Keywords: Large Language Model, Large Language Model, Prompt, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17428v1.pdf filename=2403.17428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where <b>LLMs</b> can contribute substantial value. Here, we explore the use of <b>LLMs</b> for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether <b>LLMs</b> can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) <b>summarize</b> stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of <b>LLMs.</b> Our experimental results show that appropriately <b>prompted</b> <b>LLMs</b> can achieve high performance on both the symptom delineation task and the <b>summarization</b> task. This research contributes to the nascent field of applying <b>LLMs</b> to psychiatric interview and demonstrates their potential effectiveness in aiding mental health practitioners.</p></p class="citation"></blockquote><h3 id=726--136347-alisa-accelerating-large-language-model-inference-via-sparsity-aware-kv-caching-youpeng-zhao-et-al-2024>(7/26 | 136/347) ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching (Youpeng Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youpeng Zhao, Di Wu, Jun Wang. (2024)<br><strong>ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching</strong><br><button class=copy-to-clipboard title="ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-PF, cs.AI<br>Keyword Score: 40<br>Keywords: LLaMA, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17312v1.pdf filename=2403.17312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>Transformer</b> architecture has significantly advanced natural language processing (NLP) and has been foundational in developing <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>LLaMA</b> and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, <b>LLMs</b> present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of <b>LLM</b> inference, KV caching for the attention layers in <b>Transformers</b> can effectively accelerate <b>LLM</b> inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-design solution to address the challenges imposed by KV caching. On the algorithm level, ALISA prioritizes tokens that are most important in generating a new token via a Sparse Window Attention (SWA) algorithm. SWA introduces high sparsity in attention layers and reduces the memory footprint of KV caching at negligible accuracy loss. On the system level, ALISA employs three-phase token-level dynamical scheduling and optimizes the trade-off between caching and recomputation, thus maximizing the overall performance in resource-constrained systems. In a single GPU-CPU system, we demonstrate that under varying workloads, ALISA improves the throughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X, respectively.</p></p class="citation"></blockquote><h3 id=826--137347-towards-explainable-clustering-a-constrained-declarative-based-approach-mathieu-guilbert-et-al-2024>(8/26 | 137/347) Towards Explainable Clustering: A Constrained Declarative based Approach (Mathieu Guilbert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathieu Guilbert, Christel Vrain, Thi-Bich-Hanh Dao. (2024)<br><strong>Towards Explainable Clustering: A Constrained Declarative based Approach</strong><br><button class=copy-to-clipboard title="Towards Explainable Clustering: A Constrained Declarative based Approach" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 33<br>Keywords: Clustering, Explainable AI, Pruning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18101v1.pdf filename=2403.18101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The domain of <b>explainable</b> <b>AI</b> is of interest in all Machine Learning fields, and it is all the more important in <b>clustering,</b> an <b>unsupervised</b> task whose result must be validated by a domain expert. We aim at finding a <b>clustering</b> that has high quality in terms of classic <b>clustering</b> criteria and that is <b>explainable,</b> <b>and</b> we argue that these two dimensions must be considered when building the <b>clustering.</b> We consider that a good global explanation of a <b>clustering</b> should give the characteristics of each cluster taking into account their abilities to describe its objects (coverage) while distinguishing it from the other clusters (discrimination). Furthermore, we aim at leveraging expert knowledge, at different levels, on the structure of the expected <b>clustering</b> or on its explanations. In our framework an explanation of a cluster is a set of patterns, and we propose a novel interpretable constrained <b>clustering</b> method called ECS for declarative <b>clustering</b> with Explainabilty-driven Cluster Selection that integrates structural or domain expert knowledge expressed by means of constraints. It is based on the notion of coverage and discrimination that are formalized at different levels (cluster / <b>clustering),</b> each allowing for exceptions through parameterized thresholds. Our method relies on four steps: generation of a set of partitions, computation of frequent patterns for each cluster, <b>pruning</b> clusters that violates some constraints, and selection of clusters and associated patterns to build an interpretable <b>clustering.</b> This last step is combinatorial and we have developed a Constraint-Programming (CP) model to solve it. The method can integrate prior knowledge in the form of user constraints, both before or in the CP model.</p></p class="citation"></blockquote><h3 id=926--138347-explainable-graph-neural-networks-for-observation-impact-analysis-in-atmospheric-state-estimation-hyeon-ju-jeon-et-al-2024>(9/26 | 138/347) Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation (Hyeon-Ju Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeon-Ju Jeon, Jeon-Ho Kang, In-Hyuk Kwon, O-Joun Lee. (2024)<br><strong>Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation</strong><br><button class=copy-to-clipboard title="Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs.AI<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17384v1.pdf filename=2403.17384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> and explainability methods. We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological <b>graph,</b> <b>extracting</b> <b>$k$-hop</b> subgraphs centered on NWP points. <b>Self-supervised</b> <b>GNNs</b> are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii. The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process. Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.</p></p class="citation"></blockquote><h3 id=1026--139347-addressing-social-misattributions-of-large-language-models-an-hcxai-based-approach-andrea-ferrario-et-al-2024>(10/26 | 139/347) Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach (Andrea Ferrario et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Ferrario, Alberto Termine, Alessandro Facchini. (2024)<br><strong>Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach</strong><br><button class=copy-to-clipboard title="Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Explainable AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17873v1.pdf filename=2403.17873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-centered <b>explainable</b> <b>AI</b> (HCXAI) advocates for the integration of social aspects into AI explanations. Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users. In this work, we suggest extending the ST framework to address the risks of social misattributions in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> particularly in sensitive areas like mental health. In fact <b>LLMs,</b> which are remarkably capable of simulating roles and personas, may lead to mismatches between designers&rsquo; intentions and users&rsquo; perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To address these issues, we propose enhancing the ST framework with a fifth &lsquo;W-question&rsquo; to clarify the specific social attributions assigned to <b>LLMs</b> by its designers and users. This addition aims to bridge the gap between <b>LLM</b> capabilities and user perceptions, promoting the ethically responsible development and use of <b>LLM-based</b> technology.</p></p class="citation"></blockquote><h3 id=1126--140347-practical-applications-of-advanced-cloud-services-and-generative-ai-systems-in-medical-image-analysis-jingyu-xu-et-al-2024>(11/26 | 140/347) Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis (Jingyu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyu Xu, Binbin Wu, Jiaxin Huang, Yulu Gong, Yifan Zhang, Bo Liu. (2024)<br><strong>Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis</strong><br><button class=copy-to-clipboard title="Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs.AI<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Generative AI, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17549v1.pdf filename=2403.17549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The medical field is one of the important fields in the application of artificial intelligence technology. With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field. Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services.The article explores the transformative potential of <b>generative</b> <b>AI</b> in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in <b>anomaly</b> <b>detection,</b> and facilitate image-to-image translation. Despite challenges like model complexity, the applications of <b>generative</b> <b>models</b> in healthcare, including Med-PaLM 2 technology, show promising results. By addressing limitations in dataset size and diversity, these models contribute to more accurate diagnoses and improved patient outcomes. However, ethical considerations and collaboration among stakeholders are essential for responsible implementation. Through experiments leveraging <b>GANs</b> to augment brain tumor MRI datasets, the study demonstrates how <b>generative</b> <b>AI</b> can enhance image quality and diversity, ultimately advancing medical diagnostics and patient care.</p></p class="citation"></blockquote><h3 id=1226--141347-knowledge-powered-recommendation-for-an-improved-diet-water-footprint-saurav-joshi-et-al-2024>(12/26 | 141/347) Knowledge-Powered Recommendation for an Improved Diet Water Footprint (Saurav Joshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saurav Joshi, Filip Ilievski, Jay Pujara. (2024)<br><strong>Knowledge-Powered Recommendation for an Improved Diet Water Footprint</strong><br><button class=copy-to-clipboard title="Knowledge-Powered Recommendation for an Improved Diet Water Footprint" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Recommendation, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17426v1.pdf filename=2403.17426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>According to WWF, 1.1 billion people lack access to water, and 2.7 billion experience water scarcity at least one month a year. By 2025, two-thirds of the world&rsquo;s population may be facing water shortages. This highlights the urgency of managing water usage efficiently, especially in water-intensive sectors like food. This paper proposes a <b>recommendation</b> engine, powered by <b>knowledge</b> <b>graphs,</b> aiming to facilitate sustainable and healthy food consumption. The engine recommends ingredient substitutes in user recipes that improve nutritional value and reduce environmental impact, particularly water footprint. The system architecture includes source identification, <b>information</b> <b>extraction,</b> schema alignment, <b>knowledge</b> <b>graph</b> construction, and user interface development. The research offers a promising tool for promoting healthier eating habits and contributing to water conservation efforts.</p></p class="citation"></blockquote><h3 id=1326--142347-self-clustering-hierarchical-multi-agent-reinforcement-learning-with-extensible-cooperation-graph-qingxu-fu-et-al-2024>(13/26 | 142/347) Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph (Qingxu Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingxu Fu, Tenghai Qiu, Jianqiang Yi, Zhiqiang Pu, Xiaolin Ai. (2024)<br><strong>Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph</strong><br><button class=copy-to-clipboard title="Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Reinforcement Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18056v1.pdf filename=2403.18056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL) has been successful in solving many cooperative challenges. However, classic non-hierarchical MARL algorithms still cannot address various complex multi-agent problems that require hierarchical cooperative behaviors. The cooperative knowledge and policies learned in non-hierarchical algorithms are implicit and not interpretable, thereby restricting the integration of existing knowledge. This paper proposes a novel hierarchical MARL model called Hierarchical Cooperation <b>Graph</b> Learning (HCGL) for solving general multi-agent problems. HCGL has three components: a dynamic Extensible Cooperation <b>Graph</b> (ECG) for achieving self-clustering cooperation; a group of <b>graph</b> operators for adjusting the topology of ECG; and an MARL optimizer for training these <b>graph</b> operators. HCGL&rsquo;s key distinction from other MARL models is that the behaviors of agents are guided by the topology of ECG instead of policy neural networks. ECG is a three-layer <b>graph</b> consisting of an agent node layer, a cluster node layer, and a target node layer. To manipulate the ECG topology in response to changing environmental conditions, four <b>graph</b> operators are trained to adjust the edge connections of ECG dynamically. The hierarchical feature of ECG provides a unique approach to merge primitive actions (actions executed by the agents) and cooperative actions (actions executed by the clusters) into a unified action space, allowing us to integrate fundamental cooperative knowledge into an extensible interface. In our experiments, the HCGL model has shown outstanding performance in multi-agent <b>benchmarks</b> with sparse rewards. We also verify that HCGL can easily be transferred to large-scale scenarios with high <b>zero-shot</b> transfer success rates.</p></p class="citation"></blockquote><h3 id=1426--143347-out-of-distribution-rumor-detection-via-test-time-adaptation-xiang-tao-et-al-2024>(14/26 | 143/347) Out-of-distribution Rumor Detection via Test-Time Adaptation (Xiang Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Tao, Mingqing Zhang, Qiang Liu, Shu Wu, Liang Wang. (2024)<br><strong>Out-of-distribution Rumor Detection via Test-Time Adaptation</strong><br><button class=copy-to-clipboard title="Out-of-distribution Rumor Detection via Test-Time Adaptation" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Graph, Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17735v1.pdf filename=2403.17735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Existing methods for rumor detection have achieved good performance, as they have collected enough corpus from the same data <b>distribution</b> <b>for</b> model training. However, significant <b>distribution</b> <b>shifts</b> between the training data and real-world test data occur due to differences in news topics, social media platforms, languages and the variance in propagation scale caused by news popularity. This leads to a substantial decline in the performance of these existing methods in <b>Out-Of-Distribution</b> (OOD) situations. To address this problem, we propose a simple and efficient method named Test-time Adaptation for Rumor Detection under <b>distribution</b> <b>shifts</b> (TARD). This method models the propagation of news in the form of a propagation <b>graph,</b> and builds propagation <b>graph</b> test-time adaptation framework, enhancing the model&rsquo;s adaptability and robustness when facing OOD problems. Extensive experiments conducted on two group datasets collected from real-world social platforms demonstrate that our framework outperforms the state-of-the-art methods in performance.</p></p class="citation"></blockquote><h3 id=1526--144347-a-real-time-rescheduling-algorithm-for-multi-robot-plan-execution-ying-feng-et-al-2024>(15/26 | 144/347) A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution (Ying Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Feng, Adittyo Paul, Zhe Chen, Jiaoyang Li. (2024)<br><strong>A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution</strong><br><button class=copy-to-clipboard title="A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs-RO, cs.AI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18145v1.pdf filename=2403.18145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One area of research in multi-agent path finding is to determine how replanning can be efficiently achieved in the case of agents being delayed during execution. One option is to reschedule the passing order of agents, i.e., the sequence in which agents visit the same location. In response, we propose Switchable-Edge Search (SES), an A*-style algorithm designed to find optimal passing orders. We prove the optimality of SES and evaluate its efficiency via <b>simulations.</b> The best variant of SES takes less than 1 second for small- and medium-sized problems and runs up to 4 times faster than baselines for large-sized problems.</p></p class="citation"></blockquote><h3 id=1626--145347-agentstudio-a-toolkit-for-building-general-virtual-agents-longtao-zheng-et-al-2024>(16/26 | 145/347) AgentStudio: A Toolkit for Building General Virtual Agents (Longtao Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, Shuicheng Yan. (2024)<br><strong>AgentStudio: A Toolkit for Building General Virtual Agents</strong><br><button class=copy-to-clipboard title="AgentStudio: A Toolkit for Building General Virtual Agents" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17918v1.pdf filename=2403.17918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence. Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities. To address this, we introduce AgentStudio, an online, realistic, and <b>multimodal</b> toolkit that covers the entire lifecycle of agent development. This includes environment setups, data collection, agent evaluation, and visualization. The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces. This versatility is further enhanced by AgentStudio&rsquo;s graphical user interfaces, which allow efficient development of datasets and <b>benchmarks</b> in real-world settings. To illustrate, we introduce a visual <b>grounding</b> dataset and a real-world <b>benchmark</b> suite, both created with our graphical interfaces. Furthermore, we present several actionable insights derived from AgentStudio, e.g., general visual <b>grounding,</b> open-ended tool creation, learning from videos, etc. We have open-sourced the environments, datasets, <b>benchmarks,</b> and interfaces to promote research towards developing general virtual agents for the future.</p></p class="citation"></blockquote><h3 id=1726--146347-solution-for-emotion-prediction-competition-of-workshop-on-emotionally-and-culturally-intelligent-ai-shengdong-xu-et-al-2024>(17/26 | 146/347) Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI (Shengdong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengdong Xu, Zhouyang Chi, Yang Yang. (2024)<br><strong>Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI</strong><br><button class=copy-to-clipboard title="Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17683v1.pdf filename=2403.17683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This report provide a detailed description of the method that we explored and proposed in the WECIA Emotion Prediction Competition (EPC), which predicts a person&rsquo;s emotion through an artistic work with a comment. The dataset of this competition is ArtELingo, designed to encourage work on diversity across languages and cultures. The dataset has two main challenges, namely modal imbalance problem and language-cultural differences problem. In order to address this issue, we propose a simple yet effective approach called single-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses on using the single modal message to enhance the performance of <b>multimodal</b> models and a well-designed <b>prompt</b> to reduce cultural differences problem. To clarify, our approach contains two main blocks: (1)XLM-R\cite{conneau2019unsupervised} based unimodal model and X$^2$-VLM\cite{zeng2022x} based <b>multimodal</b> model (2) Emotion-Cultural specific <b>prompt.</b> Our approach ranked first in the final test with a score of 0.627.</p></p class="citation"></blockquote><h3 id=1826--147347-prioritized-league-reinforcement-learning-for-large-scale-heterogeneous-multiagent-systems-qingxu-fu-et-al-2024>(18/26 | 147/347) Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems (Qingxu Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingxu Fu, Zhiqiang Pu, Min Chen, Tenghai Qiu, Jianqiang Yi. (2024)<br><strong>Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems</strong><br><button class=copy-to-clipboard title="Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18057v1.pdf filename=2403.18057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale heterogeneous multiagent systems feature various realistic factors in the real world, such as agents with diverse abilities and overall system cost. In comparison to homogeneous systems, heterogeneous systems offer significant practical advantages. Nonetheless, they also present challenges for multiagent <b>reinforcement</b> <b>learning,</b> including addressing the non-stationary problem and managing an imbalanced number of agents with different types. We propose a Prioritized Heterogeneous League <b>Reinforcement</b> <b>Learning</b> (PHLRL) method to address large-scale heterogeneous cooperation problems. PHLRL maintains a record of various policies that agents have explored during their training and establishes a heterogeneous league consisting of diverse policies to aid in future policy optimization. Furthermore, we design a prioritized policy gradient approach to compensate for the gap caused by differences in the number of different types of agents. Next, we use Unreal Engine to design a large-scale heterogeneous cooperation <b>benchmark</b> named Large-Scale Multiagent Operation (LSMO), which is a complex two-team competition scenario that requires collaboration from both ground and airborne agents. We use experiments to show that PHLRL outperforms state-of-the-art methods, including QTRAN and QPLEX in LSMO.</p></p class="citation"></blockquote><h3 id=1926--148347-hierarchical-multi-label-classification-for-fine-level-event-extraction-from-aviation-accident-reports-xinyu-zhao-et-al-2024>(19/26 | 148/347) Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports (Xinyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Zhao, Hao Yan, Yongming Liu. (2024)<br><strong>Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports</strong><br><button class=copy-to-clipboard title="Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17914v1.pdf filename=2403.17914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A large volume of accident reports is recorded in the aviation domain, which greatly values improving aviation safety. To better use those reports, we need to understand the most important events or impact factors according to the accident reports. However, the increasing number of accident reports requires large efforts from domain experts to label those reports. In order to make the labeling process more efficient, many researchers have started developing algorithms to identify the underlying events from accident reports automatically. This article argues that we can identify the events more accurately by leveraging the event taxonomy. More specifically, we consider the problem a hierarchical classification task where we first identify the coarse-level information and then predict the fine-level information. We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into <b>BERT.</b> To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels. The effectiveness of our framework is evaluated with the data collected by National Transportation Safety Board (NTSB). It has been shown that fine-level prediction accuracy is highly improved, and the regularization term can be beneficial to the rare event identification problem.</p></p class="citation"></blockquote><h3 id=2026--149347-using-stratified-sampling-to-improve-lime-image-explanations-muhammad-rashid-et-al-2024>(20/26 | 149/347) Using Stratified Sampling to Improve LIME Image Explanations (Muhammad Rashid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Rashid, Elvio G. Amparore, Enrico Ferrari, Damiano Verda. (2024)<br><strong>Using Stratified Sampling to Improve LIME Image Explanations</strong><br><button class=copy-to-clipboard title="Using Stratified Sampling to Improve LIME Image Explanations" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17742v1.pdf filename=2403.17742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic <b>explainable</b> <b>AI</b> method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling. Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data. We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past. We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator. Experiments show the efficacy of the proposed approach.</p></p class="citation"></blockquote><h3 id=2126--150347-tiny-models-are-the-computational-saver-for-large-models-qingyuan-wang-et-al-2024>(21/26 | 150/347) Tiny Models are the Computational Saver for Large Models (Qingyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyuan Wang, Barry Cardiff, Antoine Frappé, Benoit Larras, Deepu John. (2024)<br><strong>Tiny Models are the Computational Saver for Large Models</strong><br><button class=copy-to-clipboard title="Tiny Models are the Computational Saver for Large Models" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Model Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17726v1.pdf filename=2403.17726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces TinySaver, an early-exit-like dynamic <b>model</b> <b>compression</b> approach which employs tiny <b>models</b> <b>to</b> substitute large <b>models</b> <b>adaptively.</b> Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources. Most existing early exit designs are implemented by attaching additional network branches to the <b>model&rsquo;s</b> <b>backbone.</b> Our study, however, reveals that completely independent tiny <b>models</b> <b>can</b> replace a substantial portion of the larger <b>models&rsquo;</b> <b>job</b> with minimal impact on performance. Employing them as the first exit can remarkably enhance computational efficiency. By searching and employing the most appropriate tiny <b>model</b> <b>as</b> the computational saver for a given large <b>model,</b> <b>the</b> proposed approaches work as a novel and generic method to <b>model</b> <b>compression.</b> This finding will help the research community in exploring new compression methods to address the escalating computational demands posed by rapidly evolving AI <b>models.</b> <b>Our</b> evaluation of this approach in ImageNet-1k classification demonstrates its potential to reduce the number of compute operations by up to 90%, with only negligible losses in performance, across various modern vision <b>models.</b> <b>The</b> code of this work will be available.</p></p class="citation"></blockquote><h3 id=2226--151347-an-extension-based-approach-for-computing-and-verifying-preferences-in-abstract-argumentation-quratul-ain-mahesar-et-al-2024>(22/26 | 151/347) An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation (Quratul-ain Mahesar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quratul-ain Mahesar, Nir Oren, Wamberto W. Vasconcelos. (2024)<br><strong>An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation</strong><br><button class=copy-to-clipboard title="An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17653v1.pdf filename=2403.17653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an extension-based approach for computing and verifying preferences in an abstract argumentation system. Although numerous argumentation semantics have been developed previously for identifying acceptable sets of arguments from an argumentation framework, there is a lack of justification behind their acceptability based on implicit argument preferences. Preference-based argumentation frameworks allow one to determine what arguments are justified given a set of preferences. Our research considers the inverse of the standard <b>reasoning</b> problem, i.e., given an abstract argumentation framework and a set of justified arguments, we compute what the possible preferences over arguments are. Furthermore, there is a need to verify (i.e., assess) that the computed preferences would lead to the acceptable sets of arguments. This paper presents a novel approach and algorithm for exhaustively computing and enumerating all possible sets of preferences (restricted to three identified cases) for a conflict-free set of arguments in an abstract argumentation framework. We prove the soundness, completeness and termination of the algorithm. The research establishes that preferences are determined using an extension-based approach after the evaluation phase (acceptability of arguments) rather than stated beforehand. In this work, we focus our research study on grounded, preferred and stable semantics. We show that the complexity of computing sets of preferences is exponential in the number of arguments, and thus, describe an approximate approach and algorithm to compute the preferences. Furthermore, we present novel algorithms for verifying (i.e., assessing) the computed preferences. We provide details of the implementation of the algorithms (source code has been made available), various experiments performed to evaluate the algorithms and the analysis of the results.</p></p class="citation"></blockquote><h3 id=2326--152347-an-open-source-end-to-end-logic-optimization-framework-for-large-scale-boolean-network-with-reinforcement-learning-zhen-li-et-al-2024>(23/26 | 152/347) An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning (Zhen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Li, Kaixiang Zhu, Xuegong Zhou, Lingli Wang. (2024)<br><strong>An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17395v1.pdf filename=2403.17395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose an open-source end-to-end logic optimization framework for large-scale boolean network with <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=2426--153347-the-pursuit-of-fairness-in-artificial-intelligence-models-a-survey-tahsin-alamgir-kheya-et-al-2024>(24/26 | 153/347) The Pursuit of Fairness in Artificial Intelligence Models: A Survey (Tahsin Alamgir Kheya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tahsin Alamgir Kheya, Mohamed Reda Bouadjenek, Sunil Aryal. (2024)<br><strong>The Pursuit of Fairness in Artificial Intelligence Models: A Survey</strong><br><button class=copy-to-clipboard title="The Pursuit of Fairness in Artificial Intelligence Models: A Survey" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17333v1.pdf filename=2403.17333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence (AI) models are now being utilized in all facets of our lives such as healthcare, education and employment. Since they are used in numerous sensitive environments and make decisions that can be life altering, potential biased outcomes are a pressing matter. Developers should ensure that such models don&rsquo;t manifest any unexpected discriminatory practices like partiality for certain genders, ethnicities or disabled people. With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them. Significant research has been conducted in addressing such issues to ensure models don&rsquo;t intentionally or unintentionally perpetuate bias. This survey offers a synopsis of the different ways researchers have promoted <b>fairness</b> in AI systems. We explore the different definitions of <b>fairness</b> existing in the current literature. We create a comprehensive taxonomy by categorizing different types of bias and investigate cases of biased AI in different application domains. A thorough study is conducted of the approaches and techniques employed by researchers to mitigate bias in AI models. Moreover, we also delve into the impact of biased models on user experience and the ethical considerations to contemplate when developing and deploying such models. We hope this survey helps researchers and practitioners understand the intricate details of <b>fairness</b> and bias in AI systems. By sharing this thorough survey, we aim to promote additional discourse in the domain of equitable and responsible AI.</p></p class="citation"></blockquote><h3 id=2526--154347-learning-traffic-signal-control-via-genetic-programming-xiao-cheng-liao-et-al-2024>(25/26 | 154/347) Learning Traffic Signal Control via Genetic Programming (Xiao-Cheng Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao-Cheng Liao, Yi Mei, Mengjie Zhang. (2024)<br><strong>Learning Traffic Signal Control via Genetic Programming</strong><br><button class=copy-to-clipboard title="Learning Traffic Signal Control via Genetic Programming" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-NE, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17328v1.pdf filename=2403.17328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The control of traffic signals is crucial for improving transportation efficiency. Recently, learning-based methods, especially Deep <b>Reinforcement</b> <b>Learning</b> (DRL), garnered substantial success in the quest for more efficient traffic signal control strategies. However, the design of rewards in DRL highly demands domain knowledge to converge to an effective policy, and the final policy also presents difficulties in terms of explainability. In this work, a new learning-based method for signal control in complex intersections is proposed. In our approach, we design a concept of phase urgency for each signal phase. During signal transitions, the traffic light control strategy selects the next phase to be activated based on the phase urgency. We then proposed to represent the urgency function as an explainable tree structure. The urgency function can calculate the phase urgency for a specific phase based on the current road conditions. Genetic programming is adopted to perform gradient-free optimization of the urgency function. We test our algorithm on multiple public traffic signal control datasets. The experimental results indicate that the tree-shaped urgency function evolved by genetic programming outperforms the baselines, including a state-of-the-art method in the transportation field and a well-known DRL-based method.</p></p class="citation"></blockquote><h3 id=2626--155347-towards-a-fair-documentation-of-workflows-and-models-in-applied-mathematics-marco-reidelbach-et-al-2024>(26/26 | 155/347) Towards a FAIR Documentation of Workflows and Models in Applied Mathematics (Marco Reidelbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Reidelbach, Björn Schembera, Marcus Weber. (2024)<br><strong>Towards a FAIR Documentation of Workflows and Models in Applied Mathematics</strong><br><button class=copy-to-clipboard title="Towards a FAIR Documentation of Workflows and Models in Applied Mathematics" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: H-3-3; H-3-7; E-0, cs-AI, cs-DB, cs-DL, cs.AI<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17778v1.pdf filename=2403.17778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling-Simulation-Optimization workflows play a fundamental role in applied mathematics. The Mathematical Research Data Initiative, MaRDI, responded to this by developing a FAIR and machine-interpretable template for a comprehensive documentation of such workflows. MaRDMO, a Plugin for the Research Data Management Organiser, enables scientists from diverse fields to document and publish their workflows on the MaRDI Portal seamlessly using the MaRDI template. Central to these workflows are mathematical models. MaRDI addresses them with the MathModDB ontology, offering a structured formal model description. Here, we showcase the interaction between MaRDMO and the MathModDB <b>Knowledge</b> <b>Graph</b> through an algebraic modeling workflow from the Digital Humanities. This demonstration underscores the versatility of both services beyond their original numerical domain.</p></p class="citation"></blockquote><h2 id=eessiv-18>eess.IV (18)</h2><h3 id=118--156347-integrative-graph-transformer-framework-for-histopathology-whole-slide-image-representation-and-classification-zhan-shi-et-al-2024>(1/18 | 156/347) Integrative Graph-Transformer Framework for Histopathology Whole Slide Image Representation and Classification (Zhan Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhan Shi, Jingwei Zhang, Jun Kong, Fusheng Wang. (2024)<br><strong>Integrative Graph-Transformer Framework for Histopathology Whole Slide Image Representation and Classification</strong><br><button class=copy-to-clipboard title="Integrative Graph-Transformer Framework for Histopathology Whole Slide Image Representation and Classification" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 83<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Multiple Instance Learning, Supervised Learning, Weakly-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18134v1.pdf filename=2403.18134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In digital pathology, the <b>multiple</b> <b>instance</b> <b>learning</b> (MIL) strategy is widely used in the weakly <b>supervised</b> histopathology whole slide image (WSI) classification task where giga-pixel WSIs are only labeled at the slide level. However, existing attention-based MIL approaches often overlook contextual information and intrinsic spatial relationships between neighboring tissue tiles, while <b>graph-based</b> <b>MIL</b> <b>frameworks</b> have limited power to recognize the long-range dependencies. In this paper, we introduce the integrative <b>graph-transformer</b> <b>framework</b> <b>that</b> simultaneously captures the context-aware relational features and global WSI representations through a novel <b>Graph</b> <b>Transformer</b> <b>Integration</b> (GTI) block. Specifically, each GTI block consists of a <b>Graph</b> <b>Convolutional</b> <b>Network</b> <b>(GCN)</b> layer modeling neighboring relations at the local instance level and an efficient global attention model capturing comprehensive global information from extensive feature embeddings. Extensive experiments on three publicly available WSI datasets: TCGA-NSCLC, TCGA-RCC and BRIGHT, demonstrate the superiority of our approach over current state-of-the-art MIL methods, achieving an improvement of 1.0% to 2.6% in accuracy and 0.7%-1.6% in AUROC.</p></p class="citation"></blockquote><h3 id=218--157347-automated-report-generation-for-lung-cytological-images-using-a-cnn-vision-classifier-and-multiple-transformer-text-decoders-preliminary-study-atsushi-teramoto-et-al-2024>(2/18 | 157/347) Automated Report Generation for Lung Cytological Images Using a CNN Vision Classifier and Multiple-Transformer Text Decoders: Preliminary Study (Atsushi Teramoto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atsushi Teramoto, Ayano Michiba, Yuka Kiriyama, Tetsuya Tsukamoto, Kazuyoshi Imaizumi, Hiroshi Fujita. (2024)<br><strong>Automated Report Generation for Lung Cytological Images Using a CNN Vision Classifier and Multiple-Transformer Text Decoders: Preliminary Study</strong><br><button class=copy-to-clipboard title="Automated Report Generation for Lung Cytological Images Using a CNN Vision Classifier and Multiple-Transformer Text Decoders: Preliminary Study" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, physics-med-ph<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18151v1.pdf filename=2403.18151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cytology plays a crucial role in lung cancer diagnosis. Pulmonary cytology involves cell morphological characterization in the specimen and reporting the corresponding findings, which are extremely burdensome tasks. In this study, we propose a report-generation technique for lung cytology images. In total, 71 benign and 135 malignant pulmonary cytology specimens were collected. Patch images were extracted from the captured specimen images, and the findings were assigned to each image as a dataset for report generation. The proposed method consists of a vision model and a <b>text</b> <b>decoder.</b> In the former, a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> is used to classify a given image as benign or malignant, and the features related to the image are extracted from the intermediate layer. Independent <b>text</b> <b>decoders</b> for benign and malignant cells are prepared for <b>text</b> <b>generation,</b> and the <b>text</b> <b>decoder</b> switches according to the <b>CNN</b> classification results. The <b>text</b> <b>decoder</b> is configured using a <b>Transformer</b> that uses the features obtained from the <b>CNN</b> for report generation. Based on the evaluation results, the sensitivity and specificity were 100% and 96.4%, respectively, for automated benign and malignant case classification, and the saliency map indicated characteristic benign and malignant areas. The grammar and style of the generated <b>texts</b> <b>were</b> confirmed as correct and in better agreement with gold standard compared to existing <b>LLM-based</b> image-captioning methods and single-text-decoder ablation model. These results indicate that the proposed method is useful for pulmonary cytology classification and reporting.</p></p class="citation"></blockquote><h3 id=318--158347-integrating-mamba-sequence-model-and-hierarchical-upsampling-network-for-accurate-semantic-segmentation-of-multiple-sclerosis-legion-kazi-shahriar-sanjid-et-al-2024>(3/18 | 158/347) Integrating Mamba Sequence Model and Hierarchical Upsampling Network for Accurate Semantic Segmentation of Multiple Sclerosis Legion (Kazi Shahriar Sanjid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kazi Shahriar Sanjid, Md. Tanzim Hossain, Md. Shakib Shahariar Junayed, Dr. Mohammad Monir Uddin. (2024)<br><strong>Integrating Mamba Sequence Model and Hierarchical Upsampling Network for Accurate Semantic Segmentation of Multiple Sclerosis Legion</strong><br><button class=copy-to-clipboard title="Integrating Mamba Sequence Model and Hierarchical Upsampling Network for Accurate Semantic Segmentation of Multiple Sclerosis Legion" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17432v1.pdf filename=2403.17432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating components from <b>convolutional</b> <b>neural</b> <b>networks</b> and state space models in medical image segmentation presents a compelling approach to enhance accuracy and efficiency. We introduce Mamba HUNet, a novel architecture tailored for robust and efficient segmentation tasks. Leveraging strengths from Mamba UNet and the lighter version of Hierarchical Upsampling Network (HUNet), Mamba HUNet combines <b>convolutional</b> <b>neural</b> <b>networks</b> local feature extraction power with state space models long range dependency modeling capabilities. We first converted HUNet into a lighter version, maintaining performance parity and then integrated this lighter HUNet into Mamba HUNet, further enhancing its efficiency. The architecture partitions input grayscale images into patches, transforming them into 1D sequences for processing efficiency akin to <b>Vision</b> <b>Transformers</b> and Mamba models. Through Visual State Space blocks and patch merging layers, hierarchical features are extracted while preserving spatial information. Experimental results on publicly available Magnetic Resonance Imaging scans, notably in Multiple Sclerosis lesion segmentation, demonstrate Mamba HUNet&rsquo;s effectiveness across diverse segmentation tasks. The model&rsquo;s robustness and flexibility underscore its potential in handling complex anatomical structures. These findings establish Mamba HUNet as a promising solution in advancing medical image segmentation, with implications for improving clinical decision making processes.</p></p class="citation"></blockquote><h3 id=418--159347-rotate-to-scan-unet-like-mamba-with-triplet-ssm-module-for-medical-image-segmentation-hao-tang-et-al-2024>(4/18 | 159/347) Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation (Hao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Tang, Lianglun Cheng, Guoheng Huang, Zhengguang Tan, Junhao Lu, Kaihong Wu. (2024)<br><strong>Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17701v1.pdf filename=2403.17701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and <b>Transformer</b> models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets, demonstrating the superior segmentation performance of our proposed TM-UNet. Additionally, compared to the previous VM-UNet, our model achieves a one-third reduction in parameters.</p></p class="citation"></blockquote><h3 id=518--160347-predicting-risk-of-cardiovascular-disease-using-retinal-oct-imaging-cynthia-maldonado-garcia-et-al-2024>(5/18 | 160/347) Predicting risk of cardiovascular disease using retinal OCT imaging (Cynthia Maldonado-Garcia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cynthia Maldonado-Garcia, Rodrigo Bonazzola, Enzo Ferrante, Thomas H Julian, Panagiotis I Sergouniotis, Nishant Ravikumara, Alejandro F Frangi. (2024)<br><strong>Predicting risk of cardiovascular disease using retinal OCT imaging</strong><br><button class=copy-to-clipboard title="Predicting risk of cardiovascular disease using retinal OCT imaging" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 36<br>Keywords: Autoencoder, Multi-modal, Multi-modal, Self-supervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18873v1.pdf filename=2403.18873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigated the potential of optical coherence tomography (OCT) as an additional imaging technique to predict future cardiovascular disease (CVD). We utilised a <b>self-supervised</b> deep learning approach based on <b>Variational</b> <b>Autoencoders</b> (VAE) to learn low-dimensional representations of high-dimensional 3D OCT images and to capture distinct characteristics of different retinal layers within the OCT image. A Random Forest (RF) classifier was subsequently trained using the learned latent features and participant demographic and clinical data, to differentiate between patients at risk of CVD events (MI or stroke) and non-CVD cases. Our predictive model, trained on <b>multimodal</b> data, was assessed based on its ability to correctly identify individuals likely to suffer from a CVD event(MI or stroke), within a 5-year interval after image acquisition. Our <b>self-supervised</b> VAE feature selection and <b>multimodal</b> Random Forest classifier differentiate between patients at risk of future CVD events and the control group with an AUC of 0.75, outperforming the clinically established QRISK3 score (AUC= 0.597). The choroidal layer visible in OCT images was identified as an important predictor of future CVD events using a novel approach to model explanability. Retinal OCT imaging provides a cost-effective and non-invasive alternative to predict the risk of cardiovascular disease and is readily accessible in optometry practices and hospitals.</p></p class="citation"></blockquote><h3 id=618--161347-onboard-deep-lossless-and-near-lossless-predictive-coding-of-hyperspectral-images-with-line-based-attention-diego-valsesia-et-al-2024>(6/18 | 161/347) Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention (Diego Valsesia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diego Valsesia, Tiziano Bianchi, Enrico Magli. (2024)<br><strong>Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention</strong><br><button class=copy-to-clipboard title="Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Autoencoder, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17677v1.pdf filename=2403.17677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning methods have traditionally been difficult to apply to compression of hyperspectral images onboard of spacecrafts, due to the large computational complexity needed to achieve adequate representational power, as well as the lack of suitable datasets for training and testing. In this paper, we depart from the traditional <b>autoencoder</b> approach and we design a predictive neural network, called LineRWKV, that works recursively line-by-line to limit memory consumption. In order to achieve that, we adopt a novel hybrid attentive-recursive operation that combines the representational advantages of <b>Transformers</b> with the linear complexity and recursive implementation of <b>recurrent</b> <b>neural</b> <b>networks.</b> The compression algorithm performs prediction of each pixel using LineRWKV, followed by entropy coding of the residual. Experiments on the HySpecNet-11k dataset and PRISMA images show that LineRWKV is the first deep-learning method to outperform CCSDS-123.0-B-2 at lossless and near-lossless compression. Promising throughput results are also evaluated on a 7W embedded system.</p></p class="citation"></blockquote><h3 id=718--162347-tracing-and-segmentation-of-molecular-patterns-in-3-dimensional-cryo-etem-density-maps-through-algorithmic-image-processing-and-deep-learning-based-techniques-salim-sazzed-2024>(7/18 | 162/347) Tracing and segmentation of molecular patterns in 3-dimensional cryo-et/em density maps through algorithmic image processing and deep learning-based techniques (Salim Sazzed, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salim Sazzed. (2024)<br><strong>Tracing and segmentation of molecular patterns in 3-dimensional cryo-et/em density maps through algorithmic image processing and deep learning-based techniques</strong><br><button class=copy-to-clipboard title="Tracing and segmentation of molecular patterns in 3-dimensional cryo-et/em density maps through algorithmic image processing and deep learning-based techniques" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, physics-bio-ph<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17293v1.pdf filename=2403.17293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the structures of biological macromolecules is highly important as they are closely associated with cellular functionalities. Comprehending the precise organization actin filaments is crucial because they form the dynamic cytoskeleton, which offers structural support to cells and connects the cell&rsquo;s interior with its surroundings. However, determining the precise organization of actin filaments is challenging due to the poor quality of cryo-electron tomography (cryo-ET) images, which suffer from low signal-to-noise (SNR) ratios and the presence of missing wedge, as well as diverse shape characteristics of actin filaments. To address these formidable challenges, the primary component of this dissertation focuses on developing sophisticated computational techniques for tracing actin filaments. In particular, three novel methodologies have been developed: i) BundleTrac, for tracing bundle-like actin filaments found in Stereocilium, ii) Spaghetti Tracer, for tracing filaments that move individually with loosely cohesive movements, and iii) Struwwel Tracer, for tracing randomly orientated actin filaments in the actin network. The second component of the dissertation introduces a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> based segmentation model to determine the location of protein secondary structures, such as helices and beta-sheets, in medium-resolution (5-10 Angstrom) 3-dimensional cryo-electron microscopy (cryo-EM) images. This methodology later evolved into a tool named DeepSSETracer. The final component of the dissertation presents a novel algorithm, cylindrical fit measure, to estimate image structure match at helix regions in medium-resolution cryo-EM images. Overall, my dissertation has made significant contributions to addressing critical research challenges in structural biology by introducing various computational methods and tools.</p></p class="citation"></blockquote><h3 id=818--163347-cross-system-biological-image-quality-enhancement-based-on-the-generative-adversarial-network-as-a-foundation-for-establishing-a-multi-institute-microscopy-cooperative-network-dominik-panek-et-al-2024>(8/18 | 163/347) Cross-system biological image quality enhancement based on the generative adversarial network as a foundation for establishing a multi-institute microscopy cooperative network (Dominik Panek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik Panek, Carina Rząca, Maksymilian Szczypior, Joanna Sorysz, Krzysztof Misztal, Zbigniew Baster, Zenon Rajfur. (2024)<br><strong>Cross-system biological image quality enhancement based on the generative adversarial network as a foundation for establishing a multi-institute microscopy cooperative network</strong><br><button class=copy-to-clipboard title="Cross-system biological image quality enhancement based on the generative adversarial network as a foundation for establishing a multi-institute microscopy cooperative network" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV, q-bio-QM<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18026v1.pdf filename=2403.18026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-quality fluorescence imaging of biological systems is limited by processes like photobleaching and phototoxicity, and also in many cases, by limited access to the latest generations of microscopes. Moreover, low temporal resolution can lead to a motion blur effect in living systems. Our work presents a deep learning (DL) <b>generative-adversarial</b> <b>approach</b> <b>to</b> the problem of obtaining high-quality (HQ) images based on their low-quality (LQ) equivalents. We propose a <b>generative-adversarial</b> <b>network</b> <b>(GAN)</b> for contrast transfer between two different separate microscopy systems: a confocal microscope (producing HQ images) and a wide-field fluorescence microscope (producing LQ images). Our model proves that such transfer is possible, allowing us to receive HQ-generated images characterized by low mean squared error (MSE) values, high structural similarity index (SSIM), and high peak signal-to-noise ratio (PSNR) values. For our best model in the case of comparing HQ-generated images and HQ-ground truth images, the median values of the metrics are 6x10-4, 0.9413, and 31.87, for MSE, SSIM, and PSNR, respectively. In contrast, in the case of comparison between LQ and HQ ground truth median values of the metrics are equal to 0.0071, 0.8304, and 21.48 for MSE, SSIM, and PSNR respectively. Therefore, we observe a significant increase ranging from 14% to 49% for SSIM and PSNR respectively. These results, together with other single-system cross-modality studies, provide proof of concept for further implementation of a cross-system biological image quality enhancement.</p></p class="citation"></blockquote><h3 id=918--164347-scalable-non-cartesian-magnetic-resonance-imaging-with-r2d2-yiwei-chen-et-al-2024>(9/18 | 164/347) Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 (Yiwei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux. (2024)<br><strong>Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2</strong><br><button class=copy-to-clipboard title="Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess-SP, eess.IV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17905v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17905v2.pdf filename=2403.17905v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new approach for non-Cartesian magnetic resonance image reconstruction. While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability. To address this scalability challenge, we leverage the &ldquo;Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)&rdquo; approach recently introduced in astronomical imaging. R2D2&rsquo;s reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration&rsquo;s image estimate and associated data residual as inputs. The method can be interpreted as a learned version of the Matching Pursuit algorithm. We demonstrate R2D2 in <b>simulation,</b> considering radial k-space sampling acquisition sequences. Our preliminary results suggest that R2D2 achieves: (i) suboptimal performance compared to its unrolled incarnation R2D2-Net, which is however non-scalable due to the necessary embedding of NUFFT-based data-consistency layers; (ii) superior reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based approximation for data consistency; (iii) superior reconstruction quality to PnP, while only requiring few iterations.</p></p class="citation"></blockquote><h3 id=1018--165347-annotated-biomedical-video-generation-using-denoising-diffusion-probabilistic-models-and-flow-fields-rüveyda-yilmaz-et-al-2024>(10/18 | 165/347) Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields (Rüveyda Yilmaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rüveyda Yilmaz, Dennis Eschweiler, Johannes Stegmaier. (2024)<br><strong>Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields</strong><br><button class=copy-to-clipboard title="Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17808v1.pdf filename=2403.17808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The segmentation and tracking of living cells play a vital role within the biomedical domain, particularly in cancer research, drug development, and developmental biology. These are usually tedious and time-consuming tasks that are traditionally done by biomedical experts. Recently, to automatize these processes, deep learning based segmentation and tracking methods have been proposed. These methods require large-scale datasets and their full potential is constrained by the scarcity of annotated data in the biomedical imaging domain. To address this limitation, we propose Biomedical Video <b>Diffusion</b> <b>Model</b> (BVDM), capable of generating realistic-looking synthetic microscopy videos. Trained only on a single real video, BVDM can generate videos of arbitrary length with pixel-level annotations that can be used for training data-hungry models. It is composed of a denoising <b>diffusion</b> <b>probabilistic</b> <b>model</b> (DDPM) generating high-fidelity synthetic cell microscopy images and a flow prediction model (FPM) predicting the non-rigid transformation between consecutive video frames. During inference, initially, the DDPM imposes realistic cell textures on synthetic cell masks which are generated based on real data statistics. The flow prediction model predicts the flow field between consecutive masks and applies that to the DDPM output from the previous time frame to create the next one while keeping temporal consistency. BVDM outperforms state-of-the-art synthetic live cell microscopy video generation models. Furthermore, we demonstrate that a sufficiently large synthetic dataset enhances the performance of cell segmentation and tracking models compared to using a limited amount of available real data.</p></p class="citation"></blockquote><h3 id=1118--166347-ct-synthesis-with-conditional-diffusion-models-for-abdominal-lymph-node-segmentation-yongrui-yu-et-al-2024>(11/18 | 166/347) CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node Segmentation (Yongrui Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongrui Yu, Hanyu Chen, Zitian Zhang, Qiong Xiao, Wenhui Lei, Linrui Dai, Yu Fu, Hui Tan, Guan Wang, Peng Gao, Xiaofan Zhang. (2024)<br><strong>CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node Segmentation</strong><br><button class=copy-to-clipboard title="CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node Segmentation" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17770v1.pdf filename=2403.17770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the significant success achieved by deep learning methods in medical image segmentation, researchers still struggle in the computer-aided diagnosis of abdominal lymph nodes due to the complex abdominal environment, small and indistinguishable lesions, and limited annotated data. To address these problems, we present a pipeline that integrates the conditional <b>diffusion</b> <b>model</b> for lymph node generation and the nnU-Net model for lymph node segmentation to improve the segmentation performance of abdominal lymph nodes through synthesizing a diversity of realistic abdominal lymph node data. We propose LN-DDPM, a conditional denoising <b>diffusion</b> <b>probabilistic</b> <b>model</b> (DDPM) for lymph node (LN) generation. LN-DDPM utilizes lymph node masks and anatomical structure masks as model conditions. These conditions work in two conditioning mechanisms: global structure conditioning and local detail conditioning, to distinguish between lymph nodes and their surroundings and better capture lymph node characteristics. The obtained paired abdominal lymph node images and masks are used for the downstream segmentation task. Experimental results on the abdominal lymph node datasets demonstrate that LN-DDPM outperforms other generative methods in the abdominal lymph node image synthesis and better assists the downstream abdominal lymph node segmentation task.</p></p class="citation"></blockquote><h3 id=1218--167347-grad-camo-learning-interpretable-single-cell-morphological-profiles-from-3d-cell-painting-images-vivek-gopalakrishnan-et-al-2024>(12/18 | 167/347) Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles from 3D Cell Painting Images (Vivek Gopalakrishnan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vivek Gopalakrishnan, Jingzhe Ma, Zhiyong Xie. (2024)<br><strong>Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles from 3D Cell Painting Images</strong><br><button class=copy-to-clipboard title="Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles from 3D Cell Painting Images" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, q-bio-QM<br>Keyword Score: 20<br>Keywords: Black Box, Representation Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17615v1.pdf filename=2403.17615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite their <b>black-box</b> <b>nature,</b> deep learning models are extensively used in image-based drug discovery to extract feature vectors from single cells in microscopy images. To better understand how these networks perform <b>representation</b> <b>learning,</b> we employ visual explainability techniques (e.g., Grad-CAM). Our analyses reveal several mechanisms by which <b>supervised</b> models cheat, exploiting biologically irrelevant pixels when extracting morphological features from images, such as noise in the background. This raises doubts regarding the fidelity of learned single-cell <b>representations</b> <b>and</b> their relevance when investigating downstream biological questions. To address this misalignment between researcher expectations and machine behavior, we introduce Grad-CAMO, a novel single-cell interpretability score for <b>supervised</b> feature extractors. Grad-CAMO measures the proportion of a model&rsquo;s attention that is concentrated on the cell of interest versus the background. This metric can be assessed per-cell or averaged across a validation set, offering a tool to audit individual features vectors or guide the improved design of deep learning architectures. Importantly, Grad-CAMO seamlessly integrates into existing workflows, requiring no dataset or model modifications, and is compatible with both 2D and 3D Cell Painting data. Additional results are available at <a href=https://github.com/eigenvivek/Grad-CAMO>https://github.com/eigenvivek/Grad-CAMO</a>.</p></p class="citation"></blockquote><h3 id=1318--168347-pseudo-mri-guided-pet-image-reconstruction-method-based-on-a-diffusion-probabilistic-model-weijie-gan-et-al-2024>(13/18 | 168/347) Pseudo-MRI-Guided PET Image Reconstruction Method Based on a Diffusion Probabilistic Model (Weijie Gan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijie Gan, Huidong Xie, Carl von Gall, Günther Platsch, Michael T. Jurkiewicz, Andrea Andrade, Udunna C. Anazodo, Ulugbek S. Kamilov, Hongyu An, Jorge Cabello. (2024)<br><strong>Pseudo-MRI-Guided PET Image Reconstruction Method Based on a Diffusion Probabilistic Model</strong><br><button class=copy-to-clipboard title="Pseudo-MRI-Guided PET Image Reconstruction Method Based on a Diffusion Probabilistic Model" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18139v1.pdf filename=2403.18139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anatomically guided PET reconstruction using MRI information has been shown to have the potential to improve PET image quality. However, these improvements are limited to PET scans with paired MRI information. In this work we employed a diffusion <b>probabilistic</b> <b>model</b> (DPM) to infer T1-weighted-MRI (deep-MRI) images from FDG-PET brain images. We then use the DPM-generated T1w-MRI to guide the PET reconstruction. The model was trained with brain FDG scans, and tested in datasets containing multiple levels of counts. Deep-MRI images appeared somewhat degraded than the acquired MRI images. Regarding PET image quality, volume of interest analysis in different brain regions showed that both PET reconstructed images using the acquired and the deep-MRI images improved image quality compared to OSEM. Same conclusions were found analysing the decimated datasets. A subjective evaluation performed by two physicians confirmed that OSEM scored consistently worse than the MRI-guided PET images and no significant differences were observed between the MRI-guided PET images. This proof of concept shows that it is possible to infer DPM-based MRI imagery to guide the PET reconstruction, enabling the possibility of changing reconstruction parameters such as the strength of the prior on anatomically guided PET reconstruction in the absence of MRI.</p></p class="citation"></blockquote><h3 id=1418--169347-serpent-scalable-and-efficient-image-restoration-via-multi-scale-structured-state-space-models-mohammad-shahab-sepehri-et-al-2024>(14/18 | 169/347) Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models (Mohammad Shahab Sepehri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Shahab Sepehri, Zalan Fabian, Mahdi Soltanolkotabi. (2024)<br><strong>Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models</strong><br><button class=copy-to-clipboard title="Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: I-4-4; I-4-5, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17902v1.pdf filename=2403.17902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The landscape of computational building blocks of efficient image restoration architectures is dominated by a combination of <b>convolutional</b> processing and various attention mechanisms. However, <b>convolutional</b> filters are inherently local and therefore struggle at modeling long-range dependencies in images. On the other hand, attention excels at capturing global interactions between arbitrary image regions, however at a quadratic cost in image dimension. In this work, we propose Serpent, an architecture that leverages recent advances in state space models (SSMs) in its core computational block. SSMs, originally introduced for sequence modeling, can maintain a global receptive field with a favorable linear scaling in input size. Our preliminary results demonstrate that Serpent can achieve reconstruction quality on par with state-of-the-art techniques, while requiring orders of magnitude less compute (up to $150$ fold reduction in FLOPS) and a factor of up to $5\times$ less GPU memory while maintaining a compact model size.</p></p class="citation"></blockquote><h3 id=1518--170347-paired-diffusion-generation-of-related-synthetic-pet-ct-segmentation-scans-using-linked-denoising-diffusion-probabilistic-models-rowan-bradbury-et-al-2024>(15/18 | 170/347) Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation scans using Linked Denoising Diffusion Probabilistic Models (Rowan Bradbury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rowan Bradbury, Katherine A. Vallis, Bartlomiej W. Papiez. (2024)<br><strong>Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation scans using Linked Denoising Diffusion Probabilistic Models</strong><br><button class=copy-to-clipboard title="Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation scans using Linked Denoising Diffusion Probabilistic Models" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17734v1.pdf filename=2403.17734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of Artificial Intelligence (AI) in biomedical imaging and radiotherapy is hindered by the limited availability of large imaging data repositories. With recent research and improvements in denoising diffusion <b>probabilistic</b> <b>models</b> (DDPM), high quality synthetic medical scans are now possible. Despite this, there is currently no way of generating multiple related images, such as a corresponding ground truth which can be used to train models, so synthetic scans are often manually annotated before use. This research introduces a novel architecture that is able to generate multiple, related PET-CT-tumour mask pairs using paired networks and conditional encoders. Our approach includes innovative, time step-controlled mechanisms and a `noise-seeding&rsquo; strategy to improve DDPM sampling consistency. While our model requires a modified perceptual loss function to ensure accurate feature alignment we show generation of clearly aligned synthetic images and improvement in segmentation accuracy with generated images.</p></p class="citation"></blockquote><h3 id=1618--171347-high-resolution-image-translation-model-based-on-grayscale-redefinition-xixian-wu-et-al-2024>(16/18 | 171/347) High-Resolution Image Translation Model Based on Grayscale Redefinition (Xixian Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xixian Wu, Dian Chao, Yang Yang. (2024)<br><strong>High-Resolution Image Translation Model Based on Grayscale Redefinition</strong><br><button class=copy-to-clipboard title="High-Resolution Image Translation Model Based on Grayscale Redefinition" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17639v1.pdf filename=2403.17639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Image-to-image</b> <b>translation</b> is a technique that focuses on transferring <b>images</b> <b>from</b> one domain to another while maintaining the essential content representations. In recent years, <b>image-to-image</b> <b>translation</b> has gained significant attention and achieved remarkable advancements due to its diverse applications in computer vision and <b>image</b> <b>processing</b> tasks. In this work, we propose an innovative method for <b>image</b> <b>translation</b> between different domains. For high-resolution <b>image</b> <b>translation</b> tasks, we use a grayscale adjustment method to achieve pixel-level translation. For other tasks, we utilize the Pix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and improved loss to enhance the <b>image</b> <b>translation</b> performance. On the other hand, to tackle the issue of sparse training data, we adopt model weight initialization from other task to optimize the performance of the current task.</p></p class="citation"></blockquote><h3 id=1718--172347-building-bridges-across-spatial-and-temporal-resolutions-reference-based-super-resolution-via-change-priors-and-conditional-diffusion-model-runmin-dong-et-al-2024>(17/18 | 172/347) Building Bridges across Spatial and Temporal Resolutions: Reference-Based Super-Resolution via Change Priors and Conditional Diffusion Model (Runmin Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runmin Dong, Shuai Yuan, Bin Luo, Mengxuan Chen, Jinxiao Zhang, Lixian Zhang, Weijia Li, Juepeng Zheng, Haohuan Fu. (2024)<br><strong>Building Bridges across Spatial and Temporal Resolutions: Reference-Based Super-Resolution via Change Priors and Conditional Diffusion Model</strong><br><button class=copy-to-clipboard title="Building Bridges across Spatial and Temporal Resolutions: Reference-Based Super-Resolution via Change Priors and Conditional Diffusion Model" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17460v1.pdf filename=2403.17460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reference-based super-resolution (RefSR) has the potential to build bridges across spatial and temporal resolutions of remote sensing images. However, existing RefSR methods are limited by the faithfulness of content reconstruction and the effectiveness of texture transfer in large scaling factors. Conditional <b>diffusion</b> <b>models</b> have opened up new opportunities for generating realistic high-resolution images, but effectively utilizing reference images within these models remains an area for further exploration. Furthermore, content fidelity is difficult to guarantee in areas without relevant reference information. To solve these issues, we propose a change-aware <b>diffusion</b> <b>model</b> named Ref-Diff for RefSR, using the land cover change priors to guide the denoising process explicitly. Specifically, we inject the priors into the denoising model to improve the utilization of reference information in unchanged areas and regulate the reconstruction of semantically relevant content in changed areas. With this powerful guidance, we decouple the semantics-guided denoising and reference texture-guided denoising processes to improve the model performance. Extensive experiments demonstrate the superior effectiveness and robustness of the proposed method compared with state-of-the-art RefSR methods in both quantitative and qualitative evaluations. The code and data are available at <a href=https://github.com/dongrunmin/RefDiff>https://github.com/dongrunmin/RefDiff</a>.</p></p class="citation"></blockquote><h3 id=1818--173347-labeling-subtypes-in-a-parkinsons-cohort-using-multifeatures-in-mri----integrating-grey-and-white-matter-information-tanmayee-samantaray-et-al-2024>(18/18 | 173/347) Labeling subtypes in a Parkinson&rsquo;s Cohort using Multifeatures in MRI &ndash; Integrating Grey and White Matter Information (Tanmayee Samantaray et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanmayee Samantaray, Jitender Saini, Pramod Kumar Pal, Bithiah Grace Jaganathan, Vijaya V Saradhi, Gupta CN. (2024)<br><strong>Labeling subtypes in a Parkinson&rsquo;s Cohort using Multifeatures in MRI &ndash; Integrating Grey and White Matter Information</strong><br><button class=copy-to-clipboard title="Labeling subtypes in a Parkinson's Cohort using Multifeatures in MRI -- Integrating Grey and White Matter Information" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, q-bio-NC<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17332v1.pdf filename=2403.17332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thresholding of networks has long posed a challenge in brain connectivity analysis. Weighted networks are typically binarized using threshold measures to facilitate network analysis. Previous studies on MRI-based brain networks have predominantly utilized density or sparsity-based thresholding techniques, optimized within specific ranges derived from network metrics such as path length, <b>clustering</b> coefficient, and small-world index. Thus, determination of a single threshold value for facilitating comparative analysis of networks remains elusive. To address this, our study introduces Mutual K-Nearest Neighbor (MKNN)-based thresholding for brain network analysis. Here, nearest neighbor selection is based on the highest correlation between features of brain regions. Construction of brain networks was accomplished by computing Pearson correlations between grey matter volume and white matter volume for each pair of brain regions. Structural MRI data from 180 Parkinsons patients and 70 controls from the NIMHANS, India were analyzed. Subtypes within Parkinsons disease were identified based on grey and white matter volume atrophy using source-based morphometric decomposition. The loading coefficients were correlated with clinical features to discern clinical relationship with the deciphered subtypes. Our data-mining approach revealed: Subtype A (N = 51, intermediate type), Subtype B (N = 57, mild-severe type with mild motor symptoms), and Subtype AB (N = 36, most-severe type with predominance in motor impairment). Subtype-specific weighted matrices were binarized using MKNN-based thresholding for brain network analysis. Permutation tests on network metrics of resulting bipartite <b>graphs</b> demonstrated significant group differences in betweenness centrality and participation coefficient. The identified hubs were specific to each subtype, with some hubs conserved across different subtypes.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--174347-magis-llm-based-multi-agent-framework-for-github-issue-resolution-wei-tao-et-al-2024>(1/5 | 174/347) MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution (Wei Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Tao, Yucheng Zhou, Wenqiang Zhang, Yu Cheng. (2024)<br><strong>MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution</strong><br><button class=copy-to-clipboard title="MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 83<br>Keywords: Benchmarking, Claude, GPT, GPT-3, GPT-3.5, GPT-4, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17927v1.pdf filename=2403.17927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new <b>code</b> <b>but</b> also the maintenance of existing functionalities. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown promise in <b>code</b> <b>generation</b> and understanding but face difficulties in <b>code</b> <b>change,</b> particularly at the repository level. To overcome these challenges, we empirically study the reason why <b>LLMs</b> mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel <b>LLM-based</b> Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of <b>LLMs</b> to resolve GitHub issues. In experiments, we employ the SWE-bench <b>benchmark</b> to compare MAGIS with popular <b>LLMs,</b> including <b>GPT-3.5,</b> <b>GPT-4,</b> and <b>Claude-2.</b> MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of <b>GPT-4,</b> the based <b>LLM</b> of our method. We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc.</p></p class="citation"></blockquote><h3 id=25--175347-an-empirical-study-of-chatgpt-related-projects-on-github-zheng-lin-et-al-2024>(2/5 | 175/347) An Empirical Study of ChatGPT-related projects on GitHub (Zheng Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Lin, Neng Zhang. (2024)<br><strong>An Empirical Study of ChatGPT-related projects on GitHub</strong><br><button class=copy-to-clipboard title="An Empirical Study of ChatGPT-related projects on GitHub" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Topic Model, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17437v1.pdf filename=2403.17437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>ChatGPT</b> possesses powerful capabilities in natural language processing and code analysis, it has received widespread attention since its launch. Developers have applied its powerful capabilities to various domains through software projects which are hosted on the largest open-source platform (GitHub) worldwide. Simultaneously, these projects have triggered extensive discussions. In order to comprehend the research content of these projects and understand the potential requirements discussed, we collected <b>ChatGPT-related</b> projects from the GitHub platform and utilized the LDA <b>topic</b> <b>model</b> to identify the discussion <b>topics.</b> <b>Specifically,</b> we selected 200 projects, categorizing them into three primary categories through analyzing their descriptions: <b>ChatGPT</b> implementation & training, <b>ChatGPT</b> application, <b>ChatGPT</b> improvement & extension. Subsequently, we employed the LDA <b>topic</b> <b>model</b> to identify 10 <b>topics</b> <b>from</b> issue texts, and compared the distribution and evolution trend of the discovered <b>topics</b> <b>within</b> the three primary project categories. Our observations include (1) The number of projects growing in a single month for the three primary project categories are closely associated with the development of <b>ChatGPT.</b> (2) There exist significant variations in the popularity of each <b>topic</b> <b>for</b> the three primary project categories. (3) The monthly changes in the absolute impact of each <b>topic</b> <b>for</b> the three primary project categories are diverse, which is often closely associated with the variation in the number of projects owned by that category. (4) With the passage of time, the relative impact of each <b>topic</b> <b>exhibits</b> different development trends in the three primary project categories. Based on these findings, we discuss implications for developers and users.</p></p class="citation"></blockquote><h3 id=35--176347-spes-towards-optimizing-performance-resource-trade-off-for-serverless-functions-cheryl-lee-et-al-2024>(3/5 | 176/347) SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless Functions (Cheryl Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheryl Lee, Zhouruixin Zhu, Tianyi Yang, Yintong Huo, Yuxin Su, Pinjia He, Michael R. Lyu. (2024)<br><strong>SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless Functions</strong><br><button class=copy-to-clipboard title="SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless Functions" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-DC, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17574v1.pdf filename=2403.17574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As an emerging cloud computing deployment paradigm, serverless computing is gaining traction due to its efficiency and ability to harness on-demand cloud resources. However, a significant hurdle remains in the form of the cold start problem, causing latency when launching new function instances from scratch. Existing solutions tend to use over-simplistic strategies for function pre-loading/unloading without full invocation pattern exploitation, rendering unsatisfactory optimization of the trade-off between cold start latency and resource waste. To bridge this gap, we propose SPES, the first differentiated scheduler for runtime cold start mitigation by optimizing serverless function provision. Our insight is that the common architecture of serverless systems <b>prompts</b> the concentration of certain invocation patterns, leading to predictable invocation behaviors. This allows us to categorize functions and pre-load/unload proper function instances with finer-grained strategies based on accurate invocation prediction. Experiments demonstrate the success of SPES in optimizing serverless function provision on both sides: reducing the 75th-percentile cold start rates by 49.77% and the wasted memory time by 56.43%, compared to the state-of-the-art. By mitigating the cold start issue, SPES is a promising advancement in facilitating cloud services deployed on serverless architectures.</p></p class="citation"></blockquote><h3 id=45--177347-natural-language-requirements-testability-measurement-based-on-requirement-smells-morteza-zakeri-nasrabadi-et-al-2024>(4/5 | 177/347) Natural Language Requirements Testability Measurement Based on Requirement Smells (Morteza Zakeri-Nasrabadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Morteza Zakeri-Nasrabadi, Saeed Parsa. (2024)<br><strong>Natural Language Requirements Testability Measurement Based on Requirement Smells</strong><br><button class=copy-to-clipboard title="Natural Language Requirements Testability Measurement Based on Requirement Smells" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17479v1.pdf filename=2403.17479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Requirements form the basis for defining software systems&rsquo; obligations and tasks. Testable requirements help prevent failures, reduce maintenance costs, and make it easier to perform acceptance tests. However, despite the importance of measuring and quantifying requirements testability, no automatic approach for measuring requirements testability has been proposed based on the requirements smells, which are at odds with the requirements testability. This paper presents a mathematical model to evaluate and rank the natural language requirements testability based on an extensive set of nine requirements smells, detected automatically, and acceptance test efforts determined by requirement length and its application domain. Most of the smells stem from uncountable adjectives, context-sensitive, and ambiguous <b>words.</b> <b>A</b> comprehensive dictionary is required to detect such <b>words.</b> <b>We</b> offer a neural <b>word-embedding</b> <b>technique</b> to generate such a dictionary automatically. Using the dictionary, we could automatically detect Polysemy smell (domain-specific ambiguity) for the first time in 10 application domains. Our empirical study on nearly 1000 software requirements from six well-known industrial and academic projects demonstrates that the proposed smell detection approach outperforms Smella, a state-of-the-art tool, in detecting requirements smells. The precision and recall of smell detection are improved with an average of 0.03 and 0.33, respectively, compared to the state-of-the-art. The proposed requirement testability model measures the testability of 985 requirements with a mean absolute error of 0.12 and a mean squared error of 0.03, demonstrating the model&rsquo;s potential for practical use.</p></p class="citation"></blockquote><h3 id=55--178347-mesia-understanding-and-leveraging-supplementary-nature-of-method-level-comments-for-automatic-comment-generation-xinglu-pan-et-al-2024>(5/5 | 178/347) MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation (Xinglu Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinglu Pan, Chenxiao Liu, Yanzhen Zou, Tao Xie, Bing Xie. (2024)<br><strong>MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation</strong><br><button class=copy-to-clipboard title="MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17357v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17357v1.pdf filename=2403.17357v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code comments are important for developers in program comprehension. In scenarios of comprehending and reusing a method, developers expect code comments to provide supplementary information beyond the method signature. However, the extent of such supplementary information varies a lot in different code comments. In this paper, we raise the awareness of the supplementary nature of method-level comments and propose a new metric named MESIA (Mean Supplementary Information Amount) to assess the extent of supplementary information that a code comment can provide. With the MESIA metric, we conduct experiments on a popular code-comment dataset and three common types of neural approaches to generate method-level comments. Our experimental results demonstrate the value of our proposed work with a number of findings. (1) Small-MESIA comments occupy around 20% of the dataset and mostly fall into only the WHAT comment category. (2) Being able to provide various kinds of essential information, large-MESIA comments in the dataset are difficult for existing neural approaches to generate. (3) We can improve the capability of existing neural approaches to generate large-MESIA comments by reducing the proportion of small-MESIA comments in the training set. (4) The retrained model can generate large-MESIA comments that convey essential meaningful supplementary information for methods in the small-MESIA test set, but will get a lower <b>BLEU</b> score in evaluation. These findings indicate that with good training data, auto-generated comments can sometimes even surpass human-written reference comments, and having no appropriate ground truth for evaluation is an issue that needs to be addressed by future work on automatic comment generation.</p></p class="citation"></blockquote><h2 id=cscv-81>cs.CV (81)</h2><h3 id=181--179347-wordrobe-text-guided-generation-of-textured-3d-garments-astitva-srivastava-et-al-2024>(1/81 | 179/347) WordRobe: Text-Guided Generation of Textured 3D Garments (Astitva Srivastava et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Astitva Srivastava, Pranav Manu, Amit Raj, Varun Jampani, Avinash Sharma. (2024)<br><strong>WordRobe: Text-Guided Generation of Textured 3D Garments</strong><br><button class=copy-to-clipboard title="WordRobe: Text-Guided Generation of Textured 3D Garments" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 70<br>Keywords: ControlNet, Simulation, Simulator, Supervised Learning, Weakly-supervised Learning, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17541v1.pdf filename=2403.17541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we tackle a new and challenging problem of text-driven generation of 3D garments with high-quality textures. We propose &ldquo;WordRobe&rdquo;, a novel framework for the generation of unposed & textured 3D garment meshes from user-friendly text <b>prompts.</b> We achieve this by first learning a latent representation of 3D garments using a novel coarse-to-fine training strategy and a loss for latent disentanglement, promoting better latent interpolation. Subsequently, we align the garment latent space to the CLIP embedding space in a weakly <b>supervised</b> manner, enabling text-driven 3D garment generation and editing. For appearance modeling, we leverage the <b>zero-shot</b> generation capability of <b>ControlNet</b> to synthesize view-consistent texture maps in a single feed-forward inference step, thereby drastically decreasing the generation time as compared to existing methods. We demonstrate superior performance over current SOTAs for learning 3D garment latent space, garment interpolation, and text-driven texture synthesis, supported by quantitative evaluation and qualitative user study. The unposed 3D garment meshes generated using WordRobe can be directly fed to standard cloth <b>simulation</b> & animation pipelines without any post-processing.</p></p class="citation"></blockquote><h3 id=281--180347-boosting-few-shot-learning-with-disentangled-self-supervised-learning-and-meta-learning-for-medical-image-classification-eva-pachetti-et-al-2024>(2/81 | 180/347) Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification (Eva Pachetti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eva Pachetti, Sotirios A. Tsaftaris, Sara Colantonio. (2024)<br><strong>Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification</strong><br><button class=copy-to-clipboard title="Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2; I-4; I-5; J-3, cs-AI, cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Distribution Shift, Distribution Shift, Few-shot, Few-shot Learning, Fine-tuning, Meta Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17530v1.pdf filename=2403.17530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data. We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes. Methods: The proposed method starts with a pre-training phase, where features learned in a <b>self-supervised</b> <b>learning</b> setting are disentangled to improve the robustness of the representations for downstream tasks. We then introduce a <b>meta-fine-tuning</b> <b>step,</b> leveraging related classes between <b>meta-training</b> <b>and</b> <b>meta-testing</b> <b>phases</b> but varying the granularity level. This approach aims to enhance the model&rsquo;s generalization capabilities by exposing it to more challenging classification tasks during <b>meta-training</b> <b>and</b> evaluating it on easier tasks but holding greater clinical relevance during <b>meta-testing.</b> <b>We</b> demonstrate the effectiveness of the proposed approach through a series of experiments exploring several backbones, as well as diverse pre-training and <b>fine-tuning</b> schemes, on two distinct medical tasks, i.e., classification of prostate cancer aggressiveness from MRI data and classification of breast cancer malignity from microscopic images. Results: Our results indicate that the proposed approach consistently yields superior performance w.r.t. ablation experiments, maintaining competitiveness even when a <b>distribution</b> <b>shift</b> between training and evaluation data occurs. Conclusion: Extensive experiments demonstrate the effectiveness and wide applicability of the proposed approach. We hope that this work will add another solution to the arsenal of addressing learning issues in data-scarce imaging domains.</p></p class="citation"></blockquote><h3 id=381--181347-language-models-are-free-boosters-for-biomedical-imaging-tasks-zhixin-lai-et-al-2024>(3/81 | 181/347) Language Models are Free Boosters for Biomedical Imaging Tasks (Zhixin Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Naira Hovakimyan. (2024)<br><strong>Language Models are Free Boosters for Biomedical Imaging Tasks</strong><br><button class=copy-to-clipboard title="Language Models are Free Boosters for Biomedical Imaging Tasks" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Multi-modal, Transformer, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17343v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17343v2.pdf filename=2403.17343v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we uncover the unexpected efficacy of residual-based <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen <b>transformer</b> block, extracted from pre-trained <b>LLMs,</b> as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard <b>multi-modal</b> <b>vision-language</b> frameworks, which typically hinge on language-driven <b>prompts</b> and inputs. We found that these <b>LLMs</b> could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we aim to open new avenues for employing <b>LLMs</b> in biomedical imaging and enriching the understanding of their potential in this specialized domain.</p></p class="citation"></blockquote><h3 id=481--182347-spectral-convolutional-transformer-harmonizing-real-vs-complex-multi-view-spectral-operators-for-vision-transformer-badri-n-patro-et-al-2024>(4/81 | 182/347) Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer (Badri N. Patro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Badri N. Patro, Vinay P. Namboodiri, Vijay S. Agneeswaran. (2024)<br><strong>Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer</strong><br><button class=copy-to-clipboard title="Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-MM, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Transfer Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18063v1.pdf filename=2403.18063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> used in <b>vision</b> <b>have</b> been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating <b>convolutions</b> in <b>transformers</b> such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a <b>convolutional</b> operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral <b>convolution</b> <b>transformer</b> (SCT) that provides improved performance over the state-of-the-art methods while reducing the number of parameters. Through extensive experiments, we show that SCT-C-small gives state-of-the-art performance on the ImageNet dataset and reaches 84.5% top-1 accuracy, while SCT-C-Large reaches 85.9% and SCT-C-Huge reaches 86.4%. We evaluate SCT on <b>transfer</b> <b>learning</b> on datasets such as CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on downstream tasks i.e. instance segmentation on the MSCOCO dataset. The project page is available on this webpage.\url{https://github.com/badripatro/sct}</p></p class="citation"></blockquote><h3 id=581--183347-improving-text-to-image-consistency-via-automatic-prompt-optimization-oscar-mañas-et-al-2024>(5/81 | 183/347) Improving Text-to-Image Consistency via Automatic Prompt Optimization (Oscar Mañas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal Drozdzal. (2024)<br><strong>Improving Text-to-Image Consistency via Automatic Prompt Optimization</strong><br><button class=copy-to-clipboard title="Improving Text-to-Image Consistency via Automatic Prompt Optimization" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Text2image, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17804v1.pdf filename=2403.17804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Impressive advances in <b>text-to-image</b> (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input <b>prompt,</b> oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve <b>prompt-image</b> consistency suffer from the following challenges: (1) they oftentimes require model <b>fine-tuning,</b> (2) they only focus on nearby <b>prompt</b> samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and <b>prompt-image</b> consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to improve <b>prompt-image</b> consistency in T2I models. Our framework starts from a user <b>prompt</b> and iteratively generates revised <b>prompts</b> with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=681--184347-equipping-sketch-patches-with-context-aware-positional-encoding-for-graphic-sketch-representation-sicong-zang-et-al-2024>(6/81 | 184/347) Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation (Sicong Zang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sicong Zang, Zhijun Fang. (2024)<br><strong>Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation</strong><br><button class=copy-to-clipboard title="Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 48<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Representation Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17525v1.pdf filename=2403.17525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The drawing order of a sketch records how it is created stroke-by-stroke by a human being. For graphic sketch <b>representation</b> <b>learning,</b> recent studies have injected sketch drawing orders into <b>graph</b> <b>edge</b> <b>construction</b> by linking each patch to another in accordance to a temporal-based nearest neighboring strategy. However, such constructed <b>graph</b> <b>edges</b> <b>may</b> be unreliable, since a sketch could have variants of drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for learning graphic sketch <b>representation.</b> <b>Instead</b> of injecting sketch drawings into <b>graph</b> <b>edges,</b> <b>we</b> embed these sequential information into <b>graph</b> <b>nodes</b> <b>only.</b> More specifically, each patch embedding is equipped with a sinusoidal absolute PE to highlight the sequential position in the drawing order. And its neighboring patches, ranked by the values of <b>self-attention</b> scores between patch embeddings, are equipped with learnable relative PEs to restore the contextual positions within a neighborhood. During message aggregation via <b>graph</b> <b>convolutional</b> <b>networks,</b> a node receives both semantic contents from patch embeddings and contextual patterns from PEs by its neighbors, arriving at drawing-order-enhanced sketch <b>representations.</b> <b>Experimental</b> results indicate that our method significantly improves sketch healing and controllable sketch synthesis.</p></p class="citation"></blockquote><h3 id=781--185347-assessment-of-multimodal-large-language-models-in-alignment-with-human-values-zhelun-shi-et-al-2024>(7/81 | 185/347) Assessment of Multimodal Large Language Models in Alignment with Human Values (Zhelun Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu Sheng, Yu Qiao, Jing Shao. (2024)<br><strong>Assessment of Multimodal Large Language Models in Alignment with Human Values</strong><br><button class=copy-to-clipboard title="Assessment of Multimodal Large Language Models in Alignment with Human Values" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Reasoning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17830v1.pdf filename=2403.17830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> aim to serve as versatile assistants aligned with human values, as defined by the principles of being helpful, honest, and harmless (hhh). However, in terms of <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), despite their commendable performance in perception and <b>reasoning</b> tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations. To address this gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human expectations. Ch3Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the hhh principle. We also present a unified evaluation strategy supporting assessment across various scenarios and different perspectives. Based on the evaluation results, we <b>summarize</b> over 10 key findings that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between evaluation levels, guiding future advancements in the field.</p></p class="citation"></blockquote><h3 id=881--186347-elgc-net-efficient-local-global-context-aggregation-for-remote-sensing-change-detection-mubashir-noman-et-al-2024>(8/81 | 186/347) ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing Change Detection (Mubashir Noman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mubashir Noman, Mustansar Fiaz, Hisham Cholakkal, Salman Khan, Fahad Shahbaz Khan. (2024)<br><strong>ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing Change Detection</strong><br><button class=copy-to-clipboard title="ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing Change Detection" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17909v1.pdf filename=2403.17909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning has shown remarkable success in remote sensing change detection (CD), aiming to identify semantic change regions between co-registered satellite image pairs acquired at distinct time stamps. However, existing <b>convolutional</b> <b>neural</b> <b>network</b> and <b>transformer-based</b> frameworks often struggle to accurately segment semantic change regions. Moreover, <b>transformers-based</b> methods with standard <b>self-attention</b> suffer from quadratic computational complexity with respect to the image resolution, making them less practical for CD tasks with limited training data. To address these issues, we propose an efficient change detection framework, ELGC-Net, which leverages rich contextual information to precisely estimate change regions while reducing the model size. Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The focus of our design is the introduction of an Efficient Local-Global Context Aggregator module within the encoder, capturing enhanced global context and local spatial information through a novel pooled-transpose (PT) attention and depthwise <b>convolution,</b> respectively. The PT attention employs pooling operations for robust feature extraction and minimizes computational cost with transposed attention. Extensive experiments on three challenging CD datasets demonstrate that ELGC-Net outperforms existing methods. Compared to the recent <b>transformer-based</b> CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in intersection over union metric on the LEVIR-CD dataset, while significantly reducing trainable parameters. Our proposed ELGC-Net sets a new state-of-the-art performance in remote sensing change detection <b>benchmarks.</b> Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly reduced computational complexity, suitable for resource-constrained settings, while achieving comparable performance. Project url <a href=https://github.com/techmn/elgcnet>https://github.com/techmn/elgcnet</a>.</p></p class="citation"></blockquote><h3 id=981--187347-aid-attention-interpolation-of-text-to-image-diffusion-qiyuan-he-et-al-2024>(9/81 | 187/347) AID: Attention Interpolation of Text-to-Image Diffusion (Qiyuan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao. (2024)<br><strong>AID: Attention Interpolation of Text-to-Image Diffusion</strong><br><button class=copy-to-clipboard title="AID: Attention Interpolation of Text-to-Image Diffusion" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Text2image, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17924v1.pdf filename=2403.17924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conditional <b>diffusion</b> <b>models</b> can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via <b>Diffusion</b> <b>(AID).</b> Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with <b>self-attention</b> to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, <b>Prompt-guided</b> Attention Interpolation via <b>Diffusion</b> <b>(PAID),</b> that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation. Code and demo are available at <a href=https://github.com/QY-H00/attention-interpolation-diffusion>https://github.com/QY-H00/attention-interpolation-diffusion</a>.</p></p class="citation"></blockquote><h3 id=1081--188347-a-foundation-model-utilizing-chest-ct-volumes-and-radiology-reports-for-supervised-level-zero-shot-detection-of-abnormalities-ibrahim-ethem-hamamci-et-al-2024>(10/81 | 188/347) A foundation model utilizing chest CT volumes and radiology reports for supervised-level zero-shot detection of abnormalities (Ibrahim Ethem Hamamci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Dogan, Muhammed Furkan Dasdelen, Bastian Wittmann, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, Anjany Sekuboyina, Berkan Lafci, Mehmet K. Ozdemir, Bjoern Menze. (2024)<br><strong>A foundation model utilizing chest CT volumes and radiology reports for supervised-level zero-shot detection of abnormalities</strong><br><button class=copy-to-clipboard title="A foundation model utilizing chest CT volumes and radiology reports for supervised-level zero-shot detection of abnormalities" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Foundation Model, Self-supervised Learning, Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17834v1.pdf filename=2403.17834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A major challenge in computational research in 3D medical imaging is the lack of comprehensive datasets. Addressing this issue, our study introduces CT-RATE, the first 3D medical imaging dataset that pairs images with textual reports. CT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188 through various reconstructions, from 21,304 unique patients, along with corresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP, a CT-focused contrastive language-image pre-training framework. As a versatile, <b>self-supervised</b> model, CT-CLIP is designed for broad application and does not require task-specific training. Remarkably, CT-CLIP outperforms state-of-the-art, fully <b>supervised</b> methods in multi-abnormality detection across all key metrics, thus eliminating the need for manual annotation. We also demonstrate its utility in case retrieval, whether using imagery or textual queries, thereby advancing knowledge dissemination. The open-source release of CT-RATE and CT-CLIP marks a significant advancement in medical AI, enhancing 3D imaging analysis and fostering innovation in healthcare.</p></p class="citation"></blockquote><h3 id=1181--189347-efficient-image-pre-training-with-siamese-cropped-masked-autoencoders-alexandre-eymaël-et-al-2024>(11/81 | 189/347) Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders (Alexandre Eymaël et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Eymaël, Renaud Vandeghen, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck. (2024)<br><strong>Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders</strong><br><button class=copy-to-clipboard title="Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-6; I-2-10, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Autoencoder, Self-supervised Learning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17823v1.pdf filename=2403.17823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>pre-training</b> of image encoders is omnipresent in the literature, particularly following the introduction of Masked <b>autoencoders</b> (MAE). Current efforts attempt to learn object-centric representations from motion in videos. In particular, SiamMAE recently introduced a Siamese network, training a shared-weight encoder from two frames of a video with a high asymmetric masking ratio (95%). In this work, we propose CropMAE, an alternative approach to the Siamese pre-training introduced by SiamMAE. Our method specifically differs by exclusively considering pairs of cropped images sourced from the same image but cropped differently, deviating from the conventional pairs of frames extracted from a video. CropMAE therefore alleviates the need for video datasets, while maintaining competitive performances and drastically reducing pre-training time. Furthermore, we demonstrate that CropMAE learns similar object-centric representations without explicit motion, showing that current <b>self-supervised</b> <b>learning</b> methods do not learn objects from motion, but rather thanks to the Siamese architecture. Finally, CropMAE achieves the highest masking ratio to date (98.5%), enabling the reconstruction of images using only two visible patches. Our code is available at <a href=https://github.com/alexandre-eymael/CropMAE>https://github.com/alexandre-eymael/CropMAE</a>.</p></p class="citation"></blockquote><h3 id=1281--190347-the-solution-for-the-cvpr-2023-1st-foundation-model-challenge-track2-haonan-xu-et-al-2024>(12/81 | 190/347) The Solution for the CVPR 2023 1st foundation model challenge-Track2 (Haonan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haonan Xu, Yurui Huang, Sishun Pan, Zhihao Guan, Yi Xu, Yang Yang. (2024)<br><strong>The Solution for the CVPR 2023 1st foundation model challenge-Track2</strong><br><button class=copy-to-clipboard title="The Solution for the CVPR 2023 1st foundation model challenge-Track2" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Data Augmentation, Foundation Model, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17702v1.pdf filename=2403.17702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a solution for cross-modal transportation retrieval. Due to the cross-domain problem of traffic images, we divide the problem into two sub-tasks of pedestrian retrieval and vehicle retrieval through a simple strategy. In pedestrian retrieval tasks, we use IRRA as the base model and specifically design an Attribute Classification to mine the knowledge implied by attribute labels. More importantly, We use the strategy of Inclusion Relation Matching to make the <b>image-text</b> pairs with inclusion relation have similar representation in the feature space. For the vehicle retrieval task, we use BLIP as the base model. Since aligning the color attributes of vehicles is challenging, we introduce attribute-based <b>object</b> <b>detection</b> techniques to add color patch blocks to vehicle images for color <b>data</b> <b>augmentation.</b> This serves as strong prior information, helping the model perform the <b>image-text</b> alignment. At the same time, we incorporate labeled attributes into the <b>image-text</b> alignment loss to learn fine-grained alignment and prevent similar images and texts from being incorrectly separated. Our approach ranked first in the final B-board test with a score of 70.9.</p></p class="citation"></blockquote><h3 id=1381--191347-dual-memory-networks-a-versatile-adaptation-approach-for-vision-language-models-yabin-zhang-et-al-2024>(13/81 | 191/347) Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models (Yabin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang. (2024)<br><strong>Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models</strong><br><button class=copy-to-clipboard title="Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs-MM, cs.CV<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Few-shot, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17589v1.pdf filename=2403.17589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the emergence of pre-trained <b>vision-language</b> models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: <b>zero-shot</b> adaptation, <b>few-shot</b> adaptation, and the recently-proposed training-free <b>few-shot</b> adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free <b>few-shot</b> adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the training set. This novel capability enhances model performance in the <b>few-shot</b> setting and enables model usability in the absence of training data. The two memory networks employ the same flexible memory interactive strategy, which can operate in a training-free mode and can be further enhanced by incorporating learnable projection layers. Our approach is tested across 11 datasets under the three task settings. Remarkably, in the <b>zero-shot</b> scenario, it outperforms existing methods by over 3% and even shows superior results against methods utilizing external training data. Additionally, our method exhibits robust performance against natural <b>distribution</b> <b>shifts.</b> Codes are available at \url{https://github.com/YBZh/DMN}.</p></p class="citation"></blockquote><h3 id=1481--192347-self-rectifying-diffusion-sampling-with-perturbed-attention-guidance-donghoon-ahn-et-al-2024>(14/81 | 192/347) Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance (Donghoon Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim. (2024)<br><strong>Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance</strong><br><button class=copy-to-clipboard title="Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: ControlNet, Diffusion Model, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17377v1.pdf filename=2403.17377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have demonstrated that <b>diffusion</b> <b>models</b> are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves <b>diffusion</b> <b>sample</b> quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected <b>self-attention</b> maps in <b>diffusion</b> <b>U-Net</b> with an identity matrix, by considering the <b>self-attention</b> mechanisms&rsquo; ability to capture structural information, and guiding the denoising process away from these degraded samples. In both ADM and Stable <b>Diffusion,</b> <b>PAG</b> surprisingly improves sample quality in conditional and even unconditional scenarios. Moreover, PAG significantly improves the baseline performance in various downstream tasks where existing guidances such as CG or CFG cannot be fully utilized, including <b>ControlNet</b> with empty <b>prompts</b> and image restoration such as inpainting and deblurring.</p></p class="citation"></blockquote><h3 id=1581--193347-the-solution-for-the-iccv-2023-1st-scientific-figure-captioning-challenge-dian-chao-et-al-2024>(15/81 | 193/347) The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge (Dian Chao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dian Chao, Xin Song, Shupeng Zhong, Boyuan Wang, Xiangyu Wu, Chen Zhu, Yang Yang. (2024)<br><strong>The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge</strong><br><button class=copy-to-clipboard title="The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Optical Character Recognition, LLaMA, Text Generation, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17342v1.pdf filename=2403.17342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a solution for improving the quality of captions generated for figures in papers. We adopt the approach of summarizing the textual content in the paper to generate image captions. Throughout our study, we encounter discrepancies in the <b>OCR</b> information provided in the official dataset. To rectify this, we employ the PaddleOCR toolkit to extract <b>OCR</b> information from all images. Moreover, we observe that certain textual content in the official paper pertains to images that are not relevant for captioning, thereby introducing noise during caption generation. To mitigate this issue, we leverage <b>LLaMA</b> to extract image-specific information by querying the textual content based on image mentions, effectively filtering out extraneous information. Additionally, we recognize a discrepancy between the primary use of maximum likelihood estimation during <b>text</b> <b>generation</b> and the evaluation metrics such as <b>ROUGE</b> employed to assess the quality of generated captions. To bridge this gap, we integrate the BRIO model framework, enabling a more coherent alignment between the generation and evaluation processes. Our approach ranked first in the final test with a score of 4.49.</p></p class="citation"></blockquote><h3 id=1681--194347-omnivid-a-generative-framework-for-universal-video-understanding-junke-wang-et-al-2024>(16/81 | 194/347) OmniVid: A Generative Framework for Universal Video Understanding (Junke Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang. (2024)<br><strong>OmniVid: A Generative Framework for Universal Video Understanding</strong><br><button class=copy-to-clipboard title="OmniVid: A Generative Framework for Universal Video Understanding" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, GPT, GPT-3, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17935v1.pdf filename=2403.17935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The core of video understanding tasks, such as recognition, captioning, and tracking, is to automatically detect objects or actions in a video and analyze their temporal evolution. Despite sharing a common goal, different tasks often rely on distinct model architectures and annotation formats. In contrast, natural language processing benefits from a unified output space, i.e., text sequences, which simplifies the training of powerful foundational language models, such as <b>GPT-3,</b> with extensive training corpora. Inspired by this, we seek to unify the output space of video understanding tasks by using languages as labels and additionally introducing time and box tokens. In this way, a variety of video tasks could be formulated as video-grounded token generation. This enables us to address various types of video tasks, including classification (such as action recognition), captioning (covering clip captioning, video <b>question</b> <b>answering,</b> and dense video captioning), and localization tasks (such as visual object tracking) within a fully shared encoder-decoder architecture, following a generative framework. Through comprehensive experiments, we demonstrate such a simple and straightforward idea is quite effective and can achieve state-of-the-art or competitive results on seven video <b>benchmarks,</b> providing a novel perspective for more universal video understanding. Code is available at <a href=https://github.com/wangjk666/OmniVid>https://github.com/wangjk666/OmniVid</a>.</p></p class="citation"></blockquote><h3 id=1781--195347-semi-supervised-image-captioning-considering-wasserstein-graph-matching-yang-yang-2024>(17/81 | 195/347) Semi-Supervised Image Captioning Considering Wasserstein Graph Matching (Yang Yang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Yang. (2024)<br><strong>Semi-Supervised Image Captioning Considering Wasserstein Graph Matching</strong><br><button class=copy-to-clipboard title="Semi-Supervised Image Captioning Considering Wasserstein Graph Matching" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Graph, Data Augmentation, Supervised Learning, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17995v1.pdf filename=2403.17995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image captioning can automatically generate captions for the given images, and the key challenge is to learn a mapping function from visual features to natural language features. Existing approaches are mostly <b>supervised</b> ones, i.e., each image has a corresponding sentence in the training set. However, considering that describing images always requires a huge of manpower, we usually have limited amount of described images (i.e., <b>image-text</b> pairs) and a large number of undescribed images in real-world applications. Thereby, a dilemma is the &ldquo;Semi-Supervised Image Captioning&rdquo;. To solve this problem, we propose a novel Semi-Supervised Image Captioning method considering Wasserstein <b>Graph</b> Matching (SSIC-WGM), which turns to adopt the raw image inputs to supervise the generated sentences. Different from traditional single modal semi-supervised methods, the difficulty of semi-supervised cross-modal learning lies in constructing intermediately comparable information among heterogeneous modalities. In this paper, SSIC-WGM adopts the successful scene <b>graphs</b> as intermediate information, and constrains the generated sentences from two aspects: 1) inter-modal consistency. SSIC-WGM constructs the scene <b>graphs</b> of the raw image and generated sentence respectively, then employs the wasserstein distance to better measure the similarity between region embeddings of different <b>graphs.</b> 2) intra-modal consistency. SSIC-WGM takes the <b>data</b> <b>augmentation</b> techniques for the raw images, then constrains the consistency among augmented images and generated sentences. Consequently, SSIC-WGM combines the cross-modal pseudo supervision and structure invariant measure for efficiently using the undescribed images, and learns more reasonable mapping function.</p></p class="citation"></blockquote><h3 id=1881--196347-exploring-dynamic-transformer-for-efficient-object-tracking-jiawen-zhu-et-al-2024>(18/81 | 196/347) Exploring Dynamic Transformer for Efficient Object Tracking (Jiawen Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawen Zhu, Xin Chen, Haiwen Diao, Shuai Li, Jun-Yan He, Chenyang Li, Bin Luo, Dong Wang, Huchuan Lu. (2024)<br><strong>Exploring Dynamic Transformer for Efficient Object Tracking</strong><br><button class=copy-to-clipboard title="Exploring Dynamic Transformer for Efficient Object Tracking" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Self-Distillation, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17651v1.pdf filename=2403.17651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The speed-precision trade-off is a critical problem for visual object tracking which usually requires low latency and deployment on constrained resources. Existing solutions for efficient tracking mainly focus on adopting light-weight backbones or modules, which nevertheless come at the cost of a sacrifice in precision. In this paper, inspired by dynamic network routing, we propose DyTrack, a dynamic <b>transformer</b> framework for efficient tracking. Real-world tracking scenarios exhibit diverse levels of complexity. We argue that a simple network is sufficient for easy frames in video sequences, while more computation could be assigned to difficult ones. DyTrack automatically learns to configure proper <b>reasoning</b> routes for various inputs, gaining better utilization of the available computational budget. Thus, it can achieve higher performance with the same running speed. We formulate instance-specific tracking as a sequential decision problem and attach terminating branches to intermediate layers of the entire model. Especially, to fully utilize the computations, we introduce the feature recycling mechanism to reuse the outputs of predecessors. Furthermore, a target-aware <b>self-distillation</b> strategy is designed to enhance the discriminating capabilities of early predictions by effectively mimicking the representation pattern of the deep model. Extensive experiments on multiple <b>benchmarks</b> demonstrate that DyTrack achieves promising speed-precision trade-offs with only a single model. For instance, DyTrack obtains 64.9% AUC on LaSOT with a speed of 256 fps.</p></p class="citation"></blockquote><h3 id=1981--197347-aide-an-automatic-data-engine-for-object-detection-in-autonomous-driving-mingfu-liang-et-al-2024>(19/81 | 197/347) AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving (Mingfu Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingfu Liang, Jong-Chyi Su, Samuel Schulter, Sparsh Garg, Shiyu Zhao, Ying Wu, Manmohan Chandraker. (2024)<br><strong>AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving</strong><br><button class=copy-to-clipboard title="AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Object Detection, Benchmarking, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17373v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17373v1.pdf filename=2403.17373v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance. However, <b>objects</b> <b>encountered</b> on the road exhibit a long-tailed distribution, with rare or unseen categories posing challenges to a deployed perception model. This necessitates an expensive process of continuously curating and annotating data with significant human effort. We propose to leverage recent advances in <b>vision-language</b> and <b>large</b> <b>language</b> <b>models</b> to design an Automatic Data Engine (AIDE) that automatically identifies issues, efficiently curates data, improves the model through auto-labeling, and verifies the model through generation of diverse scenarios. This process operates iteratively, allowing for continuous self-improvement of the model. We further establish a <b>benchmark</b> for open-world detection on AV datasets to comprehensively evaluate various learning paradigms, demonstrating our method&rsquo;s superior performance at a reduced cost.</p></p class="citation"></blockquote><h3 id=2081--198347-coda-instructive-chain-of-domain-adaptation-with-severity-aware-visual-prompt-tuning-ziyang-gong-et-al-2024>(20/81 | 198/347) CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning (Ziyang Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Gong, Fuhao Li, Yupeng Deng, Deblina Bhattacharjee, Xiangwei Zhu, Zhenming Ji. (2024)<br><strong>CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning</strong><br><button class=copy-to-clipboard title="CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Unsupervised Learning, Domain Adaptation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17369v1.pdf filename=2403.17369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (UDA) aims to adapt models from labeled source <b>domains</b> <b>to</b> unlabeled target <b>domains.</b> <b>When</b> adapting to adverse scenes, existing UDA methods fail to perform well due to the lack of instructions, leading their models to overlook discrepancies within all adverse scenes. To tackle this, we propose CoDA which instructs models to distinguish, focus, and learn from these discrepancies at scene and image levels. Specifically, CoDA consists of a Chain-of-Domain (CoD) strategy and a Severity-Aware Visual <b>Prompt</b> Tuning (SAVPT) mechanism. CoD focuses on scene-level instructions to divide all adverse scenes into easy and hard scenes, guiding models to adapt from source to easy <b>domains</b> <b>with</b> easy scene images, and then to hard <b>domains</b> <b>with</b> hard scene images, thereby laying a solid foundation for whole adaptations. Building upon this foundation, we employ SAVPT to dive into more detailed image-level instructions to boost performance. SAVPT features a novel metric Severity that divides all adverse scene images into low-severity and high-severity images. Then Severity directs visual <b>prompts</b> and adapters, instructing models to concentrate on unified severity features instead of scene-specific features, without adding complexity to the model architecture. CoDA achieves SOTA performances on widely-used <b>benchmarks</b> under all adverse scenes. Notably, CoDA outperforms the existing ones by 4.6%, and 10.3% mIoU on the Foggy Driving, and Foggy Zurich <b>benchmarks,</b> respectively. Our code is available at <a href=https://github.com/Cuzyoung/CoDA>https://github.com/Cuzyoung/CoDA</a></p></p class="citation"></blockquote><h3 id=2181--199347-over-nav-elevating-iterative-vision-and-language-navigation-with-open-vocabulary-detection-and-structured-representation-ganlong-zhao-et-al-2024>(21/81 | 199/347) OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation (Ganlong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ganlong Zhao, Guanbin Li, Weikai Chen, Yizhou Yu. (2024)<br><strong>OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation</strong><br><button class=copy-to-clipboard title="OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Knowledge Distillation, Multi-modal, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17334v1.pdf filename=2403.17334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in Iterative <b>Vision-and-Language</b> Navigation (IVLN) introduce a more meaningful and practical paradigm of VLN by maintaining the agent&rsquo;s memory across tours of scenes. Although the long-term memory aligns better with the persistent nature of the VLN task, it poses more challenges on how to utilize the highly unstructured navigation memory with extremely sparse supervision. Towards this end, we propose OVER-NAV, which aims to go over and beyond the current arts of IVLN techniques. In particular, we propose to incorporate <b>LLMs</b> and open-vocabulary detectors to <b>distill</b> key information and establish correspondence between <b>multi-modal</b> signals. Such a mechanism introduces reliable cross-modal supervision and enables on-the-fly generalization to unseen scenes without the need of extra annotation and re-training. To fully exploit the interpreted navigation data, we further introduce a structured representation, coded Omnigraph, to effectively integrate <b>multi-modal</b> information along the tour. Accompanied with a novel omnigraph fusion mechanism, OVER-NAV is able to extract the most relevant knowledge from omnigraph for a more accurate navigating action. In addition, OVER-NAV seamlessly supports both discrete and continuous environments under a unified framework. We demonstrate the superiority of OVER-NAV in extensive experiments.</p></p class="citation"></blockquote><h3 id=2281--200347-segment-any-medical-model-extended-yihao-liu-et-al-2024>(22/81 | 200/347) Segment Any Medical Model Extended (Yihao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Liu, Jiaming Zhang, Andres Diaz-Pinto, Haowei Li, Alejandro Martin-Gomez, Amir Kheradmand, Mehran Armand. (2024)<br><strong>Segment Any Medical Model Extended</strong><br><button class=copy-to-clipboard title="Segment Any Medical Model Extended" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18114v1.pdf filename=2403.18114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Segment Anything Model (SAM) has drawn significant attention from researchers who work on medical image segmentation because of its generalizability. However, researchers have found that SAM may have limited performance on medical images compared to state-of-the-art non-foundation models. Regardless, the community sees potential in extending, <b>fine-tuning,</b> modifying, and evaluating SAM for analysis of medical imaging. An increasing number of works have been published focusing on the mentioned four directions, where variants of SAM are proposed. To this end, a unified platform helps push the boundary of the <b>foundation</b> <b>model</b> for medical images, facilitating the use, modification, and validation of SAM and its variants in medical image segmentation. In this work, we introduce SAMM Extended (SAMME), a platform that integrates new SAM variant models, adopts faster communication protocols, accommodates new interactive modes, and allows for <b>fine-tuning</b> of subcomponents of the models. These features can expand the potential of <b>foundation</b> <b>models</b> like SAM, and the results can be translated to applications such as image-guided therapy, mixed reality interaction, robotic navigation, and <b>data</b> <b>augmentation.</b></p></p class="citation"></blockquote><h3 id=2381--201347-state-of-the-art-applications-of-deep-learning-within-tracking-and-detecting-marine-debris-a-survey-zoe-moorton-et-al-2024>(23/81 | 201/347) State of the art applications of deep learning within tracking and detecting marine debris: A survey (Zoe Moorton et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zoe Moorton, Dr. Zeyneb Kurt, Dr. Wai Lok Woo. (2024)<br><strong>State of the art applications of deep learning within tracking and detecting marine debris: A survey</strong><br><button class=copy-to-clipboard title="State of the art applications of deep learning within tracking and detecting marine debris: A survey" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Yolo, Object Detection, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18067v1.pdf filename=2403.18067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning techniques have been explored within the marine litter problem for approximately 20 years but the majority of the research has developed rapidly in the last five years. We provide an in-depth, up to date, summary and analysis of 28 of the most recent and significant contributions of deep learning in marine debris. From cross referencing the research paper results, the <b>YOLO</b> family significantly outperforms all other methods of <b>object</b> <b>detection</b> but there are many respected contributions to this field that have categorically agreed that a comprehensive database of underwater debris is not currently available for machine learning. Using a small dataset curated and labelled by us, we tested YOLOv5 on a binary classification task and found the accuracy was low and the rate of false positives was high; highlighting the importance of a comprehensive database. We conclude this survey with over 40 future research <b>recommendations</b> and open challenges.</p></p class="citation"></blockquote><h3 id=2481--202347-leveraging-near-field-lighting-for-monocular-depth-estimation-from-endoscopy-videos-akshay-paruchuri-et-al-2024>(24/81 | 202/347) Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos (Akshay Paruchuri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshay Paruchuri, Samuel Ehrenstein, Shuxian Wang, Inbar Fried, Stephen M. Pizer, Marc Niethammer, Roni Sengupta. (2024)<br><strong>Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos</strong><br><button class=copy-to-clipboard title="Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17915v1.pdf filename=2403.17915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular depth estimation in endoscopy videos can enable assistive and robotic surgery to obtain better coverage of the organ and detection of various health issues. Despite promising progress on mainstream, natural image depth estimation, techniques perform poorly on endoscopy images due to a lack of strong geometric features and challenging illumination effects. In this paper, we utilize the photometric cues, i.e., the light emitted from an endoscope and reflected by the surface, to improve monocular depth estimation. We first create two novel loss functions with <b>supervised</b> and <b>self-supervised</b> variants that utilize a per-pixel shading representation. We then propose a novel depth refinement network (PPSNet) that leverages the same per-pixel shading representation. Finally, we introduce teacher-student <b>transfer</b> <b>learning</b> to produce better depth maps from both synthetic data with supervision and clinical data with self-supervision. We achieve state-of-the-art results on the C3VD dataset while estimating high-quality depth maps from clinical data. Our code, pre-trained models, and supplementary materials can be found on our project page: <a href=https://ppsnet.github.io/>https://ppsnet.github.io/</a></p></p class="citation"></blockquote><h3 id=2581--203347-to-supervise-or-not-to-supervise-understanding-and-addressing-the-key-challenges-of-3d-transfer-learning-souhail-hadgi-et-al-2024>(25/81 | 203/347) To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning (Souhail Hadgi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Souhail Hadgi, Lei Li, Maks Ovsjanikov. (2024)<br><strong>To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning</strong><br><button class=copy-to-clipboard title="To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17869v1.pdf filename=2403.17869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transfer</b> <b>learning</b> has long been a key factor in the advancement of many fields including 2D image analysis. Unfortunately, its applicability in 3D data processing has been relatively limited. While several approaches for 3D <b>transfer</b> <b>learning</b> have been proposed in recent literature, with <b>contrastive</b> <b>learning</b> gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios. Most importantly, there is currently a lack of principled understanding of both when and why 3D <b>transfer</b> <b>learning</b> methods are applicable. Remarkably, even the applicability of standard <b>supervised</b> pre-training is poorly understood. In this work, we conduct the first in-depth quantitative and qualitative investigation of <b>supervised</b> and <b>contrastive</b> <b>pre-training</b> strategies and their utility in downstream 3D tasks. We demonstrate that layer-wise analysis of learned features provides significant insight into the downstream utility of trained networks. Informed by this analysis, we propose a simple geometric regularization strategy, which improves the transferability of <b>supervised</b> pre-training. Our work thus sheds light onto both the specific challenges of 3D <b>transfer</b> <b>learning,</b> as well as strategies to overcome them.</p></p class="citation"></blockquote><h3 id=2681--204347-not-all-similarities-are-created-equal-leveraging-data-driven-biases-to-inform-genai-copyright-disputes-uri-hacohen-et-al-2024>(26/81 | 204/347) Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes (Uri Hacohen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano. (2024)<br><strong>Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes</strong><br><button class=copy-to-clipboard title="Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, GPT, GPT-2<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17691v1.pdf filename=2403.17691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI <b>GPT,</b> and Stable <b>Diffusion,</b> <b>has</b> revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with <b>GPT2</b> and Stable <b>Diffusion</b> <b>models.</b> Copyright law distinguishes between original expressions and generic ones (Sc`enes `a faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works. We propose a data-driven approach to identify the genericity of works created by GenAI, employing &ldquo;data-driven bias&rdquo; to assess the genericity of expressive compositions. This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model&rsquo;s dataset. The potential implications of measuring expressive genericity for copyright law are profound. Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals. More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI.</p></p class="citation"></blockquote><h3 id=2781--205347-uada3d-unsupervised-adversarial-domain-adaptation-for-3d-object-detection-with-sparse-lidar-and-large-domain-gaps-maciej-k-wozniak-et-al-2024>(27/81 | 205/347) UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps (Maciej K Wozniak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt. (2024)<br><strong>UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps</strong><br><button class=copy-to-clipboard title="UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17633v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17633v2.pdf filename=2403.17633v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we address a gap in existing <b>unsupervised</b> <b>domain</b> <b>adaptation</b> approaches on LiDAR-based 3D <b>object</b> <b>detection,</b> which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce <b>Unsupervised</b> Adversarial <b>Domain</b> <b>Adaptation</b> for 3D <b>Object</b> <b>Detection</b> (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn <b>domain-invariant</b> <b>features.</b> We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot <b>domains.</b> <b>Our</b> code is open-source and will be available soon.</p></p class="citation"></blockquote><h3 id=2881--206347-move-as-you-say-interact-as-you-can-language-guided-human-motion-generation-with-scene-affordance-zan-wang-et-al-2024>(28/81 | 206/347) Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance (Zan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang. (2024)<br><strong>Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance</strong><br><button class=copy-to-clipboard title="Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Diffusion Model, Benchmarking, Multi-modal, Multi-modal, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18036v1.pdf filename=2403.18036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models&rsquo; intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene <b>grounding</b> and conditional motion generation. Our framework comprises an Affordance <b>Diffusion</b> <b>Model</b> (ADM) for predicting explicit affordance map and an Affordance-to-Motion <b>Diffusion</b> <b>Model</b> (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under <b>multimodal</b> condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established <b>benchmarks,</b> including HumanML3D and HUMANISE. Additionally, we validate our model&rsquo;s exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.</p></p class="citation"></blockquote><h3 id=2981--207347-remamber-referring-image-segmentation-with-mamba-twister-yuhuan-yang-et-al-2024>(29/81 | 207/347) ReMamber: Referring Image Segmentation with Mamba Twister (Yuhuan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhuan Yang, Chaofan Ma, Jiangchao Yao, Zhun Zhong, Ya Zhang, Yanfeng Wang. (2024)<br><strong>ReMamber: Referring Image Segmentation with Mamba Twister</strong><br><button class=copy-to-clipboard title="ReMamber: Referring Image Segmentation with Mamba Twister" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Transformer, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17839v1.pdf filename=2403.17839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Referring Image Segmentation (RIS) leveraging <b>transformers</b> has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to <b>multi-modal</b> interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of <b>multi-modal</b> data. In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a <b>multi-modal</b> Mamba Twister block. The Mamba Twister explicitly models <b>image-text</b> interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve the state-of-the-art on three challenging <b>benchmarks.</b> Moreover, we conduct thorough analyses of ReMamber and discuss other fusion designs using Mamba. These provide valuable perspectives for future research.</p></p class="citation"></blockquote><h3 id=3081--208347-hierarchical-light-transformer-ensembles-for-multimodal-trajectory-forecasting-adrien-lafage-et-al-2024>(30/81 | 208/347) Hierarchical Light Transformer Ensembles for Multimodal Trajectory Forecasting (Adrien Lafage et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrien Lafage, Mathieu Barbier, Gianni Franchi, David Filliat. (2024)<br><strong>Hierarchical Light Transformer Ensembles for Multimodal Trajectory Forecasting</strong><br><button class=copy-to-clipboard title="Hierarchical Light Transformer Ensembles for Multimodal Trajectory Forecasting" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Convolution, Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17678v1.pdf filename=2403.17678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate trajectory forecasting is crucial for the performance of various systems, such as advanced driver-assistance systems and self-driving vehicles. These forecasts allow to anticipate events leading to collisions and, therefore, to mitigate them. Deep Neural Networks have excelled in motion forecasting, but issues like overconfidence and uncertainty quantification persist. Deep Ensembles address these concerns, yet applying them to <b>multimodal</b> distributions remains challenging. In this paper, we propose a novel approach named Hierarchical Light <b>Transformer</b> Ensembles (HLT-Ens), aimed at efficiently training an ensemble of <b>Transformer</b> architectures using a novel hierarchical loss function. HLT-Ens leverages grouped fully connected layers, inspired by grouped <b>convolution</b> techniques, to capture <b>multimodal</b> distributions, effectively. Through extensive experimentation, we demonstrate that HLT-Ens achieves state-of-the-art performance levels, offering a promising avenue for improving trajectory forecasting techniques.</p></p class="citation"></blockquote><h3 id=3181--209347-diffh2o-diffusion-based-synthesis-of-hand-object-interactions-from-textual-descriptions-sammy-christen-et-al-2024>(31/81 | 209/347) DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions (Sammy Christen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin. (2024)<br><strong>DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions</strong><br><button class=copy-to-clipboard title="DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Geometry, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17827v1.pdf filename=2403.17827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text <b>prompts</b> and <b>geometry</b> of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate <b>diffusion</b> <b>models</b> for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance. Grasp guidance takes a single target grasping pose and guides the <b>diffusion</b> <b>model</b> to reach this grasp at the end of the grasping stage, which provides control over the grasping pose. Given a grasping motion from this stage, multiple different actions can be <b>prompted</b> in the interaction phase. For textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions. Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions. Moreover, we demonstrate the practicality of our framework by utilizing a hand pose estimate from an off-the-shelf pose estimator for guidance, and then sampling multiple different actions in the interaction stage.</p></p class="citation"></blockquote><h3 id=3281--210347-ocai-improving-optical-flow-estimation-by-occlusion-and-consistency-aware-interpolation-jisoo-jeong-et-al-2024>(32/81 | 210/347) OCAI: Improving Optical Flow Estimation by Occlusion and Consistency Aware Interpolation (Jisoo Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jisoo Jeong, Hong Cai, Risheek Garrepalli, Jamie Menjay Lin, Munawar Hayat, Fatih Porikli. (2024)<br><strong>OCAI: Improving Optical Flow Estimation by Occlusion and Consistency Aware Interpolation</strong><br><button class=copy-to-clipboard title="OCAI: Improving Optical Flow Estimation by Occlusion and Consistency Aware Interpolation" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Data Augmentation, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18092v1.pdf filename=2403.18092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The scarcity of ground-truth labels poses one major challenge in developing optical flow estimation models that are both generalizable and robust. While current methods rely on <b>data</b> <b>augmentation,</b> they have yet to fully exploit the rich information available in labeled video sequences. We propose OCAI, a method that supports robust frame interpolation by generating intermediate video frames alongside optical flows in between. Utilizing a forward warping approach, OCAI employs occlusion awareness to resolve ambiguities in pixel values and fills in missing values by leveraging the forward-backward consistency of optical flows. Additionally, we introduce a teacher-student style <b>semi-supervised</b> <b>learning</b> method on top of the interpolated frames. Using a pair of unlabeled frames and the teacher model&rsquo;s predicted optical flow, we generate interpolated frames and flows to train a student model. The teacher&rsquo;s weights are maintained using Exponential Moving Averaging of the student. Our evaluations demonstrate perceptually superior interpolation quality and enhanced optical flow accuracy on established <b>benchmarks</b> such as Sintel and KITTI.</p></p class="citation"></blockquote><h3 id=3381--211347-multi-task-dense-prediction-via-mixture-of-low-rank-experts-yuqi-yang-et-al-2024>(33/81 | 211/347) Multi-Task Dense Prediction via Mixture of Low-Rank Experts (Yuqi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Yang, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Bo Li. (2024)<br><strong>Multi-Task Dense Prediction via Mixture of Low-Rank Experts</strong><br><button class=copy-to-clipboard title="Multi-Task Dense Prediction via Mixture of Low-Rank Experts" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolution, Parameter Sharing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17749v1.pdf filename=2403.17749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous multi-task dense prediction methods based on the Mixture of Experts (MoE) have received great performance but they neglect the importance of explicitly modeling the global relations among all tasks. In this paper, we present a novel decoder-focused method for multi-task dense prediction, called Mixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships, MLoRE adds a generic <b>convolution</b> path to the original MoE structure, where each task feature can go through this path for explicit <b>parameter</b> <b>sharing.</b> Furthermore, to control the <b>parameters</b> <b>and</b> computational cost brought by the increase in the number of experts, we take inspiration from LoRA and propose to leverage the low-rank format of a vanilla <b>convolution</b> in the expert network. Since the low-rank experts have fewer <b>parameters</b> <b>and</b> can be dynamically parameterized into the generic <b>convolution,</b> the <b>parameters</b> <b>and</b> computational cost do not change much with the increase of experts. Benefiting from this design, we increase the number of experts and its reception field to enlarge the representation capacity, facilitating multiple dense tasks learning in a unified network. Extensive experiments on the PASCAL-Context and NYUD-v2 <b>benchmarks</b> show that our MLoRE achieves superior performance compared to previous state-of-the-art methods on all metrics. Our code is available at <a href=https://github.com/YuqiYang213/MLoRE>https://github.com/YuqiYang213/MLoRE</a>.</p></p class="citation"></blockquote><h3 id=3481--212347-diffgaze-a-diffusion-model-for-continuous-gaze-sequence-generation-on-360-images-chuhan-jiao-et-al-2024>(34/81 | 212/347) DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on 360° Images (Chuhan Jiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai Bâce, Zhiming Hu, Andreas Bulling. (2024)<br><strong>DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on 360° Images</strong><br><button class=copy-to-clipboard title="DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on 360° Images" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17477v1.pdf filename=2403.17477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DiffGaze, a novel method for generating realistic and diverse continuous human gaze sequences on 360{\deg} images based on a conditional score-based denoising <b>diffusion</b> <b>model.</b> Generating human gaze on 360{\deg} images is important for various human-computer interaction and computer graphics applications, e.g. for creating large-scale eye tracking datasets or for realistic animation of virtual humans. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting crucial parts of natural gaze behaviour. Our method uses features extracted from 360{\deg} images as condition and uses two <b>transformers</b> to model the temporal and spatial dependencies of continuous human gaze. We evaluate DiffGaze on two 360{\deg} image <b>benchmarks</b> for gaze sequence generation as well as scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms state-of-the-art methods on all tasks on both <b>benchmarks.</b> We also report a 21-participant user study showing that our method generates gaze sequences that are indistinguishable from real human sequences.</p></p class="citation"></blockquote><h3 id=3581--213347-boosting-diffusion-models-with-moving-average-sampling-in-frequency-domain-yurui-qian-et-al-2024>(35/81 | 213/347) Boosting Diffusion Models with Moving Average Sampling in Frequency Domain (Yurui Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, Tao Mei. (2024)<br><strong>Boosting Diffusion Models with Moving Average Sampling in Frequency Domain</strong><br><button class=copy-to-clipboard title="Boosting Diffusion Models with Moving Average Sampling in Frequency Domain" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17870v1.pdf filename=2403.17870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have recently brought a powerful revolution in image generation. Despite showing impressive generative capabilities, most of these models rely on the current sample to denoise the next one, possibly resulting in denoising instability. In this paper, we reinterpret the iterative denoising process as model optimization and leverage a moving average mechanism to ensemble all the prior samples. Instead of simply applying moving average to the denoised samples at different timesteps, we first map the denoised samples to data space and then perform moving average to avoid <b>distribution</b> <b>shift</b> across timesteps. In view that <b>diffusion</b> <b>models</b> evolve the recovery from low-frequency components to high-frequency details, we further decompose the samples into different frequency components and execute moving average separately on each component. We name the complete approach &ldquo;Moving Average Sampling in Frequency domain (MASF)&rdquo;. MASF could be seamlessly integrated into mainstream pre-trained <b>diffusion</b> <b>models</b> and sampling schedules. Extensive experiments on both unconditional and conditional <b>diffusion</b> <b>models</b> demonstrate that our MASF leads to superior performances compared to the baselines, with almost negligible additional complexity cost.</p></p class="citation"></blockquote><h3 id=3681--214347-genesistex-adapting-image-denoising-diffusion-to-texture-space-chenjian-gao-et-al-2024>(36/81 | 214/347) GenesisTex: Adapting Image Denoising Diffusion to Texture Space (Chenjian Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenjian Gao, Boyan Jiang, Xinghui Li, Yingpeng Zhang, Qian Yu. (2024)<br><strong>GenesisTex: Adapting Image Denoising Diffusion to Texture Space</strong><br><button class=copy-to-clipboard title="GenesisTex: Adapting Image Denoising Diffusion to Texture Space" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17782v1.pdf filename=2403.17782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present GenesisTex, a novel method for synthesizing textures for 3D geometries from text descriptions. GenesisTex adapts the pretrained image <b>diffusion</b> <b>model</b> to texture space by texture space sampling. Specifically, we maintain a latent texture map for each viewpoint, which is updated with predicted noise on the rendering of the corresponding viewpoint. The sampled latent texture maps are then decoded into a final texture map. During the sampling process, we focus on both global and local consistency across multiple viewpoints: global consistency is achieved through the integration of style consistency mechanisms within the noise prediction network, and low-level consistency is achieved by dynamically aligning latent textures. Finally, we apply reference-based inpainting and img2img on denser views for texture refinement. Our approach overcomes the limitations of slow optimization in <b>distillation-based</b> methods and instability in inpainting-based methods. Experiments on meshes from various sources demonstrate that our method surpasses the baseline methods quantitatively and qualitatively.</p></p class="citation"></blockquote><h3 id=3781--215347-manifold-guided-lyapunov-control-with-diffusion-models-amartya-mukherjee-et-al-2024>(37/81 | 215/347) Manifold-Guided Lyapunov Control with Diffusion Models (Amartya Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amartya Mukherjee, Thanin Quartz, Jun Liu. (2024)<br><strong>Manifold-Guided Lyapunov Control with Diffusion Models</strong><br><button class=copy-to-clipboard title="Manifold-Guided Lyapunov Control with Diffusion Models" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, math-DG, math-OC, stat-CO<br>Keyword Score: 20<br>Keywords: Diffusion Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17692v1.pdf filename=2403.17692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach to generating stabilizing controllers for a large class of dynamical systems using <b>diffusion</b> <b>models.</b> The core objective is to develop stabilizing control functions by identifying the closest asymptotically stable vector field relative to a predetermined manifold and adjusting the control function based on this finding. To achieve this, we employ a <b>diffusion</b> <b>model</b> trained on pairs consisting of asymptotically stable vector fields and their corresponding Lyapunov functions. Our numerical results demonstrate that this pre-trained model can achieve stabilization over previously unseen systems efficiently and rapidly, showcasing the potential of our approach in fast <b>zero-shot</b> control and generalizability.</p></p class="citation"></blockquote><h3 id=3881--216347-senm-vae-semi-supervised-noise-modeling-with-hierarchical-variational-autoencoder-dihan-zheng-et-al-2024>(38/81 | 216/347) SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder (Dihan Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dihan Zheng, Yihang Zou, Xiaowen Zhang, Chenglong Bao. (2024)<br><strong>SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder</strong><br><button class=copy-to-clipboard title="SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17502v1.pdf filename=2403.17502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The data bottleneck has emerged as a fundamental challenge in learning based image restoration methods. Researchers have attempted to generate synthesized training data using paired or unpaired samples to address this challenge. This study proposes SeNM-VAE, a semi-supervised noise modeling method that leverages both paired and unpaired datasets to generate realistic degraded data. Our approach is based on modeling the conditional distribution of degraded and clean images with a specially designed graphical model. Under the <b>variational</b> <b>inference</b> framework, we develop an objective function for handling both paired and unpaired data. We employ our method to generate paired training samples for real-world image denoising and super-resolution tasks. Our approach excels in the quality of synthetic degraded images compared to other unpaired and paired noise modeling methods. Furthermore, our approach demonstrates remarkable performance in downstream image restoration tasks, even with limited paired data. With more paired data, our method achieves the best performance on the SIDD dataset.</p></p class="citation"></blockquote><h3 id=3981--217347-track-everything-everywhere-fast-and-robustly-yunzhou-song-et-al-2024>(39/81 | 217/347) Track Everything Everywhere Fast and Robustly (Yunzhou Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, Kostas Daniilidis. (2024)<br><strong>Track Everything Everywhere Fast and Robustly</strong><br><button class=copy-to-clipboard title="Track Everything Everywhere Fast and Robustly" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Foundation Model, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17931v1.pdf filename=2403.17931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel test-time optimization approach for efficiently and robustly tracking any pixel at any time in a video. The latest state-of-the-art optimization-based tracking technique, OmniMotion, requires a prohibitively long optimization time, rendering it impractical for downstream applications. OmniMotion is sensitive to the choice of random seeds, leading to unstable convergence. To improve efficiency and robustness, we introduce a novel invertible deformation network, CaDeX++, which factorizes the function representation into a local spatial-temporal feature grid and enhances the expressivity of the coupling blocks with non-linear functions. While CaDeX++ incorporates a stronger geometric bias within its architectural design, it also takes advantage of the inductive bias provided by the vision <b>foundation</b> <b>models.</b> Our system utilizes monocular depth estimation to represent scene <b>geometry</b> and enhances the objective by incorporating DINOv2 long-term semantics to regulate the optimization process. Our experiments demonstrate a substantial improvement in training speed (more than \textbf{10 times} faster), robustness, and accuracy in tracking over the SoTA optimization-based method OmniMotion.</p></p class="citation"></blockquote><h3 id=4081--218347-dn-splatter-depth-and-normal-priors-for-gaussian-splatting-and-meshing-matias-turkulainen-et-al-2024>(40/81 | 218/347) DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing (Matias Turkulainen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala. (2024)<br><strong>DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing</strong><br><button class=copy-to-clipboard title="DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17822v1.pdf filename=2403.17822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the <b>geometry</b> of the 3D Gaussians <b>supervised</b> by normal cues to achieve better alignment with the true scene <b>geometry.</b> We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in <a href=https://github.com/maturk/dn-splatter>https://github.com/maturk/dn-splatter</a>.</p></p class="citation"></blockquote><h3 id=4181--219347-egolifter-open-world-3d-segmentation-for-egocentric-perception-qiao-gu-et-al-2024>(41/81 | 219/347) EgoLifter: Open-world 3D Segmentation for Egocentric Perception (Qiao Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney. (2024)<br><strong>EgoLifter: Open-world 3D Segmentation for Egocentric Perception</strong><br><button class=copy-to-clipboard title="EgoLifter: Open-world 3D Segmentation for Egocentric Perception" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18118v1.pdf filename=2403.18118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as <b>weak</b> <b>supervision</b> to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new <b>benchmark</b> on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale.</p></p class="citation"></blockquote><h3 id=4281--220347-efficient-video-object-segmentation-via-modulated-cross-attention-memory-abdelrahman-shaker-et-al-2024>(42/81 | 220/347) Efficient Video Object Segmentation via Modulated Cross-Attention Memory (Abdelrahman Shaker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelrahman Shaker, Syed Talal Wasim, Martin Danelljan, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. (2024)<br><strong>Efficient Video Object Segmentation via Modulated Cross-Attention Memory</strong><br><button class=copy-to-clipboard title="Efficient Video Object Segmentation via Modulated Cross-Attention Memory" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17937v1.pdf filename=2403.17937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>transformer-based</b> approaches have shown promising results for semi-supervised video object segmentation. However, these approaches typically struggle on long videos due to increased GPU memory demands, as they frequently expand the memory bank every few frames. We propose a <b>transformer-based</b> approach, named MAVOS, that introduces an optimized and dynamic long-term modulated cross-attention (MCA) memory to model temporal smoothness without requiring frequent memory expansion. The proposed MCA effectively encodes both local and global features at various levels of granularity while efficiently maintaining consistent speed regardless of the video length. Extensive experiments on multiple <b>benchmarks,</b> LVOS, Long-Time Video, and DAVIS 2017, demonstrate the effectiveness of our proposed contributions leading to real-time inference and markedly reduced memory demands without any degradation in segmentation accuracy on long videos. Compared to the best existing <b>transformer-based</b> approach, our MAVOS increases the speed by 7.6x, while significantly reducing the GPU memory by 87% with comparable segmentation performance on short and long video datasets. Notably on the LVOS dataset, our MAVOS achieves a J&amp;F score of 63.3% while operating at 37 frames per second (FPS) on a single V100 GPU. Our code and models will be publicly available at: <a href=https://github.com/Amshaker/MAVOS>https://github.com/Amshaker/MAVOS</a>.</p></p class="citation"></blockquote><h3 id=4381--221347-text-is-mass-modeling-as-stochastic-embedding-for-text-video-retrieval-jiamian-wang-et-al-2024>(43/81 | 221/347) Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval (Jiamian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao. (2024)<br><strong>Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval</strong><br><button class=copy-to-clipboard title="Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17998v1.pdf filename=2403.17998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing prevalence of video clips has sparked growing interest in <b>text-video</b> <b>retrieval.</b> Recent advances focus on establishing a joint embedding space for <b>text</b> <b>and</b> video, relying on consistent embedding representations to compute similarity. However, the <b>text</b> <b>content</b> in existing datasets is generally short and concise, making it hard to fully describe the redundant semantics of a video. Correspondingly, a single <b>text</b> <b>embedding</b> may be less expressive to capture the video embedding and empower the retrieval. In this study, we propose a new stochastic <b>text</b> <b>modeling</b> method T-MASS, i.e., <b>text</b> <b>is</b> modeled as a stochastic embedding, to enrich <b>text</b> <b>embedding</b> with a flexible and resilient semantic range, yielding a <b>text</b> <b>mass.</b> To be specific, we introduce a similarity-aware radius module to adapt the scale of the <b>text</b> <b>mass</b> upon the given <b>text-video</b> <b>pairs.</b> Plus, we design and develop a support <b>text</b> <b>regularization</b> to further control the <b>text</b> <b>mass</b> during the training. The inference pipeline is also tailored to fully exploit the <b>text</b> <b>mass</b> for accurate retrieval. Empirical evidence suggests that T-MASS not only effectively attracts relevant <b>text-video</b> <b>pairs</b> while distancing irrelevant ones, but also enables the determination of precise <b>text</b> <b>embeddings</b> for relevant pairs. Our experimental results show a substantial improvement of T-MASS over baseline (3% to 6.3% by R@1). Also, T-MASS achieves state-of-the-art performance on five <b>benchmark</b> datasets, including MSRVTT, LSMDC, DiDeMo, VATEX, and Charades.</p></p class="citation"></blockquote><h3 id=4481--222347-noise2noise-denoising-of-crism-hyperspectral-data-robert-platt-et-al-2024>(44/81 | 222/347) Noise2Noise Denoising of CRISM Hyperspectral Data (Robert Platt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Platt, Rossella Arcucci, Cédric M. John. (2024)<br><strong>Noise2Noise Denoising of CRISM Hyperspectral Data</strong><br><button class=copy-to-clipboard title="Noise2Noise Denoising of CRISM Hyperspectral Data" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17757v1.pdf filename=2403.17757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars. Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable. Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images. Our model is <b>self-supervised</b> and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce. We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming <b>benchmark</b> methods on most metrics. This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites.</p></p class="citation"></blockquote><h3 id=4581--223347-groupwise-query-specialization-and-quality-aware-multi-assignment-for-transformer-based-visual-relationship-detection-jongha-kim-et-al-2024>(45/81 | 223/347) Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection (Jongha Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongha Kim, Jihwan Park, Jinyoung Park, Jinyoung Kim, Sehyung Kim, Hyunwoo J. Kim. (2024)<br><strong>Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection</strong><br><button class=copy-to-clipboard title="Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17709v1.pdf filename=2403.17709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual Relationship Detection (VRD) has seen significant advancements with <b>Transformer-based</b> architectures recently. However, we identify two key limitations in a conventional label assignment for training <b>Transformer-based</b> VRD models, which is a process of mapping a ground-truth (GT) to a prediction. Under the conventional assignment, an unspecialized query is trained since a query is expected to detect every relation, which makes it difficult for a query to specialize in specific relations. Furthermore, a query is also insufficiently trained since a GT is assigned only to a single prediction, therefore near-correct or even correct predictions are suppressed by being assigned no relation as a GT. To address these issues, we propose Groupwise Query Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise Query Specialization trains a specialized query by dividing queries and relations into disjoint groups and directing a query in a specific query group solely toward relations in the corresponding relation group. Quality-Aware Multi-Assignment further facilitates the training by assigning a GT to multiple predictions that are significantly close to a GT in terms of a subject, an object, and the relation in between. Experimental results and analyses show that SpeaQ effectively trains specialized queries, which better utilize the capacity of a model, resulting in consistent performance gains with zero additional inference cost across multiple VRD models and <b>benchmarks.</b> Code is available at <a href=https://github.com/mlvlab/SpeaQ>https://github.com/mlvlab/SpeaQ</a>.</p></p class="citation"></blockquote><h3 id=4681--224347-lare2-latent-reconstruction-error-based-method-for-diffusion-generated-image-detection-yunpeng-luo-et-al-2024>(46/81 | 224/347) LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection (Yunpeng Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding. (2024)<br><strong>LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection</strong><br><button class=copy-to-clipboard title="LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17465v1.pdf filename=2403.17465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of <b>Diffusion</b> <b>Models</b> has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the <b>diffusion-generated</b> <b>images.</b> We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives. Extensive experiments on the large-scale GenImage <b>benchmark</b> demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators. LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times.</p></p class="citation"></blockquote><h3 id=4781--225347-decoupled-pseudo-labeling-for-semi-supervised-monocular-3d-object-detection-jiacheng-zhang-et-al-2024>(47/81 | 225/347) Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object Detection (Jiacheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Zhang, Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li. (2024)<br><strong>Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object Detection</strong><br><button class=copy-to-clipboard title="Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object Detection" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17387v1.pdf filename=2403.17387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We delve into pseudo-labeling for semi-supervised monocular 3D <b>object</b> <b>detection</b> (SSM3OD) and discover two primary issues: a misalignment between the prediction quality of 3D and 2D attributes and the tendency of depth supervision derived from pseudo-labels to be noisy, leading to significant optimization conflicts with other reliable forms of supervision. We introduce a novel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach features a Decoupled Pseudo-label Generation (DPG) module, designed to efficiently generate pseudo-labels by separately processing 2D and 3D attributes. This module incorporates a unique homography-based method for identifying dependable pseudo-labels in BEV space, specifically for 3D attributes. Additionally, we present a DepthGradient Projection (DGP) module to mitigate optimization conflicts caused by noisy depth supervision of pseudo-labels, effectively decoupling the depth gradient and removing conflicting gradients. This dual decoupling strategy-at both the pseudo-label generation and gradient levels-significantly improves the utilization of pseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI <b>benchmark</b> demonstrate the superiority of our method over existing approaches.</p></p class="citation"></blockquote><h3 id=4881--226347-quakeset-a-dataset-and-low-resource-models-to-monitor-earthquakes-through-sentinel-1-daniele-rege-cambrin-et-al-2024>(48/81 | 226/347) QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1 (Daniele Rege Cambrin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniele Rege Cambrin, Paolo Garza. (2024)<br><strong>QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1</strong><br><button class=copy-to-clipboard title="QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18116v1.pdf filename=2403.18116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Earthquake monitoring is necessary to promptly identify the affected areas, the severity of the events, and, finally, to estimate damages and plan the actions needed for the restoration process. The use of seismic stations to monitor the strength and origin of earthquakes is limited when dealing with remote areas (we cannot have global capillary coverage). Identification and analysis of all affected areas is mandatory to support areas not monitored by traditional stations. Using social media images in crisis management has proven effective in various situations. However, they are still limited by the possibility of using communication infrastructures in case of an earthquake and by the presence of people in the area. Moreover, social media images and messages cannot be used to estimate the actual severity of earthquakes and their characteristics effectively. The employment of satellites to monitor changes around the globe grants the possibility of exploiting instrumentation that is not limited by the visible spectrum, the presence of land infrastructures, and people in the affected areas. In this work, we propose a new dataset composed of images taken from Sentinel-1 and a new series of tasks to help monitor earthquakes from a new detailed view. Coupled with the data, we provide a series of traditional machine learning and deep learning models as baselines to assess the effectiveness of ML-based models in earthquake analysis.</p></p class="citation"></blockquote><h3 id=4981--227347-egoposeformer-a-simple-baseline-for-egocentric-3d-human-pose-estimation-chenhongyi-yang-et-al-2024>(49/81 | 227/347) EgoPoseFormer: A Simple Baseline for Egocentric 3D Human Pose Estimation (Chenhongyi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhongyi Yang, Anastasia Tkach, Shreyas Hampali, Linguang Zhang, Elliot J. Crowley, Cem Keskin. (2024)<br><strong>EgoPoseFormer: A Simple Baseline for Egocentric 3D Human Pose Estimation</strong><br><button class=copy-to-clipboard title="EgoPoseFormer: A Simple Baseline for Egocentric 3D Human Pose Estimation" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18080v1.pdf filename=2403.18080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present EgoPoseFormer, a simple yet effective <b>transformer-based</b> model for stereo egocentric human pose estimation. The main challenge in egocentric pose estimation is overcoming joint invisibility, which is caused by self-occlusion or a limited field of view (FOV) of head-mounted cameras. Our approach overcomes this challenge by incorporating a two-stage pose estimation paradigm: in the first stage, our model leverages the global information to estimate each joint&rsquo;s coarse location, then in the second stage, it employs a DETR style <b>transformer</b> to refine the coarse locations by exploiting fine-grained stereo visual features. In addition, we present a deformable stereo operation to enable our <b>transformer</b> to effectively process multi-view features, which enables it to accurately localize each joint in the 3D world. We evaluate our method on the stereo UnrealEgo dataset and show it significantly outperforms previous approaches while being computationally efficient: it improves MPJPE by 27.4mm (45% improvement) with only 7.9% model parameters and 13.1% FLOPs compared to the state-of-the-art. Surprisingly, with proper training techniques, we find that even our first-stage pose proposal network can achieve superior performance compared to previous arts. We also show that our method can be seamlessly extended to monocular settings, which achieves state-of-the-art performance on the SceneEgo dataset, improving MPJPE by 25.5mm (21% improvement) compared to the best existing method with only 60.7% model parameters and 36.4% FLOPs.</p></p class="citation"></blockquote><h3 id=5081--228347-every-shot-counts-using-exemplars-for-repetition-counting-in-videos-saptarshi-sinha-et-al-2024>(50/81 | 228/347) Every Shot Counts: Using Exemplars for Repetition Counting in Videos (Saptarshi Sinha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saptarshi Sinha, Alexandros Stergiou, Dima Damen. (2024)<br><strong>Every Shot Counts: Using Exemplars for Repetition Counting in Videos</strong><br><button class=copy-to-clipboard title="Every Shot Counts: Using Exemplars for Repetition Counting in Videos" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18074v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18074v1.pdf filename=2403.18074v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video repetition counting infers the number of repetitions of recurring actions or motion within a video. We propose an exemplar-based approach that discovers visual correspondence of video exemplars across repetitions within target videos. Our proposed Every Shot Counts (ESCounts) model is an attention-based encoder-decoder that encodes videos of varying lengths alongside exemplars from the same and different videos. In training, ESCounts regresses locations of high correspondence to the exemplars within the video. In tandem, our method learns a latent that encodes representations of general repetitive motions, which we use for exemplar-free, <b>zero-shot</b> inference. Extensive experiments over commonly used datasets (RepCount, Countix, and UCFRep) showcase ESCounts obtaining state-of-the-art performance across all three datasets. On RepCount, ESCounts increases the off-by-one from 0.39 to 0.56 and decreases the mean absolute error from 0.38 to 0.21. Detailed ablations further demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=5181--229347-aios-all-in-one-stage-expressive-human-pose-and-shape-estimation-qingping-sun-et-al-2024>(51/81 | 229/347) AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation (Qingping Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingping Sun, Yanjun Wang, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiyi Mei, Chi Sing Leung, Ziwei Liu, Lei Yang, Zhongang Cai. (2024)<br><strong>AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation</strong><br><button class=copy-to-clipboard title="AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17934v1.pdf filename=2403.17934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh recovery) involves the human body, hand, and expression estimation. Most existing methods have tackled this task in a two-stage manner, first detecting the human body part with an off-the-shelf detection model and inferring the different human body parts individually. Despite the impressive results achieved, these methods suffer from 1) loss of valuable contextual information via cropping, 2) introducing distractions, and 3) lacking inter-association among different persons and body parts, inevitably causing performance degradation, especially for crowded scenes. To address these issues, we introduce a novel all-in-one-stage framework, AiOS, for multiple expressive human pose and shape recovery without an additional human detection step. Specifically, our method is built upon DETR, which treats multi-person whole-body mesh recovery task as a progressive set prediction problem with various sequential detection. We devise the decoder tokens and extend them to our task. Specifically, we first employ a human token to probe a human location in the image and encode global features for each instance, which provides a coarse location for the later <b>transformer</b> block. Then, we introduce a joint-related token to probe the human joint in the image and encoder a fine-grained local feature, which collaborates with the global feature to regress the whole-body mesh. This straightforward but effective model outperforms previous state-of-the-art methods by a 9% reduction in NMVE on AGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a 3% reduction in PVE on EgoBody.</p></p class="citation"></blockquote><h3 id=5281--230347-superior-and-pragmatic-talking-face-generation-with-teacher-student-framework-chao-liang-et-al-2024>(52/81 | 230/347) Superior and Pragmatic Talking Face Generation with Teacher-Student Framework (Chao Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Liang, Jianwen Jiang, Tianyun Zhong, Gaojie Lin, Zhengkun Rong, Jiaqi Yang, Yongming Zhu. (2024)<br><strong>Superior and Pragmatic Talking Face Generation with Teacher-Student Framework</strong><br><button class=copy-to-clipboard title="Superior and Pragmatic Talking Face Generation with Teacher-Student Framework" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17883v1.pdf filename=2403.17883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Talking face generation technology creates talking videos from arbitrary appearance and motion signal, with the &ldquo;arbitrary&rdquo; offering ease of use but also introducing challenges in practical applications. Existing methods work well with standard inputs but suffer serious performance degradation with intricate real-world ones. Moreover, efficiency is also an important concern in deployment. To comprehensively address these issues, we introduce SuperFace, a teacher-student framework that balances quality, robustness, cost and editability. We first propose a simple but effective teacher model capable of handling inputs of varying qualities to generate high-quality results. Building on this, we devise an efficient <b>distillation</b> strategy to acquire an identity-specific student model that maintains quality with significantly reduced computational load. Our experiments validate that SuperFace offers a more comprehensive solution than existing methods for the four mentioned objectives, especially in reducing FLOPs by 99% with the student model. SuperFace can be driven by both video and audio and allows for localized facial attributes editing.</p></p class="citation"></blockquote><h3 id=5381--231347-low-latency-neural-stereo-streaming-qiqi-hou-et-al-2024>(53/81 | 231/347) Low-Latency Neural Stereo Streaming (Qiqi Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiqi Hou, Farzad Farhadzadeh, Amir Said, Guillaume Sautiere, Hoang Le. (2024)<br><strong>Low-Latency Neural Stereo Streaming</strong><br><button class=copy-to-clipboard title="Low-Latency Neural Stereo Streaming" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17879v1.pdf filename=2403.17879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit <b>mutual</b> <b>information</b> among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.</p></p class="citation"></blockquote><h3 id=5481--232347-fastperson-enhancing-video-learning-through-effective-video-summarization-that-preserves-linguistic-and-visual-contexts-kazuki-kawamura-et-al-2024>(54/81 | 232/347) FastPerson: Enhancing Video Learning through Effective Video Summarization that Preserves Linguistic and Visual Contexts (Kazuki Kawamura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kazuki Kawamura, Jun Rekimoto. (2024)<br><strong>FastPerson: Enhancing Video Learning through Effective Video Summarization that Preserves Linguistic and Visual Contexts</strong><br><button class=copy-to-clipboard title="FastPerson: Enhancing Video Learning through Effective Video Summarization that Preserves Linguistic and Visual Contexts" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-HC, cs-MM, cs.CV<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17727v1.pdf filename=2403.17727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quickly understanding lengthy lecture videos is essential for learners with limited time and interest in various topics to improve their learning efficiency. To this end, video <b>summarization</b> has been actively researched to enable users to view only important scenes from a video. However, these studies focus on either the visual or audio information of a video and extract important segments in the video. Therefore, there is a risk of missing important information when both the teacher&rsquo;s speech and visual information on the blackboard or slides are important, such as in a lecture video. To tackle this issue, we propose FastPerson, a video <b>summarization</b> approach that considers both the visual and auditory information in lecture videos. FastPerson creates summary videos by utilizing audio transcriptions along with on-screen images and text, minimizing the risk of overlooking crucial information for learners. Further, it provides a feature that allows learners to switch between the summary and original videos for each chapter of the video, enabling them to adjust the pace of learning based on their interests and level of understanding. We conducted an evaluation with 40 participants to assess the effectiveness of our method and confirmed that it reduced viewing time by 53% at the same level of comprehension as that when using traditional video playback methods.</p></p class="citation"></blockquote><h3 id=5581--233347-plainmamba-improving-non-hierarchical-mamba-in-visual-recognition-chenhongyi-yang-et-al-2024>(55/81 | 233/347) PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition (Chenhongyi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, Elliot J. Crowley. (2024)<br><strong>PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition</strong><br><button class=copy-to-clipboard title="PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17695v1.pdf filename=2403.17695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the need for special tokens. We evaluate PlainMamba on a variety of visual recognition tasks including image classification, semantic segmentation, <b>object</b> <b>detection,</b> and instance segmentation. Our method achieves performance gains over previous non-hierarchical models and is competitive with hierarchical alternatives. For tasks requiring high-resolution inputs, in particular, PlainMamba requires much less computing while maintaining high performance. Code and models are available at <a href=https://github.com/ChenhongyiYang/PlainMamba>https://github.com/ChenhongyiYang/PlainMamba</a></p></p class="citation"></blockquote><h3 id=5681--234347-aniportrait-audio-driven-synthesis-of-photorealistic-portrait-animation-huawei-wei-et-al-2024>(56/81 | 234/347) AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation (Huawei Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huawei Wei, Zejun Yang, Zhisheng Wang. (2024)<br><strong>AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</strong><br><button class=copy-to-clipboard title="AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17694v1.pdf filename=2403.17694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust <b>diffusion</b> <b>model,</b> coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at <a href=https://github.com/scutzzj/AniPortrait>https://github.com/scutzzj/AniPortrait</a></p></p class="citation"></blockquote><h3 id=5781--235347-difffae-advancing-high-fidelity-one-shot-facial-appearance-editing-with-space-sensitive-customization-and-semantic-preservation-qilin-wang-et-al-2024>(57/81 | 235/347) DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with Space-sensitive Customization and Semantic Preservation (Qilin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qilin Wang, Jiangning Zhang, Chengming Xu, Weijian Cao, Ying Tai, Yue Han, Yanhao Ge, Hong Gu, Chengjie Wang, Yanwei Fu. (2024)<br><strong>DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with Space-sensitive Customization and Semantic Preservation</strong><br><button class=copy-to-clipboard title="DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with Space-sensitive Customization and Semantic Preservation" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17664v1.pdf filename=2403.17664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial Appearance Editing (FAE) aims to modify physical attributes, such as pose, expression and lighting, of human facial images while preserving attributes like identity and background, showing great importance in photograph. In spite of the great progress in this area, current researches generally meet three challenges: low generation fidelity, poor attribute preservation, and inefficient inference. To overcome above challenges, this paper presents DiffFAE, a one-stage and highly-efficient <b>diffusion-based</b> <b>framework</b> tailored for high-fidelity FAE. For high-fidelity query attributes transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures the fidelity and generalization ability by utilizing rendering texture derived from 3D Morphable Model (3DMM). In order to preserve source attributes, we introduce the Region-responsive Semantic Composition (RSC). This module is guided to learn decoupled source-regarding features, thereby better preserving the identity and alleviating artifacts from non-facial attributes such as hair, clothes, and background. We further introduce a consistency regularization for our pipeline to enhance editing controllability by leveraging prior knowledge in the attention matrices of <b>diffusion</b> <b>model.</b> Extensive experiments demonstrate the superiority of DiffFAE over existing methods, achieving state-of-the-art performance in facial appearance editing.</p></p class="citation"></blockquote><h3 id=5881--236347-learning-with-unreliability-fast-few-shot-voxel-radiance-fields-with-relative-geometric-consistency-yingjie-xu-et-al-2024>(58/81 | 236/347) Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency (Yingjie Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingjie Xu, Bangzhen Liu, Hao Tang, Bailin Deng, Shengfeng He. (2024)<br><strong>Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency</strong><br><button class=copy-to-clipboard title="Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17638v1.pdf filename=2403.17638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a voxel-based optimization framework, ReVoRF, for <b>few-shot</b> radiance fields that strategically address the unreliability in pseudo novel view synthesis. Our method pivots on the insight that relative depth relationships within neighboring regions are more reliable than the absolute color values in disoccluded areas. Consequently, we devise a bilateral geometric consistency loss that carefully navigates the trade-off between color fidelity and geometric accuracy in the context of depth consistency for uncertain regions. Moreover, we present a reliability-guided learning strategy to discern and utilize the variable quality across synthesized views, complemented by a reliability-aware voxel smoothing algorithm that smoothens the transition between reliable and unreliable data patches. Our approach allows for a more nuanced use of all available data, promoting enhanced learning from regions previously considered unsuitable for high-quality reconstruction. Extensive experiments across diverse datasets reveal that our approach attains significant gains in efficiency and accuracy, delivering rendering speeds of 3 FPS, 7 mins to train a $360^\circ$ scene, and a 5% improvement in PSNR over existing <b>few-shot</b> methods. Code is available at <a href=https://github.com/HKCLynn/ReVoRF>https://github.com/HKCLynn/ReVoRF</a>.</p></p class="citation"></blockquote><h3 id=5981--237347-aniartavatar-animatable-3d-art-avatar-from-a-single-image-shaoxu-li-2024>(59/81 | 237/347) AniArtAvatar: Animatable 3D Art Avatar from a Single Image (Shaoxu Li, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoxu Li. (2024)<br><strong>AniArtAvatar: Animatable 3D Art Avatar from a Single Image</strong><br><button class=copy-to-clipboard title="AniArtAvatar: Animatable 3D Art Avatar from a Single Image" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17631v1.pdf filename=2403.17631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel approach for generating animatable 3D-aware art avatars from a single image, with controllable facial expressions, head poses, and shoulder movements. Unlike previous reenactment methods, our approach utilizes a view-conditioned 2D <b>diffusion</b> <b>model</b> to synthesize multi-view images from a single art portrait with a neutral expression. With the generated colors and normals, we synthesize a static avatar using an SDF-based neural surface. For avatar animation, we extract control points, transfer the motion with these points, and deform the implicit canonical space. Firstly, we render the front image of the avatar, extract the 2D landmarks, and project them to the 3D space using a trained SDF network. We extract 3D driving landmarks using 3DMM and transfer the motion to the avatar landmarks. To animate the avatar pose, we manually set the body height and bound the head and torso of an avatar with two cages. The head and torso can be animated by transforming the two cages. Our approach is a one-shot pipeline that can be applied to various styles. Experiments demonstrate that our method can generate high-quality 3D art avatars with desired control over different motions.</p></p class="citation"></blockquote><h3 id=6081--238347-test-time-adaptation-meets-image-enhancement-improving-accuracy-via-uncertainty-aware-logit-switching-shohei-enomoto-et-al-2024>(60/81 | 238/347) Test-time Adaptation Meets Image Enhancement: Improving Accuracy via Uncertainty-aware Logit Switching (Shohei Enomoto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shohei Enomoto, Naoya Hasegawa, Kazuki Adachi, Taku Sasaki, Shin&rsquo;ya Yamaguchi, Satoshi Suzuki, Takeharu Eda. (2024)<br><strong>Test-time Adaptation Meets Image Enhancement: Improving Accuracy via Uncertainty-aware Logit Switching</strong><br><button class=copy-to-clipboard title="Test-time Adaptation Meets Image Enhancement: Improving Accuracy via Uncertainty-aware Logit Switching" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, stat-ML<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17423v1.pdf filename=2403.17423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks have achieved remarkable success in a variety of computer vision applications. However, there is a problem of degrading accuracy when the data <b>distribution</b> <b>shifts</b> between training and testing. As a solution of this problem, Test-time Adaptation~(TTA) has been well studied because of its practicality. Although TTA methods increase accuracy under <b>distribution</b> <b>shift</b> by updating the model at test time, using high-uncertainty predictions is known to degrade accuracy. Since the input image is the root of the <b>distribution</b> <b>shift,</b> we incorporate a new perspective on enhancing the input image into TTA methods to reduce the prediction&rsquo;s uncertainty. We hypothesize that enhancing the input image reduces prediction&rsquo;s uncertainty and increase the accuracy of TTA methods. On the basis of our hypothesis, we propose a novel method: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the classification model is combined with the image enhancement model that transforms input images into recognition-friendly ones, and these models are updated by existing TTA methods. Furthermore, we found that the prediction from the enhanced image does not always have lower uncertainty than the prediction from the original image. Thus, we propose logit switching, which compares the uncertainty measure of these predictions and outputs the lower one. In our experiments, we evaluate TECA with various TTA methods and show that TECA reduces prediction&rsquo;s uncertainty and increases accuracy of TTA methods despite having no hyperparameters and little parameter overhead.</p></p class="citation"></blockquote><h3 id=6181--239347-interhandgen-two-hand-interaction-generation-via-cascaded-reverse-diffusion-jihyun-lee-et-al-2024>(61/81 | 239/347) InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion (Jihyun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim. (2024)<br><strong>InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion</strong><br><button class=copy-to-clipboard title="InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17422v1.pdf filename=2403.17422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular, we introduce a <b>diffusion</b> <b>model</b> that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our <b>diffusion</b> <b>prior</b> can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy.</p></p class="citation"></blockquote><h3 id=6281--240347-ssf3d-strict-semi-supervised-3d-object-detection-with-switching-filter-songbur-wong-2024>(62/81 | 240/347) SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter (Songbur Wong, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Songbur Wong. (2024)<br><strong>SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter</strong><br><button class=copy-to-clipboard title="SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17390v1.pdf filename=2403.17390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>SSF3D modified the semi-supervised 3D <b>object</b> <b>detection</b> (SS3DOD) framework, which designed specifically for point cloud data. Leveraging the characteristics of non-coincidence and weak correlation of target <b>objects</b> <b>in</b> point cloud, we adopt a strategy of retaining only the truth-determining pseudo labels and trimming the other fuzzy labels with points, instead of pursuing a balance between the quantity and quality of pseudo labels. Besides, we notice that changing the filter will make the model meet different distributed targets, which is beneficial to break the training bottleneck. Two mechanism are introduced to achieve above ideas: strict threshold and filter switching. The experiments are conducted to analyze the effectiveness of above approaches and their impact on the overall performance of the system. Evaluating on the KITTI dataset, SSF3D exhibits superior performance compared to the current state-of-the-art methods. The code will be released here.</p></p class="citation"></blockquote><h3 id=6381--241347-tram-global-trajectory-and-motion-of-3d-humans-from-in-the-wild-videos-yufu-wang-et-al-2024>(63/81 | 241/347) TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos (Yufu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufu Wang, Ziyun Wang, Lingjie Liu, Kostas Daniilidis. (2024)<br><strong>TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos</strong><br><button class=copy-to-clipboard title="TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17346v1.pdf filename=2403.17346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose TRAM, a two-stage method to reconstruct a human&rsquo;s global trajectory and motion from in-the-wild videos. TRAM robustifies SLAM to recover the camera motion in the presence of dynamic humans and uses the scene background to derive the motion scale. Using the recovered camera as a metric-scale reference frame, we introduce a video <b>transformer</b> model (VIMO) to regress the kinematic body motion of a human. By composing the two motions, we achieve accurate recovery of 3D humans in the world space, reducing global motion errors by 60% from prior work. <a href=https://yufu-wang.github.io/tram4d/>https://yufu-wang.github.io/tram4d/</a></p></p class="citation"></blockquote><h3 id=6481--242347-staircase-localization-for-autonomous-exploration-in-urban-environments-jinrae-kim-et-al-2024>(64/81 | 242/347) Staircase Localization for Autonomous Exploration in Urban Environments (Jinrae Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinrae Kim, Sunggoo Jung, Sung-Kyun Kim, Youdan Kim, Ali-akbar Agha-mohammadi. (2024)<br><strong>Staircase Localization for Autonomous Exploration in Urban Environments</strong><br><button class=copy-to-clipboard title="Staircase Localization for Autonomous Exploration in Urban Environments" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17330v1.pdf filename=2403.17330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A staircase localization method is proposed for robots to explore urban environments autonomously. The proposed method employs a modular design in the form of a cascade pipeline consisting of three modules of stair detection, line segment detection, and stair localization modules. The stair detection module utilizes an <b>object</b> <b>detection</b> algorithm based on deep learning to generate a region of interest (ROI). From the ROI, line segment features are extracted using a deep line segment detection algorithm. The extracted line segments are used to localize a staircase in terms of position, orientation, and stair direction. The stair detection and localization are performed only with a single RGB-D camera. Each component of the proposed pipeline does not need to be designed particularly for staircases, which makes it easy to maintain the whole pipeline and replace each component with state-of-the-art deep learning detection techniques. The results of real-world experiments show that the proposed method can perform accurate stair detection and localization during autonomous exploration for various structured and unstructured upstairs and downstairs with shadows, dirt, and occlusions by artificial and natural objects.</p></p class="citation"></blockquote><h3 id=6581--243347-physical-3d-adversarial-attacks-against-monocular-depth-estimation-in-autonomous-driving-junhao-zheng-et-al-2024>(65/81 | 243/347) Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving (Junhao Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhao Zheng, Chenhao Lin, Jiahao Sun, Zhengyu Zhao, Qian Li, Chao Shen. (2024)<br><strong>Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving</strong><br><button class=copy-to-clipboard title="Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17301v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17301v2.pdf filename=2403.17301v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based monocular depth estimation (MDE), extensively applied in autonomous driving, is known to be vulnerable to <b>adversarial</b> <b>attacks.</b> Previous physical attacks against MDE models rely on 2D <b>adversarial</b> <b>patches,</b> so they only affect a small, localized region in the MDE map but fail under various viewpoints. To address these limitations, we propose 3D Depth Fool (3D$^2$Fool), the first 3D texture-based <b>adversarial</b> <b>attack</b> against MDE models. 3D$^2$Fool is specifically optimized to generate 3D <b>adversarial</b> <b>textures</b> agnostic to model types of vehicles and to have improved robustness in bad weather conditions, such as rain and fog. Experimental results validate the superior performance of our 3D$^2$Fool across various scenarios, including vehicles, MDE models, weather conditions, and viewpoints. Real-world experiments with printed 3D textures on physical vehicle models further demonstrate that our 3D$^2$Fool can cause an MDE error of over 10 meters.</p></p class="citation"></blockquote><h3 id=6681--244347-neural-clustering-based-visual-representation-learning-guikun-chen-et-al-2024>(66/81 | 244/347) Neural Clustering based Visual Representation Learning (Guikun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guikun Chen, Xia Li, Yi Yang, Wenguan Wang. (2024)<br><strong>Neural Clustering based Visual Representation Learning</strong><br><button class=copy-to-clipboard title="Neural Clustering based Visual Representation Learning" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Clustering, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17409v1.pdf filename=2403.17409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate a fundamental aspect of machine vision: the measurement of features, by revisiting <b>clustering,</b> one of the most classic approaches in machine learning and data analysis. Existing visual feature extractors, including ConvNets, ViTs, and MLPs, represent an image as rectangular regions. Though prevalent, such a grid-style paradigm is built upon engineering practice and lacks explicit modeling of data distribution. In this work, we propose feature extraction with <b>clustering</b> (FEC), a conceptually elegant yet surprisingly ad-hoc interpretable neural <b>clustering</b> framework, which views feature extraction as a process of selecting representatives from data and thus automatically captures the underlying data distribution. Given an image, FEC alternates between grouping pixels into individual clusters to abstract representatives and updating the deep features of pixels with current representatives. Such an iterative working mechanism is implemented in the form of several neural layers and the final representatives can be used for downstream tasks. The cluster assignments across layers, which can be viewed and inspected by humans, make the forward process of FEC fully transparent and empower it with promising ad-hoc interpretability. Extensive experiments on various visual recognition models and tasks verify the effectiveness, generality, and interpretability of FEC. We expect this work will provoke a rethink of the current de facto grid-style paradigm.</p></p class="citation"></blockquote><h3 id=6781--245347-tgglinesplus-a-robust-topological-graph-guided-computer-vision-algorithm-for-line-detection-from-images-liping-yang-et-al-2024>(67/81 | 245/347) TGGLinesPlus: A robust topological graph-guided computer vision algorithm for line detection from images (Liping Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liping Yang, Joshua Driscol, Ming Gong, Shujie Wang, Catherine G. Potts. (2024)<br><strong>TGGLinesPlus: A robust topological graph-guided computer vision algorithm for line detection from images</strong><br><button class=copy-to-clipboard title="TGGLinesPlus: A robust topological graph-guided computer vision algorithm for line detection from images" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18038v1.pdf filename=2403.18038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Line detection is a classic and essential problem in image processing, computer vision and machine intelligence. Line detection has many important applications, including image vectorization (e.g., document recognition and art design), indoor mapping, and important societal challenges (e.g., sea ice fracture line extraction from satellite imagery). Many line detection algorithms and methods have been developed, but robust and intuitive methods are still lacking. In this paper, we proposed and implemented a topological <b>graph-guided</b> algorithm, named TGGLinesPlus, for line detection. Our experiments on images from a wide range of domains have demonstrated the flexibility of our TGGLinesPlus algorithm. We also <b>benchmarked</b> our algorithm with five classic and state-of-the-art line detection methods and the results demonstrate the robustness of TGGLinesPlus. We hope our open-source implementation of TGGLinesPlus will inspire and pave the way for many applications where spatial science matters.</p></p class="citation"></blockquote><h3 id=6881--246347-spectralwaste-dataset-multimodal-data-for-waste-sorting-automation-sara-casao-et-al-2024>(68/81 | 246/347) SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation (Sara Casao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Casao, Fernando Peña, Alberto Sabater, Rosa Castillón, Darío Suárez, Eduardo Montijano, Ana C. Murillo. (2024)<br><strong>SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation</strong><br><button class=copy-to-clipboard title="SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18033v1.pdf filename=2403.18033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increase in non-biodegradable waste is a worldwide concern. Recycling facilities play a crucial role, but their automation is hindered by the complex characteristics of waste recycling lines like clutter or object deformation. In addition, the lack of publicly available labeled data for these environments makes developing robust perception systems challenging. Our work explores the benefits of <b>multimodal</b> perception for object segmentation in real waste management scenarios. First, we present SpectralWaste, the first dataset collected from an operational plastic waste sorting facility that provides synchronized hyperspectral and conventional RGB images. This dataset contains labels for several categories of objects that commonly appear in sorting plants and need to be detected and separated from the main trash flow for several reasons, such as security in the management line or reuse. Additionally, we propose a pipeline employing different object segmentation architectures and evaluate the alternatives on our dataset, conducting an extensive analysis for both <b>multimodal</b> and unimodal alternatives. Our evaluation pays special attention to efficiency and suitability for real-time processing and demonstrates how HSI can bring a boost to RGB-only perception in these realistic industrial settings without much computational overhead.</p></p class="citation"></blockquote><h3 id=6981--247347-mmvp-a-multimodal-mocap-dataset-with-vision-and-pressure-sensors-he-zhang-et-al-2024>(69/81 | 247/347) MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors (He Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He Zhang, Shenghao Ren, Haolei Yuan, Jianhui Zhao, Fan Li, Shuangpeng Sun, Zhenghao Liang, Tao Yu, Qiu Shen, Xun Cao. (2024)<br><strong>MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors</strong><br><button class=copy-to-clipboard title="MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17610v1.pdf filename=2403.17610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Foot contact is an important cue not only for human motion capture but also for motion understanding and physically plausible motion generation. However, most of the foot-contact annotations in existing datasets are estimated by purely visual matching and distance thresholding, which results in low accuracy and coarse granularity. Even though existing <b>multimodal</b> datasets synergistically capture plantar pressure (foot contact) and visual signals, they are specifically designed for small-range and slow motion such as Taiji Quan and Yoga. Therefore, there is still a lack of a vision-pressure <b>multimodal</b> dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation. To fill this gap, we propose a <b>Multimodal</b> MoCap Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking. To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture. Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy. We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains. Project page: <a href=https://haolyuan.github.io/MMVP-Dataset/>https://haolyuan.github.io/MMVP-Dataset/</a>.</p></p class="citation"></blockquote><h3 id=7081--248347-tdip-tunable-deep-image-processing-a-real-time-melt-pool-monitoring-solution-javid-akhavan-et-al-2024>(70/81 | 248/347) TDIP: Tunable Deep Image Processing, a Real Time Melt Pool Monitoring Solution (Javid Akhavan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Javid Akhavan, Youmna Mahmoud, Ke Xu, Jiaqi Lyu, Souran Manoochehri. (2024)<br><strong>TDIP: Tunable Deep Image Processing, a Real Time Melt Pool Monitoring Solution</strong><br><button class=copy-to-clipboard title="TDIP: Tunable Deep Image Processing, a Real Time Melt Pool Monitoring Solution" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-9, cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18117v1.pdf filename=2403.18117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of Industry 4.0, Additive Manufacturing (AM), particularly metal AM, has emerged as a significant contributor due to its innovative and cost-effective approach to fabricate highly intricate geometries. Despite its potential, this industry still lacks real-time capable process monitoring algorithms. Recent advancements in this field suggest that Melt Pool (MP) signatures during the fabrication process contain crucial information about process dynamics and quality. To obtain this information, various sensory approaches, such as high-speed cameras-based vision modules are employed for online fabrication monitoring. However, many conventional in-depth analyses still cannot process all the recorded data simultaneously. Although conventional Image Processing (ImP) solutions provide a targeted tunable approach, they pose a trade-off between convergence certainty and convergence speed. As a result, conventional methods are not suitable for a dynamically changing application like MP monitoring. Therefore, this article proposes the implementation of a Tunable Deep Image Processing (TDIP) method to address the data-rich monitoring needs in real-time. The proposed model is first trained to replicate an ImP algorithm with tunable features and methodology. The TDIP model is then further improved to account for MP geometries and fabrication quality based on the vision input and process parameters. The TDIP model achieved over 94% estimation accuracy with more than 96% R2 score for quality, <b>geometry,</b> and MP signature estimation and isolation. The TDIP model can process 500 images per second, while conventional methods taking a few minutes per image. This significant processing time reduction enables the integration of vision-based monitoring in real-time for processes and quality estimation.</p></p class="citation"></blockquote><h3 id=7181--249347-global-point-cloud-registration-network-for-large-transformations-hanz-cuevas-velasquez-et-al-2024>(71/81 | 249/347) Global Point Cloud Registration Network for Large Transformations (Hanz Cuevas-Velasquez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanz Cuevas-Velasquez, Alejandro Galán-Cuenca, Antonio Javier Gallego, Marcelo Saval-Calvo, Robert B. Fisher. (2024)<br><strong>Global Point Cloud Registration Network for Large Transformations</strong><br><button class=copy-to-clipboard title="Global Point Cloud Registration Network for Large Transformations" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18040v1.pdf filename=2403.18040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Three-dimensional data registration is an established yet challenging problem that is key in many different applications, such as mapping the environment for autonomous vehicles, and modeling objects and people for avatar creation, among many others. Registration refers to the process of mapping multiple data into the same coordinate system by means of matching correspondences and transformation estimation. Novel proposals exploit the benefits of deep learning architectures for this purpose, as they learn the best features for the data, providing better matches and hence results. However, the state of the art is usually focused on cases of relatively small transformations, although in certain applications and in a real and practical environment, large transformations are very common. In this paper, we present ReLaTo (Registration for Large Transformations), an architecture that faces the cases where large transformations happen while maintaining good performance for local transformations. This proposal uses a novel Softmax pooling layer to find correspondences in a bilateral consensus manner between two point sets, sampling the most confident matches. These matches are used to estimate a coarse and global registration using weighted Singular Value Decomposition (SVD). A target-guided denoising step is then applied to both the obtained matches and latent features, estimating the final fine registration considering the local <b>geometry.</b> All these steps are carried out following an end-to-end approach, which has been shown to improve 10 state-of-the-art registration methods in two datasets commonly used for this task (ModelNet40 and KITTI), especially in the case of large transformations.</p></p class="citation"></blockquote><h3 id=7281--250347-2d-gaussian-splatting-for-geometrically-accurate-radiance-fields-binbin-huang-et-al-2024>(72/81 | 250/347) 2D Gaussian Splatting for Geometrically Accurate Radiance Fields (Binbin Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao. (2024)<br><strong>2D Gaussian Splatting for Geometrically Accurate Radiance Fields</strong><br><button class=copy-to-clipboard title="2D Gaussian Splatting for Geometrically Accurate Radiance Fields" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17888v1.pdf filename=2403.17888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent <b>geometry</b> while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed <b>geometry</b> reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering. Our code will be made publicly available.</p></p class="citation"></blockquote><h3 id=7381--251347-towards-3d-vision-with-low-cost-single-photon-cameras-fangzhou-mu-et-al-2024>(73/81 | 251/347) Towards 3D Vision with Low-Cost Single-Photon Cameras (Fangzhou Mu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangzhou Mu, Carter Sifferman, Sacha Jungerman, Yiquan Li, Mark Han, Michael Gleicher, Mohit Gupta, Yin Li. (2024)<br><strong>Towards 3D Vision with Low-Cost Single-Photon Cameras</strong><br><button class=copy-to-clipboard title="Towards 3D Vision with Low-Cost Single-Photon Cameras" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17801v1.pdf filename=2403.17801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a method for reconstructing 3D shape of arbitrary Lambertian objects based on measurements by miniature, energy-efficient, low-cost single-photon cameras. These cameras, operating as time resolved image sensors, illuminate the scene with a very fast pulse of diffuse light and record the shape of that pulse as it returns back from the scene at a high temporal resolution. We propose to model this image formation process, account for its non-idealities, and adapt neural rendering to reconstruct 3D <b>geometry</b> from a set of spatially distributed sensors with known poses. We show that our approach can successfully recover complex 3D shapes from simulated data. We further demonstrate 3D object reconstruction from real-world captures, utilizing measurements from a commodity proximity sensor. Our work draws a connection between image-based modeling and active range scanning and is a step towards 3D vision with single-photon cameras.</p></p class="citation"></blockquote><h3 id=7481--252347-a-personalized-video-based-hand-taxonomy-application-for-individuals-with-spinal-cord-injury-mehdy-dousty-et-al-2024>(74/81 | 252/347) A Personalized Video-Based Hand Taxonomy: Application for Individuals with Spinal Cord Injury (Mehdy Dousty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehdy Dousty, David J. Fleet, José Zariffa. (2024)<br><strong>A Personalized Video-Based Hand Taxonomy: Application for Individuals with Spinal Cord Injury</strong><br><button class=copy-to-clipboard title="A Personalized Video-Based Hand Taxonomy: Application for Individuals with Spinal Cord Injury" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18094v1.pdf filename=2403.18094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hand function is critical for our interactions and quality of life. Spinal cord injuries (SCI) can impair hand function, reducing independence. A comprehensive evaluation of function in home and community settings requires a hand grasp taxonomy for individuals with impaired hand function. Developing such a taxonomy is challenging due to unrepresented grasp types in standard taxonomies, uneven data distribution across injury levels, and limited data. This study aims to automatically identify the dominant distinct hand grasps in egocentric video using semantic <b>clustering.</b> Egocentric video recordings collected in the homes of 19 individual with cervical SCI were used to cluster grasping actions with semantic significance. A deep learning model integrating posture and appearance data was employed to create a personalized hand taxonomy. Quantitative analysis reveals a cluster purity of 67.6% +- 24.2% with with 18.0% +- 21.8% redundancy. Qualitative assessment revealed meaningful clusters in video content. This methodology provides a flexible and effective strategy to analyze hand function in the wild. It offers researchers and clinicians an efficient tool for evaluating hand function, aiding sensitive assessments and tailored intervention plans.</p></p class="citation"></blockquote><h3 id=7581--253347-convofusion-multi-modal-conversational-diffusion-for-co-speech-gesture-synthesis-muhammad-hamza-mughal-et-al-2024>(75/81 | 253/347) ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis (Muhammad Hamza Mughal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Hamza Mughal, Rishabh Dabral, Ikhsanul Habibie, Lucia Donatelli, Marc Habermann, Christian Theobalt. (2024)<br><strong>ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis</strong><br><button class=copy-to-clipboard title="ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17936v1.pdf filename=2403.17936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gestures play a key role in human communication. Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance. Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words. Therefore, we present ConvoFusion, a diffusion-based approach for <b>multi-modal</b> gesture synthesis, which can not only generate gestures based on <b>multi-modal</b> speech inputs, but can also facilitate controllability in gesture synthesis. Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing. Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures. To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another. We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks. We urge the reader to watch our supplementary video at our website.</p></p class="citation"></blockquote><h3 id=7681--254347-sen2fire-a-challenging-benchmark-dataset-for-wildfire-detection-using-sentinel-data-yonghao-xu-et-al-2024>(76/81 | 254/347) Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data (Yonghao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yonghao Xu, Amanda Berg, Leif Haglund. (2024)<br><strong>Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data</strong><br><button class=copy-to-clipboard title="Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17884v1.pdf filename=2403.17884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Utilizing satellite imagery for wildfire detection presents substantial potential for practical applications. To advance the development of machine learning algorithms in this domain, our study introduces the \textit{Sen2Fire} dataset&ndash;a challenging satellite remote sensing dataset tailored for wildfire detection. This dataset is curated from Sentinel-2 multi-spectral data and Sentinel-5P aerosol product, comprising a total of 2466 image patches. Each patch has a size of 512$\times$512 pixels with 13 bands. Given the distinctive sensitivities of various wavebands to wildfire responses, our research focuses on optimizing wildfire detection by evaluating different wavebands and employing a combination of spectral indices, such as normalized burn ratio (NBR) and normalized difference vegetation index (NDVI). The results suggest that, in contrast to using all bands for wildfire detection, selecting specific band combinations yields superior performance. Additionally, our study underscores the positive impact of integrating Sentinel-5 aerosol data for wildfire detection. The code and dataset are available online (<a href=https://zenodo.org/records/10881058)>https://zenodo.org/records/10881058)</a>.</p></p class="citation"></blockquote><h3 id=7781--255347-deepfake-generation-and-detection-a-benchmark-and-survey-gan-pei-et-al-2024>(77/81 | 255/347) Deepfake Generation and Detection: A Benchmark and Survey (Gan Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gan Pei, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, Dacheng Tao. (2024)<br><strong>Deepfake Generation and Detection: A Benchmark and Survey</strong><br><button class=copy-to-clipboard title="Deepfake Generation and Detection: A Benchmark and Survey" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17881v1.pdf filename=2403.17881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively <b>benchmark</b> representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href=https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection>https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>.</p></p class="citation"></blockquote><h3 id=7881--256347-invisible-gas-detection-an-rgb-thermal-cross-attention-network-and-a-new-benchmark-jue-wang-et-al-2024>(78/81 | 256/347) Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A New Benchmark (Jue Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jue Wang, Yuxiang Lin, Qi Zhao, Dong Luo, Shuaibao Chen, Wei Chen, Xiaojiang Peng. (2024)<br><strong>Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A New Benchmark</strong><br><button class=copy-to-clipboard title="Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A New Benchmark" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17712v1.pdf filename=2403.17712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread use of various chemical gases in industrial processes necessitates effective measures to prevent their leakage during transportation and storage, given their high toxicity. Thermal infrared-based computer vision detection techniques provide a straightforward approach to identify gas leakage areas. However, the development of high-quality algorithms has been challenging due to the low texture in thermal images and the lack of open-source datasets. In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN), which employs an RGB-assisted two-stream network architecture to integrate texture information from RGB images and gas area information from thermal images. Additionally, to facilitate the research of invisible gas detection, we introduce Gas-DB, an extensive open-source gas detection database including about 1.3K well-annotated RGB-thermal images with eight variant collection scenes. Experimental results demonstrate that our method successfully leverages the advantages of both modalities, achieving state-of-the-art (SOTA) performance among RGB-thermal methods, surpassing single-stream SOTA models in terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%, and 4.88%, respectively. The code and data will be made available soon.</p></p class="citation"></blockquote><h3 id=7981--257347-clinical-domain-knowledge-derived-template-improves-post-hoc-ai-explanations-in-pneumothorax-classification-han-yuan-et-al-2024>(79/81 | 257/347) Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification (Han Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Yuan, Chuan Hong, Pengtao Jiang, Gangming Zhao, Nguyen Tuan Anh Tran, Xinxing Xu, Yet Yen Yan, Nan Liu. (2024)<br><strong>Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification</strong><br><button class=copy-to-clipboard title="Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18871v1.pdf filename=2403.18871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Pneumothorax is an acute thoracic disease caused by abnormal air collection between the lungs and chest wall. To address the opaqueness often associated with deep learning (DL) models, explainable artificial intelligence (XAI) methods have been introduced to outline regions related to pneumothorax diagnoses made by DL models. However, these explanations sometimes diverge from actual lesion areas, highlighting the need for further improvement. Method: We propose a template-guided approach to incorporate the clinical knowledge of pneumothorax into model explanations generated by XAI methods, thereby enhancing the quality of these explanations. Utilizing one lesion delineation created by radiologists, our approach first generates a template that represents potential areas of pneumothorax occurrence. This template is then superimposed on model explanations to filter out extraneous explanations that fall outside the template&rsquo;s boundaries. To validate its efficacy, we carried out a comparative analysis of three XAI methods with and without our template guidance when explaining two DL models in two real-world datasets. Results: The proposed approach consistently improved baseline XAI methods across twelve <b>benchmark</b> scenarios built on three XAI methods, two DL models, and two datasets. The average incremental percentages, calculated by the performance improvements over the baseline performance, were 97.8% in Intersection over Union (IoU) and 94.1% in Dice Similarity Coefficient (DSC) when comparing model explanations and ground-truth lesion areas. Conclusions: In the context of pneumothorax diagnoses, we proposed a template-guided approach for improving AI explanations. We anticipate that our template guidance will forge a fresh approach to elucidating AI models by integrating clinical domain expertise.</p></p class="citation"></blockquote><h3 id=8081--258347-deepmif-deep-monotonic-implicit-fields-for-large-scale-lidar-3d-mapping-kutay-yılmaz-et-al-2024>(80/81 | 258/347) DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping (Kutay Yılmaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov. (2024)<br><strong>DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping</strong><br><button class=copy-to-clipboard title="DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17550v1.pdf filename=2403.17550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as captured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI <b>benchmarks.</b> The code of our approach will be made publicly available.</p></p class="citation"></blockquote><h3 id=8181--259347-activity-biometrics-person-identification-from-daily-activities-shehreen-azad-et-al-2024>(81/81 | 259/347) Activity-Biometrics: Person Identification from Daily Activities (Shehreen Azad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shehreen Azad, Yogesh Singh Rawat. (2024)<br><strong>Activity-Biometrics: Person Identification from Daily Activities</strong><br><button class=copy-to-clipboard title="Activity-Biometrics: Person Identification from Daily Activities" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17360v1.pdf filename=2403.17360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we study a novel problem which focuses on person identification while performing daily activities. Learning biometric features from RGB videos is challenging due to spatio-temporal complexity and presence of appearance biases such as clothing color and background. We propose ABNet, a novel framework which leverages disentanglement of biometric and non-biometric features to perform effective person identification from daily activities. ABNet relies on a bias-less teacher to learn biometric features from RGB videos and explicitly disentangle non-biometric features with the help of biometric distortion. In addition, ABNet also exploits activity prior for biometrics which is enabled by joint biometric and activity learning. We perform comprehensive evaluation of the proposed approach across five different datasets which are derived from existing activity recognition <b>benchmarks.</b> Furthermore, we extensively compare ABNet with existing works in person identification and demonstrate its effectiveness for activity-based biometrics across all five datasets. The code and dataset can be accessed at: \url{https://github.com/sacrcv/Activity-Biometrics/}</p></p class="citation"></blockquote><h2 id=cssd-3>cs.SD (3)</h2><h3 id=13--260347-accuracy-enhancement-method-for-speech-emotion-recognition-from-spectrogram-using-temporal-frequency-correlation-and-positional-information-learning-through-knowledge-transfer-jeong-yoon-kim-et-al-2024>(1/3 | 260/347) Accuracy enhancement method for speech emotion recognition from spectrogram using temporal frequency correlation and positional information learning through knowledge transfer (Jeong-Yoon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeong-Yoon Kim, Seung-Ho Lee. (2024)<br><strong>Accuracy enhancement method for speech emotion recognition from spectrogram using temporal frequency correlation and positional information learning through knowledge transfer</strong><br><button class=copy-to-clipboard title="Accuracy enhancement method for speech emotion recognition from spectrogram using temporal frequency correlation and positional information learning through knowledge transfer" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CV, cs-SD, cs.SD, eess-AS<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Knowledge Transfer, Transformer, Emotion Recognition, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17327v1.pdf filename=2403.17327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a method to improve the accuracy of speech <b>emotion</b> <b>recognition</b> (SER) by using <b>vision</b> <b>transformer</b> (ViT) to attend to the correlation of frequency (y-axis) with time (x-axis) in spectrogram and transferring positional information between ViT through <b>knowledge</b> <b>transfer.</b> The proposed method has the following originality i) We use vertically segmented patches of log-Mel spectrogram to analyze the correlation of frequencies over time. This type of patch allows us to correlate the most relevant frequencies for a particular <b>emotion</b> <b>with</b> the time they were uttered. ii) We propose the use of image coordinate encoding, an absolute positional encoding suitable for ViT. By normalizing the x, y coordinates of the image to -1 to 1 and concatenating them to the image, we can effectively provide valid absolute positional information for ViT. iii) Through feature map matching, the locality and location information of the teacher network is effectively transmitted to the student network. Teacher network is a ViT that contains locality of <b>convolutional</b> stem and absolute position information through image coordinate encoding, and student network is a structure that lacks positional encoding in the basic ViT structure. In feature map matching stage, we train through the mean absolute error (L1 loss) to minimize the difference between the feature maps of the two networks. To validate the proposed method, three <b>emotion</b> <b>datasets</b> (SAVEE, EmoDB, and CREMA-D) consisting of speech were converted into log-Mel spectrograms for comparison experiments. The experimental results show that the proposed method significantly outperforms the state-of-the-art methods in terms of weighted accuracy while requiring significantly fewer floating point operations (FLOPs). Overall, the proposed method offers an promising solution for SER by providing improved efficiency and performance.</p></p class="citation"></blockquote><h3 id=23--261347-low-latency-neural-speech-phase-prediction-based-on-parallel-estimation-architecture-and-anti-wrapping-losses-for-speech-generation-tasks-yang-ai-et-al-2024>(2/3 | 261/347) Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks (Yang Ai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Ai, Zhen-Hua Ling. (2024)<br><strong>Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks</strong><br><button class=copy-to-clipboard title="Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17378v1.pdf filename=2403.17378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel neural speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra. The proposed model is a cascade of a residual <b>convolutional</b> <b>network</b> and a parallel estimation architecture. The parallel estimation architecture is a core module for direct wrapped phase prediction. This architecture consists of two parallel linear <b>convolutional</b> <b>layers</b> and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval. To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function. We mathematically demonstrate that the anti-wrapping function should possess three properties, namely parity, periodicity and monotonicity. We also achieve low-latency streamable phase prediction by combining causal <b>convolutions</b> and <b>knowledge</b> <b>distillation</b> training strategies. For both analysis-synthesis and specific speech generation tasks, experimental results show that our proposed neural speech phase prediction model outperforms the iterative phase estimation algorithms and neural network-based phase prediction methods in terms of phase prediction precision, efficiency and robustness. Compared with HiFi-GAN-based waveform reconstruction method, our proposed model also shows outstanding efficiency advantages while ensuring the quality of synthesized speech. To the best of our <b>knowledge,</b> <b>we</b> are the first to directly predict speech phase spectra from amplitude spectra only via neural networks.</p></p class="citation"></blockquote><h3 id=33--262347-deep-functional-multiple-index-models-with-an-application-to-ser-matthieu-saumard-et-al-2024>(3/3 | 262/347) Deep functional multiple index models with an application to SER (Matthieu Saumard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthieu Saumard, Abir El Haj, Thibault Napoleon. (2024)<br><strong>Deep functional multiple index models with an application to SER</strong><br><button class=copy-to-clipboard title="Deep functional multiple index models with an application to SER" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS, stat-AP<br>Keyword Score: 33<br>Keywords: Benchmarking, Simulation, Simulator, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17562v1.pdf filename=2403.17562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech <b>Emotion</b> <b>Recognition</b> (SER) plays a crucial role in advancing human-computer interaction and speech processing capabilities. We introduce a novel deep-learning architecture designed specifically for the functional data model known as the multiple-index functional model. Our key innovation lies in integrating adaptive basis layers and an automated data transformation search within the deep learning framework. <b>Simulations</b> for this new model show good performances. This allows us to extract features tailored for chunk-level SER, based on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the effectiveness of our approach on the <b>benchmark</b> IEMOCAP database, achieving good performance compared to existing methods.</p></p class="citation"></blockquote><h2 id=csro-16>cs.RO (16)</h2><h3 id=116--263347-leveraging-symmetry-in-rl-based-legged-locomotion-control-zhi-su-et-al-2024>(1/16 | 263/347) Leveraging Symmetry in RL-based Legged Locomotion Control (Zhi Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhi Su, Xiaoyu Huang, Daniel Ordoñez-Apraez, Yunfei Li, Zhongyu Li, Qiayuan Liao, Giulio Turrisi, Massimiliano Pontil, Claudio Semini, Yi Wu, Koushil Sreenath. (2024)<br><strong>Leveraging Symmetry in RL-based Legged Locomotion Control</strong><br><button class=copy-to-clipboard title="Leveraging Symmetry in RL-based Legged Locomotion Control" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Data Augmentation, Reinforcement Learning, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17320v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17320v2.pdf filename=2403.17320v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model-free <b>reinforcement</b> <b>learning</b> is a promising approach for autonomously solving challenging robotics control problems, but faces exploration difficulty without information of the robot&rsquo;s kinematics and dynamics morphology. The under-exploration of multiple modalities with symmetric states leads to behaviors that are often unnatural and sub-optimal. This issue becomes particularly pronounced in the context of robotic systems with morphological symmetries, such as legged robots for which the resulting asymmetric and aperiodic behaviors compromise performance, robustness, and transferability to real hardware. To mitigate this challenge, we can leverage symmetry to guide and improve the exploration in policy learning via equivariance/invariance constraints. In this paper, we investigate the efficacy of two approaches to incorporate symmetry: modifying the network architectures to be strictly equivariant/invariant, and leveraging <b>data</b> <b>augmentation</b> to approximate equivariant/invariant actor-critics. We implement the methods on challenging loco-manipulation and bipedal locomotion tasks and compare with an unconstrained baseline. We find that the strictly equivariant policy consistently outperforms other methods in sample efficiency and task performance in <b>simulation.</b> In addition, symmetry-incorporated approaches exhibit better gait quality, higher robustness and can be deployed <b>zero-shot</b> in real-world experiments.</p></p class="citation"></blockquote><h3 id=216--264347-shapegrasp-zero-shot-task-oriented-grasping-with-large-language-models-through-geometric-decomposition-samuel-li-et-al-2024>(2/16 | 264/347) ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition (Samuel Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Li, Sarthak Bhagat, Joseph Campbell, Yaqi Xie, Woojun Kim, Katia Sycara, Simon Stepputtis. (2024)<br><strong>ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition</strong><br><button class=copy-to-clipboard title="ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 43<br>Keywords: Graph, Zero-shot, Common-sense Reasoning, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18062v1.pdf filename=2403.18062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel <b>zero-shot</b> task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a <b>graph</b> structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information - the object&rsquo;s name and the intended task - to facilitate <b>zero-shot</b> task-oriented grasping. We utilize the <b>commonsense</b> <b>reasoning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach&rsquo;s decomposition and <b>reasoning</b> pipeline is capable of selecting the correct part in 92% of the cases and successfully grasping the object in 82% of the tasks we evaluate. Additional videos, experiments, code, and data are available on our project website: <a href=https://shapegrasp.github.io/>https://shapegrasp.github.io/</a>.</p></p class="citation"></blockquote><h3 id=316--265347-sledge-synthesizing-simulation-environments-for-driving-agents-with-generative-models-kashyap-chitta-et-al-2024>(3/16 | 265/347) SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models (Kashyap Chitta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kashyap Chitta, Daniel Dauner, Andreas Geiger. (2024)<br><strong>SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models</strong><br><button class=copy-to-clipboard title="SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 43<br>Keywords: Graph, Autoencoder, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17933v1.pdf filename=2403.17933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane <b>graphs.</b> The model&rsquo;s outputs serve as an initial state for traffic <b>simulation.</b> The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane <b>graph</b> representations, we introduce a novel raster-to-vector <b>autoencoder</b> (RVAE). It encodes agents and the lane <b>graph</b> into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion <b>Transformer.</b> Using generated entities in SLEDGE enables greater control over the <b>simulation,</b> e.g. upsampling turns or increasing traffic density. Further, SLEDGE can support 500m long routes, a capability not found in existing data-driven simulators like nuPlan. It presents new challenges for planning algorithms, evidenced by failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and dense traffic generated by our model. Compared to nuPlan, SLEDGE requires 500$\times$ less storage to set up (&lt;4GB), making it a more accessible option and helping with democratizing future research in this field.</p></p class="citation"></blockquote><h3 id=416--266347-scenario-based-curriculum-generation-for-multi-agent-autonomous-driving-axel-brunnbauer-et-al-2024>(4/16 | 266/347) Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving (Axel Brunnbauer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Axel Brunnbauer, Luigi Berducci, Peter Priller, Dejan Nickovic, Radu Grosu. (2024)<br><strong>Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving</strong><br><button class=copy-to-clipboard title="Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-MA, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17805v1.pdf filename=2403.17805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The automated generation of diverse and complex training scenarios has been an important ingredient in many complex learning tasks. Especially in real-world application domains, such as autonomous driving, auto-curriculum generation is considered vital for obtaining robust and general policies. However, crafting traffic scenarios with multiple, heterogeneous agents is typically considered as a tedious and time-consuming task, especially in more complex <b>simulation</b> environments. In our work, we introduce MATS-Gym, a Multi-Agent Traffic Scenario framework to train agents in CARLA, a high-fidelity driving simulator. MATS-Gym is a multi-agent training framework for autonomous driving that uses partial scenario specifications to generate traffic scenarios with variable numbers of agents. This paper unifies various existing approaches to traffic scenario description into a single training framework and demonstrates how it can be integrated with techniques from <b>unsupervised</b> environment design to automate the generation of adaptive auto-curricula. The code is available at <a href=https://github.com/AutonomousDrivingExaminer/mats-gym>https://github.com/AutonomousDrivingExaminer/mats-gym</a>.</p></p class="citation"></blockquote><h3 id=516--267347-unified-path-and-gait-planning-for-safe-bipedal-robot-navigation-chengyang-peng-et-al-2024>(5/16 | 267/347) Unified Path and Gait Planning for Safe Bipedal Robot Navigation (Chengyang Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengyang Peng, Victor Paredes, Ayonga Hereid. (2024)<br><strong>Unified Path and Gait Planning for Safe Bipedal Robot Navigation</strong><br><button class=copy-to-clipboard title="Unified Path and Gait Planning for Safe Bipedal Robot Navigation" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17347v1.pdf filename=2403.17347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safe path and gait planning are essential for bipedal robots to navigate complex real-world environments. The prevailing approaches often plan the path and gait separately in a hierarchical fashion, potentially resulting in unsafe movements due to neglecting the physical constraints of walking robots. A safety-critical path must not only avoid obstacles but also ensure that the robot&rsquo;s gaits are subject to its dynamic and kinematic constraints. This work presents a novel approach that unifies path planning and gait planning via a Model Predictive Control (MPC) using the Linear Inverted Pendulum (LIP) model representing bipedal locomotion. This approach considers environmental constraints, such as obstacles, and the robot&rsquo;s kinematics and dynamics constraints. By using <b>discrete-time</b> <b>Control</b> Barrier Functions for obstacle avoidance, our approach generates the next foot landing position, ensuring robust walking gaits and a safe navigation path within clustered environments. We validated our proposed approach in <b>simulation</b> using a Digit robot in 20 randomly created environments. The results demonstrate improved performance in terms of safety and robustness when compared to hierarchical path and gait planning frameworks.</p></p class="citation"></blockquote><h3 id=616--268347-sparse-graph-enabled-formation-planning-for-large-scale-aerial-swarms-yuan-zhou-et-al-2024>(6/16 | 268/347) Sparse-Graph-Enabled Formation Planning for Large-Scale Aerial Swarms (Yuan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Zhou, Lun Quan, Chao Xu, Guangtong Xu, Fei Gao. (2024)<br><strong>Sparse-Graph-Enabled Formation Planning for Large-Scale Aerial Swarms</strong><br><button class=copy-to-clipboard title="Sparse-Graph-Enabled Formation Planning for Large-Scale Aerial Swarms" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17288v1.pdf filename=2403.17288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The formation trajectory planning using complete <b>graphs</b> to model collaborative constraints becomes computationally intractable as the number of drones increases due to the curse of dimensionality. To tackle this issue, this paper presents a sparse <b>graph</b> construction method for formation planning to realize better efficiency-performance trade-off. Firstly, a sparsification mechanism for complete <b>graphs</b> is designed to ensure the global rigidity of sparsified <b>graphs,</b> which is a necessary condition for uniquely corresponding to a geometric shape. Secondly, a good sparse <b>graph</b> is constructed to preserve the main structural feature of complete <b>graphs</b> sufficiently. Since the <b>graph-based</b> formation constraint is described by Laplacian matrix, the sparse <b>graph</b> construction problem is equivalent to submatrix selection, which has combinatorial time complexity and needs a scoring metric. Via comparative <b>simulations,</b> the Max-Trace matrix-revealing metric shows the promising performance. The sparse <b>graph</b> is integrated into the formation planning. <b>Simulation</b> results with 72 drones in complex environments demonstrate that when preserving 30% connection edges, our method has comparative formation error and recovery performance w.r.t. complete <b>graphs.</b> Meanwhile, the planning efficiency is improved by approximate an order of magnitude. <b>Benchmark</b> comparisons and ablation studies are conducted to fully validate the merits of our method.</p></p class="citation"></blockquote><h3 id=716--269347-a-study-on-the-use-of-simulation-in-synthesizing-path-following-control-policies-for-autonomous-ground-robots-harry-zhang-et-al-2024>(7/16 | 269/347) A Study on the Use of Simulation in Synthesizing Path-Following Control Policies for Autonomous Ground Robots (Harry Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harry Zhang, Stefan Caldararu, Aaron Young, Alexis Ruiz, Huzaifa Unjhawala, Ishaan Mahajan, Sriram Ashokkumar, Nevindu Batagoda, Zhenhao Zhou, Luning Bakke, Dan Negrut. (2024)<br><strong>A Study on the Use of Simulation in Synthesizing Path-Following Control Policies for Autonomous Ground Robots</strong><br><button class=copy-to-clipboard title="A Study on the Use of Simulation in Synthesizing Path-Following Control Policies for Autonomous Ground Robots" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18021v1.pdf filename=2403.18021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We report results obtained and insights gained while answering the following question: how effective is it to use a simulator to establish path following control policies for an autonomous ground robot? While the quality of the simulator conditions the answer to this question, we found that for the <b>simulation</b> platform used herein, producing four control policies for path planning was straightforward once a digital twin of the controlled robot was available. The control policies established in <b>simulation</b> and subsequently demonstrated in the real world are PID control, MPC, and two neural network (NN) based controllers. Training the two NN controllers via imitation learning was accomplished expeditiously using seven simple maneuvers: follow three circles clockwise, follow the same circles counter-clockwise, and drive straight. A test randomization process that employs random micro-simulations is used to rank the ``goodness&rsquo;&rsquo; of the four control policies. The policy ranking noted in <b>simulation</b> correlates well with the ranking observed when the control policies were tested in the real world. The <b>simulation</b> platform used is publicly available and BSD3-released as open source; a public Docker image is available for reproducibility studies. It contains a dynamics engine, a sensor simulator, a ROS2 bridge, and a ROS2 autonomy stack the latter employed both in the simulator and the real world experiments.</p></p class="citation"></blockquote><h3 id=816--270347-time-optimal-flight-with-safety-constraints-and-data-driven-dynamics-maria-krinner-et-al-2024>(8/16 | 270/347) Time-Optimal Flight with Safety Constraints and Data-driven Dynamics (Maria Krinner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria Krinner, Angel Romero, Leonard Bauersfeld, Melanie Zeilinger, Andrea Carron, Davide Scaramuzza. (2024)<br><strong>Time-Optimal Flight with Safety Constraints and Data-driven Dynamics</strong><br><button class=copy-to-clipboard title="Time-Optimal Flight with Safety Constraints and Data-driven Dynamics" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17551v1.pdf filename=2403.17551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time-optimal quadrotor flight is an extremely challenging problem due to the limited control authority encountered at the limit of handling. Model Predictive Contouring Control (MPCC) has emerged as a leading model-based approach for time optimization problems such as drone racing. However, the standard MPCC formulation used in quadrotor racing introduces the notion of the gates directly in the cost function, creating a multi-objective optimization that continuously trades off between maximizing progress and tracking the path accurately. This paper introduces three key components that enhance the MPCC approach for drone racing. First and foremost, we provide safety guarantees in the form of a constraint and terminal set. The safety set is designed as a spatial constraint which prevents gate collisions while allowing for time-optimization only in the cost function. Second, we augment the existing first principles dynamics with a residual term that captures complex aerodynamic effects and thrust forces learned directly from real world data. Third, we use Trust Region Bayesian Optimization (TuRBO), a state of the art global Bayesian Optimization algorithm, to tune the hyperparameters of the MPC controller given a sparse reward based on lap time minimization. The proposed approach achieves similar lap times to the best state-of-the-art RL and outperforms the best time-optimal controller while satisfying constraints. In both <b>simulation</b> and real-world, our approach consistently prevents gate crashes with 100% success rate, while pushing the quadrotor to its physical limit reaching speeds of more than 80km/h.</p></p class="citation"></blockquote><h3 id=916--271347-learning-goal-directed-object-pushing-in-cluttered-scenes-with-location-based-attention-nils-dengler-et-al-2024>(9/16 | 271/347) Learning Goal-Directed Object Pushing in Cluttered Scenes with Location-Based Attention (Nils Dengler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nils Dengler, Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar, Maren Bennewitz. (2024)<br><strong>Learning Goal-Directed Object Pushing in Cluttered Scenes with Location-Based Attention</strong><br><button class=copy-to-clipboard title="Learning Goal-Directed Object Pushing in Cluttered Scenes with Location-Based Attention" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17667v1.pdf filename=2403.17667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-prehensile planar pushing is a challenging task due to its underactuated nature with hybrid-dynamics, where a robot needs to reason about an object&rsquo;s long-term behaviour and contact-switching, while being robust to contact uncertainty. The presence of clutter in the environment further complicates this task, introducing the need to include more sophisticated spatial analysis to avoid collisions. Building upon prior work on <b>reinforcement</b> <b>learning</b> (RL) with <b>multimodal</b> categorical exploration for planar pushing, in this paper we incorporate location-based attention to enable robust navigation through clutter. Unlike previous RL literature addressing this obstacle avoidance pushing task, our framework requires no predefined global paths and considers the target orientation of the manipulated object. Our results demonstrate that the learned policies successfully navigate through a wide range of complex obstacle configurations, including dynamic obstacles, with smooth motions, achieving the desired target object pose. We also validate the transferability of the learned policies to robotic hardware using the KUKA iiwa robot arm.</p></p class="citation"></blockquote><h3 id=1016--272347-code-generation-for-conic-model-predictive-control-on-microcontrollers-with-tinympc-sam-schoedel-et-al-2024>(10/16 | 272/347) Code Generation for Conic Model-Predictive Control on Microcontrollers with TinyMPC (Sam Schoedel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sam Schoedel, Khai Nguyen, Elakhya Nedumaran, Brian Plancher, Zachary Manchester. (2024)<br><strong>Code Generation for Conic Model-Predictive Control on Microcontrollers with TinyMPC</strong><br><button class=copy-to-clipboard title="Code Generation for Conic Model-Predictive Control on Microcontrollers with TinyMPC" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY, math-OC<br>Keyword Score: 13<br>Keywords: Benchmarking, Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18149v1.pdf filename=2403.18149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conic constraints appear in many important control applications like legged locomotion, robotic manipulation, and autonomous rocket landing. However, current solvers for conic optimization problems have relatively heavy computational demands in terms of both floating-point operations and memory footprint, making them impractical for use on small embedded devices. We extend TinyMPC, an open-source, high-speed solver targeting low-power embedded control applications, to handle second-order cone constraints. We also present <b>code-generation</b> <b>software</b> to enable deployment of TinyMPC on a variety of microcontrollers. We <b>benchmark</b> our generated <b>code</b> <b>against</b> state-of-the-art embedded QP and SOCP solvers, demonstrating a two-order-of-magnitude speed increase over ECOS while consuming less memory. Finally, we demonstrate TinyMPC&rsquo;s efficacy on the Crazyflie, a lightweight, resource-constrained quadrotor with fast dynamics. TinyMPC and its <b>code-generation</b> <b>tools</b> are publicly available at <a href=https://tinympc.org>https://tinympc.org</a>.</p></p class="citation"></blockquote><h3 id=1116--273347-hierarchical-open-vocabulary-3d-scene-graphs-for-language-grounded-robot-navigation-abdelrhman-werby-et-al-2024>(11/16 | 273/347) Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation (Abdelrhman Werby et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard. (2024)<br><strong>Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation</strong><br><button class=copy-to-clipboard title="Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17846v1.pdf filename=2403.17846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene <b>graph</b> mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision <b>foundation</b> <b>models,</b> we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene <b>graph</b> hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi <b>graph.</b> HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps. In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-storage environments. We provide code and trial video data at <a href=http://hovsg.github.io/>http://hovsg.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1216--274347-lidar-based-crop-row-detection-algorithm-for-over-canopy-autonomous-navigation-in-agriculture-fields-ruiji-liu-et-al-2024>(12/16 | 274/347) LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous Navigation in Agriculture Fields (Ruiji Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiji Liu, Francisco Yandun, George Kantor. (2024)<br><strong>LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous Navigation in Agriculture Fields</strong><br><button class=copy-to-clipboard title="LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous Navigation in Agriculture Fields" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17774v1.pdf filename=2403.17774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous navigation is crucial for various robotics applications in agriculture. However, many existing methods depend on RTK-GPS systems, which are expensive and susceptible to poor signal coverage. This paper introduces a state-of-the-art LiDAR-based navigation system that can achieve over-canopy autonomous navigation in row-crop fields, even when the canopy fully blocks the interrow spacing. Our crop row detection algorithm can detect crop rows across diverse scenarios, encompassing various crop types, growth stages, weed presence, and discontinuities within the crop rows. Without utilizing the global localization of the robot, our navigation system can perform autonomous navigation in these challenging scenarios, detect the end of the crop rows, and navigate to the next crop row autonomously, providing a crop-agnostic approach to navigate the whole row-crop field. This navigation system has undergone tests in various simulated agricultural fields, achieving an average of $2.98cm$ autonomous driving accuracy without <b>human</b> <b>intervention</b> on the custom Amiga robot. In addition, the qualitative results of our crop row detection algorithm from the actual soybean fields validate our LiDAR-based crop row detection algorithm&rsquo;s potential for practical agricultural applications.</p></p class="citation"></blockquote><h3 id=1316--275347-roboduet-a-framework-affording-mobile-manipulation-and-cross-embodiment-guoping-pan-et-al-2024>(13/16 | 275/347) RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment (Guoping Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoping Pan, Qingwei Ben, Zhecheng Yuan, Guangqi Jiang, Yandong Ji, Jiangmiao Pang, Houde Liu, Huazhe Xu. (2024)<br><strong>RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment</strong><br><button class=copy-to-clipboard title="RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17367v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17367v2.pdf filename=2403.17367v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Combining the mobility of legged robots with the manipulation skills of arms has the potential to significantly expand the operational range and enhance the capabilities of robotic systems in performing various mobile manipulation tasks. Existing approaches are confined to imprecise six degrees of freedom (DoF) manipulation and possess a limited arm workspace. In this paper, we propose a novel framework, RoboDuet, which employs two collaborative policies to realize locomotion and manipulation simultaneously, achieving whole-body control through interactions between each other. Surprisingly, going beyond the large-range pose tracking, we find that the two-policy framework may enable cross-embodiment deployment such as using different quadrupedal robots or other arms. Our experiments demonstrate that the policies trained through RoboDuet can accomplish stable gaits, agile 6D end-effector pose tracking, and <b>zero-shot</b> exchange of legged robots, and can be deployed in the real world to perform various mobile manipulation tasks. Our project page with demo videos is at <a href=https://locomanip-duet.github.io>https://locomanip-duet.github.io</a> .</p></p class="citation"></blockquote><h3 id=1416--276347-multi-objective-trajectory-planning-with-dual-encoder-beibei-zhang-et-al-2024>(14/16 | 276/347) Multi-Objective Trajectory Planning with Dual-Encoder (Beibei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beibei Zhang, Tian Xiang, Chentao Mao, Yuhua Zheng, Shuai Li, Haoyi Niu, Xiangming Xi, Wenyuan Bai, Feng Gao. (2024)<br><strong>Multi-Objective Trajectory Planning with Dual-Encoder</strong><br><button class=copy-to-clipboard title="Multi-Objective Trajectory Planning with Dual-Encoder" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17353v1.pdf filename=2403.17353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time-jerk optimal trajectory planning is crucial in advancing robotic arms&rsquo; performance in dynamic tasks. Traditional methods rely on solving complex nonlinear programming problems, bringing significant delays in generating optimized trajectories. In this paper, we propose a two-stage approach to accelerate time-jerk optimal trajectory planning. Firstly, we introduce a dual-encoder based <b>transformer</b> model to establish a good preliminary trajectory. This trajectory is subsequently refined through sequential quadratic programming to improve its optimality and robustness. Our approach outperforms the state-of-the-art by up to 79.72% in reducing trajectory planning time. Compared with existing methods, our method shrinks the optimality gap with the objective function value decreasing by up to 29.9%.</p></p class="citation"></blockquote><h3 id=1516--277347-online-tree-reconstruction-and-forest-inventory-on-a-mobile-robotic-system-leonard-freißmuth-et-al-2024>(15/16 | 277/347) Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System (Leonard Freißmuth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonard Freißmuth, Matias Mattamala, Nived Chebrolu, Simon Schaefer, Stefan Leutenegger, Maurice Fallon. (2024)<br><strong>Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System</strong><br><button class=copy-to-clipboard title="Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17622v1.pdf filename=2403.17622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories. However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data. In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots. Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired <b>clustering</b> algorithm. Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem. Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose <b>graph</b> LiDAR SLAM system. This enables us to refine our estimates of the tree traits if an area is revisited later during a mission. We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests. Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete.</p></p class="citation"></blockquote><h3 id=1616--278347-system-calibration-of-a-field-phenotyping-robot-with-multiple-high-precision-profile-laser-scanners-felix-esser-et-al-2024>(16/16 | 278/347) System Calibration of a Field Phenotyping Robot with Multiple High-Precision Profile Laser Scanners (Felix Esser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Esser, Gereon Tombrink, Andre Cornelißen, Lasse Klingbeil, Heiner Kuhlmann. (2024)<br><strong>System Calibration of a Field Phenotyping Robot with Multiple High-Precision Profile Laser Scanners</strong><br><button class=copy-to-clipboard title="System Calibration of a Field Phenotyping Robot with Multiple High-Precision Profile Laser Scanners" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17788v1.pdf filename=2403.17788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The creation of precise and high-resolution crop point clouds in agricultural fields has become a key challenge for high-throughput phenotyping applications. This work implements a novel calibration method to calibrate the laser scanning system of an agricultural field robot consisting of two industrial-grade laser scanners used for high-precise 3D crop point cloud creation. The calibration method optimizes the transformation between the scanner origins and the robot pose by minimizing 3D point omnivariances within the point cloud. Moreover, we present a novel factor <b>graph-based</b> pose estimation method that fuses total station prism measurements with IMU and GNSS heading information for high-precise pose determination during calibration. The root-mean-square error of the distances to a georeferenced ground truth point cloud results in 0.8 cm after parameter optimization. Furthermore, our results show the importance of a reference point cloud in the calibration method needed to estimate the vertical translation of the calibration. Challenges arise due to non-static parameters while the robot moves, indicated by systematic deviations to a ground truth terrestrial laser scan.</p></p class="citation"></blockquote><h2 id=cscr-9>cs.CR (9)</h2><h3 id=19--279347-faultguard-a-generative-approach-to-resilient-fault-prediction-in-smart-electrical-grids-emad-efatinasab-et-al-2024>(1/9 | 279/347) FaultGuard: A Generative Approach to Resilient Fault Prediction in Smart Electrical Grids (Emad Efatinasab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emad Efatinasab, Francesco Marchiori, Alessandro Brighente, Mirco Rampazzo, Mauro Conti. (2024)<br><strong>FaultGuard: A Generative Approach to Resilient Fault Prediction in Smart Electrical Grids</strong><br><button class=copy-to-clipboard title="FaultGuard: A Generative Approach to Resilient Fault Prediction in Smart Electrical Grids" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR, eess-SP<br>Keyword Score: 46<br>Keywords: Adversarial Learning, Anomaly Detection, Benchmarking, Benchmarking, Generative Adversarial Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17494v1.pdf filename=2403.17494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting and classifying faults in electricity networks is crucial for uninterrupted provision and keeping maintenance costs at a minimum. Thanks to the advancements in the field provided by the smart grid, several data-driven approaches have been proposed in the literature to tackle fault prediction tasks. Implementing these systems brought several improvements, such as optimal energy consumption and quick restoration. Thus, they have become an essential component of the smart grid. However, the robustness and security of these systems against <b>adversarial</b> <b>attacks</b> have not yet been extensively investigated. These attacks can impair the whole grid and cause additional damage to the infrastructure, deceiving fault detection systems and disrupting restoration. In this paper, we present FaultGuard, the first framework for fault type and zone classification resilient to <b>adversarial</b> <b>attacks.</b> To ensure the security of our system, we employ an <b>Anomaly</b> <b>Detection</b> System (ADS) leveraging a novel <b>Generative</b> <b>Adversarial</b> <b>Network</b> training layer to identify attacks. Furthermore, we propose a low-complexity fault prediction model and an online <b>adversarial</b> <b>training</b> technique to enhance robustness. We comprehensively evaluate the framework&rsquo;s performance against various <b>adversarial</b> <b>attacks</b> using the IEEE13-AdvAttack dataset, which constitutes the state-of-the-art for resilient fault prediction <b>benchmarking.</b> Our model outclasses the state-of-the-art even without considering adversaries, with an accuracy of up to 0.958. Furthermore, our ADS shows attack detection capabilities with an accuracy of up to 1.000. Finally, we demonstrate how our novel training layers drastically increase performances across the whole framework, with a mean increase of 154% in ADS accuracy and 118% in model accuracy.</p></p class="citation"></blockquote><h3 id=29--280347-dont-listen-to-me-understanding-and-exploring-jailbreak-prompts-of-large-language-models-zhiyuan-yu-et-al-2024>(2/9 | 280/347) Don&rsquo;t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models (Zhiyuan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach Cameron, Chaowei Xiao, Ning Zhang. (2024)<br><strong>Don&rsquo;t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models</strong><br><button class=copy-to-clipboard title="Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Generative AI, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17336v1.pdf filename=2403.17336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>generative</b> <b>AI</b> have enabled ubiquitous access to <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Empowered by their exceptional capabilities to understand and generate human-like text, these models are being increasingly integrated into our society. At the same time, there are also concerns on the potential misuse of this powerful technology, <b>prompting</b> defensive measures from service providers. To overcome such protection, jailbreaking <b>prompts</b> have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited. Due to the rapid development of <b>LLMs</b> and their ease of access via natural languages, the frontline of jailbreak <b>prompts</b> is largely seen in online forums and among hobbyists. To gain a better understanding of the threat landscape of semantically meaningful jailbreak <b>prompts,</b> we systemized existing <b>prompts</b> and measured their jailbreak effectiveness empirically. Further, we conducted a user study involving 92 participants with diverse backgrounds to unveil the process of manually creating jailbreak <b>prompts.</b> We observed that users often succeeded in jailbreak <b>prompts</b> generation regardless of their expertise in <b>LLMs.</b> Building on the insights from the user study, we also developed a system using AI as the assistant to automate the process of jailbreak <b>prompt</b> generation.</p></p class="citation"></blockquote><h3 id=39--281347-optimization-based-prompt-injection-attack-to-llm-as-a-judge-jiawen-shi-et-al-2024>(3/9 | 281/347) Optimization-based Prompt Injection Attack to LLM-as-a-Judge (Jiawen Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, Neil Zhenqiang Gong. (2024)<br><strong>Optimization-based Prompt Injection Attack to LLM-as-a-Judge</strong><br><button class=copy-to-clipboard title="Optimization-based Prompt Injection Attack to LLM-as-a-Judge" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17710v1.pdf filename=2403.17710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLM-as-a-Judge</b> is a novel solution that can assess textual information with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Based on existing research studies, <b>LLMs</b> demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against <b>prompt</b> injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based <b>prompt</b> injection attack tailored to <b>LLM-as-a-Judge.</b> Our method formulates a precise optimization objective for attacking the decision-making process of <b>LLM-as-a-Judge</b> and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft <b>prompt</b> injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of <b>LLM-based</b> judgment systems. Through extensive experiments, we showcase the capability of JudgeDeceiver in altering decision outcomes across various cases, highlighting the vulnerability of <b>LLM-as-a-Judge</b> systems to the optimization-based <b>prompt</b> injection attack.</p></p class="citation"></blockquote><h3 id=49--282347-hawk-accurate-and-fast-privacy-preserving-machine-learning-using-secure-lookup-table-computation-hamza-saleem-et-al-2024>(4/9 | 282/347) Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation (Hamza Saleem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamza Saleem, Amir Ziashahabi, Muhammad Naveed, Salman Avestimehr. (2024)<br><strong>Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation</strong><br><button class=copy-to-clipboard title="Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 23<br>Keywords: MNIST, Benchmarking, Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17296v1.pdf filename=2403.17296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training machine learning models on data from multiple entities without direct data sharing can unlock applications otherwise hindered by business, legal, or ethical constraints. In this work, we design and implement new privacy-preserving machine learning protocols for <b>logistic</b> <b>regression</b> and neural network models. We adopt a two-server model where data owners secret-share their data between two servers that train and evaluate the model on the joint data. A significant source of inefficiency and inaccuracy in existing methods arises from using Yao&rsquo;s garbled circuits to compute non-linear activation functions. We propose new methods for computing non-linear functions based on secret-shared lookup tables, offering both computational efficiency and improved accuracy. Beyond introducing leakage-free techniques, we initiate the exploration of relaxed security measures for privacy-preserving machine learning. Instead of claiming that the servers gain no knowledge during the computation, we contend that while some information is revealed about access patterns to lookup tables, it maintains epsilon-dX-privacy. Leveraging this relaxation significantly reduces the computational resources needed for training. We present new cryptographic protocols tailored to this relaxed security paradigm and define and analyze the leakage. Our evaluations show that our <b>logistic</b> <b>regression</b> protocol is up to 9x faster, and the neural network training is up to 688x faster than SecureML. Notably, our neural network achieves an accuracy of 96.6% on <b>MNIST</b> in 15 epochs, outperforming prior <b>benchmarks</b> that capped at 93.4% using the same architecture.</p></p class="citation"></blockquote><h3 id=59--283347-depending-on-yourself-when-you-should-mentoring-llm-with-rl-agents-to-become-the-master-in-cybersecurity-games-yikuan-yan-et-al-2024>(5/9 | 283/347) Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games (Yikuan Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yikuan Yan, Yaolun Zhang, Keman Huang. (2024)<br><strong>Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games</strong><br><button class=copy-to-clipboard title="Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-MA, cs.CR<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17674v1.pdf filename=2403.17674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating <b>LLM</b> and <b>reinforcement</b> <b>learning</b> (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations. In this study, we introduce SecurityBot, a <b>LLM</b> agent mentored by pre-trained RL agents, to support cybersecurity operations. In particularly, the <b>LLM</b> agent is supported with a profile module to generated behavior guidelines, a memory module to accumulate local experiences, a reflection module to re-evaluate choices, and an action module to reduce action space. Additionally, it adopts the collaboration mechanism to take suggestions from pre-trained RL agents, including a cursor for dynamic suggestion taken, an aggregator for multiple mentors&rsquo; suggestions ranking and a caller for proactive suggestion asking. Building on the CybORG experiment framework, our experiences show that SecurityBot demonstrates significant performance improvement compared with <b>LLM</b> or RL standalone, achieving the complementary performance in the cybersecurity games.</p></p class="citation"></blockquote><h3 id=69--284347-leak-and-learn-an-attackers-cookbook-to-train-using-leaked-data-from-federated-learning-joshua-c-zhao-et-al-2024>(6/9 | 284/347) Leak and Learn: An Attacker&rsquo;s Cookbook to Train Using Leaked Data from Federated Learning (Joshua C. Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua C. Zhao, Ahaan Dabholkar, Atul Sharma, Saurabh Bagchi. (2024)<br><strong>Leak and Learn: An Attacker&rsquo;s Cookbook to Train Using Leaked Data from Federated Learning</strong><br><button class=copy-to-clipboard title="Leak and Learn: An Attacker's Cookbook to Train Using Leaked Data from Federated Learning" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18144v1.pdf filename=2403.18144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> is a decentralized learning paradigm introduced to preserve privacy of client data. Despite this, prior work has shown that an attacker at the server can still reconstruct the private training data using only the client updates. These attacks are known as data reconstruction attacks and fall into two major categories: gradient inversion (GI) and linear layer leakage attacks (LLL). However, despite demonstrating the effectiveness of these attacks in breaching privacy, prior work has not investigated the usefulness of the reconstructed data for downstream tasks. In this work, we explore data reconstruction attacks through the lens of training and improving models with leaked data. We demonstrate the effectiveness of both GI and LLL attacks in maliciously training models using the leaked data more accurately than a benign <b>federated</b> <b>learning</b> strategy. Counter-intuitively, this bump in training quality can occur despite limited reconstruction quality or a small total number of leaked images. Finally, we show the limitations of these attacks for downstream training, individually for GI attacks and for LLL attacks.</p></p class="citation"></blockquote><h3 id=79--285347-ransomware-analysis-and-evaluation-of-live-forensic-techniques-and-the-impact-on-linux-based-iot-systems-salko-korac-et-al-2024>(7/9 | 285/347) Ransomware: Analysis and Evaluation of Live Forensic Techniques and the Impact on Linux based IoT Systems (Salko Korac et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salko Korac, Leandros Maglaras, Naghmeh Moradpoor, Bill Buchanan, Berk Canberk. (2024)<br><strong>Ransomware: Analysis and Evaluation of Live Forensic Techniques and the Impact on Linux based IoT Systems</strong><br><button class=copy-to-clipboard title="Ransomware: Analysis and Evaluation of Live Forensic Techniques and the Impact on Linux based IoT Systems" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Ransomware<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17571v1.pdf filename=2403.17571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Ransomware</b> has been predominantly a threat to Windows systems. But, Linux systems became interesting for cybercriminals and this trend is expected to continue. This endangers IoT ecosystems, whereas many IoT systems are based on Linux (e.g. cloud infrastructure and gateways). This paper researches how currently employed forensic techniques can be applied to Linux <b>ransomware</b> and evaluates the maturity as well as the impact on the system. While Windows-based <b>ransomware</b> predominantly uses RSA and AES for key management, a variety of approaches was identified for Linux. Cybercriminals appear to be deliberately moving away from RSA and AES to make Live forensic investigations more difficult. Linux <b>ransomware</b> is developed for a predefined goal and does not exploit the full potential of damage. It appears in an early stage and is expected to reach a similar potential to Windows-based malware. The results generated provided an excellent basic understanding to discuss and assess implications on the IoT industry at an early stage of development.</p></p class="citation"></blockquote><h3 id=89--286347-provably-secure-disambiguating-neural-linguistic-steganography-yuang-qi-et-al-2024>(8/9 | 286/347) Provably Secure Disambiguating Neural Linguistic Steganography (Yuang Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuang Qi, Kejiang Chen, Kai Zeng, Weiming Zhang, Nenghai Yu. (2024)<br><strong>Provably Secure Disambiguating Neural Linguistic Steganography</strong><br><button class=copy-to-clipboard title="Provably Secure Disambiguating Neural Linguistic Steganography" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Disambiguation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17524v1.pdf filename=2403.17524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research in provably secure neural linguistic steganography has overlooked a crucial aspect: the sender must detokenize stegotexts to avoid raising suspicion from the eavesdropper. The segmentation ambiguity problem, which arises when using language models based on subwords, leads to occasional decoding failures in all neural language steganography implementations based on these models. Current solutions to this issue involve altering the probability distribution of candidate words, rendering them incompatible with provably secure steganography. We propose a novel secure <b>disambiguation</b> method named SyncPool, which effectively addresses the segmentation ambiguity problem. We group all tokens with prefix relationships in the candidate pool before the steganographic embedding algorithm runs to eliminate uncertainty among ambiguous tokens. To enable the receiver to synchronize the sampling process of the sender, a shared cryptographically-secure pseudorandom number generator (CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does not change the size of the candidate pool or the distribution of tokens and thus is applicable to provably secure language steganography methods. We provide theoretical proofs and experimentally demonstrate the applicability of our solution to various languages and models, showing its potential to significantly improve the reliability and security of neural linguistic steganography systems.</p></p class="citation"></blockquote><h3 id=99--287347-two-birds-with-one-stone-differential-privacy-by-low-power-sram-memory-jianqing-liu-et-al-2024>(9/9 | 287/347) Two Birds with One Stone: Differential Privacy by Low-power SRAM Memory (Jianqing Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianqing Liu, Na Gong, Hritom Das. (2024)<br><strong>Two Birds with One Stone: Differential Privacy by Low-power SRAM Memory</strong><br><button class=copy-to-clipboard title="Two Birds with One Stone: Differential Privacy by Low-power SRAM Memory" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17303v1.pdf filename=2403.17303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The software-based implementation of <b>differential</b> <b>privacy</b> mechanisms has been shown to be neither friendly for lightweight devices nor secure against side-channel attacks. In this work, we aim to develop a hardware-based technique to achieve <b>differential</b> <b>privacy</b> by design. In contrary to the conventional software-based noise generation and injection process, our design realizes local <b>differential</b> <b>privacy</b> (LDP) by harnessing the inherent hardware noise into controlled LDP noise when data is stored in the memory. Specifically, the noise is tamed through a novel memory design and power downscaling technique, which leads to double-faceted gains in privacy and power efficiency. A well-round study that consists of theoretical design and analysis and chip implementation and experiments is presented. The results confirm that the developed technique is differentially private, saves 88.58% system power, speeds up software-based DP mechanisms by more than 10^6 times, while only incurring 2.46% chip overhead and 7.81% estimation errors in data recovery.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--288347-eternagram-probing-player-attitudes-in-alternate-climate-scenarios-through-a-chatgpt-driven-text-adventure-suifang-zhou-et-al-2024>(1/5 | 288/347) Eternagram: Probing Player Attitudes in Alternate Climate Scenarios Through a ChatGPT-Driven Text Adventure (Suifang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suifang Zhou, Latisha Besariani Hendra, Qinshi Zhang, Jussi Holopainen, RAY LC. (2024)<br><strong>Eternagram: Probing Player Attitudes in Alternate Climate Scenarios Through a ChatGPT-Driven Text Adventure</strong><br><button class=copy-to-clipboard title="Eternagram: Probing Player Attitudes in Alternate Climate Scenarios Through a ChatGPT-Driven Text Adventure" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: ChatGPT, GPT, Chatbot, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18160v1.pdf filename=2403.18160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional methods of assessing attitudes towards climate change are limited in capturing authentic opinions, primarily <b>stemming</b> from a lack of context-specific assessment strategies and an overreliance on simplistic surveys. Game-based Assessments (GBA) have demonstrated the ability to overcome these issues by immersing participants in engaging gameplay within carefully crafted, scenario-based environments. Concurrently, advancements in AI and Natural Language Processing (NLP) show promise in enhancing the gamified testing environment, achieving this by generating context-aware, human-like dialogues that contribute to a more natural and effective assessment. Our study introduces a new technique for probing climate change attitudes by actualizing a <b>GPT-driven</b> <b>chatbot</b> system in harmony with a game design depicting a futuristic climate scenario. The correlation analysis reveals an assimilation effect, where players&rsquo; post-game climate awareness tends to align with their in-game perceptions. Key predictors of pro-climate attitudes are identified as traits like &lsquo;Openness&rsquo; and &lsquo;Agreeableness&rsquo;, and a preference for democratic values.</p></p class="citation"></blockquote><h3 id=25--289347-expressedit-video-editing-with-natural-language-and-sketching-bekzat-tilekbay-et-al-2024>(2/5 | 289/347) ExpressEdit: Video Editing with Natural Language and Sketching (Bekzat Tilekbay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bekzat Tilekbay, Saelyne Yang, Michal Lewkowicz, Alex Suryapranata, Juho Kim. (2024)<br><strong>ExpressEdit: Video Editing with Natural Language and Sketching</strong><br><button class=copy-to-clipboard title="ExpressEdit: Video Editing with Natural Language and Sketching" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17693v1.pdf filename=2403.17693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike. When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging. However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas. To address this challenge, we first explored how multimodality$-$natural language (NL) and sketching, which are natural modalities humans use for expression$-$can be utilized to support video editors in expressing video editing ideas. We gathered 176 <b>multimodal</b> expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents. Based on the findings, we present ExpressEdit, a system that enables editing videos via NL text and sketching on the video frame. Powered by <b>LLM</b> and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching. The system implements the interpreted edits, which then the user can iterate on. An observational study (N=10) showed that ExpressEdit enhanced the ability of novice video editors to express and implement their edit ideas. The system allowed participants to perform edits more efficiently and generate more ideas by generating edits based on user&rsquo;s <b>multimodal</b> edit commands and supporting iterations on the editing commands. This work offers insights into the design of future <b>multimodal</b> interfaces and AI-based pipelines for video editing.</p></p class="citation"></blockquote><h3 id=35--290347-towards-inclusive-video-commenting-introducing-signmaku-for-the-deaf-and-hard-of-hearing-si-chen-et-al-2024>(3/5 | 290/347) Towards Inclusive Video Commenting: Introducing Signmaku for the Deaf and Hard-of-Hearing (Si Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Si Chen, Haocong Cheng, Jason Situ, Desirée Kirst, Suzy Su, Saumya Malhotra, Lawrence Angrave, Qi Wang, Yun Huang. (2024)<br><strong>Towards Inclusive Video Commenting: Introducing Signmaku for the Deaf and Hard-of-Hearing</strong><br><button class=copy-to-clipboard title="Towards Inclusive Video Commenting: Introducing Signmaku for the Deaf and Hard-of-Hearing" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: F-2-2; I-2-7, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17807v1.pdf filename=2403.17807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous research underscored the potential of danmaku&ndash;a text-based commenting feature on videos&ndash;in engaging hearing audiences. Yet, for many Deaf and hard-of-hearing (DHH) individuals, American Sign Language (ASL) takes precedence over English. To improve inclusivity, we introduce &ldquo;Signmaku,&rdquo; a new commenting mechanism that uses ASL, serving as a sign language counterpart to danmaku. Through a need-finding study (N=12) and a within-subject experiment (N=20), we evaluated three design styles: real human faces, cartoon-like figures, and robotic representations. The results showed that cartoon-like signmaku not only entertained but also encouraged participants to create and share ASL comments, with fewer privacy concerns compared to the other designs. Conversely, the robotic representations faced challenges in accurately depicting hand movements and facial expressions, resulting in higher cognitive demands on users. Signmaku featuring real human faces elicited the lowest cognitive load and was the most comprehensible among all three types. Our findings offered novel design implications for leveraging <b>generative</b> <b>AI</b> to create signmaku comments, enriching co-learning experiences for DHH individuals.</p></p class="citation"></blockquote><h3 id=45--291347-scicapenter-supporting-caption-composition-for-scientific-figures-with-machine-generated-captions-and-ratings-ting-yao-hsu-et-al-2024>(4/5 | 291/347) SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings (Ting-Yao Hsu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting-Yao Hsu, Chieh-Yang Huang, Shih-Hong Huang, Ryan Rossi, Sungchul Kim, Tong Yu, C. Lee Giles, Ting-Hao K. Huang. (2024)<br><strong>SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings</strong><br><button class=copy-to-clipboard title="SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Optical Character Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17784v1.pdf filename=2403.17784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crafting effective captions for figures is important. Readers heavily depend on these captions to grasp the figure&rsquo;s message. However, despite a well-developed set of AI technologies for figures and captions, these have rarely been tested for usefulness in aiding caption writing. This paper introduces SciCapenter, an interactive system that puts together cutting-edge AI technologies for scientific figure captions to aid caption composition. SciCapenter generates a variety of captions for each figure in a scholarly article, providing scores and a comprehensive checklist to assess caption quality across multiple critical aspects, such as helpfulness, <b>OCR</b> mention, key takeaways, and visual properties reference. Users can directly edit captions in SciCapenter, resubmit for revised evaluations, and iteratively refine them. A user study with Ph.D. students indicates that SciCapenter significantly lowers the cognitive load of caption writing. Participants&rsquo; feedback further offers valuable design insights for future systems aiming to enhance caption writing.</p></p class="citation"></blockquote><h3 id=55--292347-evaluating-authoring-tools-with-the-explorable-authoring-requirements-frederic-salmen-et-al-2024>(5/5 | 292/347) Evaluating Authoring Tools with the Explorable Authoring Requirements (Frederic Salmen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frederic Salmen, Ulrik Schroeder. (2024)<br><strong>Evaluating Authoring Tools with the Explorable Authoring Requirements</strong><br><button class=copy-to-clipboard title="Evaluating Authoring Tools with the Explorable Authoring Requirements" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17714v1.pdf filename=2403.17714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explorables with interactive, <b>multimodal</b> content, openly available on the web, are a promising medium for education. Yet authoring such explorables requires web development expertise, excluding most educators and students from the authoring and remixing process. Some tools are available to reduce this barrier of entry and others are in development, making a method to evaluate these new tools necessary. On the basis of the software quality model ISO 25010, empirical results, and domain modeling, we derive the Explorable Authoring Requirements (EAR) as a requirements catalogue explorable authoring tools should implement. We then outline a future research design to operationalize EAR.</p></p class="citation"></blockquote><h2 id=csit-3>cs.IT (3)</h2><h3 id=13--293347-adaptive-ttd-configurations-for-near-field-communications-an-unsupervised-transformer-approach-hsienchih-ting-et-al-2024>(1/3 | 293/347) Adaptive TTD Configurations for Near-Field Communications: An Unsupervised Transformer Approach (Hsienchih Ting et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hsienchih Ting, Zhaolin Wang, Yuanwei Liu. (2024)<br><strong>Adaptive TTD Configurations for Near-Field Communications: An Unsupervised Transformer Approach</strong><br><button class=copy-to-clipboard title="Adaptive TTD Configurations for Near-Field Communications: An Unsupervised Transformer Approach" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18146v1.pdf filename=2403.18146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>True-time delayers (TTDs) are popular analog devices for facilitating near-field wideband beamforming subject to the spatial-wideband effect. In this paper, an adaptive TTD configuration is proposed for short-range TTDs. Compared to the existing TTD configurations, the proposed one can effectively combat the spatial-widebandd effect for arbitrary user locations and array shapes with the aid of a switch network. A novel end-to-end deep neural network is proposed to optimize the hybrid beamforming with adaptive TTDs for maximizing spectral efficiency. 1) First, based on the U-Net architecture, a near-field channel learning module (NFC-LM) is proposed for adaptive beamformer design through extracting the latent channel response features of various users across different frequencies. In the NFC-LM, an improved cross attention (CA) is introduced to further optimize beamformer design by enhancing the latent feature connection between near-field channel and different beamformers. 2) Second, a switch multi-user <b>transformer</b> (S-MT) is proposed to adaptively control the connection between TTDs and phase shifters (PSs). In the S-MT, an improved multi-head attention, namely multi-user attention (MSA), is introduced to optimize the switch network through exploring the latent channel relations among various users. 3) Third, a multi feature cross attention (MCA) is introduced to simultaneously optimize the NFC-LM and S-MT by enhancing the latent feature correlation between beamformers and switch network. Numerical <b>simulation</b> results show that 1) the proposed adaptive TTD configuration effectively eliminates the spatial-wideband effect under uniform linear array (ULA) and uniform circular array (UCA) architectures, and 2) the proposed deep neural network can provide near optimal spectral efficiency, and solve the multi-user bemformer design and dynamical connection problem in real-time.</p></p class="citation"></blockquote><h3 id=23--294347-robust-analysis-of-full-duplex-two-way-space-shift-keying-with-ris-systems-xusheng-zhu-et-al-2024>(2/3 | 294/347) Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS Systems (Xusheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xusheng Zhu, Wen Chen, Qingqing Wu, Wen Fang, Chaoying Huang, Jun Li. (2024)<br><strong>Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS Systems</strong><br><button class=copy-to-clipboard title="Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS Systems" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17751v1.pdf filename=2403.17751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS)-assisted index modulation system schemes are considered a promising technology for sixth-generation (6G) wireless communication systems, which can enhance various system capabilities such as coverage and reliability. However, obtaining perfect channel state information (CSI) is challenging due to the lack of a radio frequency chain in RIS. In this paper, we investigate the RIS-assisted full-duplex (FD) two-way space shift keying (SSK) system under imperfect CSI, where the signal emissions are augmented by deploying RISs in the vicinity of two FD users. The maximum likelihood detector is utilized to recover the transmit antenna index. With this in mind, we derive closed-form average bit error probability (ABEP) expression based on the Gaussian-Chebyshev quadrature (GCQ) method and provide the upper bound and asymptotic ABEP expressions in the presence of channel estimation errors. To gain more insights, we also derive the outage probability and provide the throughput of the proposed scheme with imperfect CSI. The correctness of the analytical derivation results is confirmed via Monte Carlo <b>simulations.</b> It is demonstrated that increasing the number of elements of RIS can significantly improve the ABEP performance of the FD system over the half-duplex (HD) system. Furthermore, in the high SNR region, the ABEP performance of the FD system is better than that of the HD system.</p></p class="citation"></blockquote><h3 id=33--295347-computer-classification-of-linear-codes-based-on-lattice-point-enumeration-and-integer-linear-programming-sascha-kurz-2024>(3/3 | 295/347) Computer classification of linear codes based on lattice point enumeration and integer linear programming (Sascha Kurz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sascha Kurz. (2024)<br><strong>Computer classification of linear codes based on lattice point enumeration and integer linear programming</strong><br><button class=copy-to-clipboard title="Computer classification of linear codes based on lattice point enumeration and integer linear programming" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: 94B05, 05E20, cs-IT, cs.IT, math-CO, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17509v1.pdf filename=2403.17509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Linear codes play a central role in coding theory and have applications in several branches of mathematics. For error correction purposes the minimum Hamming distance should be as large as possible. Linear codes related to applications in Galois <b>Geometry</b> often require a certain divisibility of the occurring weights. In this paper we present an algorithmic framework for the classification of linear codes over finite fields with restricted sets of weights. The underlying algorithms are based on lattice point enumeration and integer linear programming. We present new enumeration and non-existence results for projective two-weight codes, divisible codes, and additive $\mathbb{F}_4$-codes.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--296347-fedmil-federated-multiple-instance-learning-for-video-analysis-with-optimized-dpp-scheduling-ashish-bastola-et-al-2024>(1/1 | 296/347) FedMIL: Federated-Multiple Instance Learning for Video Analysis with Optimized DPP Scheduling (Ashish Bastola et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashish Bastola, Hao Wang, Xiwen Chen, Abolfazl Razi. (2024)<br><strong>FedMIL: Federated-Multiple Instance Learning for Video Analysis with Optimized DPP Scheduling</strong><br><button class=copy-to-clipboard title="FedMIL: Federated-Multiple Instance Learning for Video Analysis with Optimized DPP Scheduling" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-IT, cs.DC, math-IT<br>Keyword Score: 40<br>Keywords: Federated Learning, Multiple Instance Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17331v1.pdf filename=2403.17331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many AI platforms, including traffic monitoring systems, use <b>Federated</b> <b>Learning</b> (FL) for decentralized sensor data processing for learning-based applications while preserving privacy and ensuring secured information transfer. On the other hand, applying <b>supervised</b> <b>learning</b> to large data samples, like high-resolution images requires intensive human labor to label different parts of a data sample. <b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) alleviates this challenge by operating over labels assigned to the &lsquo;bag&rsquo; of instances. In this paper, we introduce <b>Federated</b> <b>Multiple-Instance</b> <b>Learning</b> <b>(FedMIL).</b> This framework applies <b>federated</b> <b>learning</b> to boost the training performance in video-based MIL tasks such as vehicle accident detection using distributed CCTV networks. However, data sources in decentralized settings are not typically Independently and Identically Distributed (IID), making client selection imperative to collectively represent the entire dataset with minimal clients. To address this challenge, we propose DPPQ, a framework based on the Determinantal Point Process (DPP) with a quality-based kernel to select clients with the most diverse datasets that achieve better performance compared to both random selection and current DPP-based client selection methods even with less data utilization in the majority of non-IID cases. This offers a significant advantage for deployment on edge devices with limited computational resources, providing a reliable solution for training AI models in massive smart sensor networks.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--297347-neural-attributed-community-search-at-billion-scale-jianwei-wang-et-al-2024>(1/2 | 297/347) Neural Attributed Community Search at Billion Scale (Jianwei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianwei Wang, Kai Wang, Xuemin Lin, Wenjie Zhang, Ying Zhang. (2024)<br><strong>Neural Attributed Community Search at Billion Scale</strong><br><button class=copy-to-clipboard title="Neural Attributed Community Search at Billion Scale" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 33<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18874v1.pdf filename=2403.18874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Community search has been extensively studied in the past decades. In recent years, there is a growing interest in attributed community search that aims to identify a community based on both the query <b>nodes</b> <b>and</b> query attributes. A set of techniques have been investigated. Though the recent methods based on advanced learning models such as <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> can achieve state-of-the-art performance in terms of accuracy, we notice that 1) they suffer from severe efficiency issues; 2) they directly model community search as a <b>node</b> <b>classification</b> problem and thus cannot make good use of interdependence among different entities in the <b>graph.</b> <b>Motivated</b> <b>by</b> these, in this paper, we propose a new neurAL attrIbuted Community sEarch model for large-scale <b>graphs,</b> <b>termed</b> <b>ALICE.</b> ALICE first extracts a candidate subgraph to reduce the search scope and subsequently predicts the community by the Consistency-aware Net , termed ConNet. Specifically, in the extraction phase, we introduce the density sketch modularity that uses a unified form to combine the strengths of two existing powerful modularities, i.e., classical modularity and density modularity. Based on the new modularity metric, we first adaptively obtain the candidate subgraph, formed by the k-hop neighbors of the query <b>nodes,</b> <b>with</b> the maximum modularity. Then, we construct a <b>node-attribute</b> <b>bipartite</b> <b>graph</b> <b>to</b> <b>take</b> attributes into consideration. After that, ConNet adopts a cross-attention encoder to encode the interaction between the query and the <b>graph.</b> <b>The</b> <b>training</b> of the model is guided by the structure-attribute consistency and the local consistency to achieve better performance. Extensive experiments over 11 real-world datasets including one billion-scale <b>graph</b> <b>demonstrate</b> <b>the</b> superiority of ALICE in terms of accuracy, efficiency, and scalability.</p></p class="citation"></blockquote><h3 id=22--298347-efficient-unsupervised-community-search-with-pre-trained-graph-transformer-jianwei-wang-et-al-2024>(2/2 | 298/347) Efficient Unsupervised Community Search with Pre-trained Graph Transformer (Jianwei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianwei Wang, Kai Wang, Xuemin Lin, Wenjie Zhang, Ying Zhang. (2024)<br><strong>Efficient Unsupervised Community Search with Pre-trained Graph Transformer</strong><br><button class=copy-to-clipboard title="Efficient Unsupervised Community Search with Pre-trained Graph Transformer" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-DB, cs-SI, cs.SI<br>Keyword Score: 33<br>Keywords: Graph, Self-supervised Learning, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18869v1.pdf filename=2403.18869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Community search has aroused widespread interest in the past decades. Among existing solutions, the learning-based models exhibit outstanding performance in terms of accuracy by leveraging labels to 1) train the model for community score learning, and 2) select the optimal threshold for community identification. However, labeled data are not always available in real-world scenarios. To address this notable limitation of learning-based models, we propose a pre-trained <b>graph</b> <b>Transformer</b> based community search framework that uses Zero label (i.e., <b>unsupervised),</b> termed TransZero. TransZero has two key phases, i.e., the offline pre-training phase and the online search phase. Specifically, in the offline pretraining phase, we design an efficient and effective community search <b>graph</b> <b>transformer</b> (CSGphormer) to learn node representation. To pre-train CSGphormer without the usage of labels, we introduce two <b>self-supervised</b> losses, i.e., personalization loss and link loss, motivated by the inherent uniqueness of node and <b>graph</b> topology, respectively. In the online search phase, with the representation learned by the pre-trained CSGphormer, we compute the community score without using labels by measuring the similarity of representations between the query nodes and the nodes in the <b>graph.</b> To free the framework from the usage of a label-based threshold, we define a new function named expected score gain to guide the community identification process. Furthermore, we propose two efficient and effective algorithms for the community identification process that run without the usage of labels. Extensive experiments over 10 public datasets illustrate the superior performance of TransZero regarding both accuracy and efficiency.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--299347-discretize-first-filter-next-learning-divergence-consistent-closure-models-for-large-eddy-simulation-syver-døving-agdestein-et-al-2024>(1/2 | 299/347) Discretize first, filter next: learning divergence-consistent closure models for large-eddy simulation (Syver Døving Agdestein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syver Døving Agdestein, Benjamin Sanderse. (2024)<br><strong>Discretize first, filter next: learning divergence-consistent closure models for large-eddy simulation</strong><br><button class=copy-to-clipboard title="Discretize first, filter next: learning divergence-consistent closure models for large-eddy simulation" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65 (Primary), 76, 35 (Secondary), cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 30<br>Keywords: Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18088v1.pdf filename=2403.18088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new neural network based large eddy <b>simulation</b> framework for the incompressible Navier-Stokes equations based on the paradigm &ldquo;discretize first, filter and close next&rdquo;. This leads to full model-data consistency and allows for employing neural closure models in the same environment as where they have been trained. Since the LES discretization error is included in the learning process, the closure models can learn to account for the discretization. Furthermore, we introduce a new divergence-consistent discrete filter defined through face-averaging. The new filter preserves the discrete divergence-free constraint by construction, unlike general discrete filters such as volume-averaging filters. We show that using a divergence-consistent LES formulation coupled with a <b>convolutional</b> neural closure model produces stable and accurate results for both a-priori and a-posteriori training, while a general (divergence-inconsistent) LES model requires a-posteriori training or other stability-enforcing measures.</p></p class="citation"></blockquote><h3 id=22--300347-numerical-analysis-of-a-fesav-scheme-for-a-caginalp-phase-field-model-with-mechanical-effects-in-stereolithography-xingguang-jin-et-al-2024>(2/2 | 300/347) Numerical analysis of a FE/SAV scheme for a Caginalp phase field model with mechanical effects in stereolithography (Xingguang Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingguang Jin, Kei Fong Lam, Changqing Ye. (2024)<br><strong>Numerical analysis of a FE/SAV scheme for a Caginalp phase field model with mechanical effects in stereolithography</strong><br><button class=copy-to-clipboard title="Numerical analysis of a FE/SAV scheme for a Caginalp phase field model with mechanical effects in stereolithography" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17434v1.pdf filename=2403.17434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we propose a phase field model based on a Caginalp system with mechanical effects to study the underlying physical and chemical processes behind stereolithography, which is an additive manufacturing (3D printing) technique that builds objects in a layer-by-layer fashion by using an ultraviolet laser to solidify liquid polymer resins. Existence of weak solutions is established by demonstrating the convergence of a numerical scheme based on a first order scalar auxiliary variable temporal discretization and a finite element spatial discretization. We further establish uniqueness and regularity of solutions, as well as optimal error estimates for the Caginalp system that are supported by numerical <b>simulations.</b> We also present some qualitative two-dimensional <b>simulations</b> of the stereolithography processes captured by the model.</p></p class="citation"></blockquote><h2 id=eesssy-11>eess.SY (11)</h2><h3 id=111--301347-learning-piecewise-residuals-of-control-barrier-functions-for-safety-of-switching-systems-using-multi-output-gaussian-processes-mohammad-aali-et-al-2024>(1/11 | 301/347) Learning Piecewise Residuals of Control Barrier Functions for Safety of Switching Systems using Multi-Output Gaussian Processes (Mohammad Aali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Aali, Jun Liu. (2024)<br><strong>Learning Piecewise Residuals of Control Barrier Functions for Safety of Switching Systems using Multi-Output Gaussian Processes</strong><br><button class=copy-to-clipboard title="Learning Piecewise Residuals of Control Barrier Functions for Safety of Switching Systems using Multi-Output Gaussian Processes" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18041v1.pdf filename=2403.18041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Control barrier functions (CBFs) have recently been introduced as a systematic tool to ensure safety by establishing set invariance. When combined with a control Lyapunov function (CLF), they form a safety-critical control mechanism. However, the effectiveness of CBFs and CLFs is closely tied to the system model. In practice, model uncertainty can jeopardize safety and stability guarantees and may lead to undesirable performance. In this paper, we develop a safe learning-based control strategy for switching systems in the face of uncertainty. We focus on the case that a nominal model is available for a true underlying switching system. This uncertainty results in piecewise residuals for each switching surface, impacting the CLF and CBF constraints. We introduce a batch multi-output <b>Gaussian</b> <b>process</b> (MOGP) framework to approximate these piecewise residuals, thereby mitigating the adverse effects of uncertainty. A particular structure of the covariance function enables us to convert the MOGP-based chance constraints CLF and CBF into second-order cone constraints, which leads to a convex optimization. We analyze the feasibility of the resulting optimization and provide the necessary and sufficient conditions for feasibility. The effectiveness of the proposed strategy is validated through a <b>simulation</b> of a switching adaptive cruise control system.</p></p class="citation"></blockquote><h3 id=211--302347-multi-agent-clarity-aware-dynamic-coverage-with-gaussian-processes-devansh-r-agrawal-et-al-2024>(2/11 | 302/347) Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes (Devansh R. Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Devansh R. Agrawal, Dimitra Panagou. (2024)<br><strong>Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes</strong><br><button class=copy-to-clipboard title="Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17917v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17917v1.pdf filename=2403.17917v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents two algorithms for multi-agent dynamic coverage in spatiotemporal environments, where the coverage algorithms are informed by the method of data assimilation. In particular, we show that by considering the information assimilation algorithm, here a Numerical <b>Gaussian</b> <b>Process</b> Kalman Filter, the influence of measurements taken at one position on the uncertainty of the estimate at another location can be computed. We use this relationship to propose new coverage algorithms. Furthermore, we show that the controllers naturally extend to the multi-agent context, allowing for a distributed-control central-information paradigm for multi-agent coverage. Finally, we demonstrate the algorithms through a realistic <b>simulation</b> of a team of UAVs collecting wind data over a region in Austria.</p></p class="citation"></blockquote><h3 id=311--303347-path-integral-control-with-rollout-clustering-and-dynamic-obstacles-steven-patrick-et-al-2024>(3/11 | 303/347) Path Integral Control with Rollout Clustering and Dynamic Obstacles (Steven Patrick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Steven Patrick, Efstathios Bakolas. (2024)<br><strong>Path Integral Control with Rollout Clustering and Dynamic Obstacles</strong><br><button class=copy-to-clipboard title="Path Integral Control with Rollout Clustering and Dynamic Obstacles" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18066v1.pdf filename=2403.18066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model Predictive Path Integral (MPPI) control has proven to be a powerful tool for the control of uncertain systems (such as systems subject to disturbances and systems with unmodeled dynamics). One important limitation of the baseline MPPI algorithm is that it does not utilize simulated trajectories to their fullest extent. For one, it assumes that the average of all trajectories weighted by their performance index will be a safe trajectory. In this paper, multiple examples are shown where the previous assumption does not hold, and a trajectory <b>clustering</b> technique is presented that reduces the chances of the weighted average crossing in an unsafe region. Secondly, MPPI does not account for dynamic obstacles, so the authors put forward a novel cost function that accounts for dynamic obstacles without adding significant computation time to the overall algorithm. The novel contributions proposed in this paper were evaluated with extensive <b>simulations</b> to demonstrate improvements upon the state-of-the-art MPPI techniques.</p></p class="citation"></blockquote><h3 id=411--304347-multiple-model-reference-adaptive-control-with-blending-for-non-square-multivariable-systems-alex-lovi-et-al-2024>(4/11 | 304/347) Multiple Model Reference Adaptive Control with Blending for Non-Square Multivariable Systems (Alex Lovi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Lovi, Baris Fidan, Christopher Nielsen. (2024)<br><strong>Multiple Model Reference Adaptive Control with Blending for Non-Square Multivariable Systems</strong><br><button class=copy-to-clipboard title="Multiple Model Reference Adaptive Control with Blending for Non-Square Multivariable Systems" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18119v1.pdf filename=2403.18119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we develop a multiple model reference adaptive controller (MMRAC) with blending. The systems under consideration are non-square, i.e., the number of inputs is not equal to the number of states; multi-input, linear, time-invariant with uncertain parameters that lie inside of a known, compact, and convex set. Moreover, the full state of the plant is available for feedback. A multiple model online identification scheme for the plant&rsquo;s state and input matrices is developed that guarantees the estimated parameters converge to the underlying plant model under the assumption of persistence of excitation. Using an exact matching condition, the parameter estimates are used in a control law such that the plant&rsquo;s states asymptotically track the reference signal generated by a state-space model reference. The control architecture is proven to provide boundedness of all closed-loop signals and to asymptotically drive the state tracking error to zero. Numerical <b>simulations</b> illustrate the stability and efficacy of the proposed MMRAC scheme.</p></p class="citation"></blockquote><h3 id=511--305347-adaptive-boundary-control-of-the-kuramoto-sivashinsky-equation-under-intermittent-sensing-mohamed-camil-belhadjoudja-et-al-2024>(5/11 | 305/347) Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under Intermittent Sensing (Mohamed Camil Belhadjoudja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Camil Belhadjoudja, Mohamed Maghenem, Emmanuel Witrant, Christophe Prieur. (2024)<br><strong>Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under Intermittent Sensing</strong><br><button class=copy-to-clipboard title="Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under Intermittent Sensing" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-AP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18055v1.pdf filename=2403.18055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study in this paper boundary stabilization, in the L2 sense, of the one-dimensional Kuramoto-Sivashinsky equation subject to intermittent sensing. We assume that we measure the state of this spatio-temporal equation on a given spatial subdomain during certain intervals of time, while we measure the state on the remaining spatial subdomain during the remaining intervals of time. As a result, we assign a feedback law at the boundary of the spatial domain and force to zero the value of the state at the junction of the two subdomains. Throughout the study, the destabilizing coefficient is assumed to be space-dependent and bounded but unknown. Adaptive boundary controllers are designed under different assumptions on the forcing term. In particular, when the forcing term is null, we guarantee global exponential stability of the origin. Furthermore, when the forcing term is bounded and admits a known upper bound, we guarantee input-to-state stability, and only global uniform ultimate boundedness is guaranteed when the upper bound is unknown. Numerical <b>simulations</b> are performed to illustrate our results</p></p class="citation"></blockquote><h3 id=611--306347-using-quantum-computers-in-control-interval-matrix-properties-jan-schneider-et-al-2024>(6/11 | 306/347) Using quantum computers in control: interval matrix properties (Jan Schneider et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Schneider, Julian Berberich. (2024)<br><strong>Using quantum computers in control: interval matrix properties</strong><br><button class=copy-to-clipboard title="Using quantum computers in control: interval matrix properties" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17711v1.pdf filename=2403.17711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum computing provides a powerful framework for tackling computational problems that are classically intractable. The goal of this paper is to explore the use of quantum computers for solving relevant problems in systems and control theory. In the recent literature, different quantum algorithms have been developed to tackle binary optimization, which plays an important role in various control-theoretic problems. As a prototypical example, we consider the verification of interval matrix properties such as non-singularity and stability on a quantum computer. We present a quantum algorithm solving these problems and we study its performance in <b>simulation.</b> Our results demonstrate that quantum computers provide a promising tool for control whose applicability to further computationally complex problems remains to be explored.</p></p class="citation"></blockquote><h3 id=711--307347-cyclic-pursuit-formation-control-for-arbitrary-desired-shapes-anna-fujioka-et-al-2024>(7/11 | 307/347) Cyclic pursuit formation control for arbitrary desired shapes (Anna Fujioka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Fujioka, Masaki Ogura, Naoki Wakamiya. (2024)<br><strong>Cyclic pursuit formation control for arbitrary desired shapes</strong><br><button class=copy-to-clipboard title="Cyclic pursuit formation control for arbitrary desired shapes" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17417v1.pdf filename=2403.17417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A multi-agent system comprises numerous agents that autonomously make decisions to collectively accomplish tasks, drawing significant attention for their wide-ranging applications. Within this context, formation control emerges as a prominent task, wherein agents collaboratively shape and maneuver while preserving formation integrity. Our focus centers on cyclic pursuit, a method facilitating the formation of circles, ellipses, and figure-eights under the assumption that agents can only perceive the relative positions of those preceding them. However, this method&rsquo;s scope has been restricted to these specific shapes, leaving the feasibility of forming other shapes uncertain. In response, our study proposes a novel method based on cyclic pursuit capable of forming a broader array of shapes, enabling agents to individually shape while pursuing preceding agents, thereby extending the repertoire of achievable formations. We present two scenarios concerning the information available to agents and devise formation control methods tailored to each scenario. Through extensive <b>simulations,</b> we demonstrate the efficacy of our proposed method in forming multiple shapes, including those represented as Fourier series, thereby underscoring the versatility and effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=811--308347-destination-constrained-linear-dynamical-system-modeling-in-set-valued-frameworks-xiaowei-yang-et-al-2024>(8/11 | 308/347) Destination-Constrained Linear Dynamical System Modeling in Set-Valued Frameworks (Xiaowei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaowei Yang, Haiqi Liu, Fanqin Meng, Xiaojing Shen. (2024)<br><strong>Destination-Constrained Linear Dynamical System Modeling in Set-Valued Frameworks</strong><br><button class=copy-to-clipboard title="Destination-Constrained Linear Dynamical System Modeling in Set-Valued Frameworks" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SP, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17337v1.pdf filename=2403.17337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Directional motion towards a specified destination is a common occurrence in physical processes and human societal activities. Utilizing this prior information can significantly improve the control and predictive performance of system models. This paper primarily focuses on reconstructing linear dynamic system models based on destination constraints in the set-valued framework. We treat destination constraints as inherent information in the state evolution process and employ convex optimization techniques to construct a coherent and robust state model. This refined model effectively captures the impact of destination constraints on the state evolution at each time step. Furthermore, we design an optimal weight matrix for the reconstructed model to ensure smoother and more natural trajectories of state evolution. We also analyze the theoretical guarantee of optimality for this weight matrix and the properties of the reconstructed model. Finally, <b>simulation</b> experiments verify that the reconstructed model has significant advantages over the unconstrained and unoptimized weighted models and constrains the evolution of state trajectories with different starting and ending points.</p></p class="citation"></blockquote><h3 id=911--309347-neural-distributed-controllers-with-port-hamiltonian-structures-muhammad-zakwan-et-al-2024>(9/11 | 309/347) Neural Distributed Controllers with Port-Hamiltonian Structures (Muhammad Zakwan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Zakwan, Giancarlo Ferrari-Trecate. (2024)<br><strong>Neural Distributed Controllers with Port-Hamiltonian Structures</strong><br><button class=copy-to-clipboard title="Neural Distributed Controllers with Port-Hamiltonian Structures" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17785v1.pdf filename=2403.17785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Controlling large-scale cyber-physical systems necessitates optimal distributed policies, relying solely on local real-time data and limited communication with neighboring agents. However, finding optimal controllers remains challenging, even in seemingly simple scenarios. Parameterizing these policies using Neural Networks (NNs) can deliver good performance, but their sensitivity to small input changes can destabilize the closed-loop system. This paper addresses this issue for a network of nonlinear dissipative systems. Specifically, we leverage well-established port-Hamiltonian structures to characterize deep distributed control policies with closed-loop stability guarantees and a finite $\mathcal{L}_2$ gain, regardless of specific NN parameters. This eliminates the need to constrain the parameters during optimization and enables training with standard methods like <b>stochastic</b> <b>gradient</b> <b>descent.</b> A numerical study on the consensus control of Kuramoto oscillators demonstrates the effectiveness of the proposed controllers.</p></p class="citation"></blockquote><h3 id=1011--310347-reinforcement-learning-based-receding-horizon-control-using-adaptive-control-barrier-functions-for-safety-critical-systems-ehsan-sabouni-et-al-2024>(10/11 | 310/347) Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems (Ehsan Sabouni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Sabouni, H. M. Sabbir Ahmad, Vittorio Giammarino, Christos G. Cassandras, Ioannis Ch. Paschalidis, Wenchao Li. (2024)<br><strong>Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems</strong><br><button class=copy-to-clipboard title="Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17338v1.pdf filename=2403.17338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced. Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. %as well as infeasibility. To address these challenges, we propose a <b>Reinforcement</b> <b>Learning</b> (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In particular, we parameterize our controller and use bilevel optimization, where RL is used to learn the optimal parameters while MPC computes the optimal control input. We validate our method by applying it to the challenging automated merging control problem for Connected and Automated Vehicles (CAVs) at conflicting roadways. Results demonstrate improved performance and a significant reduction in the number of infeasible cases compared to traditional heuristic approaches used for tuning CBF-based controllers, showcasing the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=1111--311347-neural-exponential-stabilization-of-control-affine-nonlinear-systems-muhammad-zakwan-et-al-2024>(11/11 | 311/347) Neural Exponential Stabilization of Control-affine Nonlinear Systems (Muhammad Zakwan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Zakwan, Liang Xu, Giancarlo Ferrari-Trecate. (2024)<br><strong>Neural Exponential Stabilization of Control-affine Nonlinear Systems</strong><br><button class=copy-to-clipboard title="Neural Exponential Stabilization of Control-affine Nonlinear Systems" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17793v1.pdf filename=2403.17793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel learning-based approach for achieving exponential stabilization of nonlinear control-affine systems. We leverage the Control Contraction Metrics (CCMs) framework to co-synthesize Neural Contraction Metrics (NCMs) and Neural Network (NN) controllers. First, we transform the infinite-dimensional semi-definite program (SDP) for CCM computation into a tractable inequality feasibility problem using element-wise bounds of matrix-valued functions. The terms in the inequality can be efficiently computed by our novel algorithms. Second, we propose a free parametrization of NCMs guaranteeing positive definiteness and the satisfaction of a partial differential equation, regardless of trainable parameters. Third, this parametrization and the inequality condition enable the design of contractivity-enforcing regularizers, which can be incorporated while designing the NN controller for exponential stabilization of the underlying nonlinear systems. Furthermore, when the training loss goes to zero, we provide formal guarantees on verification of the NCM and the exponentional stabilization under the NN controller. Finally, we validate our method through <b>benchmark</b> experiments on set-point stabilization and increasing the region of attraction of a locally pre-stabilized closed-loop system.</p></p class="citation"></blockquote><h2 id=csdb-2>cs.DB (2)</h2><h3 id=12--312347-disambiguate-entity-matching-through-relation-discovery-with-large-language-models-zezhou-huang-2024>(1/2 | 312/347) Disambiguate Entity Matching through Relation Discovery with Large Language Models (Zezhou Huang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zezhou Huang. (2024)<br><strong>Disambiguate Entity Matching through Relation Discovery with Large Language Models</strong><br><button class=copy-to-clipboard title="Disambiguate Entity Matching through Relation Discovery with Large Language Models" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-CL, cs-DB, cs.DB<br>Keyword Score: 30<br>Keywords: GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17344v1.pdf filename=2403.17344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity matching is a critical challenge in data integration and cleaning, central to tasks like fuzzy joins and deduplication. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>GPT.</b> However, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a &ldquo;match,&rdquo; especially when integrating with external databases. This ambiguity arises due to varying levels of detail and granularity among entities, complicating exact matches. We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the &ldquo;relations&rdquo; between entities as crucial for resolving ambiguities in matching. By predefining a set of relations relevant to the task at hand, our method allows analysts to navigate the spectrum of similarity more effectively, from exact matches to conceptually related entities.</p></p class="citation"></blockquote><h3 id=22--313347-empirical-analysis-of-eip-3675-miner-dynamics-transaction-fees-and-transaction-time-umesh-bhatt-et-al-2024>(2/2 | 313/347) Empirical Analysis of EIP-3675: Miner Dynamics, Transaction Fees, and Transaction Time (Umesh Bhatt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Umesh Bhatt, Sarvesh Pandey. (2024)<br><strong>Empirical Analysis of EIP-3675: Miner Dynamics, Transaction Fees, and Transaction Time</strong><br><button class=copy-to-clipboard title="Empirical Analysis of EIP-3675: Miner Dynamics, Transaction Fees, and Transaction Time" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17885v1.pdf filename=2403.17885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Ethereum Improvement Proposal 3675 (EIP-3675) marks a significant shift, transitioning from a Proof of Work (PoW) to a Proof of Stake (PoS) consensus mechanism. This transition resulted in a staggering 99.95% decrease in energy consumption. However, the transition <b>prompts</b> two critical questions: (1). How does EIP-3675 affect miners&rsquo; dynamics? and (2). How do users determine priority fees, considering that paying too little may cause delays or non-inclusion, yet paying too much wastes money with little to no benefits? To address the first question, we present a comprehensive empirical study examining EIP-3675&rsquo;s effect on miner dynamics (i.e., miner participation, distribution, and the degree of randomness in miner selection). Our findings reveal that the transition has encouraged broader participation of miners in block append operation, resulting in a larger pool of unique miners ($\approx50\times$ PoW), and the change in miner distribution with the increased number of unique small category miners ($\approx60\times$ PoW). However, there is an unintended consequence: a reduction in the miner selection randomness, which signifies the negative impact of the transition to PoS-Ethereum on network decentralization. Regarding the second question, we employed regression-based machine learning models; the Gradient Boosting Regressor performed best in predicting priority fees, while the K-Neighbours Regressor was worst.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--314347-sghormer-an-energy-saving-graph-transformer-driven-by-spikes-huizhe-zhang-et-al-2024>(1/1 | 314/347) SGHormer: An Energy-Saving Graph Transformer Driven by Spikes (Huizhe Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huizhe Zhang, Jintang Li, Liang Chen, Zibin Zheng. (2024)<br><strong>SGHormer: An Energy-Saving Graph Transformer Driven by Spikes</strong><br><button class=copy-to-clipboard title="SGHormer: An Energy-Saving Graph Transformer Driven by Spikes" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-LG, cs-NE, cs.NE<br>Keyword Score: 28<br>Keywords: Graph, Representation Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17656v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17656v1.pdf filename=2403.17656v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Transformers</b> (GTs) with powerful <b>representation</b> <b>learning</b> ability make a huge success in wide range of <b>graph</b> tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla <b>transformer</b> seriously hinder its scalability on the large-scale <b>graph</b> data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs&rsquo; efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based <b>graph</b> <b>transformer</b> (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking <b>graph</b> <b>self-attention</b> and spiking rectify blocks in SGHormer explicitly capture global structure information and recover the expressive power of spiking embeddings, respectively. In experiments, SGHormer achieves comparable performances to other full-precision GTs with extremely low computational energy consumption. The results show that SGHomer makes a remarkable progress in the field of low-energy GTs.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--315347-einexprs-contraction-paths-of-tensor-networks-as-symbolic-expressions-sergio-sanchez-ramirez-et-al-2024>(1/2 | 315/347) EinExprs: Contraction Paths of Tensor Networks as Symbolic Expressions (Sergio Sanchez-Ramirez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergio Sanchez-Ramirez, Jofre Vallès-Muns, Artur Garcia-Saez. (2024)<br><strong>EinExprs: Contraction Paths of Tensor Networks as Symbolic Expressions</strong><br><button class=copy-to-clipboard title="EinExprs: Contraction Paths of Tensor Networks as Symbolic Expressions" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: 81-04, G-4; J-2; I-1-1, cs-MS, quant-ph, quant-ph<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18030v1.pdf filename=2403.18030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tensor Networks are <b>graph</b> representations of summation expressions in which vertices represent tensors and edges represent tensor indices or vector spaces. In this work, we present EinExprs.jl, a Julia package for contraction path optimization that offers state-of-art optimizers. We propose a representation of the contraction path of a Tensor Network based on symbolic expressions. Using this package the user may choose among a collection of different methods such as Greedy algorithms, or an approach based on the hypergraph partitioning problem. We <b>benchmark</b> this library with examples obtained from the <b>simulation</b> of Random Quantum Circuits (RQC), a well known example where Tensor Networks provide state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=22--316347-fermihedral-on-the-optimal-compilation-for-fermion-to-qubit-encoding-yuhao-liu-et-al-2024>(2/2 | 316/347) Fermihedral: On the Optimal Compilation for Fermion-to-Qubit Encoding (Yuhao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Liu, Shize Che, Junyu Zhou, Yunong Shi, Gushu Li. (2024)<br><strong>Fermihedral: On the Optimal Compilation for Fermion-to-Qubit Encoding</strong><br><button class=copy-to-clipboard title="Fermihedral: On the Optimal Compilation for Fermion-to-Qubit Encoding" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17794v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17794v2.pdf filename=2403.17794v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces Fermihedral, a compiler framework focusing on discovering the optimal Fermion-to-qubit encoding for targeted Fermionic Hamiltonians. Fermion-to-qubit encoding is a crucial step in harnessing quantum computing for efficient <b>simulation</b> of Fermionic quantum systems. Utilizing Pauli algebra, Fermihedral redefines complex constraints and objectives of Fermion-to-qubit encoding into a Boolean Satisfiability problem which can then be solved with high-performance solvers. To accommodate larger-scale scenarios, this paper proposed two new strategies that yield approximate optimal solutions mitigating the overhead from the exponentially large number of clauses. Evaluation across diverse Fermionic systems highlights the superiority of Fermihedral, showcasing substantial reductions in implementation costs, gate counts, and circuit depth in the compiled circuits. Real-system experiments on IonQ&rsquo;s device affirm its effectiveness, notably enhancing <b>simulation</b> accuracy.</p></p class="citation"></blockquote><h2 id=mathoc-5>math.OC (5)</h2><h3 id=15--317347-generalized-maximum-entropy-differential-dynamic-programming-yuichiro-aoyama-et-al-2024>(1/5 | 317/347) Generalized Maximum Entropy Differential Dynamic Programming (Yuichiro Aoyama et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuichiro Aoyama, Evangelos A. Theodorou. (2024)<br><strong>Generalized Maximum Entropy Differential Dynamic Programming</strong><br><button class=copy-to-clipboard title="Generalized Maximum Entropy Differential Dynamic Programming" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 34H05, cs-IT, math-IT, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18130v1.pdf filename=2403.18130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a sampling-based trajectory optimization method derived from the maximum entropy formulation of Differential Dynamic Programming with Tsallis entropy. This method can be seen as a generalization of the legacy work with Shannon entropy, which leads to a Gaussian optimal control policy for exploration during optimization. With the Tsallis entropy, the optimal control policy takes the form of $q$-Gaussian, which further encourages exploration with its heavy-tailed shape. Moreover, in our formulation, the exploration variance, which was scaled by a fixed constant inverse temperature in the original formulation with Shannon entropy, is automatically scaled based on the value function of the trajectory. Due to this property, our algorithms can promote exploration when necessary, that is, the cost of the trajectory is high, rather than using the same scaling factor. The <b>simulation</b> results demonstrate the properties of the proposed algorithm described above.</p></p class="citation"></blockquote><h3 id=25--318347-a-moreau-envelope-approach-for-lqr-meta-policy-estimation-ashwin-aravind-et-al-2024>(2/5 | 318/347) A Moreau Envelope Approach for LQR Meta-Policy Estimation (Ashwin Aravind et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashwin Aravind, Mohammad Taha Toghani, César A. Uribe. (2024)<br><strong>A Moreau Envelope Approach for LQR Meta-Policy Estimation</strong><br><button class=copy-to-clipboard title="A Moreau Envelope Approach for LQR Meta-Policy Estimation" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 49M99, 93E35, 93C05, I-2-8, cs-LG, cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Discrete Time, Discrete Time, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17364v1.pdf filename=2403.17364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of policy estimation for the Linear Quadratic Regulator (LQR) in <b>discrete-time</b> <b>linear</b> time-invariant uncertain dynamical systems. We propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of realizations of the uncertain system, to define a <b>meta-policy</b> <b>efficiently</b> adjustable to new realizations. Moreover, we design an algorithm to find an approximate first-order stationary point of the <b>meta-LQR</b> <b>cost</b> function. Numerical results show that the proposed approach outperforms naive averaging of controllers on new realizations of the linear system. We also provide empirical evidence that our method has better sample complexity than Model-Agnostic <b>Meta-Learning</b> <b>(MAML)</b> approaches.</p></p class="citation"></blockquote><h3 id=35--319347-deep-polytopic-autoencoders-for-low-dimensional-linear-parameter-varying-approximations-and-nonlinear-feedback-design-jan-heiland-et-al-2024>(3/5 | 319/347) Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design (Jan Heiland et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Heiland, Yongho Kim, Steffen W. R. Werner. (2024)<br><strong>Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design</strong><br><button class=copy-to-clipboard title="Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design" index=319>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-319 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, cs-NA, math-DS, math-NA, math-OC, math.OC, physics-flu-dyn<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18044v1.pdf filename=2403.18044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Polytopic <b>autoencoders</b> provide low-dimensional parametrizations of states in a polytope. For nonlinear PDEs, this is readily applied to low-dimensional linear parameter-varying (LPV) approximations as they have been exploited for efficient nonlinear controller design via series expansions of the solution to the state-dependent Riccati equation. In this work, we develop a polytopic <b>autoencoder</b> for control applications and show how it outperforms standard linear approaches in view of LPV approximations of nonlinear systems and how the particular architecture enables higher order series expansions at little extra computational effort. We illustrate the properties and potentials of this approach to computational nonlinear controller design for large-scale systems with a thorough numerical study.</p></p class="citation"></blockquote><h3 id=45--320347-an-inexact-infeasible-arc-search-interior-point-method-for-linear-programming-problems-einosuke-iida-et-al-2024>(4/5 | 320/347) An inexact infeasible arc-search interior-point method for linear programming problems (Einosuke Iida et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Einosuke Iida, Makoto Yamashita. (2024)<br><strong>An inexact infeasible arc-search interior-point method for linear programming problems</strong><br><button class=copy-to-clipboard title="An inexact infeasible arc-search interior-point method for linear programming problems" index=320>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-320 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 90C51, 65K05, 90C05, F-2-1; G-1-6, cs-NA, math-NA, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18155v1.pdf filename=2403.18155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inexact interior-point methods (IPMs) are a type of interior-point methods that inexactly solve the linear equation system for obtaining the search direction. On the other hand,arc-search IPMs approximate the central path with an ellipsoidal arc obtained by solving two linear equation systems in each iteration, while conventional line-search IPMs solve one linear system, therefore, the improvement due to the inexact solutions of the linear equation systems can be more beneficial in arc-search IPMs than conventional IPMs. In this paper, we propose an inexact infeasible arc-search interior-point method.We establish that the proposed method is a polynomial-time algorithm through its convergence analysis. The numerical experiments with the conjugate gradient method show that the proposed method can reduce the number of iterations compared to an existing method for <b>benchmark</b> problems; the numbers of iterations are reduced to two-thirds for more than 70% of the problems.</p></p class="citation"></blockquote><h3 id=55--321347-multi-agent-pathfinding-for-noise-restricted-hybrid-fuel-unmanned-aerial-vehicles-drew-scott-et-al-2024>(5/5 | 321/347) Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial Vehicles (Drew Scott et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Drew Scott, Satyanarayana G. Manyam, David W. Casbeer, Manish Kumar, Isaac E. Weintraub. (2024)<br><strong>Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial Vehicles</strong><br><button class=copy-to-clipboard title="Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial Vehicles" index=321>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-321 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-RO, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17849v1.pdf filename=2403.17849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple agents from respective start to goal locations such that no paths conflict. We address the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles which are subject to location-dependent noise restrictions. We solve this problem by searching a constraint tree for which the subproblem at each node is a set of shortest path problems subject to the noise and fuel constraints and conflict zone avoidance. A labeling algorithm is presented to solve this subproblem, including the conflict zones which are treated as dynamic obstacles. We present the experimental results of the algorithms for various <b>graph</b> sizes and number of agents.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--322347-channel-estimation-and-beamforming-for-beyond-diagonal-reconfigurable-intelligent-surfaces-hongyu-li-et-al-2024>(1/1 | 322/347) Channel Estimation and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces (Hongyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyu Li, Shanpu Shen, Yumeng Zhang, Bruno Clerckx. (2024)<br><strong>Channel Estimation and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces</strong><br><button class=copy-to-clipboard title="Channel Estimation and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces" index=322>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-322 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18087v1.pdf filename=2403.18087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a new advance and generalization of the RIS technique. BD-RIS breaks through the isolation between RIS elements by creatively introducing inter-element connections, thereby enabling smarter wave manipulation and enlarging coverage. However, exploring proper channel estimation schemes suitable for BD-RIS aided communication systems still remains an open problem. In this paper, we study channel estimation and beamforming design for BD-RIS aided multi-antenna systems. We first describe the channel estimation strategy based on the least square (LS) method, derive the mean square error (MSE) of the LS estimation, and formulate the joint pilot sequence and BD-RIS design problem with unique constraints induced by BD-RIS architectures. Specifically, we propose an efficient pilot sequence and BD-RIS design which theoretically guarantees to achieve the minimum MSE. With the estimated channel, we then consider two BD-RIS scenarios and propose beamforming design algorithms. Finally, we provide <b>simulation</b> results to verify the effectiveness of the proposed channel estimation scheme and beamforming design algorithms. We also show that more interelement connections in BD-RIS improves the performance while increasing the training overhead for channel estimation.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--323347-r2d2-image-reconstruction-with-model-uncertainty-quantification-in-radio-astronomy-amir-aghabiglou-et-al-2024>(1/1 | 323/347) R2D2 image reconstruction with model uncertainty quantification in radio astronomy (Amir Aghabiglou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Aghabiglou, Chung San Chu, Arwa Dabbech, Yves Wiaux. (2024)<br><strong>R2D2 image reconstruction with model uncertainty quantification in radio astronomy</strong><br><button class=copy-to-clipboard title="R2D2 image reconstruction with model uncertainty quantification in radio astronomy" index=323>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-323 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-LG, eess-IV, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18052v1.pdf filename=2403.18052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <code>Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2) approach was recently introduced for Radio-Interferometric (RI) imaging in astronomy. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs. In this work, we investigate the robustness of the R2D2 image estimation process, by studying the uncertainty associated with its series of learned models. Adopting an ensemble averaging approach, multiple series can be trained, arising from different random DNN initializations of the training process at each iteration. The resulting multiple R2D2 instances can also be leveraged to generate </code>R2D2 samples&rsquo;&rsquo;, from which empirical mean and standard deviation endow the algorithm with a joint estimation and uncertainty quantification functionality. Focusing on RI imaging, and adopting a telescope-specific approach, multiple R2D2 instances were trained to encompass the most general observation setting of the Very Large Array (VLA). <b>Simulations</b> and real-data experiments confirm that: (i) R2D2&rsquo;s image estimation capability is superior to that of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction capability (arising from series with only few DNNs) makes the computation of multiple reconstruction samples and of uncertainty maps practical even at large image dimension; (iii) it is characterized by a very low model uncertainty.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--324347-accelerating-radio-spectrum-regulation-workflows-with-large-language-models-llms-amir-ghasemi-et-al-2024>(1/1 | 324/347) Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs) (Amir Ghasemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Ghasemi, Paul Guinand. (2024)<br><strong>Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)</strong><br><button class=copy-to-clipboard title="Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)" index=324>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-324 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17819v1.pdf filename=2403.17819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications. To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks. In light of these challenges, this paper demonstrates example applications of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to expedite spectrum regulatory processes. We explore various roles that <b>LLMs</b> can play in this context while identifying some of the challenges to address. The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of <b>LLMs</b> in spectrum management.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--325347-prediction-sharing-during-training-and-inference-yotam-gafni-et-al-2024>(1/1 | 325/347) Prediction-sharing During Training and Inference (Yotam Gafni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz. (2024)<br><strong>Prediction-sharing During Training and Inference</strong><br><button class=copy-to-clipboard title="Prediction-sharing During Training and Inference" index=325>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-325 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: cs-AI, cs-GT, cs-LG, cs-MA, econ-TH, econ.TH<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17515v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17515v1.pdf filename=2403.17515v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Two firms are engaged in a competitive prediction task. Each firm has two sources of data &ndash; labeled historical data and unlabeled inference-time data &ndash; and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm&rsquo;s prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the firms has a structural advantage in deducing it. Within these two settings we study optimal contract choice. More specifically, we find the individually rational and Pareto-optimal contracts for some notable cases, and describe specific settings where each of the different sharing contracts emerge as optimal. Finally, in the third level of our analysis we demonstrate the applicability of our concepts in a synthetic <b>simulation</b> using real loan data.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--326347-speaker-distance-estimation-in-enclosures-from-single-channel-audio-michael-neri-et-al-2024>(1/2 | 326/347) Speaker Distance Estimation in Enclosures from Single-Channel Audio (Michael Neri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Neri, Archontis Politis, Daniel Krause, Marco Carli, Tuomas Virtanen. (2024)<br><strong>Speaker Distance Estimation in Enclosures from Single-Channel Audio</strong><br><button class=copy-to-clipboard title="Speaker Distance Estimation in Enclosures from Single-Channel Audio" index=326>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-326 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Convolution, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17514v1.pdf filename=2403.17514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distance estimation from audio plays a crucial role in various applications, such as acoustic scene analysis, sound source localization, and room modeling. Most studies predominantly center on employing a classification approach, where distances are discretized into distinct categories, enabling smoother model training and achieving higher accuracy but imposing restrictions on the precision of the obtained sound source position. Towards this direction, in this paper we propose a novel approach for continuous distance estimation from audio signals using a <b>convolutional</b> <b>recurrent</b> <b>neural</b> <b>network</b> with an attention module. The attention mechanism enables the model to focus on relevant temporal and spectral features, enhancing its ability to capture fine-grained distance-related information. To evaluate the effectiveness of our proposed method, we conduct extensive experiments using audio recordings in controlled environments with three levels of realism (synthetic room impulse response, measured response with convolved speech, and real recordings) on four datasets (our synthetic dataset, QMULTIMIT, VoiceHome-2, and STARSS23). Experimental results show that the model achieves an absolute error of 0.11 meters in a noiseless synthetic scenario. Moreover, the results showed an absolute error of about 1.30 meters in the hybrid scenario. The algorithm&rsquo;s performance in the real scenario, where unpredictable environmental factors and noise are prevalent, yields an absolute error of approximately 0.50 meters. For reproducible research purposes we make model, code, and synthetic datasets available at <a href=https://github.com/michaelneri/audio-distance-estimation>https://github.com/michaelneri/audio-distance-estimation</a>.</p></p class="citation"></blockquote><h3 id=22--327347-infrastructure-less-localization-from-indoor-environmental-sounds-based-on-spectral-decomposition-and-spatial-likelihood-model-satoki-ogiso-et-al-2024>(2/2 | 327/347) Infrastructure-less Localization from Indoor Environmental Sounds Based on Spectral Decomposition and Spatial Likelihood Model (Satoki Ogiso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Satoki Ogiso, Yoshiaki Bando, Takeshi Kurata, Takashi Okuma. (2024)<br><strong>Infrastructure-less Localization from Indoor Environmental Sounds Based on Spectral Decomposition and Spatial Likelihood Model</strong><br><button class=copy-to-clipboard title="Infrastructure-less Localization from Indoor Environmental Sounds Based on Spectral Decomposition and Spatial Likelihood Model" index=327>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-327 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17402v1.pdf filename=2403.17402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human and/or asset tracking using an attached sensor units helps understand their activities. Most common indoor localization methods for human tracking technologies require expensive infrastructures, deployment and maintenance. To overcome this problem, environmental sounds have been used for infrastructure-free localization. While they achieve room-level classification, they suffer from two problems: low signal-to-noise-ratio (SNR) condition and non-uniqueness of sound over the coverage area. A microphone localization method was proposed using <b>supervised</b> spectral decomposition and spatial likelihood to solve these problems. The proposed method was evaluated with actual recordings in an experimental room with a size of 12 x 30 m. The results showed that the proposed method with <b>supervised</b> NMF was robust under low-SNR condition compared to a simple feature (mel frequency cepstrum coefficient: MFCC). Additionally, the proposed method could be easily integrated with prior distribution, which is available from other Bayesian localizations. The proposed method can be used to evaluate the spatial likelihood from environmental sounds.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--328347-the-recessionary-pressures-of-generative-ai-a-threat-to-wellbeing-jo-an-occhipinti-et-al-2024>(1/1 | 328/347) The recessionary pressures of generative AI: A threat to wellbeing (Jo-An Occhipinti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jo-An Occhipinti, Ante Prodan, William Hynes, Roy Green, Sharan Burrow, Harris A Eyre, Adam Skinner, Goran Ujdur, John Buchanan, Ian B Hickie, Mark Heffernan, Christine Song, Marcel Tanner. (2024)<br><strong>The recessionary pressures of generative AI: A threat to wellbeing</strong><br><button class=copy-to-clipboard title="The recessionary pressures of generative AI: A threat to wellbeing" index=328>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-328 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Generative AI, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17405v1.pdf filename=2403.17405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Artificial</b> Intelligence (AI) stands as a transformative force that presents a paradox; it offers unprecedented opportunities for productivity growth while potentially posing significant threats to economic stability and societal wellbeing. Many consider <b>generative</b> <b>AI</b> as akin to previous technological advancements, using historical precedent to argue that fears of widespread job displacement are unfounded, while others contend that <b>generative</b> <b>AI`s</b> unique capacity to undertake non-routine cognitive tasks sets it apart from other forms of automation capital and presents a threat to the quality and availability of work that underpin stable societies. This paper explores the conditions under which both may be true. We posit the existence of an AI-capital-to-labour ratio threshold beyond which a self-reinforcing cycle of recessionary pressures could be triggered, exacerbating social disparities, reducing social cohesion, heightening tensions, and requiring sustained government intervention to maintain stability. To prevent this, the paper underscores the urgent need for proactive policy responses, making <b>recommendations</b> to reduce these risks through robust regulatory frameworks and a new social contract characterised by progressive social and economic policies. This approach aims to ensure a sustainable, inclusive, and resilient economic future where human contribution to the economy is retained and integrated with <b>generative</b> <b>AI</b> to enhance the Mental Wealth of nations.</p></p class="citation"></blockquote><h2 id=statml-2>stat.ML (2)</h2><h3 id=12--329347-an-analysis-of-switchback-designs-in-reinforcement-learning-qianglin-wen-et-al-2024>(1/2 | 329/347) An Analysis of Switchback Designs in Reinforcement Learning (Qianglin Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianglin Wen, Chengchun Shi, Ying Yang, Niansheng Tang, Hongtu Zhu. (2024)<br><strong>An Analysis of Switchback Designs in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="An Analysis of Switchback Designs in Reinforcement Learning" index=329>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-329 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17285v1.pdf filename=2403.17285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper offers a detailed investigation of switchback designs in A/B testing, which alternate between baseline and new policies over time. Our aim is to thoroughly evaluate the effects of these designs on the accuracy of their resulting average treatment effect (ATE) estimators. We propose a novel &ldquo;weak signal analysis&rdquo; framework, which substantially simplifies the calculations of the mean squared errors (MSEs) of these ATEs in <b>Markov</b> <b>decision</b> <b>process</b> environments. Our findings suggest that (i) when the majority of reward errors are positively correlated, the switchback design is more efficient than the alternating-day design which switches policies in a daily basis. Additionally, increasing the frequency of policy switches tends to reduce the MSE of the ATE estimator. (ii) When the errors are uncorrelated, however, all these designs become asymptotically equivalent. (iii) In cases where the majority of errors are negative correlated, the alternating-day design becomes the optimal choice. These insights are crucial, offering guidelines for practitioners on designing experiments in A/B testing. Our analysis accommodates a variety of policy value estimators, including model-based estimators, least squares temporal difference learning estimators, and double <b>reinforcement</b> <b>learning</b> estimators, thereby offering a comprehensive understanding of optimal design strategies for policy evaluation in <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=22--330347-asymptotic-bayes-risk-of-semi-supervised-learning-with-uncertain-labeling-victor-leger-et-al-2024>(2/2 | 330/347) Asymptotic Bayes risk of semi-supervised learning with uncertain labeling (Victor Leger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Leger, Romain Couillet. (2024)<br><strong>Asymptotic Bayes risk of semi-supervised learning with uncertain labeling</strong><br><button class=copy-to-clipboard title="Asymptotic Bayes risk of semi-supervised learning with uncertain labeling" index=330>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-330 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17767v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17767v2.pdf filename=2403.17767v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article considers a <b>semi-supervised</b> <b>classification</b> setting on a Gaussian mixture model, where the data is not labeled strictly as usual, but instead with uncertain labels. Our main aim is to compute the Bayes risk for this model. We compare the behavior of the Bayes risk and the best known algorithm for this model. This comparison eventually gives new insights over the algorithm.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--331347-predicting-perceived-gloss-do-weak-labels-suffice-julia-guerrero-viu-et-al-2024>(1/1 | 331/347) Predicting Perceived Gloss: Do Weak Labels Suffice? (Julia Guerrero-Viu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julia Guerrero-Viu, J. Daniel Subias, Ana Serrano, Katherine R. Storrs, Roland W. Fleming, Belen Masia, Diego Gutierrez. (2024)<br><strong>Predicting Perceived Gloss: Do Weak Labels Suffice?</strong><br><button class=copy-to-clipboard title="Predicting Perceived Gloss: Do Weak Labels Suffice?" index=331>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-331 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-CV, cs-GR, cs.GR<br>Keyword Score: 15<br>Keywords: Geometry, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17672v1.pdf filename=2403.17672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating perceptual attributes of materials directly from images is a challenging task due to their complex, not fully-understood interactions with external factors, such as <b>geometry</b> and lighting. <b>Supervised</b> deep learning models have recently been shown to outperform traditional approaches, but rely on large datasets of human-annotated images for accurate perception predictions. Obtaining reliable annotations is a costly endeavor, aggravated by the limited ability of these models to generalise to different aspects of appearance. In this work, we show how a much smaller set of human annotations (&ldquo;strong labels&rdquo;) can be effectively augmented with automatically derived &ldquo;weak labels&rdquo; in the context of learning a low-dimensional image-computable gloss metric. We evaluate three alternative weak labels for predicting human gloss perception from limited annotated data. Incorporating weak labels enhances our gloss prediction beyond the current state of the art. Moreover, it enables a substantial reduction in human annotation costs without sacrificing accuracy, whether working with rendered images or real photographs.</p></p class="citation"></blockquote><h2 id=cscg-2>cs.CG (2)</h2><h3 id=12--332347-formal-verification-of-the-empty-hexagon-number-bernardo-subercaseaux-et-al-2024>(1/2 | 332/347) Formal Verification of the Empty Hexagon Number (Bernardo Subercaseaux et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernardo Subercaseaux, Wojciech Nawrocki, James Gallicchio, Cayden Codel, Mario Carneiro, Marijn J. H. Heule. (2024)<br><strong>Formal Verification of the Empty Hexagon Number</strong><br><button class=copy-to-clipboard title="Formal Verification of the Empty Hexagon Number" index=332>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-332 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-LO, cs.CG<br>Keyword Score: 15<br>Keywords: Geometry, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17370v1.pdf filename=2403.17370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A recent breakthrough in computer-assisted mathematics showed that every set of $30$ points in the plane in general position (i.e., without three on a common line) contains an empty convex hexagon, thus closing a line of research dating back to the 1930s. Through a combination of geometric insights and automated <b>reasoning</b> techniques, Heule and Scheucher constructed a CNF formula $\phi_n$, with $O(n^4)$ clauses, whose unsatisfiability implies that no set of $n$ points in general position can avoid an empty convex hexagon. An unsatisfiability proof for n = 30 was then found with a SAT solver using 17300 CPU hours of parallel computation, thus implying that the empty hexagon number h(6) is equal to 30. In this paper, we formalize and verify this result in the Lean theorem prover. Our formalization covers discrete computational <b>geometry</b> ideas and SAT encoding techniques that have been successfully applied to similar Erd\H{o}s-Szekeres-type problems. In particular, our framework provides tools to connect standard mathematical objects to propositional assignments, which represents a key step towards the formal verification of other SAT-based mathematical results. Overall, we hope that this work sets a new standard for verification when extensive computation is used for discrete <b>geometry</b> problems, and that it increases the trust the mathematical community has in computer-assisted proofs in this area.</p></p class="citation"></blockquote><h3 id=22--333347-robust-containment-queries-over-collections-of-rational-parametric-curves-via-generalized-winding-numbers-jacob-spainhour-et-al-2024>(2/2 | 333/347) Robust Containment Queries over Collections of Rational Parametric Curves via Generalized Winding Numbers (Jacob Spainhour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Spainhour, David Gunderman, Kenneth Weiss. (2024)<br><strong>Robust Containment Queries over Collections of Rational Parametric Curves via Generalized Winding Numbers</strong><br><button class=copy-to-clipboard title="Robust Containment Queries over Collections of Rational Parametric Curves via Generalized Winding Numbers" index=333>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-333 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: 68U05, cs-CG, cs-GR, cs-NA, cs.CG, math-NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17371v1.pdf filename=2403.17371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point containment queries for regions bound by watertight geometric surfaces, i.e. closed and without self-intersections, can be evaluated straightforwardly with a number of well-studied algorithms. However, when such assumptions on domain <b>geometry</b> are not met, these methods are theoretically unfounded at best and practically unusable at worst. More robust classification schemes utilize generalized winding numbers, a mathematical construction that is indifferent to imperfections in the often human-defined geometric model. We extend this methodology to more general curved shapes, defining a robust containment query for regions whose boundary elements are defined by a collection of rational parametric curves. In doing so, we devise an algorithm that is stable and accurate at arbitrary points in space, circumventing the typical difficulties for queries that are arbitrarily close or coincident with the model. This is done by reducing the generalized winding number problem to an integer winding number problem, which is solved by approximating each curve with a polyline that provably has the same winding number at the point of interest. We demonstrate the improvements in computational complexity granted by this method over conventional techniques, as well as the robustness induced by its application</p></p class="citation"></blockquote><h2 id=csgt-2>cs.GT (2)</h2><h3 id=12--334347-paths-to-equilibrium-in-normal-form-games-bora-yongacoglu-et-al-2024>(1/2 | 334/347) Paths to Equilibrium in Normal-Form Games (Bora Yongacoglu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bora Yongacoglu, Gürdal Arslan, Lacra Pavel, Serdar Yüksel. (2024)<br><strong>Paths to Equilibrium in Normal-Form Games</strong><br><button class=copy-to-clipboard title="Paths to Equilibrium in Normal-Form Games" index=334>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-334 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-GT, cs-LG, cs.GT<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18079v1.pdf filename=2403.18079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In multi-agent <b>reinforcement</b> <b>learning</b> (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in <b>reinforcement</b> <b>learning,</b> where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implications about the capabilities or limitations of a class of MARL algorithms. We answer this question in the affirmative for mixed extensions of finite normal-form games.%</p></p class="citation"></blockquote><h3 id=22--335347-generalizing-better-response-paths-and-weakly-acyclic-games-bora-yongacoglu-et-al-2024>(2/2 | 335/347) Generalizing Better Response Paths and Weakly Acyclic Games (Bora Yongacoglu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bora Yongacoglu, Gürdal Arslan, Lacra Pavel, Serdar Yüksel. (2024)<br><strong>Generalizing Better Response Paths and Weakly Acyclic Games</strong><br><button class=copy-to-clipboard title="Generalizing Better Response Paths and Weakly Acyclic Games" index=335>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-335 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT, econ-TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18086v1.pdf filename=2403.18086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weakly acyclic games generalize potential games and are fundamental to the study of game theoretic control. In this paper, we present a generalization of weakly acyclic games, and we observe its importance in multi-agent learning when agents employ experimental strategy updates in periods where they fail to best respond. While weak acyclicity is defined in terms of path connectivity properties of a game&rsquo;s better response <b>graph,</b> our generalization is defined using a generalized better response <b>graph.</b> We provide sufficient conditions for this notion of generalized weak acyclicity in both two-player games and $n$-player games. To demonstrate that our generalization is not trivial, we provide examples of games admitting a pure Nash equilibrium that are not generalized weakly acyclic. The generalization presented in this work is closely related to the recent theory of satisficing paths, and the counterexamples presented here constitute the first negative results in that theory.</p></p class="citation"></blockquote><h2 id=physicsapp-ph-1>physics.app-ph (1)</h2><h3 id=11--336347-analysis-on-reservoir-activation-with-the-nonlinearity-harnessed-from-solution-processed-mos2-devices-songwei-liu-et-al-2024>(1/1 | 336/347) Analysis on reservoir activation with the nonlinearity harnessed from solution-processed MoS2 devices (Songwei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Songwei Liu, Yang Liu, Yingyi Wen, Jingfang Pei, Pengyu Liu, Lekai Song, Xiaoyue Fan, Wenchen Yang, Danmei Pan, Teng Ma, Yue Lin, Gang Wang, Guohua Hu. (2024)<br><strong>Analysis on reservoir activation with the nonlinearity harnessed from solution-processed MoS2 devices</strong><br><button class=copy-to-clipboard title="Analysis on reservoir activation with the nonlinearity harnessed from solution-processed MoS2 devices" index=336>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-336 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.app-ph<br>Categories: cs-ET, physics-app-ph, physics.app-ph<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17676v1.pdf filename=2403.17676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reservoir computing is a <b>recurrent</b> <b>neural</b> <b>network</b> that has been applied across various domains in machine learning. The implementation of reservoir computing, however, often demands heavy computations for activating the reservoir. Configuring physical reservoir networks and harnessing the nonlinearity from the underlying devices for activation is an emergent solution to address the computational challenge. Herein, we analyze the feasibility of employing the nonlinearity from solution-processed molybdenum disulfide (MoS2) devices for reservoir activation. The devices, fabricated using liquid-phase exfoliated MoS2, exhibit a high-order nonlinearity achieved by Stark modulation of the MoS2 material. We demonstrate that this nonlinearity can be fitted and employed as the activation function to facilitate reservoir computing implementation. Notably, owing to the high-order nonlinearity, the network exhibits long-term synchronization and robust generalization abilities for approximating complex dynamical systems. Given the remarkable reservoir activation capability, coupled with the scalability of the device fabrication, our findings open the possibility for the physical realization of lightweight, efficient reservoir computing for, for instance, signal classification, motion tracking, and pattern recognition of complex time series as well as secure cryptography. As an example, we show the network can be appointed to generate chaotic random numbers for secure data encryption.</p></p class="citation"></blockquote><h2 id=hep-ex-1>hep-ex (1)</h2><h3 id=11--337347-particle-identification-with-machine-learning-from-incomplete-data-in-the-alice-experiment-maja-karwowska-et-al-2024>(1/1 | 337/347) Particle identification with machine learning from incomplete data in the ALICE experiment (Maja Karwowska et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maja Karwowska, Łukasz Graczykowski, Kamil Deja, Miłosz Kasak, Małgorzata Janik. (2024)<br><strong>Particle identification with machine learning from incomplete data in the ALICE experiment</strong><br><button class=copy-to-clipboard title="Particle identification with machine learning from incomplete data in the ALICE experiment" index=337>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-337 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ex<br>Categories: cs-LG, hep-ex, hep-ex<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17436v1.pdf filename=2403.17436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss <b>domain</b> <b>adaptation,</b> the ML technique needed to transfer the knowledge between simulated and real experimental data.</p></p class="citation"></blockquote><h2 id=mathco-2>math.CO (2)</h2><h3 id=12--338347-so-long-sucker-endgame-analysis-jean-lou-de-carufel-et-al-2024>(1/2 | 338/347) So Long Sucker: Endgame Analysis (Jean-Lou De Carufel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean-Lou De Carufel, Marie Rose Jerade. (2024)<br><strong>So Long Sucker: Endgame Analysis</strong><br><button class=copy-to-clipboard title="So Long Sucker: Endgame Analysis" index=338>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-338 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-GT, math-CO, math.CO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17302v1.pdf filename=2403.17302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>So Long Sucker is a strategy board game requiring 4 players, each with $c$ chips of their designated color, and a board made of $k$ empty piles. With a clear set-up come intricate rules, such as: players taking turns but not in a fixed order, agreements between some players being made and broken at any time, and a player winning the game even without any chips in hand. One of the main points of interest in studying this game, is finding when a player has a winning strategy. The game begins with four players that get eliminated successively until the winner is left. To study winning strategies, it is of interest to look at endgame situations. We present the following game set-up: there are two players left in the game, Blue and Red, and only their respective chip colors. In this paper, we characterize Blue&rsquo;s winning situations and strategies through inductive <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=22--339347-a-caro-wei-bound-for-induced-linear-forests-in-graphs-gwenaël-joret-et-al-2024>(2/2 | 339/347) A Caro-Wei bound for induced linear forests in graphs (Gwenaël Joret et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gwenaël Joret, Robin Petit. (2024)<br><strong>A Caro-Wei bound for induced linear forests in graphs</strong><br><button class=copy-to-clipboard title="A Caro-Wei bound for induced linear forests in graphs" index=339>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-339 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17568v1.pdf filename=2403.17568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A well-known result due to Caro (1979) and Wei (1981) states that every <b>graph</b> $G$ has an independent set of size at least $\sum_{v\in V(G)} \frac{1}{d(v) + 1}$, where $d(v)$ denotes the degree of vertex $v$. Alon, Kahn, and Seymour (1987) showed the following generalization: For every $k\geq 0$, every <b>graph</b> $G$ has a $k$-degenerate induced subgraph with at least $\sum_{v \in V(G)}\min{1, \frac {k+1}{d(v)+1}}$ vertices. In particular, for $k=1$, every <b>graph</b> $G$ with no isolated vertices has an induced forest with at least $\sum_{v\in V(G)} \frac{2}{d(v) + 1}$ vertices. Akbari, Amanihamedani, Mousavi, Nikpey, and Sheybani (2019) conjectured that, if $G$ has minimum degree at least $2$, then one can even find an induced linear forest of that order in $G$, that is, a forest where each component is a path. In this paper, we prove this conjecture and show a number of related results. In particular, if there is no restriction on the minimum degree of $G$, we show that there are infinitely many ``best possible&rsquo;&rsquo; functions $f$ such that $\sum_{v\in V(G)} f(d(v))$ is a lower bound on the maximum order of a linear forest in $G$, and we give a full characterization of all such functions $f$.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--340347-learning-to-visually-localize-sound-sources-from-mixtures-without-prior-source-knowledge-dongjin-kim-et-al-2024>(1/1 | 340/347) Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge (Dongjin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongjin Kim, Sung Jin Um, Sangmin Lee, Jung Uk Kim. (2024)<br><strong>Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge</strong><br><button class=copy-to-clipboard title="Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge" index=340>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-340 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-CV, cs-MM, cs-SD, cs.MM, eess-AS<br>Keyword Score: 6<br>Keywords: Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17420v1.pdf filename=2403.17420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The goal of the multi-sound source localization task is to localize sound sources from the mixture individually. While recent multi-sound source localization methods have shown improved performance, they face challenges due to their reliance on prior information about the number of objects to be separated. In this paper, to overcome this limitation, we present a novel multi-sound source localization method that can perform localization without prior knowledge of the number of sound sources. To achieve this goal, we propose an iterative object identification (IOI) module, which can recognize sound-making objects in an iterative manner. After finding the regions of sound-making objects, we devise object similarity-aware <b>clustering</b> (OSC) loss to guide the IOI module to effectively combine regions of the same object but also distinguish between different objects and backgrounds. It enables our method to perform accurate localization of sound-making objects without any prior knowledge. Extensive experimental results on the MUSIC and VGGSound <b>benchmarks</b> show the significant performance improvements of the proposed method over the existing methods for both single and multi-source. Our code is available at: <a href=https://github.com/VisualAIKHU/NoPrior_MultiSSL>https://github.com/VisualAIKHU/NoPrior_MultiSSL</a></p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--341347-online-submodular-welfare-maximization-meets-post-allocation-stochasticity-and-reusability-rajan-udwani-2024>(1/2 | 341/347) Online Submodular Welfare Maximization Meets Post-Allocation Stochasticity and Reusability (Rajan Udwani, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rajan Udwani. (2024)<br><strong>Online Submodular Welfare Maximization Meets Post-Allocation Stochasticity and Reusability</strong><br><button class=copy-to-clipboard title="Online Submodular Welfare Maximization Meets Post-Allocation Stochasticity and Reusability" index=341>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-341 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18059v1.pdf filename=2403.18059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We generalize the problem of online submodular welfare maximization to incorporate a variety of new elements arising from reusability, stochastic rewards, combinatorial actions and similar features that have received significant attention in recent years. For our general formulation, we show that a non-adaptive Greedy algorithm achieves the highest possible competitive ratio against an adaptive offline <b>benchmark</b> in the adversarial arrival model and in the unknown IID stochastic arrival model. In addition to generalizing several previous results, this shows that, in general, adaptivity to stochastic rewards (and similar features) offers no theoretical (worst-case) benefits.</p></p class="citation"></blockquote><h3 id=22--342347-generalising-the-maximum-independent-set-algorithm-via-boolean-networks-maximilien-gadouleau-et-al-2024>(2/2 | 342/347) Generalising the maximum independent set algorithm via Boolean networks (Maximilien Gadouleau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilien Gadouleau, David C. Kutner. (2024)<br><strong>Generalising the maximum independent set algorithm via Boolean networks</strong><br><button class=copy-to-clipboard title="Generalising the maximum independent set algorithm via Boolean networks" index=342>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-342 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17658v1.pdf filename=2403.17658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A simple greedy algorithm to find a maximal independent set (MIS) in a <b>graph</b> starts with the empty set and visits every vertex, adding it to the set if and only if none of its neighbours are already in the set. In this paper, we consider the generalisation of this MIS algorithm by letting it start with any set of vertices and we prove the hardness of many decision problems related to this generalisation. Our results are based on two main strategies. Firstly, we view the MIS algorithm as a sequential update of a Boolean network, which we refer to as the MIS network, according to a permutation of the vertex set. The set of fixed points of the MIS network corresponds to the set of MIS of the <b>graph.</b> Our generalisation then consists in starting from any configuration and following a sequential update given by a word of vertices. Secondly, we introduce the concept of a colony of a <b>graph,</b> that is a set of vertices that is dominated by an independent set. Deciding whether a set of vertices is a colony is NP-complete; decision problems related to the MIS algorithm will be reduced from the Colony problem. We first show that deciding whether a configuration can reach all maximal independent sets is coNP-complete. Second, we consider so-called fixing words, that allow to reach a MIS for any initial configuration, and fixing permutations, which we call permises; deciding whether a permutation is fixing is coNP-complete. Third, we show that deciding whether a <b>graph</b> has a permis is coNP-hard. Finally, we generalise the MIS algorithm to digraphs. The algorithm then uses the so-called kernel network, whose fixed points are the kernels of the digraph. Deciding whether the kernel network of a given digraph is fixable is coNP-hard, even for digraphs that have a kernel. Alternatively, we introduce two fixable Boolean networks whose sets of fixed points contain all kernels.</p></p class="citation"></blockquote><h2 id=mathst-2>math.ST (2)</h2><h3 id=12--343347-counting-stars-is-constant-degree-optimal-for-detecting-any-planted-subgraph-xifan-yu-et-al-2024>(1/2 | 343/347) Counting Stars is Constant-Degree Optimal For Detecting Any Planted Subgraph (Xifan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xifan Yu, Ilias Zadik, Peiyuan Zhang. (2024)<br><strong>Counting Stars is Constant-Degree Optimal For Detecting Any Planted Subgraph</strong><br><button class=copy-to-clipboard title="Counting Stars is Constant-Degree Optimal For Detecting Any Planted Subgraph" index=343>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-343 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-CC, cs-DS, math-ST, math.ST, stat-TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17766v1.pdf filename=2403.17766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the computational limits of the following general hypothesis testing problem. Let H=H_n be an \emph{arbitrary} undirected <b>graph</b> on n vertices. We study the detection task between a <code>null'' Erd\H{o}s-R\'{e}nyi random &lt;b>graph&lt;/b> G(n,p) and a </code>planted&rsquo;&rsquo; random <b>graph</b> which is the union of G(n,p) together with a random copy of H=H_n. Our notion of planted model is a generalization of a plethora of recently studied models initiated with the study of the planted clique model (Jerrum 1992), which corresponds to the special case where H is a k-clique and p=1/2. Over the last decade, several papers have studied the power of low-degree polynomials for limited choices of H&rsquo;s in the above task. In this work, we adopt a unifying perspective and characterize the power of \emph{constant degree} polynomials for the detection task, when \emph{H=H_n is any arbitrary graph} and for \emph{any p=\Omega(1).} Perhaps surprisingly, we prove that the optimal constant degree polynomial is always given by simply \emph{counting stars} in the input random <b>graph.</b> As a direct corollary, we conclude that the class of constant-degree polynomials is only able to ``sense&rsquo;&rsquo; the degree distribution of the planted <b>graph</b> H, and no other <b>graph</b> theoretic property of it.</p></p class="citation"></blockquote><h3 id=22--344347-geometric-planted-matchings-beyond-the-gaussian-model-lucas-da-rocha-schwengber-et-al-2024>(2/2 | 344/347) Geometric planted matchings beyond the Gaussian model (Lucas da Rocha Schwengber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas da Rocha Schwengber, Roberto Imbuzeiro Oliveira. (2024)<br><strong>Geometric planted matchings beyond the Gaussian model</strong><br><button class=copy-to-clipboard title="Geometric planted matchings beyond the Gaussian model" index=344>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-344 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-DB, cs-DM, math-CO, math-ST, math.ST, stat-TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17469v1.pdf filename=2403.17469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of recovering an unknown matching between a set of $n$ randomly placed points in $\mathbb{R}^d$ and random perturbations of these points. This can be seen as a model for particle tracking and more generally, entity resolution. We use matchings in random geometric <b>graphs</b> to derive minimax lower bounds for this problem that hold under great generality. Using these results we show that for a broad class of distributions, the order of the number of mistakes made by an estimator that minimizes the sum of squared Euclidean distances is minimax optimal when $d$ is fixed and is optimal up to $n^{o(1)}$ factors when $d = o(\log n)$. In the high-dimensional regime we consider a setup where both initial positions and perturbations have independent sub-Gaussian coordinates. In this setup we give sufficient conditions under which the same estimator makes no mistakes with high probability. We prove an analogous result for an adapted version of this estimator that incorporates information on the covariance matrix of the perturbations.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-2>physics.soc-ph (2)</h2><h3 id=12--345347-distance-based-hierarchical-cutting-of-complex-networks-with-non-preferential-and-preferential-choice-of-seeds-alexandre-benatti-et-al-2024>(1/2 | 345/347) Distance-Based Hierarchical Cutting of Complex Networks with Non-Preferential and Preferential Choice of Seeds (Alexandre Benatti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Benatti, Luciano da F. Costa. (2024)<br><strong>Distance-Based Hierarchical Cutting of Complex Networks with Non-Preferential and Preferential Choice of Seeds</strong><br><button class=copy-to-clipboard title="Distance-Based Hierarchical Cutting of Complex Networks with Non-Preferential and Preferential Choice of Seeds" index=345>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-345 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-SI, physics-soc-ph, physics.soc-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17713v1.pdf filename=2403.17713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graphs</b> and complex networks can be successively separated into connected components associated to respective seed nodes, therefore establishing a respective hierarchical organization. In the present work, we study the properties of the hierarchical structure implied by distance-based cutting of Erd\H{o}s-R'enyi, Barab'asi-Albert, and a specific geometric network. Two main situations are considered regarding the choice of the seeds: non-preferential and preferential to the respective node degree. Among the obtained findings, we have the tendency of geometrical networks yielding more balanced pairs of connected components along the network progressive separation, presenting little chaining effects, followed by the Erd\H{o}s-R'enyi and Barab'asi-Albert types of networks. The choice of seeds preferential to the node degree tended to enhance the balance of the connected components in the case of the geometrical networks.</p></p class="citation"></blockquote><h3 id=22--346347-ω_1-ω_2-temporal-random-hyperbolic-graphs-sofoclis-zambirinis-et-al-2024>(2/2 | 346/347) $(ω_1, ω_2)$-Temporal random hyperbolic graphs (Sofoclis Zambirinis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sofoclis Zambirinis, Fragkiskos Papadopoulos. (2024)<br><strong>$(ω_1, ω_2)$-Temporal random hyperbolic graphs</strong><br><button class=copy-to-clipboard title="$(ω_1, ω_2)$-Temporal random hyperbolic graphs" index=346>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-346 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cond-mat-stat-mech, cs-SI, physics-soc-ph, physics.soc-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17440v1.pdf filename=2403.17440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We extend a recent model of temporal random hyperbolic <b>graphs</b> by allowing connections and disconnections to persist across network snapshots with different probabilities, $\omega_1$ and $\omega_2$. This extension, while conceptually simple, poses analytical challenges involving the Appell $F_1$ series. Despite these challenges, we are able to analyze key properties of the model, which include the distributions of contact and intercontact durations, as well as the expected time-aggregated degree. The incorporation of $\omega_1$ and $\omega_2$ enables more flexible tuning of the average contact and intercontact durations, and of the average time-aggregated degree, providing a finer control for exploring the effect of temporal network dynamics on epidemic processes. Overall, our results provide new insights into the analysis of temporal networks and contribute to a more general representation of real-world scenarios.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--347347-piecewise-linear-expectation-analysis-via-k-induction-for-probabilistic-programs-tengshun-yang-et-al-2024>(1/1 | 347/347) Piecewise Linear Expectation Analysis via $k$-Induction for Probabilistic Programs (Tengshun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tengshun Yang, Hongfei Fu, Jingyu Ke, Naijun Zhan, Shiyang Wu. (2024)<br><strong>Piecewise Linear Expectation Analysis via $k$-Induction for Probabilistic Programs</strong><br><button class=copy-to-clipboard title="Piecewise Linear Expectation Analysis via $k$-Induction for Probabilistic Programs" index=347>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-347 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17567v1.pdf filename=2403.17567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantitative analysis of probabilistic programs aims at deriving tight numerical bounds for probabilistic properties such as expectation and assertion probability, and plays a crucial role in the verification of probabilistic programs. Along this line of research, most existing works consider numerical bounds over the whole state space monolithically and do not consider piecewise bounds. Clearly, monolithic bounds are either conservative, or not expressive and succinct enough in general. To derive more succinct, expressive and precise numerical bounds for probabilistic properties, we propose a novel approach for synthesizing piecewise linear bounds in this work. To this end, we first show how to extract a piecewise feature w.r.t. a given quantitative property from a probabilistic program using latticed $k$-induction that captures a wide and representative class of piecewise bound functions. Second, we develop an algorithmic approach to synthesize piecewise linear upper and lower bounds from the piecewise feature, for which we show that the synthesis of piecewise linear bounds can be reduced to bilinear programming. Third, we implement our approach with the bilinear programming solver Gurobi. The experimental results indicate that our approach is capable of generating tight or even accurate piecewise linear bounds for an extensive set of <b>benchmarks</b> compared with the state of the art.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.27</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.29</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-64>cs.CL (64)</a><ul><li><a href=#164--1347-ellen-extremely-lightly-supervised-learning-for-efficient-named-entity-recognition-haris-riaz-et-al-2024>(1/64 | 1/347) ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition (Haris Riaz et al., 2024)</a></li><li><a href=#264--2347-internlm2-technical-report-zheng-cai-et-al-2024>(2/64 | 2/347) InternLM2 Technical Report (Zheng Cai et al., 2024)</a></li><li><a href=#364--3347-large-language-models-are-state-of-the-art-evaluator-for-grammatical-error-correction-masamune-kobayashi-et-al-2024>(3/64 | 3/347) Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction (Masamune Kobayashi et al., 2024)</a></li><li><a href=#464--4347-illuminer-instruction-tuned-large-language-models-as-few-shot-intent-classifier-and-slot-filler-paramita-mirza-et-al-2024>(4/64 | 4/347) ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler (Paramita Mirza et al., 2024)</a></li><li><a href=#564--5347-verbing-weirds-language-models-evaluation-of-english-zero-derivation-in-five-llms-david-r-mortensen-et-al-2024>(5/64 | 5/347) Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs (David R. Mortensen et al., 2024)</a></li><li><a href=#664--6347-enhancing-legal-document-retrieval-a-multi-phase-approach-with-large-language-models-hai-long-nguyen-et-al-2024>(6/64 | 6/347) Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models (Hai-Long Nguyen et al., 2024)</a></li><li><a href=#764--7347-language-models-for-text-classification-is-in-context-learning-enough-aleksandra-edwards-et-al-2024>(7/64 | 7/347) Language Models for Text Classification: Is In-Context Learning Enough? (Aleksandra Edwards et al., 2024)</a></li><li><a href=#864--8347-extracting-biomedical-entities-from-noisy-audio-transcripts-nima-ebadi-et-al-2024>(8/64 | 8/347) Extracting Biomedical Entities from Noisy Audio Transcripts (Nima Ebadi et al., 2024)</a></li><li><a href=#964--9347-large-language-models-as-financial-data-annotators-a-study-on-effectiveness-and-efficiency-toyin-aguda-et-al-2024>(9/64 | 9/347) Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency (Toyin Aguda et al., 2024)</a></li><li><a href=#1064--10347-naive-bayes-based-context-extension-for-large-language-models-jianlin-su-et-al-2024>(10/64 | 10/347) Naive Bayes-based Context Extension for Large Language Models (Jianlin Su et al., 2024)</a></li><li><a href=#1164--11347-pctoolkit-a-unified-plug-and-play-prompt-compression-toolkit-of-large-language-models-jinyi-li-et-al-2024>(11/64 | 11/347) PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models (Jinyi Li et al., 2024)</a></li><li><a href=#1264--12347-supervisory-prompt-training-jean-ghislain-billa-et-al-2024>(12/64 | 12/347) Supervisory Prompt Training (Jean Ghislain Billa et al., 2024)</a></li><li><a href=#1364--13347-large-language-models-produce-responses-perceived-to-be-empathic-yoon-kyung-lee-et-al-2024>(13/64 | 13/347) Large Language Models Produce Responses Perceived to be Empathic (Yoon Kyung Lee et al., 2024)</a></li><li><a href=#1464--14347-constructions-are-so-difficult-that-even-large-language-models-get-them-right-for-the-wrong-reasons-shijia-zhou-et-al-2024>(14/64 | 14/347) Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons (Shijia Zhou et al., 2024)</a></li><li><a href=#1564--15347-dancer-entity-description-augmented-named-entity-corrector-for-automatic-speech-recognition-yi-cheng-wang-et-al-2024>(15/64 | 15/347) DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition (Yi-Cheng Wang et al., 2024)</a></li><li><a href=#1664--16347-kdmcse-knowledge-distillation-multimodal-sentence-embeddings-with-adaptive-angular-margin-contrastive-learning-cong-duy-nguyen-et-al-2024>(16/64 | 16/347) KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning (Cong-Duy Nguyen et al., 2024)</a></li><li><a href=#1764--17347-chain-of-action-faithful-and-multimodal-question-answering-through-large-language-models-zhenyu-pan-et-al-2024>(17/64 | 17/347) Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models (Zhenyu Pan et al., 2024)</a></li><li><a href=#1864--18347-gpts-and-language-barrier-a-cross-lingual-legal-qa-examination-ha-thanh-nguyen-et-al-2024>(18/64 | 18/347) GPTs and Language Barrier: A Cross-Lingual Legal QA Examination (Ha-Thanh Nguyen et al., 2024)</a></li><li><a href=#1964--19347-m3p-towards-multimodal-multilingual-translation-with-multimodal-prompt-jian-yang-et-al-2024>(19/64 | 19/347) m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt (Jian Yang et al., 2024)</a></li><li><a href=#2064--20347-the-unreasonable-ineffectiveness-of-the-deeper-layers-andrey-gromov-et-al-2024>(20/64 | 20/347) The Unreasonable Ineffectiveness of the Deeper Layers (Andrey Gromov et al., 2024)</a></li><li><a href=#2164--21347-decoding-probing-revealing-internal-linguistic-structures-in-neural-language-models-using-minimal-pairs-linyang-he-et-al-2024>(21/64 | 21/347) Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models using Minimal Pairs (Linyang He et al., 2024)</a></li><li><a href=#2264--22347-denoising-table-text-retrieval-for-open-domain-question-answering-deokhyung-kang-et-al-2024>(22/64 | 22/347) Denoising Table-Text Retrieval for Open-Domain Question Answering (Deokhyung Kang et al., 2024)</a></li><li><a href=#2364--23347-chatgpt-rates-natural-language-explanation-quality-like-humans-but-on-which-scales-fan-huang-et-al-2024>(23/64 | 23/347) ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales? (Fan Huang et al., 2024)</a></li><li><a href=#2464--24347-arabicaqa-a-comprehensive-dataset-for-arabic-question-answering-abdelrahman-abdallah-et-al-2024>(24/64 | 24/347) ArabicaQA: A Comprehensive Dataset for Arabic Question Answering (Abdelrahman Abdallah et al., 2024)</a></li><li><a href=#2564--25347-hill-hierarchy-aware-information-lossless-contrastive-learning-for-hierarchical-text-classification-he-zhu-et-al-2024>(25/64 | 25/347) HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification (He Zhu et al., 2024)</a></li><li><a href=#2664--26347-coig-cqia-quality-is-all-you-need-for-chinese-instruction-fine-tuning-yuelin-bai-et-al-2024>(26/64 | 26/347) COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning (Yuelin Bai et al., 2024)</a></li><li><a href=#2764--27347-chroniclingamericaqa-a-large-scale-question-answering-dataset-based-on-historical-american-newspaper-pages-bhawna-piryani-et-al-2024>(27/64 | 27/347) ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages (Bhawna Piryani et al., 2024)</a></li><li><a href=#2864--28347-can-multiple-choice-questions-really-be-useful-in-detecting-the-abilities-of-llms-wangyue-li-et-al-2024>(28/64 | 28/347) Can multiple-choice questions really be useful in detecting the abilities of LLMs? (Wangyue Li et al., 2024)</a></li><li><a href=#2964--29347-intrinsic-subgraph-generation-for-interpretable-graph-based-visual-question-answering-pascal-tilli-et-al-2024>(29/64 | 29/347) Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering (Pascal Tilli et al., 2024)</a></li><li><a href=#3064--30347-dgot-dynamic-graph-of-thoughts-for-scientific-abstract-generation-xinyu-ning-et-al-2024>(30/64 | 30/347) DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation (Xinyu Ning et al., 2024)</a></li><li><a href=#3164--31347-jmultiwoz-a-large-scale-japanese-multi-domain-task-oriented-dialogue-dataset-atsumoto-ohashi-et-al-2024>(31/64 | 31/347) JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset (Atsumoto Ohashi et al., 2024)</a></li><li><a href=#3264--32347-improving-pre-trained-language-model-sensitivity-via-mask-specific-losses-a-case-study-on-biomedical-ner-micheal-abaho-et-al-2024>(32/64 | 32/347) Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER (Micheal Abaho et al., 2024)</a></li><li><a href=#3364--33347-dore-a-dataset-for-portuguese-definition-generation-anna-beatriz-dimas-furtado-et-al-2024>(33/64 | 33/347) DORE: A Dataset For Portuguese Definition Generation (Anna Beatriz Dimas Furtado et al., 2024)</a></li><li><a href=#3464--34347-exploring-llms-as-a-source-of-targeted-synthetic-textual-data-to-minimize-high-confidence-misclassifications-philip-lippmann-et-al-2024>(34/64 | 34/347) Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications (Philip Lippmann et al., 2024)</a></li><li><a href=#3564--35347-using-domain-knowledge-to-guide-dialog-structure-induction-via-neural-probabilistic-soft-logic-connor-pryor-et-al-2024>(35/64 | 35/347) Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic (Connor Pryor et al., 2024)</a></li><li><a href=#3664--36347-enhanced-short-text-modeling-leveraging-large-language-models-for-topic-refinement-shuyu-chang-et-al-2024>(36/64 | 36/347) Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement (Shuyu Chang et al., 2024)</a></li><li><a href=#3764--37347-multilingual-sentence-t5-scalable-sentence-encoders-for-multilingual-applications-chihiro-yano-et-al-2024>(37/64 | 37/347) Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications (Chihiro Yano et al., 2024)</a></li><li><a href=#3864--38347-robust-and-scalable-model-editing-for-large-language-models-yingfa-chen-et-al-2024>(38/64 | 38/347) Robust and Scalable Model Editing for Large Language Models (Yingfa Chen et al., 2024)</a></li><li><a href=#3964--39347-lm-combiner-a-contextual-rewriting-model-for-chinese-grammatical-error-correction-yixuan-wang-et-al-2024>(39/64 | 39/347) LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction (Yixuan Wang et al., 2024)</a></li><li><a href=#4064--40347-large-language-models-for-education-a-survey-and-outlook-shen-wang-et-al-2024>(40/64 | 40/347) Large Language Models for Education: A Survey and Outlook (Shen Wang et al., 2024)</a></li><li><a href=#4164--41347-the-impact-of-syntactic-and-semantic-proximity-on-machine-translation-with-back-translation-nicolas-guerin-et-al-2024>(41/64 | 41/347) The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation (Nicolas Guerin et al., 2024)</a></li><li><a href=#4264--42347-continual-few-shot-event-detection-via-hierarchical-augmentation-networks-chenlong-zhang-et-al-2024>(42/64 | 42/347) Continual Few-shot Event Detection via Hierarchical Augmentation Networks (Chenlong Zhang et al., 2024)</a></li><li><a href=#4364--43347-mix-initiative-response-generation-with-dynamic-prefix-tuning-yuxiang-nie-et-al-2024>(43/64 | 43/347) Mix-Initiative Response Generation with Dynamic Prefix Tuning (Yuxiang Nie et al., 2024)</a></li><li><a href=#4464--44347-you-are-an-expert-annotator-automatic-best-worst-scaling-annotations-for-emotion-intensity-modeling-christopher-bagdon-et-al-2024>(44/64 | 44/347) &lsquo;You are an expert annotator&rsquo;: Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling (Christopher Bagdon et al., 2024)</a></li><li><a href=#4564--45347-decoding-excellence-mapping-the-demand-for-psychological-traits-of-operations-and-supply-chain-professionals-through-text-mining-s-di-luozzo-et-al-2024>(45/64 | 45/347) Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining (S. Di Luozzo et al., 2024)</a></li><li><a href=#4664--46347-a-gaze-grounded-visual-question-answering-dataset-for-clarifying-ambiguous-japanese-questions-shun-inadumi-et-al-2024>(46/64 | 46/347) A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions (Shun Inadumi et al., 2024)</a></li><li><a href=#4764--47347-mapguide-a-simple-yet-effective-method-to-reconstruct-continuous-language-from-brain-activities-xinpei-zhao-et-al-2024>(47/64 | 47/347) MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities (Xinpei Zhao et al., 2024)</a></li><li><a href=#4864--48347-automate-knowledge-concept-tagging-on-math-questions-with-llms-hang-li-et-al-2024>(48/64 | 48/347) Automate Knowledge Concept Tagging on Math Questions with LLMs (Hang Li et al., 2024)</a></li><li><a href=#4964--49347-neural-multimodal-topic-modeling-a-comprehensive-evaluation-felipe-gonzález-pizarro-et-al-2024>(49/64 | 49/347) Neural Multimodal Topic Modeling: A Comprehensive Evaluation (Felipe González-Pizarro et al., 2024)</a></li><li><a href=#5064--50347-juru-legal-brazilian-large-language-model-from-reputable-sources-roseval-malaquias-junior-et-al-2024>(50/64 | 50/347) Juru: Legal Brazilian Large Language Model from Reputable Sources (Roseval Malaquias Junior et al., 2024)</a></li><li><a href=#5164--51347-for-those-who-dont-know-how-to-ask-building-a-dataset-of-technology-questions-for-digital-newcomers-evan-lucas-et-al-2024>(51/64 | 51/347) For those who don&rsquo;t know (how) to ask: Building a dataset of technology questions for digital newcomers (Evan Lucas et al., 2024)</a></li><li><a href=#5264--52347-chatgpt-role-play-dataset-analysis-of-user-motives-and-model-naturalness-yufei-tao-et-al-2024>(52/64 | 52/347) ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness (Yufei Tao et al., 2024)</a></li><li><a href=#5364--53347-towards-a-zero-data-controllable-adaptive-dialog-system-dirk-väth-et-al-2024>(53/64 | 53/347) Towards a Zero-Data, Controllable, Adaptive Dialog System (Dirk Väth et al., 2024)</a></li><li><a href=#5464--54347-rubia-a-russian-language-bias-detection-dataset-veronika-grigoreva-et-al-2024>(54/64 | 54/347) RuBia: A Russian Language Bias Detection Dataset (Veronika Grigoreva et al., 2024)</a></li><li><a href=#5564--55347-transcribing-bengali-text-with-regional-dialects-to-ipa-using-district-guided-tokens-s-m-jishanul-islam-et-al-2024>(55/64 | 55/347) Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens (S M Jishanul Islam et al., 2024)</a></li><li><a href=#5664--56347-bridging-textual-and-tabular-worlds-for-fact-verification-a-lightweight-attention-based-model-shirin-dabbaghi-varnosfaderani-et-al-2024>(56/64 | 56/347) Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model (Shirin Dabbaghi Varnosfaderani et al., 2024)</a></li><li><a href=#5764--57347-project-mosla-recording-every-moment-of-second-language-acquisition-masato-hagiwara-et-al-2024>(57/64 | 57/347) Project MOSLA: Recording Every Moment of Second Language Acquisition (Masato Hagiwara et al., 2024)</a></li><li><a href=#5864--58347-enriching-word-usage-graphs-with-cluster-definitions-mariia-fedorova-et-al-2024>(58/64 | 58/347) Enriching Word Usage Graphs with Cluster Definitions (Mariia Fedorova et al., 2024)</a></li><li><a href=#5964--59347-scinews-from-scholarly-complexities-to-public-narratives----a-dataset-for-scientific-news-report-generation-dongqi-pu-et-al-2024>(59/64 | 59/347) SciNews: From Scholarly Complexities to Public Narratives &ndash; A Dataset for Scientific News Report Generation (Dongqi Pu et al., 2024)</a></li><li><a href=#6064--60347-task-oriented-paraphrase-analytics-marcel-gohsen-et-al-2024>(60/64 | 60/347) Task-Oriented Paraphrase Analytics (Marcel Gohsen et al., 2024)</a></li><li><a href=#6164--61347-sparse-logistic-regression-with-high-order-features-for-automatic-grammar-rule-extraction-from-treebanks-santiago-herrera-et-al-2024>(61/64 | 61/347) Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks (Santiago Herrera et al., 2024)</a></li><li><a href=#6264--62347-sharing-the-cost-of-success-a-game-for-evaluating-and-learning-collaborative-multi-agent-instruction-giving-and-following-policies-philipp-sadler-et-al-2024>(62/64 | 62/347) Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies (Philipp Sadler et al., 2024)</a></li><li><a href=#6364--63347-common-ground-tracking-in-multimodal-dialogue-ibrahim-khebour-et-al-2024>(63/64 | 63/347) Common Ground Tracking in Multimodal Dialogue (Ibrahim Khebour et al., 2024)</a></li><li><a href=#6464--64347-graph-language-model-glm-a-new-graph-based-approach-to-detect-social-instabilities-wallyson-lemes-de-oliveira-et-al-2024>(64/64 | 64/347) Graph Language Model (GLM): A new graph-based approach to detect social instabilities (Wallyson Lemes de Oliveira et al., 2024)</a></li></ul></li><li><a href=#cslg-49>cs.LG (49)</a><ul><li><a href=#149--65347-oh-we-freeze-improving-quantized-knowledge-distillation-via-signal-propagation-analysis-for-large-language-models-kartikeya-bhardwaj-et-al-2024>(1/49 | 65/347) Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models (Kartikeya Bhardwaj et al., 2024)</a></li><li><a href=#249--66347-variational-graph-auto-encoder-based-inductive-learning-method-for-semi-supervised-classification-hanxuan-yang-et-al-2024>(2/49 | 66/347) Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification (Hanxuan Yang et al., 2024)</a></li><li><a href=#349--67347-chain-of-compression-a-systematic-approach-to-combinationally-compress-convolutional-neural-networks-yingtao-shen-et-al-2024>(3/49 | 67/347) Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks (Yingtao Shen et al., 2024)</a></li><li><a href=#449--68347-targeted-visualization-of-the-backbone-of-encoder-llms-isaac-roberts-et-al-2024>(4/49 | 68/347) Targeted Visualization of the Backbone of Encoder LLMs (Isaac Roberts et al., 2024)</a></li><li><a href=#549--69347-the-need-for-speed-pruning-transformers-with-one-recipe-samir-khaki-et-al-2024>(5/49 | 69/347) The Need for Speed: Pruning Transformers with One Recipe (Samir Khaki et al., 2024)</a></li><li><a href=#649--70347-lisa-layerwise-importance-sampling-for-memory-efficient-large-language-model-fine-tuning-rui-pan-et-al-2024>(6/49 | 70/347) LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning (Rui Pan et al., 2024)</a></li><li><a href=#749--71347-mechanistic-design-and-scaling-of-hybrid-architectures-michael-poli-et-al-2024>(7/49 | 71/347) Mechanistic Design and Scaling of Hybrid Architectures (Michael Poli et al., 2024)</a></li><li><a href=#849--72347-leave-no-patient-behind-enhancing-medication-recommendation-for-rare-disease-patients-zihao-zhao-et-al-2024>(8/49 | 72/347) Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients (Zihao Zhao et al., 2024)</a></li><li><a href=#949--73347-el-mlffs-ensemble-learning-of-machine-leaning-force-fields-bangchen-yin-et-al-2024>(9/49 | 73/347) EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields (Bangchen Yin et al., 2024)</a></li><li><a href=#1049--74347-learn-from-heterophily-heterophilous-information-enhanced-graph-neural-network-yilun-zheng-et-al-2024>(10/49 | 74/347) Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network (Yilun Zheng et al., 2024)</a></li><li><a href=#1149--75347-are-compressed-language-models-less-subgroup-robust-leonidas-gee-et-al-2024>(11/49 | 75/347) Are Compressed Language Models Less Subgroup Robust? (Leonidas Gee et al., 2024)</a></li><li><a href=#1249--76347-masked-autoencoders-are-pde-learners-anthony-zhou-et-al-2024>(12/49 | 76/347) Masked Autoencoders are PDE Learners (Anthony Zhou et al., 2024)</a></li><li><a href=#1349--77347-peersimgym-an-environment-for-solving-the-task-offloading-problem-with-reinforcement-learning-frederico-metelo-et-al-2024>(13/49 | 77/347) PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning (Frederico Metelo et al., 2024)</a></li><li><a href=#1449--78347-on-the-benefits-of-over-parameterization-for-out-of-distribution-generalization-yifan-hao-et-al-2024>(14/49 | 78/347) On the Benefits of Over-parameterization for Out-of-Distribution Generalization (Yifan Hao et al., 2024)</a></li><li><a href=#1549--79347-bvr-gym-a-reinforcement-learning-environment-for-beyond-visual-range-air-combat-edvards-scukins-et-al-2024>(15/49 | 79/347) BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat (Edvards Scukins et al., 2024)</a></li><li><a href=#1649--80347-securing-gnns-explanation-based-identification-of-backdoored-training-graphs-jane-downer-et-al-2024>(16/49 | 80/347) Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs (Jane Downer et al., 2024)</a></li><li><a href=#1749--81347-bidirectional-consistency-models-liangchen-li-et-al-2024>(17/49 | 81/347) Bidirectional Consistency Models (Liangchen Li et al., 2024)</a></li><li><a href=#1849--82347-herta-a-high-efficiency-and-rigorous-training-algorithm-for-unfolded-graph-neural-networks-yongyi-yang-et-al-2024>(18/49 | 82/347) HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks (Yongyi Yang et al., 2024)</a></li><li><a href=#1949--83347-healthgat-node-classifications-in-electronic-health-records-using-graph-attention-networks-fahmida-liza-piya-et-al-2024>(19/49 | 83/347) HealthGAT: Node Classifications in Electronic Health Records using Graph Attention Networks (Fahmida Liza Piya et al., 2024)</a></li><li><a href=#2049--84347-learning-the-optimal-power-flow-environment-design-matters-thomas-wolgast-et-al-2024>(20/49 | 84/347) Learning the Optimal Power Flow: Environment Design Matters (Thomas Wolgast et al., 2024)</a></li><li><a href=#2149--85347-uncertainty-aware-distributional-offline-reinforcement-learning-xiaocong-chen-et-al-2024>(21/49 | 85/347) Uncertainty-aware Distributional Offline Reinforcement Learning (Xiaocong Chen et al., 2024)</a></li><li><a href=#2249--86347-tutorial-on-diffusion-models-for-imaging-and-vision-stanley-h-chan-2024>(22/49 | 86/347) Tutorial on Diffusion Models for Imaging and Vision (Stanley H. Chan, 2024)</a></li><li><a href=#2349--87347-counterfactual-fairness-through-transforming-data-orthogonal-to-bias-shuyi-chen-et-al-2024>(23/49 | 87/347) Counterfactual Fairness through Transforming Data Orthogonal to Bias (Shuyi Chen et al., 2024)</a></li><li><a href=#2449--88347-climate-downscaling-a-deep-learning-based-super-resolution-model-of-precipitation-data-with-attention-block-and-skip-connections-chia-hao-chiang-et-al-2024>(24/49 | 88/347) Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections (Chia-Hao Chiang et al., 2024)</a></li><li><a href=#2549--89347-secure-aggregation-is-not-private-against-membership-inference-attacks-khac-hoang-ngo-et-al-2024>(25/49 | 89/347) Secure Aggregation is Not Private Against Membership Inference Attacks (Khac-Hoang Ngo et al., 2024)</a></li><li><a href=#2649--90347-ccdsreformer-traffic-flow-prediction-with-a-criss-crossed-dual-stream-enhanced-rectified-transformer-model-zhiqi-shao-et-al-2024>(26/49 | 90/347) CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model (Zhiqi Shao et al., 2024)</a></li><li><a href=#2749--91347-enhancing-privacy-in-federated-learning-through-local-training-nicola-bastianello-et-al-2024>(27/49 | 91/347) Enhancing Privacy in Federated Learning through Local Training (Nicola Bastianello et al., 2024)</a></li><li><a href=#2849--92347-a-survey-on-deep-learning-and-state-of-the-arts-applications-mohd-halim-mohd-noor-et-al-2024>(28/49 | 92/347) A Survey on Deep Learning and State-of-the-arts Applications (Mohd Halim Mohd Noor et al., 2024)</a></li><li><a href=#2949--93347-deep-support-vectors-junhoo-lee-et-al-2024>(29/49 | 93/347) Deep Support Vectors (Junhoo Lee et al., 2024)</a></li><li><a href=#3049--94347-canos-a-fast-and-scalable-neural-ac-opf-solver-robust-to-n-1-perturbations-luis-piloto-et-al-2024>(30/49 | 94/347) CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1 Perturbations (Luis Piloto et al., 2024)</a></li><li><a href=#3149--95347-ae-semrl-learning-semantic-association-rules-with-autoencoders-erkan-karabulut-et-al-2024>(31/49 | 95/347) AE SemRL: Learning Semantic Association Rules with Autoencoders (Erkan Karabulut et al., 2024)</a></li><li><a href=#3249--96347-recommendation-of-data-free-class-incremental-learning-algorithms-by-simulating-future-data-eva-feillet-et-al-2024>(32/49 | 96/347) Recommendation of data-free class-incremental learning algorithms by simulating future data (Eva Feillet et al., 2024)</a></li><li><a href=#3349--97347-a-correction-of-pseudo-log-likelihood-method-shi-feng-et-al-2024>(33/49 | 97/347) A Correction of Pseudo Log-Likelihood Method (Shi Feng et al., 2024)</a></li><li><a href=#3449--98347-compressed-multi-task-embeddings-for-data-efficient-downstream-training-and-inference-in-earth-observation-carlos-gomes-et-al-2024>(34/49 | 98/347) Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation (Carlos Gomes et al., 2024)</a></li><li><a href=#3549--99347-empowering-data-mesh-with-federated-learning-haoyuan-li-et-al-2024>(35/49 | 99/347) Empowering Data Mesh with Federated Learning (Haoyuan Li et al., 2024)</a></li><li><a href=#3649--100347-tractoracle-towards-an-anatomically-informed-reward-function-for-rl-based-tractography-antoine-théberge-et-al-2024>(36/49 | 100/347) TractOracle: towards an anatomically-informed reward function for RL-based tractography (Antoine Théberge et al., 2024)</a></li><li><a href=#3749--101347-gpfl-a-gradient-projection-based-client-selection-framework-for-efficient-federated-learning-shijie-na-et-al-2024>(37/49 | 101/347) GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning (Shijie Na et al., 2024)</a></li><li><a href=#3849--102347-have-faith-in-faithfulness-going-beyond-circuit-overlap-when-finding-model-mechanisms-michael-hanna-et-al-2024>(38/49 | 102/347) Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms (Michael Hanna et al., 2024)</a></li><li><a href=#3949--103347-mep-multiple-kernel-learning-enhancing-relative-positional-encoding-length-extrapolation-weiguo-gao-2024>(39/49 | 103/347) MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation (Weiguo Gao, 2024)</a></li><li><a href=#4049--104347-how-private-is-dp-sgd-lynn-chua-et-al-2024>(40/49 | 104/347) How Private is DP-SGD? (Lynn Chua et al., 2024)</a></li><li><a href=#4149--105347-mixing-artificial-and-natural-intelligence-from-statistical-mechanics-to-ai-and-back-to-turbulence-michael-et-al-2024>(41/49 | 105/347) Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence (Michael et al., 2024)</a></li><li><a href=#4249--106347-vdsc-enhancing-exploration-timing-with-value-discrepancy-and-state-counts-marius-captari-et-al-2024>(42/49 | 106/347) VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts (Marius Captari et al., 2024)</a></li><li><a href=#4349--107347-boosting-adversarial-training-via-fisher-rao-norm-based-regularization-xiangyu-yin-et-al-2024>(43/49 | 107/347) Boosting Adversarial Training via Fisher-Rao Norm-based Regularization (Xiangyu Yin et al., 2024)</a></li><li><a href=#4449--108347-a-unified-kernel-for-neural-network-learning-shao-qun-zhang-et-al-2024>(44/49 | 108/347) A Unified Kernel for Neural Network Learning (Shao-Qun Zhang et al., 2024)</a></li><li><a href=#4549--109347-imitating-cost-constrained-behaviors-in-reinforcement-learning-qian-shao-et-al-2024>(45/49 | 109/347) Imitating Cost-Constrained Behaviors in Reinforcement Learning (Qian Shao et al., 2024)</a></li><li><a href=#4649--110347-on-permutation-invariant-neural-networks-masanari-kimura-et-al-2024>(46/49 | 110/347) On permutation-invariant neural networks (Masanari Kimura et al., 2024)</a></li><li><a href=#4749--111347-not-all-federated-learning-algorithms-are-created-equal-a-performance-evaluation-study-gustav-a-baumgart-et-al-2024>(47/49 | 111/347) Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study (Gustav A. Baumgart et al., 2024)</a></li><li><a href=#4849--112347-forest-ore-mining-optimal-rule-ensemble-to-interpret-random-forest-models-haddouchi-maissae-et-al-2024>(48/49 | 112/347) Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest models (Haddouchi Maissae et al., 2024)</a></li><li><a href=#4949--113347-incorporating-exponential-smoothing-into-mlp-a-simple-but-effective-sequence-model-jiqun-chu-et-al-2024>(49/49 | 113/347) Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model (Jiqun Chu et al., 2024)</a></li></ul></li><li><a href=#csir-16>cs.IR (16)</a><ul><li><a href=#116--114347-large-language-models-enhanced-collaborative-filtering-zhongxiang-sun-et-al-2024>(1/16 | 114/347) Large Language Models Enhanced Collaborative Filtering (Zhongxiang Sun et al., 2024)</a></li><li><a href=#216--115347-twolar-a-two-step-llm-augmented-distillation-method-for-passage-reranking-davide-baldelli-et-al-2024>(2/16 | 115/347) TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking (Davide Baldelli et al., 2024)</a></li><li><a href=#316--116347-mind-your-language-a-multilingual-dataset-for-cross-lingual-news-recommendation-andreea-iana-et-al-2024>(3/16 | 116/347) MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation (Andreea Iana et al., 2024)</a></li><li><a href=#416--117347-afdgcf-adaptive-feature-de-correlation-graph-collaborative-filtering-for-recommendations-wei-wu-et-al-2024>(4/16 | 117/347) AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations (Wei Wu et al., 2024)</a></li><li><a href=#516--118347-retentive-decision-transformer-with-adaptive-masking-for-reinforcement-learning-based-recommendation-systems-siyu-wang-et-al-2024>(5/16 | 118/347) Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems (Siyu Wang et al., 2024)</a></li><li><a href=#616--119347-an-empirical-study-of-training-id-agnostic-multi-modal-sequential-recommenders-youhua-li-et-al-2024>(6/16 | 119/347) An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders (Youhua Li et al., 2024)</a></li><li><a href=#716--120347-search-and-society-reimagining-information-access-for-radical-futures-bhaskar-mitra-2024>(7/16 | 120/347) Search and Society: Reimagining Information Access for Radical Futures (Bhaskar Mitra, 2024)</a></li><li><a href=#816--121347-eulerformer-sequential-user-behavior-modeling-with-complex-vector-attention-zhen-tian-et-al-2024>(8/16 | 121/347) EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention (Zhen Tian et al., 2024)</a></li><li><a href=#916--122347-cognitively-biased-users-interacting-with-algorithmically-biased-results-in-whole-session-search-on-controversial-topics-ben-wang-et-al-2024>(9/16 | 122/347) Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Controversial Topics (Ben Wang et al., 2024)</a></li><li><a href=#1016--123347-all-in-one-heterogeneous-interaction-modeling-for-cold-start-rating-prediction-shuheng-fang-et-al-2024>(10/16 | 123/347) All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction (Shuheng Fang et al., 2024)</a></li><li><a href=#1116--124347-touch-the-core-exploring-task-dependence-among-hybrid-targets-for-recommendation-xing-tang-et-al-2024>(11/16 | 124/347) Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation (Xing Tang et al., 2024)</a></li><li><a href=#1216--125347-end4rec-efficient-noise-decoupling-for-multi-behavior-sequential-recommendation-yongqiang-han-et-al-2024>(12/16 | 125/347) END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation (Yongqiang Han et al., 2024)</a></li><li><a href=#1316--126347-masked-multi-domain-network-multi-type-and-multi-scenario-conversion-rate-prediction-with-a-single-model-wentao-ouyang-et-al-2024>(13/16 | 126/347) Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model (Wentao Ouyang et al., 2024)</a></li><li><a href=#1416--127347-ma4div-multi-agent-reinforcement-learning-for-search-result-diversification-yiqun-chen-et-al-2024>(14/16 | 127/347) MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification (Yiqun Chen et al., 2024)</a></li><li><a href=#1516--128347-multi-domain-recommendation-to-attract-users-via-domain-preference-modeling-hyuunjun-ju-et-al-2024>(15/16 | 128/347) Multi-Domain Recommendation to Attract Users via Domain Preference Modeling (Hyuunjun Ju et al., 2024)</a></li><li><a href=#1616--129347-caselink-inductive-graph-learning-for-legal-case-retrieval-yanran-tang-et-al-2024>(16/16 | 129/347) CaseLink: Inductive Graph Learning for Legal Case Retrieval (Yanran Tang et al., 2024)</a></li></ul></li><li><a href=#csai-26>cs.AI (26)</a><ul><li><a href=#126--130347-evaluating-the-efficacy-of-prompt-engineered-large-multimodal-models-versus-fine-tuned-vision-transformers-in-image-based-security-applications-fouad-trad-et-al-2024>(1/26 | 130/347) Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications (Fouad Trad et al., 2024)</a></li><li><a href=#226--131347-dont-trust-verify----grounding-llm-quantitative-reasoning-with-autoformalization-jin-peng-zhou-et-al-2024>(2/26 | 131/347) Don&rsquo;t Trust: Verify &ndash; Grounding LLM Quantitative Reasoning with Autoformalization (Jin Peng Zhou et al., 2024)</a></li><li><a href=#326--132347-visual-hallucination-definition-quantification-and-prescriptive-remediations-vipula-rawte-et-al-2024>(3/26 | 132/347) Visual Hallucination: Definition, Quantification, and Prescriptive Remediations (Vipula Rawte et al., 2024)</a></li><li><a href=#426--133347-kc-genre-a-knowledge-constrained-generative-re-ranking-method-based-on-large-language-models-for-knowledge-graph-completion-yilin-wang-et-al-2024>(4/26 | 133/347) KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion (Yilin Wang et al., 2024)</a></li><li><a href=#526--134347-lasil-learner-aware-supervised-imitation-learning-for-long-term-microscopic-traffic-simulation-ke-guo-et-al-2024>(5/26 | 134/347) LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation (Ke Guo et al., 2024)</a></li><li><a href=#626--135347-aligning-large-language-models-for-enhancing-psychiatric-interviews-through-symptom-delineation-and-summarization-jae-hee-so-et-al-2024>(6/26 | 135/347) Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization (Jae-hee So et al., 2024)</a></li><li><a href=#726--136347-alisa-accelerating-large-language-model-inference-via-sparsity-aware-kv-caching-youpeng-zhao-et-al-2024>(7/26 | 136/347) ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching (Youpeng Zhao et al., 2024)</a></li><li><a href=#826--137347-towards-explainable-clustering-a-constrained-declarative-based-approach-mathieu-guilbert-et-al-2024>(8/26 | 137/347) Towards Explainable Clustering: A Constrained Declarative based Approach (Mathieu Guilbert et al., 2024)</a></li><li><a href=#926--138347-explainable-graph-neural-networks-for-observation-impact-analysis-in-atmospheric-state-estimation-hyeon-ju-jeon-et-al-2024>(9/26 | 138/347) Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation (Hyeon-Ju Jeon et al., 2024)</a></li><li><a href=#1026--139347-addressing-social-misattributions-of-large-language-models-an-hcxai-based-approach-andrea-ferrario-et-al-2024>(10/26 | 139/347) Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach (Andrea Ferrario et al., 2024)</a></li><li><a href=#1126--140347-practical-applications-of-advanced-cloud-services-and-generative-ai-systems-in-medical-image-analysis-jingyu-xu-et-al-2024>(11/26 | 140/347) Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis (Jingyu Xu et al., 2024)</a></li><li><a href=#1226--141347-knowledge-powered-recommendation-for-an-improved-diet-water-footprint-saurav-joshi-et-al-2024>(12/26 | 141/347) Knowledge-Powered Recommendation for an Improved Diet Water Footprint (Saurav Joshi et al., 2024)</a></li><li><a href=#1326--142347-self-clustering-hierarchical-multi-agent-reinforcement-learning-with-extensible-cooperation-graph-qingxu-fu-et-al-2024>(13/26 | 142/347) Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph (Qingxu Fu et al., 2024)</a></li><li><a href=#1426--143347-out-of-distribution-rumor-detection-via-test-time-adaptation-xiang-tao-et-al-2024>(14/26 | 143/347) Out-of-distribution Rumor Detection via Test-Time Adaptation (Xiang Tao et al., 2024)</a></li><li><a href=#1526--144347-a-real-time-rescheduling-algorithm-for-multi-robot-plan-execution-ying-feng-et-al-2024>(15/26 | 144/347) A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution (Ying Feng et al., 2024)</a></li><li><a href=#1626--145347-agentstudio-a-toolkit-for-building-general-virtual-agents-longtao-zheng-et-al-2024>(16/26 | 145/347) AgentStudio: A Toolkit for Building General Virtual Agents (Longtao Zheng et al., 2024)</a></li><li><a href=#1726--146347-solution-for-emotion-prediction-competition-of-workshop-on-emotionally-and-culturally-intelligent-ai-shengdong-xu-et-al-2024>(17/26 | 146/347) Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI (Shengdong Xu et al., 2024)</a></li><li><a href=#1826--147347-prioritized-league-reinforcement-learning-for-large-scale-heterogeneous-multiagent-systems-qingxu-fu-et-al-2024>(18/26 | 147/347) Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems (Qingxu Fu et al., 2024)</a></li><li><a href=#1926--148347-hierarchical-multi-label-classification-for-fine-level-event-extraction-from-aviation-accident-reports-xinyu-zhao-et-al-2024>(19/26 | 148/347) Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports (Xinyu Zhao et al., 2024)</a></li><li><a href=#2026--149347-using-stratified-sampling-to-improve-lime-image-explanations-muhammad-rashid-et-al-2024>(20/26 | 149/347) Using Stratified Sampling to Improve LIME Image Explanations (Muhammad Rashid et al., 2024)</a></li><li><a href=#2126--150347-tiny-models-are-the-computational-saver-for-large-models-qingyuan-wang-et-al-2024>(21/26 | 150/347) Tiny Models are the Computational Saver for Large Models (Qingyuan Wang et al., 2024)</a></li><li><a href=#2226--151347-an-extension-based-approach-for-computing-and-verifying-preferences-in-abstract-argumentation-quratul-ain-mahesar-et-al-2024>(22/26 | 151/347) An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation (Quratul-ain Mahesar et al., 2024)</a></li><li><a href=#2326--152347-an-open-source-end-to-end-logic-optimization-framework-for-large-scale-boolean-network-with-reinforcement-learning-zhen-li-et-al-2024>(23/26 | 152/347) An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning (Zhen Li et al., 2024)</a></li><li><a href=#2426--153347-the-pursuit-of-fairness-in-artificial-intelligence-models-a-survey-tahsin-alamgir-kheya-et-al-2024>(24/26 | 153/347) The Pursuit of Fairness in Artificial Intelligence Models: A Survey (Tahsin Alamgir Kheya et al., 2024)</a></li><li><a href=#2526--154347-learning-traffic-signal-control-via-genetic-programming-xiao-cheng-liao-et-al-2024>(25/26 | 154/347) Learning Traffic Signal Control via Genetic Programming (Xiao-Cheng Liao et al., 2024)</a></li><li><a href=#2626--155347-towards-a-fair-documentation-of-workflows-and-models-in-applied-mathematics-marco-reidelbach-et-al-2024>(26/26 | 155/347) Towards a FAIR Documentation of Workflows and Models in Applied Mathematics (Marco Reidelbach et al., 2024)</a></li></ul></li><li><a href=#eessiv-18>eess.IV (18)</a><ul><li><a href=#118--156347-integrative-graph-transformer-framework-for-histopathology-whole-slide-image-representation-and-classification-zhan-shi-et-al-2024>(1/18 | 156/347) Integrative Graph-Transformer Framework for Histopathology Whole Slide Image Representation and Classification (Zhan Shi et al., 2024)</a></li><li><a href=#218--157347-automated-report-generation-for-lung-cytological-images-using-a-cnn-vision-classifier-and-multiple-transformer-text-decoders-preliminary-study-atsushi-teramoto-et-al-2024>(2/18 | 157/347) Automated Report Generation for Lung Cytological Images Using a CNN Vision Classifier and Multiple-Transformer Text Decoders: Preliminary Study (Atsushi Teramoto et al., 2024)</a></li><li><a href=#318--158347-integrating-mamba-sequence-model-and-hierarchical-upsampling-network-for-accurate-semantic-segmentation-of-multiple-sclerosis-legion-kazi-shahriar-sanjid-et-al-2024>(3/18 | 158/347) Integrating Mamba Sequence Model and Hierarchical Upsampling Network for Accurate Semantic Segmentation of Multiple Sclerosis Legion (Kazi Shahriar Sanjid et al., 2024)</a></li><li><a href=#418--159347-rotate-to-scan-unet-like-mamba-with-triplet-ssm-module-for-medical-image-segmentation-hao-tang-et-al-2024>(4/18 | 159/347) Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation (Hao Tang et al., 2024)</a></li><li><a href=#518--160347-predicting-risk-of-cardiovascular-disease-using-retinal-oct-imaging-cynthia-maldonado-garcia-et-al-2024>(5/18 | 160/347) Predicting risk of cardiovascular disease using retinal OCT imaging (Cynthia Maldonado-Garcia et al., 2024)</a></li><li><a href=#618--161347-onboard-deep-lossless-and-near-lossless-predictive-coding-of-hyperspectral-images-with-line-based-attention-diego-valsesia-et-al-2024>(6/18 | 161/347) Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention (Diego Valsesia et al., 2024)</a></li><li><a href=#718--162347-tracing-and-segmentation-of-molecular-patterns-in-3-dimensional-cryo-etem-density-maps-through-algorithmic-image-processing-and-deep-learning-based-techniques-salim-sazzed-2024>(7/18 | 162/347) Tracing and segmentation of molecular patterns in 3-dimensional cryo-et/em density maps through algorithmic image processing and deep learning-based techniques (Salim Sazzed, 2024)</a></li><li><a href=#818--163347-cross-system-biological-image-quality-enhancement-based-on-the-generative-adversarial-network-as-a-foundation-for-establishing-a-multi-institute-microscopy-cooperative-network-dominik-panek-et-al-2024>(8/18 | 163/347) Cross-system biological image quality enhancement based on the generative adversarial network as a foundation for establishing a multi-institute microscopy cooperative network (Dominik Panek et al., 2024)</a></li><li><a href=#918--164347-scalable-non-cartesian-magnetic-resonance-imaging-with-r2d2-yiwei-chen-et-al-2024>(9/18 | 164/347) Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 (Yiwei Chen et al., 2024)</a></li><li><a href=#1018--165347-annotated-biomedical-video-generation-using-denoising-diffusion-probabilistic-models-and-flow-fields-rüveyda-yilmaz-et-al-2024>(10/18 | 165/347) Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields (Rüveyda Yilmaz et al., 2024)</a></li><li><a href=#1118--166347-ct-synthesis-with-conditional-diffusion-models-for-abdominal-lymph-node-segmentation-yongrui-yu-et-al-2024>(11/18 | 166/347) CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node Segmentation (Yongrui Yu et al., 2024)</a></li><li><a href=#1218--167347-grad-camo-learning-interpretable-single-cell-morphological-profiles-from-3d-cell-painting-images-vivek-gopalakrishnan-et-al-2024>(12/18 | 167/347) Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles from 3D Cell Painting Images (Vivek Gopalakrishnan et al., 2024)</a></li><li><a href=#1318--168347-pseudo-mri-guided-pet-image-reconstruction-method-based-on-a-diffusion-probabilistic-model-weijie-gan-et-al-2024>(13/18 | 168/347) Pseudo-MRI-Guided PET Image Reconstruction Method Based on a Diffusion Probabilistic Model (Weijie Gan et al., 2024)</a></li><li><a href=#1418--169347-serpent-scalable-and-efficient-image-restoration-via-multi-scale-structured-state-space-models-mohammad-shahab-sepehri-et-al-2024>(14/18 | 169/347) Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models (Mohammad Shahab Sepehri et al., 2024)</a></li><li><a href=#1518--170347-paired-diffusion-generation-of-related-synthetic-pet-ct-segmentation-scans-using-linked-denoising-diffusion-probabilistic-models-rowan-bradbury-et-al-2024>(15/18 | 170/347) Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation scans using Linked Denoising Diffusion Probabilistic Models (Rowan Bradbury et al., 2024)</a></li><li><a href=#1618--171347-high-resolution-image-translation-model-based-on-grayscale-redefinition-xixian-wu-et-al-2024>(16/18 | 171/347) High-Resolution Image Translation Model Based on Grayscale Redefinition (Xixian Wu et al., 2024)</a></li><li><a href=#1718--172347-building-bridges-across-spatial-and-temporal-resolutions-reference-based-super-resolution-via-change-priors-and-conditional-diffusion-model-runmin-dong-et-al-2024>(17/18 | 172/347) Building Bridges across Spatial and Temporal Resolutions: Reference-Based Super-Resolution via Change Priors and Conditional Diffusion Model (Runmin Dong et al., 2024)</a></li><li><a href=#1818--173347-labeling-subtypes-in-a-parkinsons-cohort-using-multifeatures-in-mri----integrating-grey-and-white-matter-information-tanmayee-samantaray-et-al-2024>(18/18 | 173/347) Labeling subtypes in a Parkinson&rsquo;s Cohort using Multifeatures in MRI &ndash; Integrating Grey and White Matter Information (Tanmayee Samantaray et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--174347-magis-llm-based-multi-agent-framework-for-github-issue-resolution-wei-tao-et-al-2024>(1/5 | 174/347) MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution (Wei Tao et al., 2024)</a></li><li><a href=#25--175347-an-empirical-study-of-chatgpt-related-projects-on-github-zheng-lin-et-al-2024>(2/5 | 175/347) An Empirical Study of ChatGPT-related projects on GitHub (Zheng Lin et al., 2024)</a></li><li><a href=#35--176347-spes-towards-optimizing-performance-resource-trade-off-for-serverless-functions-cheryl-lee-et-al-2024>(3/5 | 176/347) SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless Functions (Cheryl Lee et al., 2024)</a></li><li><a href=#45--177347-natural-language-requirements-testability-measurement-based-on-requirement-smells-morteza-zakeri-nasrabadi-et-al-2024>(4/5 | 177/347) Natural Language Requirements Testability Measurement Based on Requirement Smells (Morteza Zakeri-Nasrabadi et al., 2024)</a></li><li><a href=#55--178347-mesia-understanding-and-leveraging-supplementary-nature-of-method-level-comments-for-automatic-comment-generation-xinglu-pan-et-al-2024>(5/5 | 178/347) MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation (Xinglu Pan et al., 2024)</a></li></ul></li><li><a href=#cscv-81>cs.CV (81)</a><ul><li><a href=#181--179347-wordrobe-text-guided-generation-of-textured-3d-garments-astitva-srivastava-et-al-2024>(1/81 | 179/347) WordRobe: Text-Guided Generation of Textured 3D Garments (Astitva Srivastava et al., 2024)</a></li><li><a href=#281--180347-boosting-few-shot-learning-with-disentangled-self-supervised-learning-and-meta-learning-for-medical-image-classification-eva-pachetti-et-al-2024>(2/81 | 180/347) Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification (Eva Pachetti et al., 2024)</a></li><li><a href=#381--181347-language-models-are-free-boosters-for-biomedical-imaging-tasks-zhixin-lai-et-al-2024>(3/81 | 181/347) Language Models are Free Boosters for Biomedical Imaging Tasks (Zhixin Lai et al., 2024)</a></li><li><a href=#481--182347-spectral-convolutional-transformer-harmonizing-real-vs-complex-multi-view-spectral-operators-for-vision-transformer-badri-n-patro-et-al-2024>(4/81 | 182/347) Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer (Badri N. Patro et al., 2024)</a></li><li><a href=#581--183347-improving-text-to-image-consistency-via-automatic-prompt-optimization-oscar-mañas-et-al-2024>(5/81 | 183/347) Improving Text-to-Image Consistency via Automatic Prompt Optimization (Oscar Mañas et al., 2024)</a></li><li><a href=#681--184347-equipping-sketch-patches-with-context-aware-positional-encoding-for-graphic-sketch-representation-sicong-zang-et-al-2024>(6/81 | 184/347) Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation (Sicong Zang et al., 2024)</a></li><li><a href=#781--185347-assessment-of-multimodal-large-language-models-in-alignment-with-human-values-zhelun-shi-et-al-2024>(7/81 | 185/347) Assessment of Multimodal Large Language Models in Alignment with Human Values (Zhelun Shi et al., 2024)</a></li><li><a href=#881--186347-elgc-net-efficient-local-global-context-aggregation-for-remote-sensing-change-detection-mubashir-noman-et-al-2024>(8/81 | 186/347) ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing Change Detection (Mubashir Noman et al., 2024)</a></li><li><a href=#981--187347-aid-attention-interpolation-of-text-to-image-diffusion-qiyuan-he-et-al-2024>(9/81 | 187/347) AID: Attention Interpolation of Text-to-Image Diffusion (Qiyuan He et al., 2024)</a></li><li><a href=#1081--188347-a-foundation-model-utilizing-chest-ct-volumes-and-radiology-reports-for-supervised-level-zero-shot-detection-of-abnormalities-ibrahim-ethem-hamamci-et-al-2024>(10/81 | 188/347) A foundation model utilizing chest CT volumes and radiology reports for supervised-level zero-shot detection of abnormalities (Ibrahim Ethem Hamamci et al., 2024)</a></li><li><a href=#1181--189347-efficient-image-pre-training-with-siamese-cropped-masked-autoencoders-alexandre-eymaël-et-al-2024>(11/81 | 189/347) Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders (Alexandre Eymaël et al., 2024)</a></li><li><a href=#1281--190347-the-solution-for-the-cvpr-2023-1st-foundation-model-challenge-track2-haonan-xu-et-al-2024>(12/81 | 190/347) The Solution for the CVPR 2023 1st foundation model challenge-Track2 (Haonan Xu et al., 2024)</a></li><li><a href=#1381--191347-dual-memory-networks-a-versatile-adaptation-approach-for-vision-language-models-yabin-zhang-et-al-2024>(13/81 | 191/347) Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models (Yabin Zhang et al., 2024)</a></li><li><a href=#1481--192347-self-rectifying-diffusion-sampling-with-perturbed-attention-guidance-donghoon-ahn-et-al-2024>(14/81 | 192/347) Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance (Donghoon Ahn et al., 2024)</a></li><li><a href=#1581--193347-the-solution-for-the-iccv-2023-1st-scientific-figure-captioning-challenge-dian-chao-et-al-2024>(15/81 | 193/347) The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge (Dian Chao et al., 2024)</a></li><li><a href=#1681--194347-omnivid-a-generative-framework-for-universal-video-understanding-junke-wang-et-al-2024>(16/81 | 194/347) OmniVid: A Generative Framework for Universal Video Understanding (Junke Wang et al., 2024)</a></li><li><a href=#1781--195347-semi-supervised-image-captioning-considering-wasserstein-graph-matching-yang-yang-2024>(17/81 | 195/347) Semi-Supervised Image Captioning Considering Wasserstein Graph Matching (Yang Yang, 2024)</a></li><li><a href=#1881--196347-exploring-dynamic-transformer-for-efficient-object-tracking-jiawen-zhu-et-al-2024>(18/81 | 196/347) Exploring Dynamic Transformer for Efficient Object Tracking (Jiawen Zhu et al., 2024)</a></li><li><a href=#1981--197347-aide-an-automatic-data-engine-for-object-detection-in-autonomous-driving-mingfu-liang-et-al-2024>(19/81 | 197/347) AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving (Mingfu Liang et al., 2024)</a></li><li><a href=#2081--198347-coda-instructive-chain-of-domain-adaptation-with-severity-aware-visual-prompt-tuning-ziyang-gong-et-al-2024>(20/81 | 198/347) CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning (Ziyang Gong et al., 2024)</a></li><li><a href=#2181--199347-over-nav-elevating-iterative-vision-and-language-navigation-with-open-vocabulary-detection-and-structured-representation-ganlong-zhao-et-al-2024>(21/81 | 199/347) OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation (Ganlong Zhao et al., 2024)</a></li><li><a href=#2281--200347-segment-any-medical-model-extended-yihao-liu-et-al-2024>(22/81 | 200/347) Segment Any Medical Model Extended (Yihao Liu et al., 2024)</a></li><li><a href=#2381--201347-state-of-the-art-applications-of-deep-learning-within-tracking-and-detecting-marine-debris-a-survey-zoe-moorton-et-al-2024>(23/81 | 201/347) State of the art applications of deep learning within tracking and detecting marine debris: A survey (Zoe Moorton et al., 2024)</a></li><li><a href=#2481--202347-leveraging-near-field-lighting-for-monocular-depth-estimation-from-endoscopy-videos-akshay-paruchuri-et-al-2024>(24/81 | 202/347) Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos (Akshay Paruchuri et al., 2024)</a></li><li><a href=#2581--203347-to-supervise-or-not-to-supervise-understanding-and-addressing-the-key-challenges-of-3d-transfer-learning-souhail-hadgi-et-al-2024>(25/81 | 203/347) To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning (Souhail Hadgi et al., 2024)</a></li><li><a href=#2681--204347-not-all-similarities-are-created-equal-leveraging-data-driven-biases-to-inform-genai-copyright-disputes-uri-hacohen-et-al-2024>(26/81 | 204/347) Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes (Uri Hacohen et al., 2024)</a></li><li><a href=#2781--205347-uada3d-unsupervised-adversarial-domain-adaptation-for-3d-object-detection-with-sparse-lidar-and-large-domain-gaps-maciej-k-wozniak-et-al-2024>(27/81 | 205/347) UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps (Maciej K Wozniak et al., 2024)</a></li><li><a href=#2881--206347-move-as-you-say-interact-as-you-can-language-guided-human-motion-generation-with-scene-affordance-zan-wang-et-al-2024>(28/81 | 206/347) Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance (Zan Wang et al., 2024)</a></li><li><a href=#2981--207347-remamber-referring-image-segmentation-with-mamba-twister-yuhuan-yang-et-al-2024>(29/81 | 207/347) ReMamber: Referring Image Segmentation with Mamba Twister (Yuhuan Yang et al., 2024)</a></li><li><a href=#3081--208347-hierarchical-light-transformer-ensembles-for-multimodal-trajectory-forecasting-adrien-lafage-et-al-2024>(30/81 | 208/347) Hierarchical Light Transformer Ensembles for Multimodal Trajectory Forecasting (Adrien Lafage et al., 2024)</a></li><li><a href=#3181--209347-diffh2o-diffusion-based-synthesis-of-hand-object-interactions-from-textual-descriptions-sammy-christen-et-al-2024>(31/81 | 209/347) DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions (Sammy Christen et al., 2024)</a></li><li><a href=#3281--210347-ocai-improving-optical-flow-estimation-by-occlusion-and-consistency-aware-interpolation-jisoo-jeong-et-al-2024>(32/81 | 210/347) OCAI: Improving Optical Flow Estimation by Occlusion and Consistency Aware Interpolation (Jisoo Jeong et al., 2024)</a></li><li><a href=#3381--211347-multi-task-dense-prediction-via-mixture-of-low-rank-experts-yuqi-yang-et-al-2024>(33/81 | 211/347) Multi-Task Dense Prediction via Mixture of Low-Rank Experts (Yuqi Yang et al., 2024)</a></li><li><a href=#3481--212347-diffgaze-a-diffusion-model-for-continuous-gaze-sequence-generation-on-360-images-chuhan-jiao-et-al-2024>(34/81 | 212/347) DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on 360° Images (Chuhan Jiao et al., 2024)</a></li><li><a href=#3581--213347-boosting-diffusion-models-with-moving-average-sampling-in-frequency-domain-yurui-qian-et-al-2024>(35/81 | 213/347) Boosting Diffusion Models with Moving Average Sampling in Frequency Domain (Yurui Qian et al., 2024)</a></li><li><a href=#3681--214347-genesistex-adapting-image-denoising-diffusion-to-texture-space-chenjian-gao-et-al-2024>(36/81 | 214/347) GenesisTex: Adapting Image Denoising Diffusion to Texture Space (Chenjian Gao et al., 2024)</a></li><li><a href=#3781--215347-manifold-guided-lyapunov-control-with-diffusion-models-amartya-mukherjee-et-al-2024>(37/81 | 215/347) Manifold-Guided Lyapunov Control with Diffusion Models (Amartya Mukherjee et al., 2024)</a></li><li><a href=#3881--216347-senm-vae-semi-supervised-noise-modeling-with-hierarchical-variational-autoencoder-dihan-zheng-et-al-2024>(38/81 | 216/347) SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder (Dihan Zheng et al., 2024)</a></li><li><a href=#3981--217347-track-everything-everywhere-fast-and-robustly-yunzhou-song-et-al-2024>(39/81 | 217/347) Track Everything Everywhere Fast and Robustly (Yunzhou Song et al., 2024)</a></li><li><a href=#4081--218347-dn-splatter-depth-and-normal-priors-for-gaussian-splatting-and-meshing-matias-turkulainen-et-al-2024>(40/81 | 218/347) DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing (Matias Turkulainen et al., 2024)</a></li><li><a href=#4181--219347-egolifter-open-world-3d-segmentation-for-egocentric-perception-qiao-gu-et-al-2024>(41/81 | 219/347) EgoLifter: Open-world 3D Segmentation for Egocentric Perception (Qiao Gu et al., 2024)</a></li><li><a href=#4281--220347-efficient-video-object-segmentation-via-modulated-cross-attention-memory-abdelrahman-shaker-et-al-2024>(42/81 | 220/347) Efficient Video Object Segmentation via Modulated Cross-Attention Memory (Abdelrahman Shaker et al., 2024)</a></li><li><a href=#4381--221347-text-is-mass-modeling-as-stochastic-embedding-for-text-video-retrieval-jiamian-wang-et-al-2024>(43/81 | 221/347) Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval (Jiamian Wang et al., 2024)</a></li><li><a href=#4481--222347-noise2noise-denoising-of-crism-hyperspectral-data-robert-platt-et-al-2024>(44/81 | 222/347) Noise2Noise Denoising of CRISM Hyperspectral Data (Robert Platt et al., 2024)</a></li><li><a href=#4581--223347-groupwise-query-specialization-and-quality-aware-multi-assignment-for-transformer-based-visual-relationship-detection-jongha-kim-et-al-2024>(45/81 | 223/347) Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection (Jongha Kim et al., 2024)</a></li><li><a href=#4681--224347-lare2-latent-reconstruction-error-based-method-for-diffusion-generated-image-detection-yunpeng-luo-et-al-2024>(46/81 | 224/347) LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection (Yunpeng Luo et al., 2024)</a></li><li><a href=#4781--225347-decoupled-pseudo-labeling-for-semi-supervised-monocular-3d-object-detection-jiacheng-zhang-et-al-2024>(47/81 | 225/347) Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object Detection (Jiacheng Zhang et al., 2024)</a></li><li><a href=#4881--226347-quakeset-a-dataset-and-low-resource-models-to-monitor-earthquakes-through-sentinel-1-daniele-rege-cambrin-et-al-2024>(48/81 | 226/347) QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1 (Daniele Rege Cambrin et al., 2024)</a></li><li><a href=#4981--227347-egoposeformer-a-simple-baseline-for-egocentric-3d-human-pose-estimation-chenhongyi-yang-et-al-2024>(49/81 | 227/347) EgoPoseFormer: A Simple Baseline for Egocentric 3D Human Pose Estimation (Chenhongyi Yang et al., 2024)</a></li><li><a href=#5081--228347-every-shot-counts-using-exemplars-for-repetition-counting-in-videos-saptarshi-sinha-et-al-2024>(50/81 | 228/347) Every Shot Counts: Using Exemplars for Repetition Counting in Videos (Saptarshi Sinha et al., 2024)</a></li><li><a href=#5181--229347-aios-all-in-one-stage-expressive-human-pose-and-shape-estimation-qingping-sun-et-al-2024>(51/81 | 229/347) AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation (Qingping Sun et al., 2024)</a></li><li><a href=#5281--230347-superior-and-pragmatic-talking-face-generation-with-teacher-student-framework-chao-liang-et-al-2024>(52/81 | 230/347) Superior and Pragmatic Talking Face Generation with Teacher-Student Framework (Chao Liang et al., 2024)</a></li><li><a href=#5381--231347-low-latency-neural-stereo-streaming-qiqi-hou-et-al-2024>(53/81 | 231/347) Low-Latency Neural Stereo Streaming (Qiqi Hou et al., 2024)</a></li><li><a href=#5481--232347-fastperson-enhancing-video-learning-through-effective-video-summarization-that-preserves-linguistic-and-visual-contexts-kazuki-kawamura-et-al-2024>(54/81 | 232/347) FastPerson: Enhancing Video Learning through Effective Video Summarization that Preserves Linguistic and Visual Contexts (Kazuki Kawamura et al., 2024)</a></li><li><a href=#5581--233347-plainmamba-improving-non-hierarchical-mamba-in-visual-recognition-chenhongyi-yang-et-al-2024>(55/81 | 233/347) PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition (Chenhongyi Yang et al., 2024)</a></li><li><a href=#5681--234347-aniportrait-audio-driven-synthesis-of-photorealistic-portrait-animation-huawei-wei-et-al-2024>(56/81 | 234/347) AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation (Huawei Wei et al., 2024)</a></li><li><a href=#5781--235347-difffae-advancing-high-fidelity-one-shot-facial-appearance-editing-with-space-sensitive-customization-and-semantic-preservation-qilin-wang-et-al-2024>(57/81 | 235/347) DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with Space-sensitive Customization and Semantic Preservation (Qilin Wang et al., 2024)</a></li><li><a href=#5881--236347-learning-with-unreliability-fast-few-shot-voxel-radiance-fields-with-relative-geometric-consistency-yingjie-xu-et-al-2024>(58/81 | 236/347) Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency (Yingjie Xu et al., 2024)</a></li><li><a href=#5981--237347-aniartavatar-animatable-3d-art-avatar-from-a-single-image-shaoxu-li-2024>(59/81 | 237/347) AniArtAvatar: Animatable 3D Art Avatar from a Single Image (Shaoxu Li, 2024)</a></li><li><a href=#6081--238347-test-time-adaptation-meets-image-enhancement-improving-accuracy-via-uncertainty-aware-logit-switching-shohei-enomoto-et-al-2024>(60/81 | 238/347) Test-time Adaptation Meets Image Enhancement: Improving Accuracy via Uncertainty-aware Logit Switching (Shohei Enomoto et al., 2024)</a></li><li><a href=#6181--239347-interhandgen-two-hand-interaction-generation-via-cascaded-reverse-diffusion-jihyun-lee-et-al-2024>(61/81 | 239/347) InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion (Jihyun Lee et al., 2024)</a></li><li><a href=#6281--240347-ssf3d-strict-semi-supervised-3d-object-detection-with-switching-filter-songbur-wong-2024>(62/81 | 240/347) SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter (Songbur Wong, 2024)</a></li><li><a href=#6381--241347-tram-global-trajectory-and-motion-of-3d-humans-from-in-the-wild-videos-yufu-wang-et-al-2024>(63/81 | 241/347) TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos (Yufu Wang et al., 2024)</a></li><li><a href=#6481--242347-staircase-localization-for-autonomous-exploration-in-urban-environments-jinrae-kim-et-al-2024>(64/81 | 242/347) Staircase Localization for Autonomous Exploration in Urban Environments (Jinrae Kim et al., 2024)</a></li><li><a href=#6581--243347-physical-3d-adversarial-attacks-against-monocular-depth-estimation-in-autonomous-driving-junhao-zheng-et-al-2024>(65/81 | 243/347) Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving (Junhao Zheng et al., 2024)</a></li><li><a href=#6681--244347-neural-clustering-based-visual-representation-learning-guikun-chen-et-al-2024>(66/81 | 244/347) Neural Clustering based Visual Representation Learning (Guikun Chen et al., 2024)</a></li><li><a href=#6781--245347-tgglinesplus-a-robust-topological-graph-guided-computer-vision-algorithm-for-line-detection-from-images-liping-yang-et-al-2024>(67/81 | 245/347) TGGLinesPlus: A robust topological graph-guided computer vision algorithm for line detection from images (Liping Yang et al., 2024)</a></li><li><a href=#6881--246347-spectralwaste-dataset-multimodal-data-for-waste-sorting-automation-sara-casao-et-al-2024>(68/81 | 246/347) SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation (Sara Casao et al., 2024)</a></li><li><a href=#6981--247347-mmvp-a-multimodal-mocap-dataset-with-vision-and-pressure-sensors-he-zhang-et-al-2024>(69/81 | 247/347) MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors (He Zhang et al., 2024)</a></li><li><a href=#7081--248347-tdip-tunable-deep-image-processing-a-real-time-melt-pool-monitoring-solution-javid-akhavan-et-al-2024>(70/81 | 248/347) TDIP: Tunable Deep Image Processing, a Real Time Melt Pool Monitoring Solution (Javid Akhavan et al., 2024)</a></li><li><a href=#7181--249347-global-point-cloud-registration-network-for-large-transformations-hanz-cuevas-velasquez-et-al-2024>(71/81 | 249/347) Global Point Cloud Registration Network for Large Transformations (Hanz Cuevas-Velasquez et al., 2024)</a></li><li><a href=#7281--250347-2d-gaussian-splatting-for-geometrically-accurate-radiance-fields-binbin-huang-et-al-2024>(72/81 | 250/347) 2D Gaussian Splatting for Geometrically Accurate Radiance Fields (Binbin Huang et al., 2024)</a></li><li><a href=#7381--251347-towards-3d-vision-with-low-cost-single-photon-cameras-fangzhou-mu-et-al-2024>(73/81 | 251/347) Towards 3D Vision with Low-Cost Single-Photon Cameras (Fangzhou Mu et al., 2024)</a></li><li><a href=#7481--252347-a-personalized-video-based-hand-taxonomy-application-for-individuals-with-spinal-cord-injury-mehdy-dousty-et-al-2024>(74/81 | 252/347) A Personalized Video-Based Hand Taxonomy: Application for Individuals with Spinal Cord Injury (Mehdy Dousty et al., 2024)</a></li><li><a href=#7581--253347-convofusion-multi-modal-conversational-diffusion-for-co-speech-gesture-synthesis-muhammad-hamza-mughal-et-al-2024>(75/81 | 253/347) ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis (Muhammad Hamza Mughal et al., 2024)</a></li><li><a href=#7681--254347-sen2fire-a-challenging-benchmark-dataset-for-wildfire-detection-using-sentinel-data-yonghao-xu-et-al-2024>(76/81 | 254/347) Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data (Yonghao Xu et al., 2024)</a></li><li><a href=#7781--255347-deepfake-generation-and-detection-a-benchmark-and-survey-gan-pei-et-al-2024>(77/81 | 255/347) Deepfake Generation and Detection: A Benchmark and Survey (Gan Pei et al., 2024)</a></li><li><a href=#7881--256347-invisible-gas-detection-an-rgb-thermal-cross-attention-network-and-a-new-benchmark-jue-wang-et-al-2024>(78/81 | 256/347) Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A New Benchmark (Jue Wang et al., 2024)</a></li><li><a href=#7981--257347-clinical-domain-knowledge-derived-template-improves-post-hoc-ai-explanations-in-pneumothorax-classification-han-yuan-et-al-2024>(79/81 | 257/347) Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification (Han Yuan et al., 2024)</a></li><li><a href=#8081--258347-deepmif-deep-monotonic-implicit-fields-for-large-scale-lidar-3d-mapping-kutay-yılmaz-et-al-2024>(80/81 | 258/347) DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping (Kutay Yılmaz et al., 2024)</a></li><li><a href=#8181--259347-activity-biometrics-person-identification-from-daily-activities-shehreen-azad-et-al-2024>(81/81 | 259/347) Activity-Biometrics: Person Identification from Daily Activities (Shehreen Azad et al., 2024)</a></li></ul></li><li><a href=#cssd-3>cs.SD (3)</a><ul><li><a href=#13--260347-accuracy-enhancement-method-for-speech-emotion-recognition-from-spectrogram-using-temporal-frequency-correlation-and-positional-information-learning-through-knowledge-transfer-jeong-yoon-kim-et-al-2024>(1/3 | 260/347) Accuracy enhancement method for speech emotion recognition from spectrogram using temporal frequency correlation and positional information learning through knowledge transfer (Jeong-Yoon Kim et al., 2024)</a></li><li><a href=#23--261347-low-latency-neural-speech-phase-prediction-based-on-parallel-estimation-architecture-and-anti-wrapping-losses-for-speech-generation-tasks-yang-ai-et-al-2024>(2/3 | 261/347) Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks (Yang Ai et al., 2024)</a></li><li><a href=#33--262347-deep-functional-multiple-index-models-with-an-application-to-ser-matthieu-saumard-et-al-2024>(3/3 | 262/347) Deep functional multiple index models with an application to SER (Matthieu Saumard et al., 2024)</a></li></ul></li><li><a href=#csro-16>cs.RO (16)</a><ul><li><a href=#116--263347-leveraging-symmetry-in-rl-based-legged-locomotion-control-zhi-su-et-al-2024>(1/16 | 263/347) Leveraging Symmetry in RL-based Legged Locomotion Control (Zhi Su et al., 2024)</a></li><li><a href=#216--264347-shapegrasp-zero-shot-task-oriented-grasping-with-large-language-models-through-geometric-decomposition-samuel-li-et-al-2024>(2/16 | 264/347) ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition (Samuel Li et al., 2024)</a></li><li><a href=#316--265347-sledge-synthesizing-simulation-environments-for-driving-agents-with-generative-models-kashyap-chitta-et-al-2024>(3/16 | 265/347) SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models (Kashyap Chitta et al., 2024)</a></li><li><a href=#416--266347-scenario-based-curriculum-generation-for-multi-agent-autonomous-driving-axel-brunnbauer-et-al-2024>(4/16 | 266/347) Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving (Axel Brunnbauer et al., 2024)</a></li><li><a href=#516--267347-unified-path-and-gait-planning-for-safe-bipedal-robot-navigation-chengyang-peng-et-al-2024>(5/16 | 267/347) Unified Path and Gait Planning for Safe Bipedal Robot Navigation (Chengyang Peng et al., 2024)</a></li><li><a href=#616--268347-sparse-graph-enabled-formation-planning-for-large-scale-aerial-swarms-yuan-zhou-et-al-2024>(6/16 | 268/347) Sparse-Graph-Enabled Formation Planning for Large-Scale Aerial Swarms (Yuan Zhou et al., 2024)</a></li><li><a href=#716--269347-a-study-on-the-use-of-simulation-in-synthesizing-path-following-control-policies-for-autonomous-ground-robots-harry-zhang-et-al-2024>(7/16 | 269/347) A Study on the Use of Simulation in Synthesizing Path-Following Control Policies for Autonomous Ground Robots (Harry Zhang et al., 2024)</a></li><li><a href=#816--270347-time-optimal-flight-with-safety-constraints-and-data-driven-dynamics-maria-krinner-et-al-2024>(8/16 | 270/347) Time-Optimal Flight with Safety Constraints and Data-driven Dynamics (Maria Krinner et al., 2024)</a></li><li><a href=#916--271347-learning-goal-directed-object-pushing-in-cluttered-scenes-with-location-based-attention-nils-dengler-et-al-2024>(9/16 | 271/347) Learning Goal-Directed Object Pushing in Cluttered Scenes with Location-Based Attention (Nils Dengler et al., 2024)</a></li><li><a href=#1016--272347-code-generation-for-conic-model-predictive-control-on-microcontrollers-with-tinympc-sam-schoedel-et-al-2024>(10/16 | 272/347) Code Generation for Conic Model-Predictive Control on Microcontrollers with TinyMPC (Sam Schoedel et al., 2024)</a></li><li><a href=#1116--273347-hierarchical-open-vocabulary-3d-scene-graphs-for-language-grounded-robot-navigation-abdelrhman-werby-et-al-2024>(11/16 | 273/347) Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation (Abdelrhman Werby et al., 2024)</a></li><li><a href=#1216--274347-lidar-based-crop-row-detection-algorithm-for-over-canopy-autonomous-navigation-in-agriculture-fields-ruiji-liu-et-al-2024>(12/16 | 274/347) LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous Navigation in Agriculture Fields (Ruiji Liu et al., 2024)</a></li><li><a href=#1316--275347-roboduet-a-framework-affording-mobile-manipulation-and-cross-embodiment-guoping-pan-et-al-2024>(13/16 | 275/347) RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment (Guoping Pan et al., 2024)</a></li><li><a href=#1416--276347-multi-objective-trajectory-planning-with-dual-encoder-beibei-zhang-et-al-2024>(14/16 | 276/347) Multi-Objective Trajectory Planning with Dual-Encoder (Beibei Zhang et al., 2024)</a></li><li><a href=#1516--277347-online-tree-reconstruction-and-forest-inventory-on-a-mobile-robotic-system-leonard-freißmuth-et-al-2024>(15/16 | 277/347) Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System (Leonard Freißmuth et al., 2024)</a></li><li><a href=#1616--278347-system-calibration-of-a-field-phenotyping-robot-with-multiple-high-precision-profile-laser-scanners-felix-esser-et-al-2024>(16/16 | 278/347) System Calibration of a Field Phenotyping Robot with Multiple High-Precision Profile Laser Scanners (Felix Esser et al., 2024)</a></li></ul></li><li><a href=#cscr-9>cs.CR (9)</a><ul><li><a href=#19--279347-faultguard-a-generative-approach-to-resilient-fault-prediction-in-smart-electrical-grids-emad-efatinasab-et-al-2024>(1/9 | 279/347) FaultGuard: A Generative Approach to Resilient Fault Prediction in Smart Electrical Grids (Emad Efatinasab et al., 2024)</a></li><li><a href=#29--280347-dont-listen-to-me-understanding-and-exploring-jailbreak-prompts-of-large-language-models-zhiyuan-yu-et-al-2024>(2/9 | 280/347) Don&rsquo;t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models (Zhiyuan Yu et al., 2024)</a></li><li><a href=#39--281347-optimization-based-prompt-injection-attack-to-llm-as-a-judge-jiawen-shi-et-al-2024>(3/9 | 281/347) Optimization-based Prompt Injection Attack to LLM-as-a-Judge (Jiawen Shi et al., 2024)</a></li><li><a href=#49--282347-hawk-accurate-and-fast-privacy-preserving-machine-learning-using-secure-lookup-table-computation-hamza-saleem-et-al-2024>(4/9 | 282/347) Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation (Hamza Saleem et al., 2024)</a></li><li><a href=#59--283347-depending-on-yourself-when-you-should-mentoring-llm-with-rl-agents-to-become-the-master-in-cybersecurity-games-yikuan-yan-et-al-2024>(5/9 | 283/347) Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games (Yikuan Yan et al., 2024)</a></li><li><a href=#69--284347-leak-and-learn-an-attackers-cookbook-to-train-using-leaked-data-from-federated-learning-joshua-c-zhao-et-al-2024>(6/9 | 284/347) Leak and Learn: An Attacker&rsquo;s Cookbook to Train Using Leaked Data from Federated Learning (Joshua C. Zhao et al., 2024)</a></li><li><a href=#79--285347-ransomware-analysis-and-evaluation-of-live-forensic-techniques-and-the-impact-on-linux-based-iot-systems-salko-korac-et-al-2024>(7/9 | 285/347) Ransomware: Analysis and Evaluation of Live Forensic Techniques and the Impact on Linux based IoT Systems (Salko Korac et al., 2024)</a></li><li><a href=#89--286347-provably-secure-disambiguating-neural-linguistic-steganography-yuang-qi-et-al-2024>(8/9 | 286/347) Provably Secure Disambiguating Neural Linguistic Steganography (Yuang Qi et al., 2024)</a></li><li><a href=#99--287347-two-birds-with-one-stone-differential-privacy-by-low-power-sram-memory-jianqing-liu-et-al-2024>(9/9 | 287/347) Two Birds with One Stone: Differential Privacy by Low-power SRAM Memory (Jianqing Liu et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--288347-eternagram-probing-player-attitudes-in-alternate-climate-scenarios-through-a-chatgpt-driven-text-adventure-suifang-zhou-et-al-2024>(1/5 | 288/347) Eternagram: Probing Player Attitudes in Alternate Climate Scenarios Through a ChatGPT-Driven Text Adventure (Suifang Zhou et al., 2024)</a></li><li><a href=#25--289347-expressedit-video-editing-with-natural-language-and-sketching-bekzat-tilekbay-et-al-2024>(2/5 | 289/347) ExpressEdit: Video Editing with Natural Language and Sketching (Bekzat Tilekbay et al., 2024)</a></li><li><a href=#35--290347-towards-inclusive-video-commenting-introducing-signmaku-for-the-deaf-and-hard-of-hearing-si-chen-et-al-2024>(3/5 | 290/347) Towards Inclusive Video Commenting: Introducing Signmaku for the Deaf and Hard-of-Hearing (Si Chen et al., 2024)</a></li><li><a href=#45--291347-scicapenter-supporting-caption-composition-for-scientific-figures-with-machine-generated-captions-and-ratings-ting-yao-hsu-et-al-2024>(4/5 | 291/347) SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings (Ting-Yao Hsu et al., 2024)</a></li><li><a href=#55--292347-evaluating-authoring-tools-with-the-explorable-authoring-requirements-frederic-salmen-et-al-2024>(5/5 | 292/347) Evaluating Authoring Tools with the Explorable Authoring Requirements (Frederic Salmen et al., 2024)</a></li></ul></li><li><a href=#csit-3>cs.IT (3)</a><ul><li><a href=#13--293347-adaptive-ttd-configurations-for-near-field-communications-an-unsupervised-transformer-approach-hsienchih-ting-et-al-2024>(1/3 | 293/347) Adaptive TTD Configurations for Near-Field Communications: An Unsupervised Transformer Approach (Hsienchih Ting et al., 2024)</a></li><li><a href=#23--294347-robust-analysis-of-full-duplex-two-way-space-shift-keying-with-ris-systems-xusheng-zhu-et-al-2024>(2/3 | 294/347) Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS Systems (Xusheng Zhu et al., 2024)</a></li><li><a href=#33--295347-computer-classification-of-linear-codes-based-on-lattice-point-enumeration-and-integer-linear-programming-sascha-kurz-2024>(3/3 | 295/347) Computer classification of linear codes based on lattice point enumeration and integer linear programming (Sascha Kurz, 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--296347-fedmil-federated-multiple-instance-learning-for-video-analysis-with-optimized-dpp-scheduling-ashish-bastola-et-al-2024>(1/1 | 296/347) FedMIL: Federated-Multiple Instance Learning for Video Analysis with Optimized DPP Scheduling (Ashish Bastola et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--297347-neural-attributed-community-search-at-billion-scale-jianwei-wang-et-al-2024>(1/2 | 297/347) Neural Attributed Community Search at Billion Scale (Jianwei Wang et al., 2024)</a></li><li><a href=#22--298347-efficient-unsupervised-community-search-with-pre-trained-graph-transformer-jianwei-wang-et-al-2024>(2/2 | 298/347) Efficient Unsupervised Community Search with Pre-trained Graph Transformer (Jianwei Wang et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--299347-discretize-first-filter-next-learning-divergence-consistent-closure-models-for-large-eddy-simulation-syver-døving-agdestein-et-al-2024>(1/2 | 299/347) Discretize first, filter next: learning divergence-consistent closure models for large-eddy simulation (Syver Døving Agdestein et al., 2024)</a></li><li><a href=#22--300347-numerical-analysis-of-a-fesav-scheme-for-a-caginalp-phase-field-model-with-mechanical-effects-in-stereolithography-xingguang-jin-et-al-2024>(2/2 | 300/347) Numerical analysis of a FE/SAV scheme for a Caginalp phase field model with mechanical effects in stereolithography (Xingguang Jin et al., 2024)</a></li></ul></li><li><a href=#eesssy-11>eess.SY (11)</a><ul><li><a href=#111--301347-learning-piecewise-residuals-of-control-barrier-functions-for-safety-of-switching-systems-using-multi-output-gaussian-processes-mohammad-aali-et-al-2024>(1/11 | 301/347) Learning Piecewise Residuals of Control Barrier Functions for Safety of Switching Systems using Multi-Output Gaussian Processes (Mohammad Aali et al., 2024)</a></li><li><a href=#211--302347-multi-agent-clarity-aware-dynamic-coverage-with-gaussian-processes-devansh-r-agrawal-et-al-2024>(2/11 | 302/347) Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes (Devansh R. Agrawal et al., 2024)</a></li><li><a href=#311--303347-path-integral-control-with-rollout-clustering-and-dynamic-obstacles-steven-patrick-et-al-2024>(3/11 | 303/347) Path Integral Control with Rollout Clustering and Dynamic Obstacles (Steven Patrick et al., 2024)</a></li><li><a href=#411--304347-multiple-model-reference-adaptive-control-with-blending-for-non-square-multivariable-systems-alex-lovi-et-al-2024>(4/11 | 304/347) Multiple Model Reference Adaptive Control with Blending for Non-Square Multivariable Systems (Alex Lovi et al., 2024)</a></li><li><a href=#511--305347-adaptive-boundary-control-of-the-kuramoto-sivashinsky-equation-under-intermittent-sensing-mohamed-camil-belhadjoudja-et-al-2024>(5/11 | 305/347) Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under Intermittent Sensing (Mohamed Camil Belhadjoudja et al., 2024)</a></li><li><a href=#611--306347-using-quantum-computers-in-control-interval-matrix-properties-jan-schneider-et-al-2024>(6/11 | 306/347) Using quantum computers in control: interval matrix properties (Jan Schneider et al., 2024)</a></li><li><a href=#711--307347-cyclic-pursuit-formation-control-for-arbitrary-desired-shapes-anna-fujioka-et-al-2024>(7/11 | 307/347) Cyclic pursuit formation control for arbitrary desired shapes (Anna Fujioka et al., 2024)</a></li><li><a href=#811--308347-destination-constrained-linear-dynamical-system-modeling-in-set-valued-frameworks-xiaowei-yang-et-al-2024>(8/11 | 308/347) Destination-Constrained Linear Dynamical System Modeling in Set-Valued Frameworks (Xiaowei Yang et al., 2024)</a></li><li><a href=#911--309347-neural-distributed-controllers-with-port-hamiltonian-structures-muhammad-zakwan-et-al-2024>(9/11 | 309/347) Neural Distributed Controllers with Port-Hamiltonian Structures (Muhammad Zakwan et al., 2024)</a></li><li><a href=#1011--310347-reinforcement-learning-based-receding-horizon-control-using-adaptive-control-barrier-functions-for-safety-critical-systems-ehsan-sabouni-et-al-2024>(10/11 | 310/347) Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems (Ehsan Sabouni et al., 2024)</a></li><li><a href=#1111--311347-neural-exponential-stabilization-of-control-affine-nonlinear-systems-muhammad-zakwan-et-al-2024>(11/11 | 311/347) Neural Exponential Stabilization of Control-affine Nonlinear Systems (Muhammad Zakwan et al., 2024)</a></li></ul></li><li><a href=#csdb-2>cs.DB (2)</a><ul><li><a href=#12--312347-disambiguate-entity-matching-through-relation-discovery-with-large-language-models-zezhou-huang-2024>(1/2 | 312/347) Disambiguate Entity Matching through Relation Discovery with Large Language Models (Zezhou Huang, 2024)</a></li><li><a href=#22--313347-empirical-analysis-of-eip-3675-miner-dynamics-transaction-fees-and-transaction-time-umesh-bhatt-et-al-2024>(2/2 | 313/347) Empirical Analysis of EIP-3675: Miner Dynamics, Transaction Fees, and Transaction Time (Umesh Bhatt et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--314347-sghormer-an-energy-saving-graph-transformer-driven-by-spikes-huizhe-zhang-et-al-2024>(1/1 | 314/347) SGHormer: An Energy-Saving Graph Transformer Driven by Spikes (Huizhe Zhang et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--315347-einexprs-contraction-paths-of-tensor-networks-as-symbolic-expressions-sergio-sanchez-ramirez-et-al-2024>(1/2 | 315/347) EinExprs: Contraction Paths of Tensor Networks as Symbolic Expressions (Sergio Sanchez-Ramirez et al., 2024)</a></li><li><a href=#22--316347-fermihedral-on-the-optimal-compilation-for-fermion-to-qubit-encoding-yuhao-liu-et-al-2024>(2/2 | 316/347) Fermihedral: On the Optimal Compilation for Fermion-to-Qubit Encoding (Yuhao Liu et al., 2024)</a></li></ul></li><li><a href=#mathoc-5>math.OC (5)</a><ul><li><a href=#15--317347-generalized-maximum-entropy-differential-dynamic-programming-yuichiro-aoyama-et-al-2024>(1/5 | 317/347) Generalized Maximum Entropy Differential Dynamic Programming (Yuichiro Aoyama et al., 2024)</a></li><li><a href=#25--318347-a-moreau-envelope-approach-for-lqr-meta-policy-estimation-ashwin-aravind-et-al-2024>(2/5 | 318/347) A Moreau Envelope Approach for LQR Meta-Policy Estimation (Ashwin Aravind et al., 2024)</a></li><li><a href=#35--319347-deep-polytopic-autoencoders-for-low-dimensional-linear-parameter-varying-approximations-and-nonlinear-feedback-design-jan-heiland-et-al-2024>(3/5 | 319/347) Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design (Jan Heiland et al., 2024)</a></li><li><a href=#45--320347-an-inexact-infeasible-arc-search-interior-point-method-for-linear-programming-problems-einosuke-iida-et-al-2024>(4/5 | 320/347) An inexact infeasible arc-search interior-point method for linear programming problems (Einosuke Iida et al., 2024)</a></li><li><a href=#55--321347-multi-agent-pathfinding-for-noise-restricted-hybrid-fuel-unmanned-aerial-vehicles-drew-scott-et-al-2024>(5/5 | 321/347) Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial Vehicles (Drew Scott et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--322347-channel-estimation-and-beamforming-for-beyond-diagonal-reconfigurable-intelligent-surfaces-hongyu-li-et-al-2024>(1/1 | 322/347) Channel Estimation and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces (Hongyu Li et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--323347-r2d2-image-reconstruction-with-model-uncertainty-quantification-in-radio-astronomy-amir-aghabiglou-et-al-2024>(1/1 | 323/347) R2D2 image reconstruction with model uncertainty quantification in radio astronomy (Amir Aghabiglou et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--324347-accelerating-radio-spectrum-regulation-workflows-with-large-language-models-llms-amir-ghasemi-et-al-2024>(1/1 | 324/347) Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs) (Amir Ghasemi et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--325347-prediction-sharing-during-training-and-inference-yotam-gafni-et-al-2024>(1/1 | 325/347) Prediction-sharing During Training and Inference (Yotam Gafni et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--326347-speaker-distance-estimation-in-enclosures-from-single-channel-audio-michael-neri-et-al-2024>(1/2 | 326/347) Speaker Distance Estimation in Enclosures from Single-Channel Audio (Michael Neri et al., 2024)</a></li><li><a href=#22--327347-infrastructure-less-localization-from-indoor-environmental-sounds-based-on-spectral-decomposition-and-spatial-likelihood-model-satoki-ogiso-et-al-2024>(2/2 | 327/347) Infrastructure-less Localization from Indoor Environmental Sounds Based on Spectral Decomposition and Spatial Likelihood Model (Satoki Ogiso et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--328347-the-recessionary-pressures-of-generative-ai-a-threat-to-wellbeing-jo-an-occhipinti-et-al-2024>(1/1 | 328/347) The recessionary pressures of generative AI: A threat to wellbeing (Jo-An Occhipinti et al., 2024)</a></li></ul></li><li><a href=#statml-2>stat.ML (2)</a><ul><li><a href=#12--329347-an-analysis-of-switchback-designs-in-reinforcement-learning-qianglin-wen-et-al-2024>(1/2 | 329/347) An Analysis of Switchback Designs in Reinforcement Learning (Qianglin Wen et al., 2024)</a></li><li><a href=#22--330347-asymptotic-bayes-risk-of-semi-supervised-learning-with-uncertain-labeling-victor-leger-et-al-2024>(2/2 | 330/347) Asymptotic Bayes risk of semi-supervised learning with uncertain labeling (Victor Leger et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--331347-predicting-perceived-gloss-do-weak-labels-suffice-julia-guerrero-viu-et-al-2024>(1/1 | 331/347) Predicting Perceived Gloss: Do Weak Labels Suffice? (Julia Guerrero-Viu et al., 2024)</a></li></ul></li><li><a href=#cscg-2>cs.CG (2)</a><ul><li><a href=#12--332347-formal-verification-of-the-empty-hexagon-number-bernardo-subercaseaux-et-al-2024>(1/2 | 332/347) Formal Verification of the Empty Hexagon Number (Bernardo Subercaseaux et al., 2024)</a></li><li><a href=#22--333347-robust-containment-queries-over-collections-of-rational-parametric-curves-via-generalized-winding-numbers-jacob-spainhour-et-al-2024>(2/2 | 333/347) Robust Containment Queries over Collections of Rational Parametric Curves via Generalized Winding Numbers (Jacob Spainhour et al., 2024)</a></li></ul></li><li><a href=#csgt-2>cs.GT (2)</a><ul><li><a href=#12--334347-paths-to-equilibrium-in-normal-form-games-bora-yongacoglu-et-al-2024>(1/2 | 334/347) Paths to Equilibrium in Normal-Form Games (Bora Yongacoglu et al., 2024)</a></li><li><a href=#22--335347-generalizing-better-response-paths-and-weakly-acyclic-games-bora-yongacoglu-et-al-2024>(2/2 | 335/347) Generalizing Better Response Paths and Weakly Acyclic Games (Bora Yongacoglu et al., 2024)</a></li></ul></li><li><a href=#physicsapp-ph-1>physics.app-ph (1)</a><ul><li><a href=#11--336347-analysis-on-reservoir-activation-with-the-nonlinearity-harnessed-from-solution-processed-mos2-devices-songwei-liu-et-al-2024>(1/1 | 336/347) Analysis on reservoir activation with the nonlinearity harnessed from solution-processed MoS2 devices (Songwei Liu et al., 2024)</a></li></ul></li><li><a href=#hep-ex-1>hep-ex (1)</a><ul><li><a href=#11--337347-particle-identification-with-machine-learning-from-incomplete-data-in-the-alice-experiment-maja-karwowska-et-al-2024>(1/1 | 337/347) Particle identification with machine learning from incomplete data in the ALICE experiment (Maja Karwowska et al., 2024)</a></li></ul></li><li><a href=#mathco-2>math.CO (2)</a><ul><li><a href=#12--338347-so-long-sucker-endgame-analysis-jean-lou-de-carufel-et-al-2024>(1/2 | 338/347) So Long Sucker: Endgame Analysis (Jean-Lou De Carufel et al., 2024)</a></li><li><a href=#22--339347-a-caro-wei-bound-for-induced-linear-forests-in-graphs-gwenaël-joret-et-al-2024>(2/2 | 339/347) A Caro-Wei bound for induced linear forests in graphs (Gwenaël Joret et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--340347-learning-to-visually-localize-sound-sources-from-mixtures-without-prior-source-knowledge-dongjin-kim-et-al-2024>(1/1 | 340/347) Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge (Dongjin Kim et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--341347-online-submodular-welfare-maximization-meets-post-allocation-stochasticity-and-reusability-rajan-udwani-2024>(1/2 | 341/347) Online Submodular Welfare Maximization Meets Post-Allocation Stochasticity and Reusability (Rajan Udwani, 2024)</a></li><li><a href=#22--342347-generalising-the-maximum-independent-set-algorithm-via-boolean-networks-maximilien-gadouleau-et-al-2024>(2/2 | 342/347) Generalising the maximum independent set algorithm via Boolean networks (Maximilien Gadouleau et al., 2024)</a></li></ul></li><li><a href=#mathst-2>math.ST (2)</a><ul><li><a href=#12--343347-counting-stars-is-constant-degree-optimal-for-detecting-any-planted-subgraph-xifan-yu-et-al-2024>(1/2 | 343/347) Counting Stars is Constant-Degree Optimal For Detecting Any Planted Subgraph (Xifan Yu et al., 2024)</a></li><li><a href=#22--344347-geometric-planted-matchings-beyond-the-gaussian-model-lucas-da-rocha-schwengber-et-al-2024>(2/2 | 344/347) Geometric planted matchings beyond the Gaussian model (Lucas da Rocha Schwengber et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-2>physics.soc-ph (2)</a><ul><li><a href=#12--345347-distance-based-hierarchical-cutting-of-complex-networks-with-non-preferential-and-preferential-choice-of-seeds-alexandre-benatti-et-al-2024>(1/2 | 345/347) Distance-Based Hierarchical Cutting of Complex Networks with Non-Preferential and Preferential Choice of Seeds (Alexandre Benatti et al., 2024)</a></li><li><a href=#22--346347-ω_1-ω_2-temporal-random-hyperbolic-graphs-sofoclis-zambirinis-et-al-2024>(2/2 | 346/347) $(ω_1, ω_2)$-Temporal random hyperbolic graphs (Sofoclis Zambirinis et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--347347-piecewise-linear-expectation-analysis-via-k-induction-for-probabilistic-programs-tengshun-yang-et-al-2024>(1/1 | 347/347) Piecewise Linear Expectation Analysis via $k$-Induction for Probabilistic Programs (Tengshun Yang et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>