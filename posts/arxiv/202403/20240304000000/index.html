<!doctype html><html><head><title>arXiv @ 2024.03.04</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.04"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cs.AI (1) cs.AR (1) cs.CL (24) cs.CR (4) cs.CV (29) cs.CY (2) cs.DB (1) cs.DC (1) cs.DS (1) cs.HC (3) cs.IR (1) cs.LG (33) cs.MM (1) cs.NI (1) cs.PF (1) cs.RO (5) cs.SD (2) eess.SY (2) math.OC (2) physics.soc-ph (1) q-bio.QM (1) stat.ML (1) Keywords keyword cs.CL cs.CV cs.LG Active Learning 1 3 Adversarial Attack 1 Anomaly Detection 1 1 Aspect-based Sentiment Analysis 2 Autoencoder 2 Automatic Speech Recognition 1 BART 1 BLEU 1 Bandit Algorithm 1 Benchmarking 6 5 4 Black Box 1 2 Causal Intervention 1 ChatGPT 1 Chatbot 2 Claude 1 Content Detection 1 Continual Learning 1 Contrastive Learning 1 1 ControlNet 1 Convolution 3 Convolutional Neural Network 5 Counter-factual 1 1 Counterfactual Reasoning 1 Data Augmentation 1 1 Differential Privacy 1 Diffusion Model 4 1 Direct Preference Optimization 1 Disambiguation 1 Distribution Shift 2 Domain Adaptation 1 1 Federated Learning 1 Few-shot 1 Few-shot Learning 1 Fine-tuning 10 4 Foundation Model 1 2 GPT 4 1 1 GPT-3 1 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240304000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-04T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-04T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.04"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240304000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Monday, Mar 4, 2024</p></div><div class=title><h1>arXiv @ 2024.03.04</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#csai-1>cs.AI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#cscl-24>cs.CL (24)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#cscr-4>cs.CR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#cscv-29>cs.CV (29)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#csir-1>cs.IR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#cslg-33>cs.LG (33)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#cspf-1>cs.PF (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#csro-5>cs.RO (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#eesssy-2>eess.SY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/#statml-1>stat.ML (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td>1</td><td></td><td>3</td></tr><tr><td>Adversarial Attack</td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td>1</td></tr><tr><td>Aspect-based Sentiment Analysis</td><td>2</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>2</td></tr><tr><td>Automatic Speech Recognition</td><td>1</td><td></td><td></td></tr><tr><td>BART</td><td>1</td><td></td><td></td></tr><tr><td>BLEU</td><td>1</td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>6</td><td>5</td><td>4</td></tr><tr><td>Black Box</td><td></td><td>1</td><td>2</td></tr><tr><td>Causal Intervention</td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>1</td><td></td><td></td></tr><tr><td>Chatbot</td><td>2</td><td></td><td></td></tr><tr><td>Claude</td><td>1</td><td></td><td></td></tr><tr><td>Content Detection</td><td>1</td><td></td><td></td></tr><tr><td>Continual Learning</td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>ControlNet</td><td></td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td>3</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>5</td><td></td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td>1</td></tr><tr><td>Counterfactual Reasoning</td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td>1</td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>1</td></tr><tr><td>Diffusion Model</td><td></td><td>4</td><td>1</td></tr><tr><td>Direct Preference Optimization</td><td>1</td><td></td><td></td></tr><tr><td>Disambiguation</td><td>1</td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td></tr><tr><td>Domain Adaptation</td><td>1</td><td></td><td>1</td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td></tr><tr><td>Few-shot</td><td>1</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>10</td><td></td><td>4</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>2</td></tr><tr><td>GPT</td><td>4</td><td>1</td><td>1</td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td>4</td><td></td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>2</td></tr><tr><td>Geometry</td><td></td><td>1</td><td>1</td></tr><tr><td>Graph</td><td>1</td><td>2</td><td>11</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>9</td></tr><tr><td>Grounding</td><td></td><td>1</td><td></td></tr><tr><td>High-Resource</td><td></td><td></td><td>1</td></tr><tr><td>In-context Learning</td><td>9</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>3</td><td>4</td><td>4</td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td>2</td></tr><tr><td>LLaMA</td><td>3</td><td></td><td>1</td></tr><tr><td>Large Language Model</td><td>35</td><td>4</td><td>8</td></tr><tr><td>Low-Resource</td><td>2</td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td>1</td></tr><tr><td>Masked Language Model</td><td>1</td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>2</td></tr><tr><td>Model Distillation</td><td></td><td></td><td>1</td></tr><tr><td>Model Quantization</td><td>1</td><td></td><td>1</td></tr><tr><td>Morphological Analysis</td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>1</td><td>7</td><td>3</td></tr><tr><td>Multiple Instance Learning</td><td></td><td>2</td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td>2</td></tr><tr><td>Named Entity Recognition</td><td>1</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>1</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>2</td></tr><tr><td>Object Detection</td><td></td><td>4</td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td></tr><tr><td>Prompt</td><td>4</td><td>4</td><td>2</td></tr><tr><td>Prompt Learning</td><td></td><td>1</td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td></tr><tr><td>Quantization</td><td>2</td><td>2</td><td>2</td></tr><tr><td>Question Answering</td><td>2</td><td></td><td></td></tr><tr><td>Reasoning</td><td>4</td><td></td><td>1</td></tr><tr><td>Recommendation</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Reinforcement Learning</td><td>2</td><td></td><td>2</td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td></tr><tr><td>Scaling Law</td><td>1</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>2</td></tr><tr><td>Self-supervised Learning</td><td></td><td>4</td><td></td></tr><tr><td>Sentiment Analysis</td><td>4</td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>1</td><td>1</td></tr><tr><td>Simulator</td><td></td><td>1</td><td>1</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>2</td></tr><tr><td>Style Transfer</td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td>2</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>2</td><td>4</td><td>1</td></tr><tr><td>Temporal Knowledge Graph</td><td></td><td></td><td>1</td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td></td></tr><tr><td>Text Generation</td><td>2</td><td></td><td></td></tr><tr><td>Text Normalization</td><td>1</td><td></td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>2</td><td>2</td></tr><tr><td>Transfer Learning</td><td>1</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>1</td><td>2</td><td>3</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td></td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>2</td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>2</td><td>1</td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>3</td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>2</td><td>1</td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-24>cs.CL (24)</h2><h3 id=124--1119-distilling-text-style-transfer-with-self-explanation-from-llms-chiyu-zhang-et-al-2024>(1/24 | 1/119) Distilling Text Style Transfer With Self-Explanation From LLMs (Chiyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chiyu Zhang, Honglong Cai, Yuezhang, Li, Yuexin Wu, Le Hou, Muhammad Abdul-Mageed. (2024)<br><strong>Distilling Text Style Transfer With Self-Explanation From LLMs</strong><br><button class=copy-to-clipboard title="Distilling Text Style Transfer With Self-Explanation From LLMs" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 150<br>Keywords: Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Low-Resource, Supervised Learning, Unsupervised Learning, Reasoning, Style Transfer, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01106v1.pdf filename=2403.01106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text <b>Style</b> <b>Transfer</b> (TST) seeks to alter the <b>style</b> <b>of</b> text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> alongside chain-of-thought (CoT) <b>prompting</b> to facilitate TST. CoTeX <b>distills</b> the complex rewriting and <b>reasoning</b> capabilities of <b>LLMs</b> into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional <b>supervised</b> <b>fine-tuning</b> and <b>knowledge</b> <b>distillation</b> methods, particularly in <b>low-resource</b> settings. We conduct a comprehensive evaluation, comparing CoTeX against current <b>unsupervised,</b> <b>supervised,</b> <b>in-context</b> <b>learning</b> <b>(ICL)</b> techniques, and instruction-tuned <b>LLMs.</b> Furthermore, CoTeX distinguishes itself by offering transparent explanations for its <b>style</b> <b>transfer</b> process.</p></p class="citation"></blockquote><h3 id=224--2119-vbart-the-turkish-llm-meliksah-turker-et-al-2024>(2/24 | 2/119) VBART: The Turkish LLM (Meliksah Turker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meliksah Turker, Mehmet Erdi Ari, Aydin Han. (2024)<br><strong>VBART: The Turkish LLM</strong><br><button class=copy-to-clipboard title="VBART: The Turkish LLM" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 110<br>Keywords: Fine-tuning, Fine-tuning, BART, Question Answering, Text Generation, Text Summarization, Large Language Model, Large Language Model, Masked Language Model, Scaling Law, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01308v1.pdf filename=2403.01308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present VBART, the first Turkish sequence-to-sequence <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> pre-trained on a <b>large</b> <b>corpus</b> <b>from</b> scratch. VBART are compact <b>LLMs</b> based on good ideas leveraged from <b>BART</b> and mBART models and come in two sizes, <b>Large</b> <b>and</b> <b>XLarge.</b> <b>Fine-tuned</b> VBART models surpass the prior state-of-the-art results in abstractive <b>text</b> <b>summarization,</b> title generation, <b>text</b> <b>paraphrasing,</b> <b>question</b> <b>answering</b> and <b>question</b> <b>generation</b> tasks. They allow <b>fine-tuning</b> for future <b>text</b> <b>generation</b> tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained <b>LLM</b> for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI&rsquo;s multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trained <b>LLM</b> and <b>question</b> <b>the</b> relevancy of Chinchilla <b>Scaling</b> <b>Law</b> to sequence-to-sequence <b>masked</b> <b>language</b> <b>models.</b> Our <b>fine-tuned</b> models, tokenizer and cleaned web corpus of 135 GB are publicly available at huggingface.co/vngrs-ai.</p></p class="citation"></blockquote><h3 id=324--3119-lm4opt-unveiling-the-potential-of-large-language-models-in-formulating-mathematical-optimization-problems-tasnim-ahmed-et-al-2024>(3/24 | 3/119) LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems (Tasnim Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tasnim Ahmed, Salimur Choudhury. (2024)<br><strong>LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems</strong><br><button class=copy-to-clipboard title="LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 103<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01342v1.pdf filename=2403.01342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving field of natural language processing, the translation of linguistic descriptions into mathematical formulation of optimization problems presents a formidable challenge, demanding intricate understanding and processing capabilities from <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> This study compares prominent <b>LLMs,</b> including <b>GPT-3.5,</b> <b>GPT-4,</b> and <b>Llama-2-7b,</b> in <b>zero-shot</b> and one-shot settings for this task. Our findings show <b>GPT-4&rsquo;s</b> superior performance, particularly in the one-shot scenario. A central part of this research is the introduction of `LM4OPT,&rsquo; a progressive <b>fine-tuning</b> framework for <b>Llama-2-7b</b> that utilizes noisy embeddings and specialized datasets. However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as <b>Llama-2-7b</b> compared to larger counterparts, especially in processing lengthy and complex input contexts. Our empirical investigation, utilizing the NL4Opt dataset, unveils that <b>GPT-4</b> surpasses the baseline performance established by previous research, achieving an F1-score of 0.63, solely based on the problem description in natural language, and without relying on any additional named entity information. <b>GPT-3.5</b> follows closely, both outperforming the <b>fine-tuned</b> <b>Llama-2-7b.</b> These findings not only <b>benchmark</b> the current capabilities of <b>LLMs</b> in a novel application area but also lay the groundwork for future improvements in mathematical formulation of optimization problems from natural language input.</p></p class="citation"></blockquote><h3 id=424--4119-ragged-edges-the-double-edged-sword-of-retrieval-augmented-chatbots-philip-feldman-james-r-foulds-et-al-2024>(4/24 | 4/119) RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots (Philip Feldman. James R. Foulds et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Feldman. James R. Foulds, Shimei Pan. (2024)<br><strong>RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots</strong><br><button class=copy-to-clipboard title="RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: H-3-3; I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Recommendation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, ChatGPT, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01193v1.pdf filename=2403.01193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>ChatGPT</b> demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate &ndash; generate plausible but false information &ndash; poses a significant challenge. This issue is critical, as seen in recent court cases where <b>ChatGPT&rsquo;s</b> use led to citations of non-existent legal rulings. This paper explores how <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> can counter hallucinations by integrating external knowledge with <b>prompts.</b> We empirically evaluate <b>RAG</b> against standard <b>LLMs</b> using <b>prompts</b> designed to induce hallucinations. Our results show that <b>RAG</b> increases accuracy in some cases, but can still be misled when <b>prompts</b> directly contradict the model&rsquo;s pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure <b>LLM</b> reliability in real-world applications. We offer practical <b>recommendations</b> for <b>RAG</b> deployment and discuss implications for the development of more trustworthy <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=524--5119-star-constraint-lora-with-dynamic-active-learning-for-data-efficient-fine-tuning-of-large-language-models-linhai-zhang-et-al-2024>(5/24 | 5/119) STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models (Linhai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linhai Zhang, Jialong Wu, Deyu Zhou, Guoqiang Xu. (2024)<br><strong>STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models</strong><br><button class=copy-to-clipboard title="STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Active Learning, Few-shot, Few-shot Learning, Fine-tuning, Supervised Learning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01165v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01165v1.pdf filename=2403.01165v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Though <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated the powerful capabilities of <b>few-shot</b> <b>learning</b> through <b>prompting</b> methods, <b>supervised</b> training is still necessary for complex <b>reasoning</b> tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient <b>Fine-Tuning</b> (PEFT) methods and Memory-Efficient <b>Fine-Tuning</b> methods have been proposed for <b>LLMs.</b> Nevertheless, the issue of <b>large</b> <b>annotated</b> <b>data</b> consumption, the aim of Data-Efficient <b>Fine-Tuning,</b> remains unexplored. One obvious way is to combine the PEFT method with <b>active</b> <b>learning.</b> However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based <b>active</b> <b>learning</b> and LoRA. Specifically, for the uncertainty gap, we introduce a dynamic uncertainty measurement that combines the uncertainty of the base model and the uncertainty of the full model during the iteration of <b>active</b> <b>learning.</b> For poor model calibration, we incorporate the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation. Experimental results show that the proposed approach outperforms existing baseline models on three complex <b>reasoning</b> tasks.</p></p class="citation"></blockquote><h3 id=624--6119-faima-feature-aware-in-context-learning-for-multi-domain-aspect-based-sentiment-analysis-songhua-yang-et-al-2024>(6/24 | 6/119) FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis (Songhua Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Songhua Yang, Xinke Jiang, Hanjie Zhao, Wenxuan Zeng, Hongde Liu, Yuxiang Jia. (2024)<br><strong>FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis</strong><br><button class=copy-to-clipboard title="FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 86<br>Keywords: Graph, Benchmarking, Contrastive Learning, Aspect-based Sentiment Analysis, Sentiment Analysis, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01063v1.pdf filename=2403.01063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-domain <b>aspect-based</b> <b>sentiment</b> <b>analysis</b> (ABSA) seeks to capture fine-grained <b>sentiment</b> <b>across</b> diverse domains. While existing research narrowly focuses on single-domain applications constrained by methodological limitations and data scarcity, the reality is that <b>sentiment</b> <b>naturally</b> traverses multiple domains. Although <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> offer a promising solution for ABSA, it is difficult to integrate effectively with established techniques, including <b>graph-based</b> models and linguistics, because modifying their internal architecture is not easy. To alleviate this problem, we propose a novel framework, Feature-aware <b>In-context</b> <b>Learning</b> for Multi-domain ABSA (FaiMA). The core insight of FaiMA is to utilize <b>in-context</b> <b>learning</b> <b>(ICL)</b> as a feature-aware mechanism that facilitates adaptive learning in multi-domain ABSA tasks. Specifically, we employ a multi-head <b>graph</b> attention network as a text encoder optimized by heuristic rules for linguistic, domain, and <b>sentiment</b> <b>features.</b> Through <b>contrastive</b> <b>learning,</b> we optimize sentence representations by focusing on these diverse features. Additionally, we construct an efficient indexing mechanism, allowing FaiMA to stably retrieve highly relevant examples across multiple dimensions for any given input. To evaluate the efficacy of FaiMA, we build the first multi-domain ABSA <b>benchmark</b> dataset. Extensive experimental results demonstrate that FaiMA achieves significant performance improvements in multiple domains compared to baselines, increasing F1 by 2.07% on average. Source code and data sets are anonymously available at <a href=https://github.com/SupritYoung/FaiMA>https://github.com/SupritYoung/FaiMA</a>.</p></p class="citation"></blockquote><h3 id=724--7119-lab-large-scale-alignment-for-chatbots-shivchander-sudalairaj-et-al-2024>(7/24 | 7/119) LAB: Large-Scale Alignment for ChatBots (Shivchander Sudalairaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, Akash Srivastava. (2024)<br><strong>LAB: Large-Scale Alignment for ChatBots</strong><br><button class=copy-to-clipboard title="LAB: Large-Scale Alignment for ChatBots" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-4, Chatbot, Instruction Following, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01081v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01081v2.pdf filename=2403.01081v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work introduces LAB <b>(Large-scale</b> <b>Alignment</b> <b>for</b> <b>chatBots),</b> a novel methodology designed to overcome the scalability challenges in the <b>instruction-tuning</b> <b>phase</b> of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like <b>GPT-4.</b> We demonstrate that LAB-trained models can achieve competitive performance across several <b>benchmarks</b> compared to models trained with traditional human-annotated or <b>GPT-4</b> generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing <b>LLM</b> capabilities and <b>instruction-following</b> <b>behaviors</b> without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of <b>LLMs</b> for a wide range of applications.</p></p class="citation"></blockquote><h3 id=824--8119-improving-the-validity-of-automatically-generated-feedback-via-reinforcement-learning-alexander-scarlatos-et-al-2024>(8/24 | 8/119) Improving the Validity of Automatically Generated Feedback via Reinforcement Learning (Alexander Scarlatos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan. (2024)<br><strong>Improving the Validity of Automatically Generated Feedback via Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Improving the Validity of Automatically Generated Feedback via Reinforcement Learning" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Direct Preference Optimization, Reinforcement Learning, GPT, GPT-4, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01304v1.pdf filename=2403.01304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatically generating feedback via <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in intelligent tutoring systems and online learning platforms has the potential to improve the learning outcomes of many students. However, both feedback generation and evaluation are challenging: feedback content has to be valid especially in subjects like math, which requires models to understand the problem, the solution, and where the student&rsquo;s error lies. Feedback also has to be pedagogically valid to reflect effective tutoring strategies, such as explaining possible misconceptions and encouraging the student, among other desirable features. In this work, we address both problems of automatically generating and evaluating feedback while considering both correctness and alignment. First, we propose a rubric for evaluating math feedback and show that <b>GPT-4</b> is able to effectively use it to annotate human-written and <b>LLM-generated</b> feedback. Second, we propose a framework for feedback generation that optimizes both correctness and alignment using <b>reinforcement</b> <b>learning</b> (RL). Specifically, we use <b>GPT-4&rsquo;s</b> annotations to create preferences over feedback pairs in an augmented dataset for training via <b>direct</b> <b>preference</b> <b>optimization</b> (DPO). We show that our methods significantly increase the correctness and alignment of generated feedback with <b>Llama</b> 2, an open-source <b>LLM,</b> qualitatively analyze our generation and evaluation systems using case studies, and outline several areas for future work.</p></p class="citation"></blockquote><h3 id=924--9119-reading-subtext-evaluating-large-language-models-on-short-story-summarization-with-writers-melanie-subbiah-et-al-2024>(9/24 | 9/119) Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers (Melanie Subbiah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Melanie Subbiah, Sean Zhang, Lydia B. Chilton, Kathleen McKeown. (2024)<br><strong>Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers</strong><br><button class=copy-to-clipboard title="Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Claude, GPT, GPT-4, LLaMA, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01061v1.pdf filename=2403.01061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We evaluate recent <b>Large</b> <b>language</b> <b>Models</b> <b>(LLMs)</b> on the challenging task of summarizing short stories, which can be lengthy, and include nuanced subtext or scrambled timelines. Importantly, we work directly with authors to ensure that the stories have not been shared online (and therefore are unseen by the models), and to obtain informed evaluations of summary quality using judgments from the authors themselves. Through quantitative and qualitative analysis grounded in narrative theory, we compare <b>GPT-4,</b> <b>Claude-2.1,</b> and <b>LLama-2-70B.</b> We find that all three models make faithfulness mistakes in over 50% of summaries and struggle to interpret difficult subtext. However, at their best, the models can provide thoughtful thematic analysis of stories. We additionally demonstrate that <b>LLM</b> judgments of summary quality do not match the feedback from the writers.</p></p class="citation"></blockquote><h3 id=1024--10119-vnlp-turkish-nlp-package-meliksah-turker-et-al-2024>(10/24 | 10/119) VNLP: Turkish NLP Package (Meliksah Turker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meliksah Turker, Mehmet Erdi Ari, Aydin Han. (2024)<br><strong>VNLP: Turkish NLP Package</strong><br><button class=copy-to-clipboard title="VNLP: Turkish NLP Package" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Disambiguation, Morphological Analysis, Named Entity Recognition, Sentiment Analysis, Text Normalization, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01309v1.pdf filename=2403.01309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language. It contains a wide variety of tools, ranging from the simplest tasks, such as sentence splitting and <b>text</b> <b>normalization,</b> to the more advanced ones, such as <b>text</b> <b>and</b> token classification models. Its token classification models are based on &ldquo;Context Model&rdquo;, a novel architecture that is both an encoder and an auto-regressive model. NLP tasks solved by VNLP models include but are not limited to <b>Sentiment</b> <b>Analysis,</b> <b>Named</b> <b>Entity</b> <b>Recognition,</b> <b>Morphological</b> <b>Analysis</b> & <b>Disambiguation</b> and Part-of-Speech Tagging. Moreover, it comes with pre-trained <b>word</b> <b>embeddings</b> and corresponding SentencePiece Unigram tokenizers. VNLP has an open-source GitHub repository, ReadtheDocs documentation, PyPi package for convenient installation, Python and command-line API and a demo page to test all the functionality. Consequently, our main contribution is a complete, compact, easy-to-install and easy-to-use NLP package for Turkish.</p></p class="citation"></blockquote><h3 id=1124--11119-diner-debiasing-aspect-based-sentiment-analysis-with-multi-variable-causal-inference-jialong-wu-et-al-2024>(11/24 | 11/119) DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference (Jialong Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialong Wu, Linhai Zhang, Deyu Zhou, Guoqiang Xu. (2024)<br><strong>DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference</strong><br><button class=copy-to-clipboard title="DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Causal Intervention, Counter-factual, Aspect-based Sentiment Analysis, Counterfactual Reasoning, Reasoning, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01166v1.pdf filename=2403.01166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Though notable progress has been made, neural-based <b>aspect-based</b> <b>sentiment</b> <b>analysis</b> (ABSA) models are prone to learn spurious correlations from annotation biases, resulting in poor robustness on adversarial data transformations. Among the debiasing solutions, <b>causal</b> <b>inference-based</b> methods have attracted much research attention, which can be mainly categorized into <b>causal</b> <b>intervention</b> methods and <b>counterfactual</b> <b>reasoning</b> methods. However, most of the present debiasing methods focus on single-variable <b>causal</b> <b>inference,</b> which is not suitable for ABSA with two input variables (the target aspect and the review). In this paper, we propose a novel framework based on multi-variable <b>causal</b> <b>inference</b> for debiasing ABSA. In this framework, different types of biases are tackled based on different <b>causal</b> <b>intervention</b> methods. For the review branch, the bias is modeled as indirect confounding from context, where backdoor adjustment intervention is employed for debiasing. For the aspect branch, the bias is described as a direct correlation with labels, where <b>counterfactual</b> <b>reasoning</b> is adopted for debiasing. Extensive experiments demonstrate the effectiveness of the proposed method compared to various baselines on the two widely used real-world aspect robustness test set datasets.</p></p class="citation"></blockquote><h3 id=1224--12119-balancing-exploration-and-exploitation-in-llm-using-soft-rllf-for-enhanced-negation-understanding-ha-thanh-nguyen-et-al-2024>(12/24 | 12/119) Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding (Ha-Thanh Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ha-Thanh Nguyen, Ken Satoh. (2024)<br><strong>Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding</strong><br><button class=copy-to-clipboard title="Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Reinforcement Learning, Transfer Learning, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01185v1.pdf filename=2403.01185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Finetuning</b> approaches in NLP often focus on exploitation rather than exploration, which may lead to suboptimal models. Given the vast search space of natural language, this limited exploration can restrict their performance in complex, high-stakes domains, where accurate negation understanding and logical <b>reasoning</b> abilities are crucial. To address this issue, we leverage <b>Reinforcement</b> <b>Learning</b> from Logical Feedback (RLLF) to create an effective balance between exploration and exploitation in <b>LLMs.</b> Our approach employs an appropriate <b>benchmark</b> dataset for training and evaluation, highlighting the importance of exploration in enhancing negation understanding capabilities. We compare the performance of our RLLF-enhanced <b>LLMs</b> with baseline models trained without RLLF, demonstrating the value of this balanced approach. Furthermore, we showcase the potential of our method in legal AI applications by employing <b>transfer</b> <b>learning</b> and evaluating its impact on negation understanding. Our experimental results exhibit the effectiveness of balancing exploration and exploitation with RLLF in improving <b>LLMs&rsquo;</b> negation capabilities. This has implications for the development of more accurate, reliable, and logically consistent language models in high-stakes domains.</p></p class="citation"></blockquote><h3 id=1324--13119-mitigating-catastrophic-forgetting-in-large-language-models-with-self-synthesized-rehearsal-jianheng-huang-et-al-2024>(13/24 | 13/119) Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal (Jianheng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, Jinsong Su. (2024)<br><strong>Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal</strong><br><button class=copy-to-clipboard title="Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Continual Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01244v1.pdf filename=2403.01244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> suffer from catastrophic forgetting during <b>continual</b> <b>learning.</b> Conventional rehearsal-based methods rely on previous training data to retain the model&rsquo;s ability, which may not be feasible in real-world applications. When conducting <b>continual</b> <b>learning</b> based on a publicly-released <b>LLM</b> checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the <b>LLM</b> to generate synthetic instances for rehearsal. Concretely, we first employ the base <b>LLM</b> for <b>in-context</b> <b>learning</b> to generate synthetic instances. Subsequently, we utilize the latest <b>LLM</b> to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of <b>LLMs</b> in general domains.</p></p class="citation"></blockquote><h3 id=1424--14119-intactkv-improving-large-language-model-quantization-by-keeping-pivot-tokens-intact-ruikang-liu-et-al-2024>(14/24 | 14/119) IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact (Ruikang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan. (2024)<br><strong>IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact</strong><br><button class=copy-to-clipboard title="IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Model Quantization, Quantization, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01241v1.pdf filename=2403.01241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> excel in natural language processing but demand intensive computation. To mitigate this, various <b>quantization</b> methods have been explored, yet they compromise <b>LLM</b> performance. This paper unveils a previously overlooked type of outlier in <b>LLMs.</b> Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which is crucial to the performance of <b>quantized</b> <b>LLMs.</b> Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision <b>model.</b> <b>The</b> approach is simple and easy to combine with existing <b>quantization</b> solutions. Besides, IntactKV can be calibrated as additional <b>LLM</b> parameters to boost the <b>quantized</b> <b>LLMs</b> further. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of <b>quantization</b> error. Empirical results show that IntactKV brings consistent improvement and achieves lossless weight-only INT4 <b>quantization</b> on various downstream tasks, leading to the new state-of-the-art for <b>LLM</b> <b>quantization.</b></p></p class="citation"></blockquote><h3 id=1524--15119-machine-translation-in-the-covid-domain-an-english-irish-case-study-for-loresmt-2021-séamus-lankford-et-al-2024>(15/24 | 15/119) Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021 (Séamus Lankford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Séamus Lankford, Haithem Afli, Andy Way. (2024)<br><strong>Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021</strong><br><button class=copy-to-clipboard title="Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Transformer, Neural Machine Translation, BLEU, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01196v1.pdf filename=2403.01196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Translation models for the specific <b>domain</b> <b>of</b> translating Covid data from English to Irish were developed for the LoResMT 2021 shared task. <b>Domain</b> <b>adaptation</b> techniques, using a Covid-adapted generic 55k corpus from the Directorate General of Translation, were applied. <b>Fine-tuning,</b> mixed <b>fine-tuning</b> and combined dataset approaches were compared with models trained on an extended in-domain dataset. As part of this study, an English-Irish dataset of Covid related data, from the Health and Education <b>domains,</b> <b>was</b> developed. The highest-performing model used a <b>Transformer</b> architecture trained with an extended in-domain Covid dataset. In the context of this study, we have demonstrated that extending an 8k in-domain baseline dataset by just 5k lines improved the <b>BLEU</b> score by 27 points.</p></p class="citation"></blockquote><h3 id=1624--16119-accelerating-greedy-coordinate-gradient-via-probe-sampling-yiran-zhao-et-al-2024>(16/24 | 16/119) Accelerating Greedy Coordinate Gradient via Probe Sampling (Yiran Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiran Zhao, Wenyue Zheng, Tianle Cai, Xuan Long Do, Kenji Kawaguchi, Anirudh Goyal, Michael Shieh. (2024)<br><strong>Accelerating Greedy Coordinate Gradient via Probe Sampling</strong><br><button class=copy-to-clipboard title="Accelerating Greedy Coordinate Gradient via Probe Sampling" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Automatic Speech Recognition, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01251v1.pdf filename=2403.01251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safety of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing <b>prompts</b> containing adversarial suffixes to break the presumingly safe <b>LLMs,</b> but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of <b>LLM</b> safety, in this work, we study a new algorithm called $\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model&rsquo;s predictions are to the target model&rsquo;s predictions for <b>prompt</b> candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a <b>large</b> <b>number</b> <b>of</b> potential <b>prompt</b> candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llama2-7b and leads to equal or improved attack success rate <b>(ASR)</b> on the AdvBench.</p></p class="citation"></blockquote><h3 id=1724--17119-dmoerm-recipes-of-mixture-of-experts-for-effective-reward-modeling-shanghaoran-quan-2024>(17/24 | 17/119) DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling (Shanghaoran Quan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanghaoran Quan. (2024)<br><strong>DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling</strong><br><button class=copy-to-clipboard title="DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01197v1.pdf filename=2403.01197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of the reward model (RM) is a critical factor in improving the effectiveness of the <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> during alignment <b>fine-tuning.</b> There remain two challenges in RM training: 1) training the same RM using various categories of data may cause its generalization performance to suffer from multi-task disturbance, and 2) the human annotation consistency rate is generally only $60%$ to $75%$, causing training data to contain a lot of noise. To tackle these two challenges, we introduced the idea of Mixture-of-Experts (MoE) into the field of RM for the first time. We propose the Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After classifying an input into task categories, we route it to the corresponding inner layer task-specific model. The inner layer MoE is a dense model. We decompose the specific task into multiple capability dimensions and individually <b>fine-tune</b> a LoRA expert on each one. Their outputs are then synthesized by an MLP to compute the final rewards. To minimize costs, we call a public <b>LLM</b> API to obtain the capability preference labels. The validation on manually labeled datasets confirms that our model attains superior consistency with human preference and outstrips advanced generative approaches. Meanwhile, through BoN sampling and RL experiments, we demonstrate that our model outperforms state-of-the-art ensemble methods of RM and mitigates the overoptimization problem. Our code and dataset are available at: <a href=https://github.com/quanshr/DMoERM-v1>https://github.com/quanshr/DMoERM-v1</a>.</p></p class="citation"></blockquote><h3 id=1824--18119-a-comprehensive-cross-language-framework-for-harmful-content-detection-with-the-aid-of-sentiment-analysis-mohammad-dehghani-2024>(18/24 | 18/119) A comprehensive cross-language framework for harmful content detection with the aid of sentiment analysis (Mohammad Dehghani, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Dehghani. (2024)<br><strong>A comprehensive cross-language framework for harmful content detection with the aid of sentiment analysis</strong><br><button class=copy-to-clipboard title="A comprehensive cross-language framework for harmful content detection with the aid of sentiment analysis" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Low-Resource, Content Detection, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01270v1.pdf filename=2403.01270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s digital world, social media plays a significant role in facilitating communication and <b>content</b> <b>sharing.</b> However, the exponential rise in user-generated <b>content</b> <b>has</b> led to challenges in maintaining a respectful online environment. In some cases, users have taken advantage of anonymity in order to use harmful language, which can negatively affect the user experience and pose serious social problems. Recognizing the limitations of manual moderation, automatic detection systems have been developed to tackle this problem. Nevertheless, several obstacles persist, including the absence of a universal definition for harmful language, inadequate datasets across languages, the need for detailed annotation guideline, and most importantly, a comprehensive framework. This study aims to address these challenges by introducing, for the first time, a detailed framework adaptable to any language. This framework encompasses various aspects of harmful language detection. A key component of the framework is the development of a general and detailed annotation guideline. Additionally, the integration of <b>sentiment</b> <b>analysis</b> represents a novel approach to enhancing harmful language detection. Also, a definition of harmful language based on the review of different related concepts is presented. To demonstrate the effectiveness of the proposed framework, its implementation in a challenging <b>low-resource</b> language is conducted. We collected a Persian dataset and applied the annotation guideline for harmful detection and <b>sentiment</b> <b>analysis.</b> Next, we present baseline experiments utilizing machine and deep learning methods to set <b>benchmarks.</b> Results prove the framework&rsquo;s high performance, achieving an accuracy of 99.4% in offensive language detection and 66.2% in <b>sentiment</b> <b>analysis.</b></p></p class="citation"></blockquote><h3 id=1924--19119-api-is-enough-conformal-prediction-for-large-language-models-without-logit-access-jiayuan-su-et-al-2024>(19/24 | 19/119) API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access (Jiayuan Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayuan Su, Jing Luo, Hongwei Wang, Lu Cheng. (2024)<br><strong>API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access</strong><br><button class=copy-to-clipboard title="API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01216v1.pdf filename=2403.01216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study aims to address the pervasive challenge of quantifying uncertainty in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various <b>LLMs</b> and data distributions. However, existing CP methods for <b>LLMs</b> typically assume access to the logits, which are unavailable for some API-only <b>LLMs.</b> In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only <b>LLMs</b> without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on both close-ended and open-ended <b>Question</b> <b>Answering</b> tasks show our approach can mostly outperform the logit-based CP baselines.</p></p class="citation"></blockquote><h3 id=2024--20119-a-survey-of-ai-generated-text-forensic-systems-detection-attribution-and-characterization-tharindu-kumarage-et-al-2024>(20/24 | 20/119) A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization (Tharindu Kumarage et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tharindu Kumarage, Garima Agrawal, Paras Sheth, Raha Moraffah, Aman Chadha, Joshua Garland, Huan Liu. (2024)<br><strong>A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization</strong><br><button class=copy-to-clipboard title="A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01152v1.pdf filename=2403.01152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We have witnessed lately a rapid proliferation of advanced <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> capable of generating high-quality <b>text.</b> <b>While</b> these <b>LLMs</b> have revolutionized <b>text</b> <b>generation</b> across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale. This paper offers a review of AI-generated <b>text</b> <b>forensic</b> systems, an emerging field addressing the challenges of <b>LLM</b> misuses. We present an overview of the existing efforts in AI-generated <b>text</b> <b>forensics</b> by introducing a detailed taxonomy, focusing on three primary pillars: detection, attribution, and characterization. These pillars enable a practical understanding of AI-generated <b>text,</b> <b>from</b> identifying AI-generated content (detection), determining the specific AI model involved (attribution), and grouping the underlying intents of the <b>text</b> <b>(characterization).</b> Furthermore, we explore available resources for AI-generated <b>text</b> <b>forensics</b> research and discuss the evolving challenges and future directions of forensic systems in an AI era.</p></p class="citation"></blockquote><h3 id=2124--21119-llmcrit-teaching-large-language-models-to-use-criteria-weizhe-yuan-et-al-2024>(21/24 | 21/119) LLMCRIT: Teaching Large Language Models to Use Criteria (Weizhe Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weizhe Yuan, Pengfei Liu, Matthias Gallé. (2024)<br><strong>LLMCRIT: Teaching Large Language Models to Use Criteria</strong><br><button class=copy-to-clipboard title="LLMCRIT: Teaching Large Language Models to Use Criteria" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01069v1.pdf filename=2403.01069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans follow criteria when they execute tasks, and these criteria are directly used to assess the quality of task completion. Therefore, having models learn to use criteria to provide feedback can help humans or models to perform tasks better. However, existing research in this field tends to consider only a limited set of criteria or quality assessment aspects. To fill this gap, we propose a general framework that enables <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to use comprehensive criteria for a task in delivering natural language feedback on task execution. In particular, we present a model-in-the-loop framework that semi-automatically derives criteria from collected guidelines for different writing tasks and constructs <b>in-context</b> demonstrations for each criterion. We choose three tasks from real-world scenarios to operationalize this idea: paper introduction writing, Python code writing, and Reddit post writing, and evaluate our feedback generation framework using different <b>LLMs.</b> The results reveal the fine-grained effects of incorporating criteria and demonstrations and provide valuable insights on how to teach <b>LLMs</b> to use criteria more effectively.</p></p class="citation"></blockquote><h3 id=2224--22119-parallelparc-a-scalable-pipeline-for-generating-natural-language-analogies-oren-sultan-et-al-2024>(22/24 | 22/119) ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies (Oren Sultan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oren Sultan, Yonatan Bitton, Ron Yosef, Dafna Shahaf. (2024)<br><strong>ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies</strong><br><button class=copy-to-clipboard title="ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01139v1.pdf filename=2403.01139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Analogy-making is central to human cognition, allowing us to adapt to novel situations &ndash; an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test <b>LLMs&rsquo;</b> and humans&rsquo; analogy recognition in binary and multiple-choice settings, and found that humans outperform the best models (~13% gap) after a light supervision. We demonstrate that our silver-set is useful for training models. Lastly, we show challenging distractors confuse <b>LLMs,</b> but not humans. We hope our pipeline will encourage research in this emerging field.</p></p class="citation"></blockquote><h3 id=2324--23119-boottod-bootstrap-task-oriented-dialogue-representations-by-aligning-diverse-responses-weihao-zeng-et-al-2024>(23/24 | 23/119) BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses (Weihao Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihao Zeng, Keqing He, Yejie Wang, Dayuan Fu, Weiran Xu. (2024)<br><strong>BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses</strong><br><button class=copy-to-clipboard title="BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01163v1.pdf filename=2403.01163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>language</b> <b>models</b> have been successful in many scenarios. However, their usefulness in task-oriented dialogues is limited due to the intrinsic linguistic differences between general text and task-oriented dialogues. Current task-oriented dialogue pre-training methods rely on a contrastive framework, which faces challenges such as selecting true positives and hard negatives, as well as lacking diversity. In this paper, we propose a novel dialogue pre-training model called BootTOD. It learns task-oriented dialogue representations via a self-bootstrapping framework. Unlike contrastive counterparts, BootTOD aligns context and context+response representations and dismisses the requirements of contrastive pairs. BootTOD also uses multiple appropriate response targets to model the intrinsic one-to-many diversity of human conversations. Experimental results show that BootTOD outperforms strong TOD baselines on diverse downstream dialogue tasks.</p></p class="citation"></blockquote><h3 id=2424--24119-mulcogbench-a-multi-modal-cognitive-benchmark-dataset-for-evaluating-chinese-and-english-computational-language-models-yunhao-zhang-et-al-2024>(24/24 | 24/119) MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating Chinese and English Computational Language Models (Yunhao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunhao Zhang, Xiaohan Zhang, Chong Li, Shaonan Wang, Chengqing Zong. (2024)<br><strong>MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating Chinese and English Computational Language Models</strong><br><button class=copy-to-clipboard title="MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating Chinese and English Computational Language Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01116v1.pdf filename=2403.01116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained computational language models have recently made remarkable progress in harnessing the language abilities which were considered unique to humans. Their success has raised interest in whether these models represent and process language like humans. To answer this question, this paper proposes MulCogBench, a <b>multi-modal</b> cognitive <b>benchmark</b> dataset collected from native Chinese and English participants. It encompasses a variety of cognitive data, including subjective semantic ratings, eye-tracking, functional magnetic resonance imaging (fMRI), and magnetoencephalography (MEG). To assess the relationship between language models and cognitive data, we conducted a similarity-encoding analysis which decodes cognitive data based on its pattern similarity with textual embeddings. Results show that language models share significant similarities with human cognitive data and the similarity patterns are modulated by the data modality and stimuli complexity. Specifically, context-aware models outperform context-independent models as language stimulus complexity increases. The shallow layers of context-aware models are better aligned with the high-temporal-resolution MEG signals whereas the deeper layers show more similarity with the high-spatial-resolution fMRI. These results indicate that language models have a delicate relationship with brain language representations. Moreover, the results between Chinese and English are highly consistent, suggesting the generalizability of these findings across languages.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--25119-llamoco-instruction-tuning-of-large-language-models-for-optimization-code-generation-zeyuan-ma-et-al-2024>(1/2 | 25/119) LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation (Zeyuan Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, Yue-Jiao Gong. (2024)<br><strong>LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation</strong><br><button class=copy-to-clipboard title="LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-AI, cs-CL, cs-LG, cs-NE, cs-SE, math-OC, math.OC<br>Keyword Score: 120<br>Keywords: Contrastive Learning, Fine-tuning, Fine-tuning, CodeGen, GPT, GPT-4, GPT-4 turbo, Code Generation, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01131v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01131v2.pdf filename=2403.01131v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research explores optimization using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> by either iteratively seeking next-step solutions from <b>LLMs</b> or directly <b>prompting</b> <b>LLMs</b> for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to <b>prompt</b> design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first <b>instruction-tuning</b> <b>framework</b> designed to adapt <b>LLMs</b> for solving optimization problems in a <b>code-to-code</b> <b>manner.</b> Specifically, we establish a comprehensive <b>instruction</b> <b>set</b> containing well-described problem <b>prompts</b> and effective optimization <b>codes.</b> <b>We</b> then develop a novel two-phase learning strategy that incorporates a <b>contrastive</b> <b>learning-based</b> warm-up procedure before the <b>instruction-tuning</b> <b>phase</b> to enhance the convergence behavior during model <b>fine-tuning.</b> The experiment results demonstrate that a <b>CodeGen</b> (350M) model <b>fine-tuned</b> by our LLaMoCo achieves superior optimization performance compared to <b>GPT-4</b> <b>Turbo</b> and the other competitors across both synthetic and realistic problem sets. The <b>fine-tuned</b> model and the usage <b>instructions</b> <b>are</b> available at <a href=https://anonymous.4open.science/r/LLaMoCo-722A>https://anonymous.4open.science/r/LLaMoCo-722A</a>.</p></p class="citation"></blockquote><h3 id=22--26119-decentralized-implicit-differentiation-lucas-fuentes-valenzuela-et-al-2024>(2/2 | 26/119) Decentralized Implicit Differentiation (Lucas Fuentes Valenzuela et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Fuentes Valenzuela, Robin Brown, Marco Pavone. (2024)<br><strong>Decentralized Implicit Differentiation</strong><br><button class=copy-to-clipboard title="Decentralized Implicit Differentiation" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01260v1.pdf filename=2403.01260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to differentiate through optimization problems has unlocked numerous applications, from optimization-based layers in machine learning models to complex design problems formulated as bilevel programs. It has been shown that exploiting problem structure can yield significant computation gains for optimization and, in some cases, enable distributed computation. One should expect that this structure can be similarly exploited for gradient computation. In this work, we discuss a decentralized framework for computing gradients of constraint-coupled optimization problems. First, we show that this framework results in significant computational gains, especially for large systems, and provide sufficient conditions for its validity. Second, we leverage exponential decay of sensitivities in <b>graph-structured</b> problems towards building a fully distributed algorithm with convergence guarantees. Finally, we use the methodology to rigorously estimate marginal emissions rates in power systems models. Specifically, we demonstrate how the distributed scheme allows for accurate and efficient estimation of these important emissions metrics on large dynamic power system models.</p></p class="citation"></blockquote><h2 id=cslg-33>cs.LG (33)</h2><h3 id=133--27119-opengraph-towards-open-graph-foundation-models-lianghao-xia-et-al-2024>(1/33 | 27/119) OpenGraph: Towards Open Graph Foundation Models (Lianghao Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lianghao Xia, Ben Kao, Chao Huang. (2024)<br><strong>OpenGraph: Towards Open Graph Foundation Models</strong><br><button class=copy-to-clipboard title="OpenGraph: Towards Open Graph Foundation Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 83<br>Keywords: Node Classification, Graph, Graph Neural Network, Data Augmentation, Foundation Model, Recommendation, Zero-shot, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01121v1.pdf filename=2403.01121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> learning has become indispensable for interpreting and harnessing relational <b>data</b> <b>in</b> diverse fields, ranging from <b>recommendation</b> systems to social network analysis. In this context, a variety of <b>GNNs</b> have emerged as promising methodologies for encoding the structural information of <b>graphs.</b> By effectively capturing the <b>graph&rsquo;s</b> underlying structure, these <b>GNNs</b> have shown great potential in enhancing performance in <b>graph</b> learning tasks, such as link prediction and <b>node</b> <b>classification.</b> However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen <b>graph</b> <b>data</b> <b>that</b> significantly differs from the training instances. In this work, our aim is to advance the <b>graph</b> learning paradigm by developing a general <b>graph</b> <b>foundation</b> <b>model.</b> This model is designed to understand the complex topological patterns present in diverse <b>graph</b> <b>data,</b> <b>enabling</b> it to excel in <b>zero-shot</b> <b>graph</b> learning tasks across different downstream datasets. To achieve this goal, we address several key technical challenges in our OpenGraph model. Firstly, we propose a unified <b>graph</b> tokenizer to adapt our <b>graph</b> model to generalize well on unseen <b>graph</b> <b>data,</b> <b>even</b> when the underlying <b>graph</b> properties differ significantly from those encountered during training. Secondly, we develop a scalable <b>graph</b> <b>transformer</b> as the <b>foundational</b> <b>encoder,</b> which effectively captures <b>node-wise</b> <b>dependencies</b> within the global topological context. Thirdly, we introduce a <b>data</b> <b>augmentation</b> mechanism enhanced by a <b>LLM</b> to alleviate the limitations of <b>data</b> <b>scarcity</b> in real-world scenarios. Extensive experiments validate the effectiveness of our framework. By adapting our OpenGraph to new <b>graph</b> characteristics and comprehending the nuances of diverse <b>graphs,</b> our approach achieves remarkable <b>zero-shot</b> <b>graph</b> learning performance across various settings and domains.</p></p class="citation"></blockquote><h3 id=233--28119-evaluating-large-language-models-as-virtual-annotators-for-time-series-physical-sensing-data-aritra-hota-et-al-2024>(2/33 | 28/119) Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data (Aritra Hota et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aritra Hota, Soumyajit Chatterjee, Sandip Chakraborty. (2024)<br><strong>Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data</strong><br><button class=copy-to-clipboard title="Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, human-in-the-loop, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01133v1.pdf filename=2403.01133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional <b>human-in-the-loop-based</b> annotation for time-series data like inertial data often requires access to alternate modalities like video or audio from the environment. These alternate sources provide the necessary information to the human annotator, as the raw numeric data is often too obfuscated even for an expert. However, this traditional approach has many concerns surrounding overall cost, efficiency, storage of additional modalities, time, scalability, and privacy. Interestingly, recent <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are also trained with vast amounts of publicly available alphanumeric data, which allows them to comprehend and perform well on tasks beyond natural language processing. Naturally, this opens up a potential avenue to explore <b>LLMs</b> as virtual annotators where the <b>LLMs</b> will be directly provided the raw sensor data for annotation instead of relying on any alternate modality. Naturally, this could mitigate the problems of the traditional <b>human-in-the-loop</b> approach. Motivated by this observation, we perform a detailed study in this paper to assess whether the state-of-the-art (SOTA) <b>LLMs</b> can be used as virtual annotators for labeling time-series physical sensing data. To perform this in a principled manner, we segregate the study into two major phases. In the first phase, we investigate the challenges an <b>LLM</b> like <b>GPT-4</b> faces in comprehending raw sensor data. Considering the observations from phase 1, in the next phase, we investigate the possibility of encoding the raw sensor data using SOTA SSL approaches and utilizing the projected time-series data to get annotations from the <b>LLM.</b> Detailed evaluation with four <b>benchmark</b> HAR datasets shows that SSL-based encoding and metric-based guidance allow the <b>LLM</b> to make more reasonable decisions and provide accurate annotations without requiring computationally expensive <b>fine-tuning</b> or sophisticated <b>prompt</b> engineering.</p></p class="citation"></blockquote><h3 id=333--29119-less-is-more-hop-wise-graph-attention-for-scalable-and-generalizable-learning-on-circuits-chenhui-deng-et-al-2024>(3/33 | 29/119) Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits (Chenhui Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhui Deng, Zichao Yue, Cunxi Yu, Gokce Sarar, Ryan Carey, Rajeev Jain, Zhiru Zhang. (2024)<br><strong>Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits</strong><br><button class=copy-to-clipboard title="Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Graph Attention Networks, Graph, Graph Neural Network, Graph Neural Network, Reasoning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01317v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01317v2.pdf filename=2403.01317v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large <b>graphs</b> <b>and</b> <b>exhibit</b> limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a <b>gated</b> <b>self-attention</b> module, which adaptively learns important features among different hops without involving the <b>graph</b> <b>topology.</b> <b>As</b> a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the efficacy of HOGA, we consider two representative EDA tasks: quality of results (QoR) prediction and functional <b>reasoning.</b> Our experimental results indicate that (1) HOGA reduces estimation error over conventional <b>GNNs</b> by 46.76% for predicting QoR after logic synthesis; (2) HOGA improves 10.0% <b>reasoning</b> accuracy over <b>GNNs</b> for identifying functional blocks on unseen gate-level netlists after complex technology mapping; (3) The training time for HOGA almost linearly decreases with an increase in computing resources.</p></p class="citation"></blockquote><h3 id=433--30119-teaching-mlp-more-graph-information-a-three-stage-multitask-knowledge-distillation-framework-junxian-li-et-al-2024>(4/33 | 30/119) Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework (Junxian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junxian Li, Bin Shi, Erfei Cui, Hua Wei, Qinghua Zheng. (2024)<br><strong>Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework</strong><br><button class=copy-to-clipboard title="Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01079v1.pdf filename=2403.01079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the challenging problem for inference tasks on large-scale <b>graph</b> <b>datasets</b> <b>of</b> <b>Graph</b> <b>Neural</b> <b>Networks:</b> huge time and memory consumption, and try to overcome it by reducing reliance on <b>graph</b> <b>structure.</b> <b>Even</b> though <b>distilling</b> <b>graph</b> <b>knowledge</b> <b>to</b> student MLP is an excellent idea, it faces two major problems of positional information loss and low generalization. To solve the problems, we propose a new three-stage multitask <b>distillation</b> framework. In detail, we use Positional Encoding to capture positional information. Also, we introduce Neural Heat Kernels responsible for <b>graph</b> <b>data</b> <b>processing</b> in <b>GNN</b> and utilize hidden layer outputs matching for better performance of student MLP&rsquo;s hidden layers. To the best of our <b>knowledge,</b> <b>it</b> is the first work to include hidden layer <b>distillation</b> for student MLP on <b>graphs</b> <b>and</b> <b>to</b> combine <b>graph</b> <b>Positional</b> <b>Encoding</b> with MLP. We test its performance and robustness with several settings and draw the conclusion that our work can outperform well with good stability.</p></p class="citation"></blockquote><h3 id=533--31119-nomad-attention-efficient-llm-inference-on-cpus-through-multiply-add-free-attention-tianyi-zhang-et-al-2024>(5/33 | 31/119) NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention (Tianyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, Anshumali Shrivastava. (2024)<br><strong>NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention</strong><br><button class=copy-to-clipboard title="NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Quantization, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01273v1.pdf filename=2403.01273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>model</b> inference on Central Processing Units (CPU) is challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations. In this paper, we argue that there is a rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers, which allow for ultra-low-latency lookups in batch. We leverage this unique capability of CPUs to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD operations with in-register lookups. Through hardware-aware algorithmic designs, NoMAD-Attention achieves the computation of attention scores using repeated fast accesses to SIMD registers despite their highly limited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based <b>LLMs</b> without model <b>finetuning.</b> Empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original <b>LLMs</b> well, and speeds up the 4-bit <b>quantized</b> <b>LLaMA-7B-based</b> model by up to 2$\times$ at 16k context length. Our results are reproducible at <a href=https://github.com/tonyzhang617/nomad-dist>https://github.com/tonyzhang617/nomad-dist</a>.</p></p class="citation"></blockquote><h3 id=633--32119-polynormer-polynomial-expressive-graph-transformer-in-linear-time-chenhui-deng-et-al-2024>(6/33 | 32/119) Polynormer: Polynomial-Expressive Graph Transformer in Linear Time (Chenhui Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhui Deng, Zichao Yue, Zhiru Zhang. (2024)<br><strong>Polynormer: Polynomial-Expressive Graph Transformer in Linear Time</strong><br><button class=copy-to-clipboard title="Polynormer: Polynomial-Expressive Graph Transformer in Linear Time" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01232v1.pdf filename=2403.01232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>transformers</b> <b>(GTs)</b> have emerged as a promising architecture that is theoretically more expressive than <b>message-passing</b> <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs).</b> However, typical GT models have at least quadratic complexity and thus cannot scale to large <b>graphs.</b> <b>While</b> <b>there</b> are several linear GTs recently proposed, they still lag behind <b>GNN</b> counterparts on several popular <b>graph</b> <b>datasets,</b> <b>which</b> poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with <b>graph</b> <b>topology</b> <b>and</b> node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme to learn high-degree equivariant polynomials whose coefficients are controlled by attention scores. Polynormer has been evaluated on $13$ homophilic and heterophilic datasets, including large <b>graphs</b> <b>with</b> <b>millions</b> of nodes. Our extensive experiment results show that Polynormer outperforms state-of-the-art <b>GNN</b> and GT baselines on most datasets, even without the use of nonlinear activation functions.</p></p class="citation"></blockquote><h3 id=733--33119-dissecting-language-models-machine-unlearning-via-selective-pruning-nicholas-pochinkov-et-al-2024>(7/33 | 33/119) Dissecting Language Models: Machine Unlearning via Selective Pruning (Nicholas Pochinkov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Pochinkov, Nandi Schoots. (2024)<br><strong>Dissecting Language Models: Machine Unlearning via Selective Pruning</strong><br><button class=copy-to-clipboard title="Dissecting Language Models: Machine Unlearning via Selective Pruning" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Machine Unlearning, Pruning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01267v1.pdf filename=2403.01267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding and shaping the behaviour of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is increasingly important as applications become more powerful and more frequently adopted. This paper introduces a <b>machine</b> <b>unlearning</b> method specifically designed for <b>LLMs.</b> We introduce a selective <b>pruning</b> method for <b>LLMs</b> that removes neurons based on their relative importance on a targeted capability compared to overall network performance. This approach is a compute- and data-efficient method for identifying and removing neurons that enable specific behaviours. Our findings reveal that both feed-forward and attention neurons in <b>LLMs</b> are specialized; that is, for specific tasks, certain neurons are more crucial than others.</p></p class="citation"></blockquote><h3 id=833--34119-a-hybrid-model-for-traffic-incident-detection-based-on-generative-adversarial-networks-and-transformer-model-xinying-lu-et-al-2024>(8/33 | 34/119) A Hybrid Model for Traffic Incident Detection based on Generative Adversarial Networks and Transformer Model (Xinying Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinying Lu, Doudou Zhang, Jianli Xiao. (2024)<br><strong>A Hybrid Model for Traffic Incident Detection based on Generative Adversarial Networks and Transformer Model</strong><br><button class=copy-to-clipboard title="A Hybrid Model for Traffic Incident Detection based on Generative Adversarial Networks and Transformer Model" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01147v1.pdf filename=2403.01147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In addition to enhancing traffic safety and facilitating <b>prompt</b> emergency response, traffic incident detection plays an indispensable role in intelligent transportation systems by providing real-time traffic status information. This enables the realization of intelligent traffic control and management. Previous research has identified that apart from employing advanced algorithmic models, the effectiveness of detection is also significantly influenced by challenges related to acquiring large datasets and addressing dataset imbalances. A hybrid model combining <b>transformer</b> and <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> is proposed to address these challenges. Experiments are conducted on four real datasets to validate the superiority of the <b>transformer</b> in traffic incident detection. Additionally, <b>GANs</b> are utilized to expand the dataset and achieve a balanced ratio of 1:4, 2:3, and 1:1. The proposed model is evaluated against the baseline model. The results demonstrate that the proposed model enhances the dataset size, balances the dataset, and improves the performance of traffic incident detection in various aspects.</p></p class="citation"></blockquote><h3 id=933--35119-llm-pq-serving-llm-on-heterogeneous-clusters-with-phase-aware-partition-and-adaptive-quantization-juntao-zhao-et-al-2024>(9/33 | 35/119) LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization (Juntao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, Chuan Wu. (2024)<br><strong>LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization</strong><br><button class=copy-to-clipboard title="LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: High-Resource, Model Quantization, Quantization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01136v1.pdf filename=2403.01136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent breakthroughs in Large-scale language <b>models</b> <b>(LLMs)</b> have demonstrated impressive performance on various tasks. The immense sizes of <b>LLMs</b> have led to very high resource demand and cost for running the <b>models.</b> <b>Though</b> the <b>models</b> <b>are</b> largely served using uniform high-caliber GPUs nowadays, utilizing a heterogeneous cluster with a mix of available high- and low-capacity GPUs can potentially substantially reduce the serving cost. There is a lack of designs to support efficient <b>LLM</b> serving using a heterogeneous cluster, while the current solutions focus on <b>model</b> <b>partition</b> and uniform compression among homogeneous devices. This paper proposes <b>LLM-PQ,</b> a system that advocates adaptive <b>model</b> <b>quantization</b> and phase-aware partition to improve <b>LLM</b> serving efficiency on heterogeneous GPU clusters. We carefully decide on mixed-precision <b>model</b> <b>quantization</b> together with phase-aware <b>model</b> <b>partition</b> and micro-batch sizing in distributed <b>LLM</b> serving with an efficient algorithm, to greatly enhance inference throughput while fulfilling user-specified <b>model</b> <b>quality</b> targets. Extensive experiments on production inference workloads in 11 different clusters demonstrate that <b>LLM-PQ</b> achieves up to 2.88x (2.26x on average) throughput improvement in inference, showing great advantages over state-of-the-art works.</p></p class="citation"></blockquote><h3 id=1033--36119-pairwise-alignment-improves-graph-domain-adaptation-shikun-liu-et-al-2024>(10/33 | 36/119) Pairwise Alignment Improves Graph Domain Adaptation (Shikun Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shikun Liu, Deyu Zou, Han Zhao, Pan Li. (2024)<br><strong>Pairwise Alignment Improves Graph Domain Adaptation</strong><br><button class=copy-to-clipboard title="Pairwise Alignment Improves Graph Domain Adaptation" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Node Classification, Graph, Benchmarking, Distribution Shift, Distribution Shift, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01092v1.pdf filename=2403.01092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph-based</b> methods, pivotal for label inference over interconnected objects in many real-world applications, often encounter generalization challenges, if the <b>graph</b> used for model training differs significantly from the <b>graph</b> used for testing. This work delves into <b>Graph</b> <b>Domain</b> <b>Adaptation</b> (GDA) to address the unique complexities of <b>distribution</b> <b>shifts</b> over <b>graph</b> data, where interconnected data points experience shifts in features, labels, and in particular, connecting patterns. We propose a novel, theoretically principled method, Pairwise Alignment (Pair-Align) to counter <b>graph</b> structure shift by mitigating conditional structure shift (CSS) and label shift (LS). Pair-Align uses edge weights to recalibrate the influence among neighboring <b>nodes</b> <b>to</b> handle CSS and adjusts the classification loss with label weights to handle LS. Our method demonstrates superior performance in real-world applications, including <b>node</b> <b>classification</b> with region shift in social networks, and the pileup mitigation task in particle colliding experiments. For the first application, we also curate the largest dataset by far for GDA studies. Our method shows strong performance in synthetic and other existing <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=1133--37119-cool-a-conjoint-perspective-on-spatio-temporal-graph-neural-network-for-traffic-forecasting-wei-ju-et-al-2024>(11/33 | 37/119) COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting (Wei Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Ju, Yusheng Zhao, Yifang Qin, Siyu Yi, Jingyang Yuan, Zhiping Xiao, Xiao Luo, Xiting Yan, Ming Zhang. (2024)<br><strong>COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting</strong><br><button class=copy-to-clipboard title="COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IR, cs-LG, cs-SI, cs.LG<br>Keyword Score: 36<br>Keywords: Message-Passing, Graph, Graph Neural Network, Benchmarking, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01091v1.pdf filename=2403.01091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates traffic forecasting, which attempts to forecast the future state of traffic based on historical situations. This problem has received ever-increasing attention in various scenarios and facilitated the development of numerous downstream applications such as urban planning and transportation management. However, the efficacy of existing methods remains sub-optimal due to their tendency to model temporal and spatial relationships independently, thereby inadequately accounting for complex high-order interactions of both worlds. Moreover, the diversity of transitional patterns in traffic forecasting makes them challenging to capture for existing approaches, warranting a deeper exploration of their diversity. Toward this end, this paper proposes Conjoint Spatio-Temporal <b>graph</b> <b>neural</b> <b>network</b> (abbreviated as COOL), which models heterogeneous <b>graphs</b> <b>from</b> <b>prior</b> and posterior information to conjointly capture high-order spatio-temporal relationships. On the one hand, heterogeneous <b>graphs</b> <b>connecting</b> <b>sequential</b> observation are constructed to extract composite spatio-temporal relationships via prior message passing. On the other hand, we model dynamic relationships using constructed affinity and penalty <b>graphs,</b> <b>which</b> <b>guide</b> posterior message passing to incorporate complementary semantic information into node representations. Moreover, to capture diverse transitional properties to enhance traffic forecasting, we propose a conjoint <b>self-attention</b> decoder that models diverse temporal patterns from both multi-rank and multi-scale views. Experimental results on four popular <b>benchmark</b> datasets demonstrate that our proposed COOL provides state-of-the-art performance compared with the competitive baselines.</p></p class="citation"></blockquote><h3 id=1233--38119-defending-against-data-reconstruction-attacks-in-federated-learning-an-information-theory-approach-qi-tan-et-al-2024>(12/33 | 38/119) Defending Against Data Reconstruction Attacks in Federated Learning: An Information Theory Approach (Qi Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Tan, Qi Li, Yi Zhao, Zhuotao Liu, Xiaobing Guo, Ke Xu. (2024)<br><strong>Defending Against Data Reconstruction Attacks in Federated Learning: An Information Theory Approach</strong><br><button class=copy-to-clipboard title="Defending Against Data Reconstruction Attacks in Federated Learning: An Information Theory Approach" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 35<br>Keywords: Black Box, Federated Learning, Mutual Information, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01268v1.pdf filename=2403.01268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) trains a <b>black-box</b> <b>and</b> high-dimensional model among different clients by exchanging parameters instead of direct data sharing, which mitigates the privacy leak incurred by machine learning. However, FL still suffers from membership inference attacks (MIA) or data reconstruction attacks (DRA). In particular, an attacker can extract the information from local datasets by constructing DRA, which cannot be effectively throttled by existing techniques, e.g., <b>Differential</b> <b>Privacy</b> (DP). In this paper, we aim to ensure a strong privacy guarantee for FL under DRA. We prove that reconstruction errors under DRA are constrained by the information acquired by an attacker, which means that constraining the transmitted information can effectively throttle DRA. To quantify the information leakage incurred by FL, we establish a channel model, which depends on the upper bound of joint <b>mutual</b> <b>information</b> between the local dataset and multiple transmitted parameters. Moreover, the channel model indicates that the transmitted information can be constrained through data space operation, which can improve training efficiency and the model accuracy under constrained information. According to the channel model, we propose algorithms to constrain the information transmitted in a single round of local training. With a limited number of training rounds, the algorithms ensure that the total amount of transmitted information is limited. Furthermore, our channel model can be applied to various privacy-enhancing techniques (such as DP) to enhance privacy guarantees against DRA. Extensive experiments with real-world datasets validate the effectiveness of our methods.</p></p class="citation"></blockquote><h3 id=1333--39119-pseudo-label-calibration-semi-supervised-multi-modal-entity-alignment-luyao-wang-et-al-2024>(13/33 | 39/119) Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment (Luyao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luyao Wang, Pengnian Qi, Xigang Bao, Chunlai Zhou, Biao Qin. (2024)<br><strong>Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment</strong><br><button class=copy-to-clipboard title="Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-DB, cs-LG, cs.LG<br>Keyword Score: 31<br>Keywords: Graph, Contrastive Learning, Knowledge Graph, Multi-modal, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01203v1.pdf filename=2403.01203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> entity alignment (MMEA) aims to identify equivalent entities between two <b>multi-modal</b> <b>knowledge</b> <b>graphs</b> for integration. Unfortunately, prior arts have attempted to improve the interaction and fusion of <b>multi-modal</b> information, which have overlooked the influence of modal-specific noise and the usage of labeled and unlabeled data in semi-supervised settings. In this work, we introduce a Pseudo-label Calibration <b>Multi-modal</b> Entity Alignment (PCMEA) in a semi-supervised way. Specifically, in order to generate holistic entity representations, we first devise various embedding modules and attention mechanisms to extract visual, structural, relational, and attribute features. Different from the prior direct fusion methods, we next propose to exploit <b>mutual</b> <b>information</b> maximization to filter the modal-specific noise and to augment modal-invariant commonality. Then, we combine pseudo-label calibration with momentum-based <b>contrastive</b> <b>learning</b> to make full use of the labeled and unlabeled data, which improves the quality of pseudo-label and pulls aligned entities closer. Finally, extensive experiments on two MMEA datasets demonstrate the effectiveness of our PCMEA, which yields state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=1433--40119-improve-cost-efficiency-of-active-learning-over-noisy-dataset-zan-kai-chong-et-al-2024>(14/33 | 40/119) Improve Cost Efficiency of Active Learning over Noisy Dataset (Zan-Kai Chong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng. (2024)<br><strong>Improve Cost Efficiency of Active Learning over Noisy Dataset</strong><br><button class=copy-to-clipboard title="Improve Cost Efficiency of Active Learning over Noisy Dataset" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Active Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01346v1.pdf filename=2403.01346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Active</b> <b>learning</b> is a learning strategy whereby the machine learning algorithm actively identifies and labels data points to optimize its learning. This strategy is particularly effective in domains where an abundance of unlabeled data exists, but the cost of labeling these data points is prohibitively expensive. In this paper, we consider cases of binary classification, where acquiring a positive instance incurs a significantly higher cost compared to that of negative instances. For example, in the financial industry, such as in money-lending businesses, a defaulted loan constitutes a positive event leading to substantial financial loss. To address this issue, we propose a shifted normal distribution sampling function that samples from a wider range than typical uncertainty sampling. Our <b>simulation</b> underscores that our proposed sampling function limits both noisy and positive label selection, delivering between 20% and 32% improved cost efficiency over different test datasets.</p></p class="citation"></blockquote><h3 id=1533--41119-bespoke-non-stationary-solvers-for-fast-sampling-of-diffusion-and-flow-models-neta-shaul-et-al-2024>(15/33 | 41/119) Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models (Neta Shaul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neta Shaul, Uriel Singer, Ricky T. Q. Chen, Matthew Le, Ali Thabet, Albert Pumarola, Yaron Lipman. (2024)<br><strong>Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models</strong><br><button class=copy-to-clipboard title="Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Model Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01329v1.pdf filename=2403.01329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver <b>distillation</b> approach to improve sample efficiency of Diffusion and Flow <b>models.</b> <b>BNS</b> solvers are based on a family of non-stationary solvers that provably subsumes existing numerical ODE solvers and consequently demonstrate considerable improvement in sample approximation (PSNR) over these baselines. Compared to <b>model</b> <b>distillation,</b> BNS solvers benefit from a tiny parameter space ($&lt;$200 parameters), fast optimization (two orders of magnitude faster), maintain diversity of samples, and in contrast to previous solver <b>distillation</b> approaches nearly close the gap from standard <b>distillation</b> methods such as Progressive <b>Distillation</b> in the low-medium NFE regime. For example, BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We experimented with BNS solvers for conditional image generation, <b>text-to-image</b> generation, and text-2-audio generation showing significant improvement in sample approximation (PSNR) in all.</p></p class="citation"></blockquote><h3 id=1633--42119-active-deep-kernel-learning-of-molecular-functionalities-realizing-dynamic-structural-embeddings-ayana-ghosh-et-al-2024>(16/33 | 42/119) Active Deep Kernel Learning of Molecular Functionalities: Realizing Dynamic Structural Embeddings (Ayana Ghosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayana Ghosh, Maxim Ziatdinov and, Sergei V. Kalinin. (2024)<br><strong>Active Deep Kernel Learning of Molecular Functionalities: Realizing Dynamic Structural Embeddings</strong><br><button class=copy-to-clipboard title="Active Deep Kernel Learning of Molecular Functionalities: Realizing Dynamic Structural Embeddings" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph, physics-comp-ph, physics-data-an<br>Keyword Score: 30<br>Keywords: Active Learning, Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01234v1.pdf filename=2403.01234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploring molecular spaces is crucial for advancing our understanding of chemical properties and reactions, leading to groundbreaking innovations in materials science, medicine, and energy. This paper explores an approach for <b>active</b> <b>learning</b> in molecular discovery using Deep Kernel Learning (DKL), a novel approach surpassing the limits of classical <b>Variational</b> <b>Autoencoders</b> (VAEs). Employing the QM9 dataset, we contrast DKL with traditional VAEs, which analyze molecular structures based on similarity, revealing limitations due to sparse regularities in latent spaces. DKL, however, offers a more holistic perspective by correlating structure with properties, creating latent spaces that prioritize molecular functionality. This is achieved by recalculating embedding vectors iteratively, aligning with the experimental availability of target properties. The resulting latent spaces are not only better organized but also exhibit unique characteristics such as concentrated maxima representing molecular functionalities and a correlation between predictive uncertainty and error. Additionally, the formation of exclusion regions around certain compounds indicates unexplored areas with potential for groundbreaking functionalities. This study underscores DKL&rsquo;s potential in molecular research, offering new avenues for understanding and discovering molecular functionalities beyond classical VAE limitations.</p></p class="citation"></blockquote><h3 id=1733--43119-γ-vae-curvature-regularized-variational-autoencoders-for-uncovering-emergent-low-dimensional-geometric-structure-in-high-dimensional-data-jason-z-kim-et-al-2024>(17/33 | 43/119) $Γ$-VAE: Curvature regularized variational autoencoders for uncovering emergent low dimensional geometric structure in high dimensional data (Jason Z. Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jason Z. Kim, Nicolas Perrin-Gilbert, Erkan Narmanli, Paul Klein, Christopher R. Myers, Itai Cohen, Joshua J. Waterfall, James P. Sethna. (2024)<br><strong>$Γ$-VAE: Curvature regularized variational autoencoders for uncovering emergent low dimensional geometric structure in high dimensional data</strong><br><button class=copy-to-clipboard title="$Γ$-VAE: Curvature regularized variational autoencoders for uncovering emergent low dimensional geometric structure in high dimensional data" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, physics-bio-ph, q-bio-GN<br>Keyword Score: 30<br>Keywords: Autoencoder, Out-of-distribution, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01078v1.pdf filename=2403.01078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural systems with emergent behaviors often organize along low-dimensional subsets of high-dimensional spaces. For example, despite the tens of thousands of genes in the human genome, the principled study of genomics is fruitful because biological processes rely on coordinated organization that results in lower dimensional phenotypes. To uncover this organization, many nonlinear dimensionality reduction techniques have successfully embedded high-dimensional data into low-dimensional spaces by preserving local similarities between data points. However, the nonlinearities in these methods allow for too much curvature to preserve general trends across multiple non-neighboring data clusters, thereby limiting their interpretability and generalizability to <b>out-of-distribution</b> data. Here, we address both of these limitations by regularizing the curvature of manifolds generated by <b>variational</b> <b>autoencoders,</b> a process we coin ``$\Gamma$-VAE&rsquo;&rsquo;. We demonstrate its utility using two example data sets: bulk RNA-seq from the The Cancer Genome Atlas (TCGA) and the Genotype Tissue Expression (GTEx); and single cell RNA-seq from a lineage tracing experiment in hematopoietic stem cell differentiation. We find that the resulting regularized manifolds identify mesoscale structure associated with different cancer cell types, and accurately re-embed tissues from completely unseen, out-of distribution cancers as if they were originally trained on them. Finally, we show that preserving long-range relationships to differentiated cells separates undifferentiated cells &ndash; which have not yet specialized &ndash; according to their eventual fate. Broadly, we anticipate that regularizing the curvature of generative models will enable more consistent, predictive, and generalizable models in any high-dimensional system with emergent low-dimensional behavior.</p></p class="citation"></blockquote><h3 id=1833--44119-icc-quantifying-image-caption-concreteness-for-multimodal-dataset-curation-moran-yanuka-et-al-2024>(18/33 | 44/119) ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation (Moran Yanuka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moran Yanuka, Morris Alper, Hadar Averbuch-Elor, Raja Giryes. (2024)<br><strong>ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation</strong><br><button class=copy-to-clipboard title="ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01306v1.pdf filename=2403.01306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Web-scale training on paired <b>text-image</b> data is becoming increasingly central to <b>multimodal</b> learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched <b>text-image</b> pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in <b>multimodal</b> learning. Our approach leverages strong <b>foundation</b> <b>models</b> for measuring visual-semantic information loss in <b>multimodal</b> representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality samples from <b>multimodal</b> web-scale datasets to allow for efficient training in resource-constrained settings.</p></p class="citation"></blockquote><h3 id=1933--45119-stochastic-gradient-descent-for-streaming-linear-and-rectified-linear-systems-with-massart-noise-halyun-jeong-et-al-2024>(19/33 | 45/119) Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise (Halyun Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Halyun Jeong, Deanna Needell, Elizaveta Rebrova. (2024)<br><strong>Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise</strong><br><button class=copy-to-clipboard title="Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 65F10, 60-XX, cs-LG, cs-NA, cs.LG, math-NA, stat-ML<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01204v1.pdf filename=2403.01204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose <b>SGD-exp,</b> a <b>stochastic</b> <b>gradient</b> <b>descent</b> approach for linear and ReLU regressions under Massart noise (adversarial semi-random corruption model) for the fully streaming setting. We show novel nearly linear convergence guarantees of <b>SGD-exp</b> to the true parameter with up to $50%$ Massart corruption rate, and with any corruption rate in the case of symmetric oblivious corruptions. This is the first convergence guarantee result for robust ReLU regression in the streaming setting, and it shows the improved convergence rate over previous robust methods for $L_1$ linear regression due to a choice of an exponentially decaying step size, known for its efficiency in practice. Our analysis is based on the drift analysis of a discrete <b>stochastic</b> <b>process,</b> <b>which</b> could also be interesting on its own.</p></p class="citation"></blockquote><h3 id=2033--46119-feature-alignment-rethinking-efficient-active-learning-via-proxy-in-the-context-of-pre-trained-models-ziting-wen-et-al-2024>(20/33 | 46/119) Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models (Ziting Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziting Wen, Oscar Pizarro, Stefan Williams. (2024)<br><strong>Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models</strong><br><button class=copy-to-clipboard title="Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Active Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01101v1.pdf filename=2403.01101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> the pre-trained model with <b>active</b> <b>learning</b> holds promise for reducing annotation costs. However, this combination introduces significant computational costs, particularly with the growing scale of pre-trained models. Recent research has proposed proxy-based <b>active</b> <b>learning,</b> which pre-computes features to reduce computational costs. Yet, this approach often incurs a significant loss in <b>active</b> <b>learning</b> performance, which may even outweigh the computational cost savings. In this paper, we argue the performance drop stems not only from pre-computed features&rsquo; inability to distinguish between categories of labeled samples, resulting in the selection of redundant samples but also from the tendency to compromise valuable pre-trained information when <b>fine-tuning</b> with samples selected through the proxy model. To address this issue, we propose a novel method called aligned selection via proxy to update pre-computed features while selecting a proper training method to inherit valuable pre-training information. Extensive experiments validate that our method significantly improves the total cost of efficient <b>active</b> <b>learning</b> while maintaining computational efficiency.</p></p class="citation"></blockquote><h3 id=2133--47119-temporal-knowledge-graph-completion-with-time-sensitive-relations-in-hypercomplex-space-li-cai-et-al-2024>(21/33 | 47/119) Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space (Li Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Cai, Xin Mao, Zhihong Wang, Shangqing Zhao, Yuhao Zhou, Changxu Wu, Man Lan. (2024)<br><strong>Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space</strong><br><button class=copy-to-clipboard title="Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02355v1.pdf filename=2403.02355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Temporal</b> <b>knowledge</b> <b>graph</b> completion (TKGC) aims to fill in missing facts within a given <b>temporal</b> <b>knowledge</b> <b>graph</b> at a specific time. Existing methods, operating in real or complex spaces, have demonstrated promising performance in this task. This paper advances beyond conventional approaches by introducing more expressive quaternion representations for TKGC within hypercomplex space. Unlike existing quaternion-based methods, our study focuses on capturing time-sensitive relations rather than time-aware entities. Specifically, we model time-sensitive relations through time-aware rotation and periodic time translation, effectively capturing complex <b>temporal</b> <b>variability.</b> <b>Furthermore,</b> we theoretically demonstrate our method&rsquo;s capability to model symmetric, asymmetric, inverse, compositional, and evolutionary relation patterns. Comprehensive experiments on public datasets validate that our proposed approach achieves state-of-the-art performance in the field of TKGC.</p></p class="citation"></blockquote><h3 id=2233--48119-seeing-unseen-discover-novel-biomedical-concepts-via-geometry-constrained-probabilistic-modeling-jianan-fan-et-al-2024>(22/33 | 48/119) Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling (Jianan Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianan Fan, Dongnan Liu, Hang Chang, Heng Huang, Mei Chen, Weidong Cai. (2024)<br><strong>Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling</strong><br><button class=copy-to-clipboard title="Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 18<br>Keywords: Graph, Geometry, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01053v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01053v2.pdf filename=2403.01053v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a <b>geometry-constrained</b> <b>probabilistic</b> <b>modeling</b> treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the interference of distributional latent bias. Then, we incorporate a suite of critical geometric properties to impose proper constraints on the layout of constructed embedding space, which in turn minimizes the uncontrollable risk for unknown class learning and structuring. Furthermore, a spectral <b>graph-theoretic</b> method is devised to estimate the number of potential novel classes. It inherits two intriguing merits compared to existent approaches, namely high computational efficiency and flexibility for taxonomy-adaptive estimation. Extensive experiments across various biomedical scenarios substantiate the effectiveness and general applicability of our method.</p></p class="citation"></blockquote><h3 id=2333--49119-a-two-stage-algorithm-for-cost-efficient-multi-instance-counterfactual-explanations-andré-artelt-et-al-2024>(23/33 | 49/119) A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations (André Artelt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>André Artelt, Andreas Gregoriades. (2024)<br><strong>A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations</strong><br><button class=copy-to-clipboard title="A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01221v1.pdf filename=2403.01221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> explanations constitute among the most popular methods for analyzing the predictions of <b>black-box</b> <b>systems</b> since they can recommend cost-efficient and actionable changes to the input to turn an undesired system&rsquo;s output into a desired output. While most of the existing <b>counterfactual</b> methods explain a single instance, several real-world use cases, such as customer satisfaction, require the identification of a single <b>counterfactual</b> that can satisfy multiple instances (e.g. customers) simultaneously. In this work, we propose a flexible two-stage algorithm for finding groups of instances along with cost-efficient multi-instance <b>counterfactual</b> explanations. This is motivated by the fact that in most previous works the aspect of finding such groups is not addressed.</p></p class="citation"></blockquote><h3 id=2433--50119-can-a-confident-prior-replace-a-cold-posterior-martin-marek-et-al-2024>(24/33 | 50/119) Can a Confident Prior Replace a Cold Posterior? (Martin Marek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Marek, Brooks Paige, Pavel Izmailov. (2024)<br><strong>Can a Confident Prior Replace a Cold Posterior?</strong><br><button class=copy-to-clipboard title="Can a Confident Prior Replace a Cold Posterior?" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01272v1.pdf filename=2403.01272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Benchmark</b> datasets used for image classification tend to have very low levels of label noise. When Bayesian neural networks are trained on these datasets, they often underfit, misrepresenting the aleatoric uncertainty of the data. A common solution is to cool the posterior, which improves fit to the training data but is challenging to interpret from a Bayesian perspective. We explore whether posterior tempering can be replaced by a confidence-inducing prior distribution. First, we introduce a &ldquo;DirClip&rdquo; prior that is practical to sample and nearly matches the performance of a cold posterior. Second, we introduce a &ldquo;confidence prior&rdquo; that directly approximates a cold likelihood in the limit of decreasing temperature but cannot be easily sampled. Lastly, we provide several general insights into confidence-inducing priors, such as when they might diverge and how <b>fine-tuning</b> can mitigate numerical instability.</p></p class="citation"></blockquote><h3 id=2533--51119-spatio-temporal-field-neural-networks-for-air-quality-inference-yutong-feng-et-al-2024>(25/33 | 51/119) Spatio-Temporal Field Neural Networks for Air Quality Inference (Yutong Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutong Feng, Qiongyan Wang, Yutong Xia, Junlin Huang, Siru Zhong, Kun Wang, Shifen Cheng, Yuxuan Liang. (2024)<br><strong>Spatio-Temporal Field Neural Networks for Air Quality Inference</strong><br><button class=copy-to-clipboard title="Spatio-Temporal Field Neural Networks for Air Quality Inference" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02354v1.pdf filename=2403.02354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The air quality inference problem aims to utilize historical data from a limited number of observation sites to infer the air quality index at an unknown location. Considering the sparsity of data due to the high maintenance cost of the stations, good inference algorithms can effectively save the cost and refine the data granularity. While spatio-temporal <b>graph</b> <b>neural</b> <b>networks</b> have made excellent progress on this problem, their non-Euclidean and discrete data structure modeling of reality limits its potential. In this work, we make the first attempt to combine two different spatio-temporal perspectives, fields and <b>graphs,</b> <b>by</b> <b>proposing</b> a new model, Spatio-Temporal Field Neural Network, and its corresponding new framework, Pyramidal Inference. Extensive experiments validate that our model achieves state-of-the-art performance in nationwide air quality inference in the Chinese Mainland, demonstrating the superiority of our proposed model and framework.</p></p class="citation"></blockquote><h3 id=2633--52119-near-optimal-per-action-regret-bounds-for-sleeping-bandits-quan-nguyen-et-al-2024>(26/33 | 52/119) Near-optimal Per-Action Regret Bounds for Sleeping Bandits (Quan Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quan Nguyen, Nishant A. Mehta. (2024)<br><strong>Near-optimal Per-Action Regret Bounds for Sleeping Bandits</strong><br><button class=copy-to-clipboard title="Near-optimal Per-Action Regret Bounds for Sleeping Bandits" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01315v1.pdf filename=2403.01315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We derive near-optimal per-action regret bounds for sleeping <b>bandits,</b> in which both the sets of available arms and their losses in every round are chosen by an adversary. In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\sqrt{TA\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets. Compared to the minimax $\Omega(\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\sqrt{TA\ln{K}})$ and $O(\sqrt{T\sqrt{AK}})$. We extend our results to the setting of <b>bandits</b> with advice from sleeping experts, generalizing EXP4 along the way. This leads to new proofs for a number of existing adaptive and tracking regret bounds for standard non-sleeping <b>bandits.</b> Extending our results to the <b>bandit</b> version of experts that report their confidences leads to new bounds for the confidence regret that depends primarily on the sum of experts&rsquo; confidences. We prove a lower bound, showing that for any minimax optimal algorithms, there exists an action whose regret is sublinear in $T$ but linear in the number of its active rounds.</p></p class="citation"></blockquote><h3 id=2733--53119-acme-ad-accelerated-model-explanations-for-anomaly-detection-valentina-zaccaria-et-al-2024>(27/33 | 53/119) AcME-AD: Accelerated Model Explanations for Anomaly Detection (Valentina Zaccaria et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valentina Zaccaria, David Dandolo, Chiara Masiero, Gian Antonio Susto. (2024)<br><strong>AcME-AD: Accelerated Model Explanations for Anomaly Detection</strong><br><button class=copy-to-clipboard title="AcME-AD: Accelerated Model Explanations for Anomaly Detection" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01245v1.pdf filename=2403.01245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pursuing fast and robust interpretability in <b>Anomaly</b> <b>Detection</b> is crucial, especially due to its significance in practical applications. Traditional <b>Anomaly</b> <b>Detection</b> methods excel in outlier identification but are often black-boxes, providing scant insights into their decision-making process. This lack of transparency compromises their reliability and hampers their adoption in scenarios where comprehending the reasons behind <b>anomaly</b> <b>detection</b> is vital. At the same time, getting explanations quickly is paramount in practical scenarios. To bridge this gap, we present AcME-AD, a novel approach rooted in Explainable Artificial Intelligence principles, designed to clarify <b>Anomaly</b> <b>Detection</b> models for tabular data. AcME-AD transcends the constraints of model-specific or resource-heavy explainability techniques by delivering a model-agnostic, efficient solution for interoperability. It offers local feature importance scores and a what-if analysis tool, shedding light on the factors contributing to each <b>anomaly,</b> <b>thus</b> aiding root cause analysis and decision-making. This paper elucidates AcME-AD&rsquo;s foundation, its benefits over existing methods, and validates its effectiveness with tests on both synthetic and real datasets. AcME-AD&rsquo;s implementation and experiment replication code is accessible in a public repository.</p></p class="citation"></blockquote><h3 id=2833--54119-inexact-unlearning-needs-more-careful-evaluations-to-avoid-a-false-sense-of-privacy-jamie-hayes-et-al-2024>(28/33 | 54/119) Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy (Jamie Hayes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, Nicolas Papernot. (2024)<br><strong>Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy</strong><br><button class=copy-to-clipboard title="Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01218v1.pdf filename=2403.01218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model&rsquo;s training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their <code>U-MIA'' counterparts). We propose a categorization of existing U-MIAs into </code>population U-MIAs&rsquo;&rsquo;, where the same attacker is instantiated for all examples, and ``per-example U-MIAs&rsquo;&rsquo;, where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each example under attack, is significantly stronger. Indeed, our results show that the commonly used U-MIAs in the unlearning literature overestimate the privacy protection afforded by existing unlearning techniques on both vision and language models. Our investigation reveals a large variance in the vulnerability of different examples to per-example U-MIAs. In fact, several unlearning algorithms lead to a reduced vulnerability for some, but not all, examples that we wish to unlearn, at the expense of increasing it for other examples. Notably, we find that the privacy protection for the remaining training examples may worsen as a consequence of unlearning. We also discuss the fundamental difficulty of equally protecting all examples using existing unlearning schemes, due to the different rates at which examples are unlearned. We demonstrate that naive attempts at tailoring unlearning stopping criteria to different examples fail to alleviate these issues.</p></p class="citation"></blockquote><h3 id=2933--55119-training-unbiased-diffusion-models-from-biased-dataset-yeongmin-kim-et-al-2024>(29/33 | 55/119) Training Unbiased Diffusion Models From Biased Dataset (Yeongmin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeongmin Kim, Byeonghu Na, Minsang Park, JoonHo Jang, Dongjun Kim, Wanmo Kang, Il-Chul Moon. (2024)<br><strong>Training Unbiased Diffusion Models From Biased Dataset</strong><br><button class=copy-to-clipboard title="Training Unbiased Diffusion Models From Biased Dataset" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01189v1.pdf filename=2403.01189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With significant advancements in <b>diffusion</b> <b>models,</b> addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes time-dependent importance reweighting to mitigate the bias for the <b>diffusion</b> <b>models.</b> We demonstrate that the time-dependent density ratio becomes more precise than previous approaches, thereby minimizing error propagation in generative learning. While directly applying it to score-matching is intractable, we discover that using the time-dependent density ratio both for reweighting and score correction can lead to a tractable form of the objective function to regenerate the unbiased data density. Furthermore, we theoretically establish a connection with traditional score-matching, and we demonstrate its convergence to an unbiased distribution. The experimental evidence supports the usefulness of the proposed method, which outperforms baselines including time-independent importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various bias settings. Our code is available at <a href=https://github.com/alsdudrla10/TIW-DSM>https://github.com/alsdudrla10/TIW-DSM</a>.</p></p class="citation"></blockquote><h3 id=3033--56119-mpipn-a-multi-physics-informed-pointnet-for-solving-parametric-acoustic-structure-systems-chu-wang-et-al-2024>(30/33 | 56/119) MPIPN: A Multi Physics-Informed PointNet for solving parametric acoustic-structure systems (Chu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chu Wang, Jinhong Wu, Yanzhi Wang, Zhijian Zha, Qi Zhou. (2024)<br><strong>MPIPN: A Multi Physics-Informed PointNet for solving parametric acoustic-structure systems</strong><br><button class=copy-to-clipboard title="MPIPN: A Multi Physics-Informed PointNet for solving parametric acoustic-structure systems" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01132v1.pdf filename=2403.01132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning is employed for solving physical systems governed by general nonlinear partial differential equations (PDEs). However, complex multi-physics systems such as acoustic-structure coupling are often described by a series of PDEs that incorporate variable physical quantities, which are referred to as parametric systems. There are lack of strategies for solving parametric systems governed by PDEs that involve explicit and implicit quantities. In this paper, a deep learning-based Multi Physics-Informed PointNet (MPIPN) is proposed for solving parametric acoustic-structure systems. First, the MPIPN induces an enhanced point-cloud architecture that encompasses explicit physical quantities and geometric features of computational domains. Then, the MPIPN extracts local and global features of the reconstructed point-cloud as parts of solving criteria of parametric systems, respectively. Besides, implicit physical quantities are embedded by encoding techniques as another part of solving criteria. Finally, all solving criteria that characterize parametric systems are amalgamated to form distinctive sequences as the input of the MPIPN, whose outputs are solutions of systems. The proposed framework is trained by adaptive physics-informed loss functions for corresponding computational domains. The framework is generalized to deal with new parametric conditions of systems. The effectiveness of the MPIPN is validated by applying it to solve steady parametric acoustic-structure coupling systems governed by the Helmholtz equations. An ablation experiment has been implemented to demonstrate the efficacy of physics-informed impact with a minority of <b>supervised</b> data. The proposed method yields reasonable precision across all computational domains under constant parametric conditions and changeable combinations of parametric conditions for acoustic-structure systems.</p></p class="citation"></blockquote><h3 id=3133--57119-efficient-episodic-memory-utilization-of-cooperative-multi-agent-reinforcement-learning-hyungho-na-et-al-2024>(31/33 | 57/119) Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning (Hyungho Na et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyungho Na, Yunkyeong Seo, Il-chul Moon. (2024)<br><strong>Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MA, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01112v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01112v2.pdf filename=2403.01112v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In cooperative multi-agent <b>reinforcement</b> <b>learning</b> (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating <b>reinforcement</b> <b>learning</b> by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=3233--58119-continuous-mean-zero-disagreement-regularized-imitation-learning-cmz-dril-noah-ford-et-al-2024>(32/33 | 58/119) Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL) (Noah Ford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noah Ford, Ryan W. Gardner, Austin Juhl, Nathan Larson. (2024)<br><strong>Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL)</strong><br><button class=copy-to-clipboard title="Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL)" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01059v1.pdf filename=2403.01059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine-learning paradigms such as imitation learning and <b>reinforcement</b> <b>learning</b> can generate highly performant agents in a variety of complex environments. However, commonly used methods require large quantities of data and/or a known reward function. This paper presents a method called Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL) that employs a novel reward structure to improve the performance of imitation-learning agents that have access to only a handful of expert demonstrations. CMZ-DRIL uses <b>reinforcement</b> <b>learning</b> to minimize uncertainty among an ensemble of agents trained to model the expert demonstrations. This method does not use any environment-specific rewards, but creates a continuous and mean-zero reward function from the action disagreement of the agent ensemble. As demonstrated in a waypoint-navigation environment and in two MuJoCo environments, CMZ-DRIL can generate performant agents that behave more similarly to the expert than primary previous approaches in several key metrics.</p></p class="citation"></blockquote><h3 id=3333--59119-graphrcg-self-conditioned-graph-generation-via-bootstrapped-representations-song-wang-et-al-2024>(33/33 | 59/119) GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations (Song Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Song Wang, Zhen Tan, Xinyu Zhao, Tianlong Chen, Huan Liu, Jundong Li. (2024)<br><strong>GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations</strong><br><button class=copy-to-clipboard title="GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01071v1.pdf filename=2403.01071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> generation generally aims to create new <b>graphs</b> that closely align with a specific <b>graph</b> distribution. Existing works often implicitly capture this distribution through the optimization of generators, potentially overlooking the intricacies of the distribution itself. Furthermore, these approaches generally neglect the insights offered by the learned distribution for <b>graph</b> generation. In contrast, in this work, we propose a novel self-conditioned <b>graph</b> generation framework designed to explicitly model <b>graph</b> distributions and employ these distributions to guide the generation process. We first perform self-conditioned modeling to capture the <b>graph</b> distributions by transforming each <b>graph</b> sample into a low-dimensional representation and optimizing a representation generator to create new representations reflective of the learned distribution. Subsequently, we leverage these bootstrapped representations as self-conditioned guidance for the generation process, thereby facilitating the generation of <b>graphs</b> that more accurately reflect the learned distributions. We conduct extensive experiments on generic and molecular <b>graph</b> datasets across various fields. Our framework demonstrates superior performance over existing state-of-the-art <b>graph</b> generation methods in terms of <b>graph</b> quality and fidelity to training data.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--60119-automatic-speech-recognition-using-advanced-deep-learning-approaches-a-survey-hamza-kheddar-et-al-2024>(1/2 | 60/119) Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey (Hamza Kheddar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamza Kheddar, Mustapha Hemis, Yassine Himeur. (2024)<br><strong>Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey</strong><br><button class=copy-to-clipboard title="Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS, eess-SP<br>Keyword Score: 70<br>Keywords: Federated Learning, Reinforcement Learning, Transfer Learning, Transformer, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01255v1.pdf filename=2403.01255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in deep learning (DL) have posed a significant challenge for <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR).</b> <b>ASR</b> relies on extensive training datasets, including confidential ones, and demands substantial computational and storage resources. Enabling adaptive systems improves <b>ASR</b> performance in dynamic environments. DL techniques assume training and testing data originate from the same domain, which is not always true. Advanced DL techniques like deep <b>transfer</b> <b>learning</b> (DTL), <b>federated</b> <b>learning</b> (FL), and <b>reinforcement</b> <b>learning</b> (RL) address these issues. DTL allows high-performance models using small yet related datasets, FL enables training on confidential data without dataset possession, and RL optimizes decision-making in dynamic environments, reducing computation costs. This survey offers a comprehensive review of DTL, FL, and RL-based <b>ASR</b> frameworks, aiming to provide insights into the latest developments and aid researchers and professionals in understanding the current challenges. Additionally, <b>transformers,</b> which are advanced DL techniques heavily used in proposed <b>ASR</b> frameworks, are considered in this survey for their ability to capture extensive dependencies in the input <b>ASR</b> sequence. The paper starts by presenting the background of DTL, FL, RL, and <b>Transformers</b> and then adopts a well-designed taxonomy to outline the state-of-the-art approaches. Subsequently, a critical analysis is conducted to identify the strengths and weaknesses of each framework. Additionally, a comparative study is presented to highlight the existing challenges, paving the way for future research opportunities.</p></p class="citation"></blockquote><h3 id=22--61119-enhancing-audio-generation-diversity-with-visual-information-zeyu-xie-et-al-2024>(2/2 | 61/119) Enhancing Audio Generation Diversity with Visual Information (Zeyu Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Xie, Baihan Li, Xuenan Xu, Mengyue Wu, Kai Yu. (2024)<br><strong>Enhancing Audio Generation Diversity with Visual Information</strong><br><button class=copy-to-clipboard title="Enhancing Audio Generation Diversity with Visual Information" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: I-2, cs-SD, cs.SD, eess-AS<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01278v1.pdf filename=2403.01278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio and sound generation has garnered significant attention in recent years, with a primary focus on improving the quality of generated audios. However, there has been limited research on enhancing the diversity of generated audio, particularly when it comes to audio generation within specific categories. Current models tend to produce homogeneous audio samples within a category. This work aims to address this limitation by improving the diversity of generated audio with visual information. We propose a <b>clustering-based</b> method, leveraging visual information to guide the model in generating distinct audio content within each category. Results on seven categories indicate that extra visual input can largely enhance audio generation diversity. Audio samples are available at <a href=https://zeyuxie29.github.io/DiverseAudioGeneration>https://zeyuxie29.github.io/DiverseAudioGeneration</a>.</p></p class="citation"></blockquote><h2 id=cscv-29>cs.CV (29)</h2><h3 id=129--62119-dna-family-boosting-weight-sharing-nas-with-block-wise-supervisions-guangrun-wang-et-al-2024>(1/29 | 62/119) DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions (Guangrun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangrun Wang, Changlin Li, Liuchun Yuan, Jiefeng Peng, Xiaoyu Xian, Xiaodan Liang, Xiaojun Chang, Liang Lin. (2024)<br><strong>DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions</strong><br><button class=copy-to-clipboard title="DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Knowledge Distillation, Multi-modal, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01326v1.pdf filename=2403.01326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Architecture Search (NAS), aiming at automatically designing neural architectures by machines, has been considered a key step toward automatic machine learning. One notable NAS branch is the weight-sharing NAS, which significantly improves search efficiency and allows NAS algorithms to run on ordinary computers. Despite receiving high expectations, this category of methods suffers from low search effectiveness. By employing a generalization boundedness tool, we demonstrate that the devil behind this drawback is the untrustworthy architecture rating with the oversized search space of the possible architectures. Addressing this problem, we modularize a large search space into blocks with small search spaces and develop a family of models with the <b>distilling</b> neural architecture (DNA) techniques. These proposed models, namely a DNA family, are capable of resolving multiple dilemmas of the weight-sharing NAS, such as scalability, efficiency, and <b>multi-modal</b> compatibility. Our proposed DNA models can rate all architecture candidates, as opposed to previous works that can only access a subsearch space using heuristic algorithms. Moreover, under a certain computational complexity constraint, our method can seek architectures with different depths and widths. Extensive experimental evaluations show that our models achieve state-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobile <b>convolutional</b> <b>network</b> and a small <b>vision</b> <b>transformer,</b> respectively. Additionally, we provide in-depth empirical analysis and insights into neural architecture ratings. Codes available: \url{https://github.com/changlin31/DNA}.</p></p class="citation"></blockquote><h3 id=229--63119-data-free-multi-label-image-recognition-via-llm-powered-prompt-tuning-shuo-yang-et-al-2024>(2/29 | 63/119) Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning (Shuo Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuo Yang, Zirui Shang, Yongqi Wang, Derong Deng, Hongwei Chen, Qiyuan Cheng, Xinxiao Wu. (2024)<br><strong>Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning</strong><br><button class=copy-to-clipboard title="Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01209v1.pdf filename=2403.01209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel framework for multi-label image recognition without any training data, called data-free framework, which uses knowledge of pre-trained <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to learn <b>prompts</b> <b>to</b> adapt pretrained <b>Vision-Language</b> Model (VLM) like CLIP to multilabel classification. Through asking <b>LLM</b> by well-designed questions, we acquire comprehensive knowledge about characteristics and contexts of objects, which provides valuable text descriptions for learning <b>prompts.</b> <b>Then</b> we propose a hierarchical <b>prompt</b> <b>learning</b> method by taking the multi-label dependency into consideration, wherein a subset of category-specific <b>prompt</b> <b>tokens</b> are shared when the corresponding objects exhibit similar attributes or are more likely to co-occur. Benefiting from the remarkable alignment between visual and linguistic semantics of CLIP, the hierarchical <b>prompts</b> <b>learned</b> from text descriptions are applied to perform classification of images during inference. Our framework presents a new way to explore the synergies between multiple pre-trained models for novel category recognition. Extensive experiments on three public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that our method achieves better results than the state-of-the-art methods, especially outperforming the <b>zero-shot</b> multi-label recognition methods by 4.7% in mAP on MS-COCO.</p></p class="citation"></blockquote><h3 id=329--64119-scenecraft-an-llm-agent-for-synthesizing-3d-scene-as-blender-code-ziniu-hu-et-al-2024>(3/29 | 64/119) SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code (Ziniu Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi. (2024)<br><strong>SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code</strong><br><button class=copy-to-clipboard title="SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Graph, Foundation Model, GPT, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01248v1.pdf filename=2403.01248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces SceneCraft, a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene <b>graph</b> as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this <b>graph,</b> translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of <b>vision-language</b> <b>foundation</b> <b>models</b> like <b>GPT-V</b> to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive <b>LLM</b> parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing <b>LLM-based</b> agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.</p></p class="citation"></blockquote><h3 id=429--65119-learn-suspected-anomalies-from-event-prompts-for-video-anomaly-detection-chenchen-tao-et-al-2024>(4/29 | 65/119) Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection (Chenchen Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenchen Tao, Chong Wang, Yuexian Zou, Xiaohao Peng, Jiafei Wu, Jiangbo Qian. (2024)<br><strong>Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection</strong><br><button class=copy-to-clipboard title="Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Anomaly Detection, Multiple Instance Learning, Supervised Learning, Weakly-supervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01169v1.pdf filename=2403.01169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most models for weakly <b>supervised</b> video <b>anomaly</b> <b>detection</b> (WS-VAD) rely on <b>multiple</b> <b>instance</b> <b>learning,</b> aiming to distinguish normal and abnormal snippets without specifying the type of <b>anomaly.</b> <b>The</b> ambiguous nature of <b>anomaly</b> <b>definitions</b> across contexts introduces bias in detecting abnormal and normal snippets within the abnormal bag. Taking the first step to show the model why it is anomalous, a novel framework is proposed to guide the learning of suspected anomalies from event <b>prompts.</b> Given a textual <b>prompt</b> dictionary of potential <b>anomaly</b> <b>events</b> and the captions generated from <b>anomaly</b> <b>videos,</b> the semantic <b>anomaly</b> <b>similarity</b> between them could be calculated to identify the suspected anomalous events for each video snippet. It enables a new multi-prompt learning process to constrain the visual-semantic features across all videos, as well as provides a new way to label pseudo anomalies for self-training. To demonstrate effectiveness, comprehensive experiments and detailed ablation studies are conducted on four datasets, namely XD-Violence, UCF-Crime, TAD, and ShanghaiTech. Our proposed model outperforms most state-of-the-art methods in terms of AP or AUC (82.6%, 87.7%, 93.1%, and 97.4%). Furthermore, it shows promising performance in open-set and cross-dataset cases.</p></p class="citation"></blockquote><h3 id=529--66119-text-guided-explorable-image-super-resolution-kanchana-vaishnavi-gandikota-et-al-2024>(5/29 | 66/119) Text-guided Explorable Image Super-resolution (Kanchana Vaishnavi Gandikota et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanchana Vaishnavi Gandikota, Paramanand Chandramouli. (2024)<br><strong>Text-guided Explorable Image Super-resolution</strong><br><button class=copy-to-clipboard title="Text-guided Explorable Image Super-resolution" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Zero-shot, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01124v1.pdf filename=2403.01124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce the problem of <b>zero-shot</b> text-guided exploration of the solutions to open-domain image super-resolution. Our goal is to allow users to explore diverse, semantically accurate reconstructions that preserve data consistency with the low-resolution inputs for different large downsampling factors without explicitly training for these specific degradations. We propose two approaches for <b>zero-shot</b> text-guided super-resolution - i) modifying the generative process of <b>text-to-image</b> \textit{T2I} <b>diffusion</b> <b>models</b> to promote consistency with low-resolution inputs, and ii) incorporating language guidance into <b>zero-shot</b> <b>diffusion-based</b> <b>restoration</b> methods. We show that the proposed approaches result in diverse solutions that match the semantic meaning provided by the text <b>prompt</b> while preserving data consistency with the degraded inputs. We evaluate the proposed baselines for the task of extreme super-resolution and demonstrate advantages in terms of restoration quality, diversity, and explorability of solutions.</p></p class="citation"></blockquote><h3 id=629--67119-ela-efficient-local-attention-for-deep-convolutional-neural-networks-wei-xu-et-al-2024>(6/29 | 67/119) ELA: Efficient Local Attention for Deep Convolutional Neural Networks (Wei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Xu, Yi Wan. (2024)<br><strong>ELA: Efficient Local Attention for Deep Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="ELA: Efficient Local Attention for Deep Convolutional Neural Networks" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01123v1.pdf filename=2403.01123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The attention mechanism has gained significant recognition in the field of computer vision due to its ability to effectively enhance the performance of deep neural networks. However, existing methods often struggle to effectively utilize spatial information or, if they do, they come at the cost of reducing channel dimensions or increasing the complexity of neural networks. In order to address these limitations, this paper introduces an Efficient Local Attention (ELA) method that achieves substantial performance improvements with a simple structure. By analyzing the limitations of the Coordinate Attention method, we identify the lack of generalization ability in Batch Normalization, the adverse effects of dimension reduction on channel attention, and the complexity of attention generation process. To overcome these challenges, we propose the incorporation of 1D <b>convolution</b> and Group Normalization feature enhancement techniques. This approach enables accurate localization of regions of interest by efficiently encoding two 1D positional feature maps without the need for dimension reduction, while allowing for a lightweight implementation. We carefully design three hyperparameters in ELA, resulting in four different versions: ELA-T, ELA-B, ELA-S, and ELA-L, to cater to the specific requirements of different visual tasks such as image classification, <b>object</b> <b>detection</b> and sementic segmentation. ELA can be seamlessly integrated into deep <b>CNN</b> networks such as ResNet, MobileNet, and DeepLab. Extensive evaluations on the ImageNet, MSCOCO, and Pascal VOC datasets demonstrate the superiority of the proposed ELA module over current state-of-the-art methods in all three aforementioned visual tasks.</p></p class="citation"></blockquote><h3 id=729--68119-dual-graph-attention-based-disentanglement-multiple-instance-learning-for-brain-age-estimation-fanzhe-yan-et-al-2024>(7/29 | 68/119) Dual Graph Attention based Disentanglement Multiple Instance Learning for Brain Age Estimation (Fanzhe Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanzhe Yan, Gang Yang, Yu Li, Aiping Liu, Xun Chen. (2024)<br><strong>Dual Graph Attention based Disentanglement Multiple Instance Learning for Brain Age Estimation</strong><br><button class=copy-to-clipboard title="Dual Graph Attention based Disentanglement Multiple Instance Learning for Brain Age Estimation" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Graph, Convolution, Convolutional Neural Network, Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01246v1.pdf filename=2403.01246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning techniques have demonstrated great potential for accurately estimating brain age by analyzing Magnetic Resonance Imaging (MRI) data from healthy individuals. However, current methods for brain age estimation often directly utilize whole input images, overlooking two important considerations: 1) the heterogeneous nature of brain aging, where different brain regions may degenerate at different rates, and 2) the existence of age-independent redundancies in brain structure. To overcome these limitations, we propose a Dual <b>Graph</b> Attention based Disentanglement Multi-instance Learning (DGA-DMIL) framework for improving brain age estimation. Specifically, the 3D MRI data, treated as a bag of instances, is fed into a 2D <b>convolutional</b> <b>neural</b> <b>network</b> backbone, to capture the unique aging patterns in MRI. A dual <b>graph</b> attention aggregator is then proposed to learn the backbone features by exploiting the intra- and inter-instance relationships. Furthermore, a disentanglement branch is introduced to separate age-related features from age-independent structural representations to ameliorate the interference of redundant information on age prediction. To verify the effectiveness of the proposed framework, we evaluate it on two datasets, UK Biobank and ADNI, containing a total of 35,388 healthy individuals. Our proposed model demonstrates exceptional accuracy in estimating brain age, achieving a remarkable mean absolute error of 2.12 years in the UK Biobank. The results establish our approach as state-of-the-art compared to other competing brain age estimation models. In addition, the instance contribution scores identify the varied importance of brain areas for aging prediction, which provides deeper insights into the understanding of brain aging.</p></p class="citation"></blockquote><h3 id=829--69119-on-the-road-to-portability-compressing-end-to-end-motion-planner-for-autonomous-driving-kaituo-feng-et-al-2024>(8/29 | 69/119) On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving (Kaituo Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaituo Feng, Changsheng Li, Dongchun Ren, Ye Yuan, Guoren Wang. (2024)<br><strong>On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving</strong><br><button class=copy-to-clipboard title="On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01238v1.pdf filename=2403.01238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-end motion planning models equipped with deep neural networks have shown great potential for enabling full autonomous driving. However, the oversized neural networks render them impractical for deployment on resource-constrained systems, which unavoidably requires more computational time and resources during reference.To handle this, <b>knowledge</b> <b>distillation</b> offers a promising approach that compresses models by enabling a smaller student model to learn from a larger teacher model. Nevertheless, how to apply <b>knowledge</b> <b>distillation</b> to compress motion planners has not been explored so far. In this paper, we propose PlanKD, the first <b>knowledge</b> <b>distillation</b> framework tailored for compressing end-to-end motion planners. First, considering that driving scenes are inherently complex, often containing planning-irrelevant or even noisy information, transferring such information is not beneficial for the student planner. Thus, we design an information bottleneck based strategy to only <b>distill</b> planning-relevant information, rather than transfer all information indiscriminately. Second, different waypoints in an output planned trajectory may hold varying degrees of importance for motion planning, where a slight deviation in certain crucial waypoints might lead to a collision. Therefore, we devise a safety-aware waypoint-attentive <b>distillation</b> module that assigns adaptive weights to different waypoints based on the importance, to encourage the student to accurately mimic more crucial waypoints, thereby improving overall safety. Experiments demonstrate that our PlanKD can boost the performance of smaller planners by a large margin, and significantly reduce their reference time.</p></p class="citation"></blockquote><h3 id=929--70119-sar-ae-sfp-sar-imagery-adversarial-example-in-real-physics-domain-with-target-scattering-feature-parameters-jiahao-cui-et-al-2024>(9/29 | 70/119) SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with Target Scattering Feature Parameters (Jiahao Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Cui, Jiale Duan, Binyan Luo, Hang Cao, Wang Guo, Haifeng Li. (2024)<br><strong>SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with Target Scattering Feature Parameters</strong><br><button class=copy-to-clipboard title="SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with Target Scattering Feature Parameters" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Transformer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01210v1.pdf filename=2403.01210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural network-based Synthetic Aperture Radar (SAR) target recognition models are susceptible to <b>adversarial</b> <b>examples.</b> Current <b>adversarial</b> <b>example</b> generation methods for SAR imagery primarily operate in the 2D digital domain, known as image <b>adversarial</b> <b>examples.</b> Recent work, while considering SAR imaging scatter mechanisms, fails to account for the actual imaging process, rendering attacks in the three-dimensional physical domain infeasible, termed pseudo physics <b>adversarial</b> <b>examples.</b> To address these challenges, this paper proposes SAR-AE-SFP-Attack, a method to generate real physics <b>adversarial</b> <b>examples</b> by altering the scattering feature parameters of target objects. Specifically, we iteratively optimize the coherent energy accumulation of the target echo by perturbing the reflection coefficient and scattering coefficient in the scattering feature parameters of the three-dimensional target object, and obtain the <b>adversarial</b> <b>example</b> after echo signal processing and imaging processing in the RaySAR simulator. Experimental results show that compared to digital <b>adversarial</b> <b>attack</b> methods, SAR-AE-SFP Attack significantly improves attack efficiency on <b>CNN-based</b> models (over 30%) and <b>Transformer-based</b> models (over 13%), demonstrating significant transferability of attack effects across different models and perspectives.</p></p class="citation"></blockquote><h3 id=1029--71119-leveraging-self-supervised-learning-for-scene-recognition-in-child-sexual-abuse-imagery-pedro-h-v-valois-et-al-2024>(10/29 | 71/119) Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery (Pedro H. V. Valois et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pedro H. V. Valois, João Macedo, Leo S. F. Ribeiro, Jefersson A. dos Santos, Sandra Avila. (2024)<br><strong>Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery</strong><br><button class=copy-to-clipboard title="Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-CY, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01183v1.pdf filename=2403.01183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people&rsquo;s well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing & Exploited Children every year, and over 80% originated from online sources. Therefore, investigation centers and clearinghouses cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene recognition task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive material. The scarcity and limitations of working with child sexual abuse images lead to <b>self-supervised</b> <b>learning,</b> a machine-learning methodology that leverages unlabeled data to produce powerful representations that can be more easily transferred to target tasks. This work shows that <b>self-supervised</b> <b>deep</b> learning models pre-trained on scene-centric data can reach 71.6% balanced accuracy on our indoor scene classification task and, on average, 2.2 percentage points better performance than a fully <b>supervised</b> version. We cooperate with Brazilian Federal Police experts to evaluate our indoor classification model on actual child abuse material. The results demonstrate a notable discrepancy between the features observed in widely used scene datasets and those depicted on sensitive materials.</p></p class="citation"></blockquote><h3 id=1129--72119-face-swap-via-diffusion-model-feifei-wang-2024>(11/29 | 72/119) Face Swap via Diffusion Model (Feifei Wang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feifei Wang. (2024)<br><strong>Face Swap via Diffusion Model</strong><br><button class=copy-to-clipboard title="Face Swap via Diffusion Model" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: ControlNet, Diffusion Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01108v1.pdf filename=2403.01108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This technical report presents a <b>diffusion</b> <b>model</b> based framework for face swapping between two portrait images. The basic framework consists of three components, i.e., IP-Adapter, <b>ControlNet,</b> and Stable <b>Diffusion&rsquo;s</b> <b>inpainting</b> pipeline, for face feature encoding, multi-conditional generation, and face inpainting respectively. Besides, I introduce facial guidance optimization and CodeFormer based blending to further improve the generation quality. Specifically, we engage a recent light-weighted customization method (i.e., DreamBooth-LoRA), to guarantee the identity consistency by 1) using a rare identifier &ldquo;sks&rdquo; to represent the source identity, and 2) injecting the image features of source portrait into each cross-attention layer like the <b>text</b> <b>features.</b> Then I resort to the strong inpainting ability of Stable <b>Diffusion,</b> <b>and</b> utilize canny image and face detection annotation of the target portrait as the conditions, to guide ContorlNet&rsquo;s generation and align source portrait with the target portrait. To further correct face alignment, we add the facial guidance loss to optimize the <b>text</b> <b>embedding</b> during the sample generation.</p></p class="citation"></blockquote><h3 id=1229--73119-auxiliary-tasks-enhanced-dual-affinity-learning-for-weakly-supervised-semantic-segmentation-lian-xu-et-al-2024>(12/29 | 73/119) Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised Semantic Segmentation (Lian Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lian Xu, Mohammed Bennamoun, Farid Boussaid, Wanli Ouyang, Ferdous Sohel, Dan Xu. (2024)<br><strong>Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised Semantic Segmentation" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01156v1.pdf filename=2403.01156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most existing weakly <b>supervised</b> semantic segmentation (WSSS) methods rely on Class Activation Mapping (CAM) to extract coarse class-specific localization maps using image-level labels. Prior works have commonly used an off-line heuristic thresholding process that combines the CAM maps with off-the-shelf saliency maps produced by a general pre-trained saliency model to produce more accurate pseudo-segmentation labels. We propose AuxSegNet+, a weakly <b>supervised</b> auxiliary learning framework to explore the rich information from these saliency maps and the significant inter-task correlation between saliency detection and semantic segmentation. In the proposed AuxSegNet+, saliency detection and multi-label image classification are used as auxiliary tasks to improve the primary task of semantic segmentation with only image-level ground-truth labels. We also propose a cross-task affinity learning mechanism to learn pixel-level affinities from the saliency and segmentation feature maps. In particular, we propose a cross-task dual-affinity learning module to learn both pairwise and unary affinities, which are used to enhance the task-specific features and predictions by aggregating both query-dependent and query-independent global context for both saliency detection and semantic segmentation. The learned cross-task pairwise affinity can also be used to refine and propagate CAM maps to provide better pseudo labels for both tasks. Iterative improvement of segmentation performance is enabled by cross-task affinity learning and pseudo-label updating. Extensive experiments demonstrate the effectiveness of the proposed approach with new state-of-the-art WSSS results on the challenging PASCAL VOC and MS COCO <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1329--74119-adversarial-testing-for-visual-grounding-via-image-aware-property-reduction-zhiyuan-chang-et-al-2024>(13/29 | 74/119) Adversarial Testing for Visual Grounding via Image-Aware Property Reduction (Zhiyuan Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Chang, Mingyang Li, Junjie Wang, Cheng Li, Boyu Wu, Fanjiang Xu, Qing Wang. (2024)<br><strong>Adversarial Testing for Visual Grounding via Image-Aware Property Reduction</strong><br><button class=copy-to-clipboard title="Adversarial Testing for Visual Grounding via Image-Aware Property Reduction" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 21<br>Keywords: Black Box, Multi-modal, Multi-modal, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01118v1.pdf filename=2403.01118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the advantages of fusing information from various modalities, <b>multimodal</b> learning is gaining increasing attention. Being a fundamental task of <b>multimodal</b> learning, Visual <b>Grounding</b> (VG), aims to locate objects in images through natural language expressions. Ensuring the quality of VG models presents significant challenges due to the complex nature of the task. In the <b>black</b> <b>box</b> scenario, existing adversarial testing techniques often fail to fully exploit the potential of both modalities of information. They typically apply perturbations based solely on either the image or text information, disregarding the crucial correlation between the two modalities, which would lead to failures in test oracles or an inability to effectively challenge VG models. To this end, we propose PEELING, a text perturbation approach via image-aware property reduction for adversarial testing of the VG model. The core idea is to reduce the property-related information in the original expression meanwhile ensuring the reduced expression can still uniquely describe the original object in the image. To achieve this, PEELING first conducts the object and properties extraction and recombination to generate candidate property reduction expressions. It then selects the satisfied expressions that accurately describe the original object while ensuring no other objects in the image fulfill the expression, through querying the image with a visual understanding technique. We evaluate PEELING on the state-of-the-art VG model, i.e. OFA-VG, involving three commonly used datasets. Results show that the adversarial tests generated by PEELING achieves 21.4% in <b>MultiModal</b> Impact score (MMI), and outperforms state-of-the-art baselines for images and texts by 8.2%&ndash;15.1%.</p></p class="citation"></blockquote><h3 id=1429--75119-boosting-box-supervised-instance-segmentation-with-pseudo-depth-xinyi-yu-et-al-2024>(14/29 | 75/119) Boosting Box-supervised Instance Segmentation with Pseudo Depth (Xinyi Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyi Yu, Ling Yan, Pengtao Jiang, Hao Chen, Bo Li, Lin Yuanbo Wu, Linlin Ou. (2024)<br><strong>Boosting Box-supervised Instance Segmentation with Pseudo Depth</strong><br><button class=copy-to-clipboard title="Boosting Box-supervised Instance Segmentation with Pseudo Depth" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01214v1.pdf filename=2403.01214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The realm of Weakly <b>Supervised</b> Instance Segmentation (WSIS) under box supervision has garnered substantial attention, showcasing remarkable advancements in recent years. However, the limitations of box supervision become apparent in its inability to furnish effective information for distinguishing foreground from background within the specified target box. This research addresses this challenge by introducing pseudo-depth maps into the training process of the instance segmentation network, thereby boosting its performance by capturing depth differences between instances. These pseudo-depth maps are generated using a readily available depth predictor and are not necessary during the inference stage. To enable the network to discern depth features when predicting masks, we integrate a depth prediction layer into the mask prediction head. This innovative approach empowers the network to simultaneously predict masks and depth, enhancing its ability to capture nuanced depth-related information during the instance segmentation process. We further utilize the mask generated in the training process as supervision to distinguish the foreground from the background. When selecting the best mask for each box through the Hungarian algorithm, we use depth consistency as one calculation cost item. The proposed method achieves significant improvements on Cityscapes and COCO dataset.</p></p class="citation"></blockquote><h3 id=1529--76119-tcig-two-stage-controlled-image-generation-with-quality-enhancement-through-diffusion-salaheldin-mohamed-2024>(15/29 | 76/119) TCIG: Two-Stage Controlled Image Generation with Quality Enhancement through Diffusion (Salaheldin Mohamed, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salaheldin Mohamed. (2024)<br><strong>TCIG: Two-Stage Controlled Image Generation with Quality Enhancement through Diffusion</strong><br><button class=copy-to-clipboard title="TCIG: Two-Stage Controlled Image Generation with Quality Enhancement through Diffusion" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01212v1.pdf filename=2403.01212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, significant progress has been made in the development of <b>text-to-image</b> generation models. However, these models still face limitations when it comes to achieving full controllability during the generation process. Often, specific training or the use of limited models is required, and even then, they have certain restrictions. To address these challenges, A two-stage method that effectively combines controllability and high quality in the generation of images is proposed. This approach leverages the expertise of pre-trained models to achieve precise control over the generated images, while also harnessing the power of <b>diffusion</b> <b>models</b> to achieve state-of-the-art quality. By separating controllability from high quality, This method achieves outstanding results. It is compatible with both latent and image space <b>diffusion</b> <b>models,</b> ensuring versatility and flexibility. Moreover, This approach consistently produces comparable outcomes to the current state-of-the-art methods in the field. Overall, This proposed method represents a significant advancement in <b>text-to-image</b> generation, enabling improved controllability without compromising on the quality of the generated images.</p></p class="citation"></blockquote><h3 id=1629--77119-neural-radiance-fields-based-holography-invited-minsung-kang-et-al-2024>(16/29 | 77/119) Neural radiance fields-based holography [Invited] (Minsung Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba. (2024)<br><strong>Neural radiance fields-based holography [Invited]</strong><br><button class=copy-to-clipboard title="Neural radiance fields-based holography [Invited]" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01137v1.pdf filename=2403.01137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a novel approach for generating holograms based on the neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data is difficult in hologram computation. NeRF is a state-of-the-art technique for 3D light-field reconstruction from 2D images based on volume rendering. The NeRF can rapidly predict new-view images that do not include a training dataset. In this study, we constructed a rendering pipeline directly from a 3D light field generated from 2D images by NeRF for hologram generation using deep neural networks within a reasonable time. The pipeline comprises three main components: the NeRF, a depth predictor, and a hologram generator, all constructed using deep neural networks. The pipeline does not include any physical calculations. The predicted holograms of a 3D scene viewed from any direction were computed using the proposed pipeline. The <b>simulation</b> and experimental results are presented.</p></p class="citation"></blockquote><h3 id=1729--78119-dynamic-3d-point-cloud-sequences-as-2d-videos-yiming-zeng-et-al-2024>(17/29 | 78/119) Dynamic 3D Point Cloud Sequences as 2D Videos (Yiming Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Zeng, Junhui Hou, Qijian Zhang, Siyu Ren, Wenping Wang. (2024)<br><strong>Dynamic 3D Point Cloud Sequences as 2D Videos</strong><br><button class=copy-to-clipboard title="Dynamic 3D Point Cloud Sequences as 2D Videos" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01129v1.pdf filename=2403.01129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic 3D point cloud sequences serve as one of the most common and practical representation modalities of dynamic real-world environments. However, their unstructured nature in both spatial and temporal domains poses significant challenges to effective and efficient processing. Existing deep point cloud sequence modeling approaches imitate the mature 2D video learning mechanisms by developing complex spatio-temporal point neighbor grouping and feature aggregation schemes, often resulting in methods lacking effectiveness, efficiency, and expressive power. In this paper, we propose a novel generic representation called \textit{Structured Point Cloud Videos} (SPCVs). Intuitively, by leveraging the fact that 3D geometric shapes are essentially 2D manifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatial smoothness and temporal consistency, where the pixel values correspond to the 3D coordinates of points. The structured nature of our SPCV representation allows for the seamless adaptation of well-established 2D image/video techniques, enabling efficient and effective processing and analysis of 3D point cloud sequences. To achieve such re-organization, we design a <b>self-supervised</b> <b>learning</b> pipeline that is geometrically regularized and driven by self-reconstructive and deformation field learning objectives. Additionally, we construct SPCV-based frameworks for both low-level and high-level 3D point cloud sequence processing and analysis tasks, including action recognition, temporal interpolation, and compression. Extensive experiments demonstrate the versatility and superiority of the proposed SPCV, which has the potential to offer new possibilities for deep learning on unstructured 3D point cloud sequences. Code will be released at <a href=https://github.com/ZENGYIMING-EAMON/SPCV>https://github.com/ZENGYIMING-EAMON/SPCV</a>.</p></p class="citation"></blockquote><h3 id=1829--79119-extracting-usable-predictions-from-quantized-networks-through-uncertainty-quantification-for-ood-detection-rishi-singhal-et-al-2024>(18/29 | 79/119) Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection (Rishi Singhal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rishi Singhal, Srinath Srinivasan. (2024)<br><strong>Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection</strong><br><button class=copy-to-clipboard title="Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01076v1.pdf filename=2403.01076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>OOD detection has become more pertinent with advances in network design and increased task complexity. Identifying which parts of the data a given network is misclassifying has become as valuable as the network&rsquo;s overall performance. We can compress the model with <b>quantization,</b> but it suffers minor performance loss. The loss of performance further necessitates the need to derive the confidence estimate of the network&rsquo;s predictions. In line with this thinking, we introduce an Uncertainty Quantification(UQ) technique to quantify the uncertainty in the predictions from a pre-trained vision model. We subsequently leverage this information to extract valuable predictions while ignoring the non-confident predictions. We observe that our technique saves up to 80% of ignored samples from being misclassified. The code for the same is available here.</p></p class="citation"></blockquote><h3 id=1929--80119-diffsal-joint-audio-and-video-learning-for-diffusion-saliency-prediction-junwen-xiong-et-al-2024>(19/29 | 80/119) DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction (Junwen Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwen Xiong, Peng Zhang, Tao You, Chuanyue Li, Wei Huang, Yufei Zha. (2024)<br><strong>DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction</strong><br><button class=copy-to-clipboard title="DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Diffusion Model, Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01226v1.pdf filename=2403.01226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio-visual saliency prediction can draw support from diverse modality complements, but further performance enhancement is still challenged by customized architectures as well as task-specific loss functions. In recent studies, denoising <b>diffusion</b> <b>models</b> have shown more promising in unifying task frameworks owing to their inherent ability of generalization. Following this motivation, a novel <b>Diffusion</b> <b>architecture</b> for generalized audio-visual Saliency prediction (DiffSal) is proposed in this work, which formulates the prediction problem as a conditional generative task of the saliency map by utilizing input audio and video as the conditions. Based on the spatio-temporal audio-visual features, an extra network Saliency-UNet is designed to perform <b>multi-modal</b> attention modulation for progressive refinement of the ground-truth saliency map from the noisy map. Extensive experiments demonstrate that the proposed DiffSal can achieve excellent performance across six challenging audio-visual <b>benchmarks,</b> with an average relative improvement of 6.3% over the previous state-of-the-art results by six metrics.</p></p class="citation"></blockquote><h3 id=2029--81119-nerf-vpt-learning-novel-view-representations-with-neural-radiance-fields-via-view-prompt-tuning-linsheng-chen-et-al-2024>(20/29 | 81/119) NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning (Linsheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr. (2024)<br><strong>NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning</strong><br><button class=copy-to-clipboard title="NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01325v1.pdf filename=2403.01325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view <b>prompt</b> tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual <b>prompts</b> for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the <b>prompts</b> can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene <b>benchmarks,</b> such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at \url{https://github.com/Freedomcls/NeRF-VPT}.</p></p class="citation"></blockquote><h3 id=2129--82119-tumtraf-v2x-cooperative-perception-dataset-walter-zimmer-et-al-2024>(21/29 | 82/119) TUMTraf V2X Cooperative Perception Dataset (Walter Zimmer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, Alois C. Knoll. (2024)<br><strong>TUMTraf V2X Cooperative Perception Dataset</strong><br><button class=copy-to-clipboard title="TUMTraf V2X Cooperative Perception Dataset" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01316v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01316v1.pdf filename=2403.01316v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety. Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range. External sensors offer higher situational awareness for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative <b>multi-modal</b> fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D <b>object</b> <b>detection</b> and tracking task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, near-miss events, overtaking, and U-turns. Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website: <a href=https://tum-traffic-dataset.github.io/tumtraf-v2x>https://tum-traffic-dataset.github.io/tumtraf-v2x</a>.</p></p class="citation"></blockquote><h3 id=2229--83119-shapeboost-boosting-human-shape-estimation-with-part-based-parameterization-and-clothing-preserving-augmentation-siyuan-bian-et-al-2024>(22/29 | 83/119) ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation (Siyuan Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Bian, Jiefeng Li, Jiasheng Tang, Cewu Lu. (2024)<br><strong>ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation</strong><br><button class=copy-to-clipboard title="ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01345v1.pdf filename=2403.01345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate human shape recovery from a monocular RGB image is a challenging task because humans come in different shapes and sizes and wear different clothes. In this paper, we propose ShapeBoost, a new human shape recovery framework that achieves pixel-level alignment even for rare body shapes and high accuracy for people wearing different types of clothes. Unlike previous approaches that rely on the use of PCA-based shape coefficients, we adopt a new human shape parameterization that decomposes the human shape into bone lengths and the mean width of each part slice. This part-based parameterization technique achieves a balance between flexibility and validity using a semi-analytical shape reconstruction algorithm. Based on this new parameterization, a clothing-preserving <b>data</b> <b>augmentation</b> module is proposed to generate realistic images with diverse body shapes and accurate annotations. Experimental results show that our method outperforms other state-of-the-art methods in diverse body shape situations as well as in varied clothing situations.</p></p class="citation"></blockquote><h3 id=2329--84119-image-based-dietary-assessment-a-healthy-eating-plate-estimation-system-assylzhan-izbassar-et-al-2024>(23/29 | 84/119) Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System (Assylzhan Izbassar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Assylzhan Izbassar, Pakizar Shamoi. (2024)<br><strong>Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System</strong><br><button class=copy-to-clipboard title="Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01310v1.pdf filename=2403.01310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The nutritional quality of diets has significantly deteriorated over the past two to three decades, a decline often underestimated by the people. This deterioration, coupled with a hectic lifestyle, has contributed to escalating health concerns. Recognizing this issue, researchers at Harvard have advocated for a balanced nutritional plate model to promote health. Inspired by this research, our paper introduces an innovative Image-Based Dietary Assessment system aimed at evaluating the healthiness of meals through image analysis. Our system employs advanced image segmentation and classification techniques to analyze food items on a plate, assess their proportions, and calculate meal adherence to Harvard&rsquo;s healthy eating <b>recommendations.</b> This approach leverages machine learning and nutritional science to empower individuals with actionable insights for healthier eating choices. Our four-step framework involves segmenting the image, classifying the items, conducting a nutritional assessment based on the Harvard Healthy Eating Plate research, and offering tailored <b>recommendations.</b> The prototype system has shown promising results in promoting healthier eating habits by providing an accessible, evidence-based tool for dietary assessment.</p></p class="citation"></blockquote><h3 id=2429--85119-fast-low-parameter-video-activity-localization-in-collaborative-learning-environments-venkatesh-jatla-et-al-2024>(24/29 | 85/119) Fast Low-parameter Video Activity Localization in Collaborative Learning Environments (Venkatesh Jatla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Venkatesh Jatla, Sravani Teeparthi, Ugesh Egala, Sylvia Celedon Pattichis, Marios S. Patticis. (2024)<br><strong>Fast Low-parameter Video Activity Localization in Collaborative Learning Environments</strong><br><button class=copy-to-clipboard title="Fast Low-parameter Video Activity Localization in Collaborative Learning Environments" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01281v1.pdf filename=2403.01281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research on video activity detection has primarily focused on identifying well-defined human activities in short video segments. The majority of the research on video activity recognition is focused on the development of large parameter systems that require training on large video datasets. This paper develops a low-parameter, modular system with rapid inferencing capabilities that can be trained entirely on limited datasets without requiring <b>transfer</b> <b>learning</b> from large-parameter systems. The system can accurately detect and associate specific activities with the students who perform the activities in real-life classroom videos. Additionally, the paper develops an interactive web-based application to visualize human activity maps over long real-life classroom videos.</p></p class="citation"></blockquote><h3 id=2529--86119-run-time-introspection-of-2d-object-detection-in-automated-driving-systems-using-learning-representations-hakan-yekta-yatbaz-et-al-2024>(25/29 | 86/119) Run-time Introspection of 2D Object Detection in Automated Driving Systems Using Learning Representations (Hakan Yekta Yatbaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman. (2024)<br><strong>Run-time Introspection of 2D Object Detection in Automated Driving Systems Using Learning Representations</strong><br><button class=copy-to-clipboard title="Run-time Introspection of 2D Object Detection in Automated Driving Systems Using Learning Representations" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01172v1.pdf filename=2403.01172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reliable detection of various <b>objects</b> <b>and</b> road users in the surrounding environment is crucial for the safe operation of automated driving systems (ADS). Despite recent progresses in developing highly accurate <b>object</b> <b>detectors</b> based on Deep Neural Networks (DNNs), they still remain prone to detection errors, which can lead to fatal consequences in safety-critical applications such as ADS. An effective remedy to this problem is to equip the system with run-time monitoring, named as introspection in the context of autonomous systems. Motivated by this, we introduce a novel introspection solution, which operates at the frame level for DNN-based 2D <b>object</b> <b>detection</b> and leverages neural network activation patterns. The proposed approach pre-processes the neural activation patterns of the <b>object</b> <b>detector&rsquo;s</b> backbone using several different modes. To provide extensive comparative analysis and fair comparison, we also adapt and implement several state-of-the-art (SOTA) introspection mechanisms for error detection in 2D <b>object</b> <b>detection,</b> using one-stage and two-stage <b>object</b> <b>detectors</b> evaluated on KITTI and BDD datasets. We compare the performance of the proposed solution in terms of error detection, adaptability to dataset shift, and, computational and memory resource requirements. Our performance evaluation shows that the proposed introspection solution outperforms SOTA methods, achieving an absolute reduction in the missed error ratio of 9% to 17% in the BDD dataset.</p></p class="citation"></blockquote><h3 id=2629--87119-beyond-night-visibility-adaptive-multi-scale-fusion-of-infrared-and-visible-images-shufan-pei-et-al-2024>(26/29 | 87/119) Beyond Night Visibility: Adaptive Multi-Scale Fusion of Infrared and Visible Images (Shufan Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shufan Pei, Junhong Lin, Wenxi Liu, Tiesong Zhao, Chia-Wen Lin. (2024)<br><strong>Beyond Night Visibility: Adaptive Multi-Scale Fusion of Infrared and Visible Images</strong><br><button class=copy-to-clipboard title="Beyond Night Visibility: Adaptive Multi-Scale Fusion of Infrared and Visible Images" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01083v1.pdf filename=2403.01083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In addition to low light, night images suffer degradation from light effects (e.g., glare, floodlight, etc). However, existing nighttime visibility enhancement methods generally focus on low-light regions, which neglects, or even amplifies the light effects. To address this issue, we propose an Adaptive Multi-scale Fusion network (AMFusion) with infrared and visible images, which designs fusion rules according to different illumination regions. First, we separately fuse spatial and semantic features from infrared and visible images, where the former are used for the adjustment of light distribution and the latter are used for the improvement of detection accuracy. Thereby, we obtain an image free of low light and light effects, which improves the performance of nighttime <b>object</b> <b>detection.</b> Second, we utilize detection features extracted by a pre-trained backbone that guide the fusion of semantic features. Hereby, we design a Detection-guided Semantic Fusion Module (DSFM) to bridge the domain gap between detection and semantic features. Third, we propose a new illumination loss to constrain fusion image with normal light intensity. Experimental results demonstrate the superiority of AMFusion with better visual quality and detection accuracy. The source code will be released after the peer review process.</p></p class="citation"></blockquote><h3 id=2729--88119-benchmarking-segmentation-models-with-mask-preserved-attribute-editing-zijin-yin-et-al-2024>(27/29 | 88/119) Benchmarking Segmentation Models with Mask-Preserved Attribute Editing (Zijin Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijin Yin, Kongming Liang, Bing Li, Zhanyu Ma, Jun Guo. (2024)<br><strong>Benchmarking Segmentation Models with Mask-Preserved Attribute Editing</strong><br><button class=copy-to-clipboard title="Benchmarking Segmentation Models with Mask-Preserved Attribute Editing" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01231v1.pdf filename=2403.01231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When deploying segmentation models in practice, it is critical to evaluate their behaviors in varied and complex scenes. Different from the previous evaluation paradigms only in consideration of global attribute variations (e.g. adverse weather), we investigate both local and global attribute variations for robustness evaluation. To achieve this, we construct a mask-preserved attribute editing pipeline to edit visual attributes of real images with precise control of structural information. Therefore, the original segmentation labels can be reused for the edited images. Using our pipeline, we construct a <b>benchmark</b> covering both object and image attributes (e.g. color, material, pattern, style). We evaluate a broad variety of semantic segmentation models, spanning from conventional close-set models to recent open-vocabulary large models on their robustness to different types of variations. We find that both local and global attribute variations affect segmentation performances, and the sensitivity of models diverges across different variation types. We argue that local attributes have the same importance as global attributes, and should be considered in the robustness evaluation of segmentation models. Code: <a href=https://github.com/PRIS-CV/Pascal-EA>https://github.com/PRIS-CV/Pascal-EA</a>.</p></p class="citation"></blockquote><h3 id=2829--89119-rewind-dataset-privacy-preserving-speaking-status-segmentation-from-multimodal-body-movement-signals-in-the-wild-jose-vargas-quiros-et-al-2024>(28/29 | 89/119) REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild (Jose Vargas Quiros et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose Vargas Quiros, Chirag Raman, Stephanie Tan, Ekin Gedik, Laura Cabrera-Quiros, Hayley Hung. (2024)<br><strong>REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild</strong><br><button class=copy-to-clipboard title="REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, eess-SP<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01229v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01229v1.pdf filename=2403.01229v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize speech by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the speech signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking status estimation by presenting the first publicly available <b>multimodal</b> dataset with high-quality individual speech recordings of 33 subjects in a professional networking event. We present three baselines for no-audio speaking status segmentation: a) from video, b) from body acceleration (chest-worn accelerometer), c) from body pose tracks. In all cases we predict a 20Hz binary speaking status signal extracted from the audio, a time resolution not available in previous datasets. In addition to providing the signals and ground truth necessary to evaluate a wide range of speaking status detection methods, the availability of audio in REWIND makes it suitable for cross-modality studies not feasible with previous mingling datasets. Finally, our flexible data consent setup creates new challenges for <b>multimodal</b> systems under missing modalities.</p></p class="citation"></blockquote><h3 id=2929--90119-neural-field-classifiers-via-target-encoding-and-classification-loss-xindi-yang-et-al-2024>(29/29 | 90/119) Neural Field Classifiers via Target Encoding and Classification Loss (Xindi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun. (2024)<br><strong>Neural Field Classifiers via Target Encoding and Classification Loss</strong><br><button class=copy-to-clipboard title="Neural Field Classifiers via Target Encoding and Classification Loss" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01058v1.pdf filename=2403.01058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and <b>geometry</b> reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural Radiance Field (NeRF), all of these methods are regression models and are optimized by some regression loss. However, are regression models really better than classification models for neural field methods? In this work, we try to visit this very fundamental but overlooked question for neural fields from a machine learning perspective. We successfully propose a novel Neural Field Classifier (NFC) framework which formulates existing neural field methods as classification tasks rather than regression tasks. The proposed NFC can easily transform arbitrary Neural Field Regressor (NFR) into its classification variant via employing a novel Target Encoding module and optimizing a classification loss. By encoding a continuous regression target into a high-dimensional discrete encoding, we naturally formulate a multi-label classification task. Extensive experiments demonstrate the impressive effectiveness of NFC at the nearly free extra computational costs. Moreover, NFC also shows robustness to sparse inputs, corrupted images, and dynamic scenes.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--91119-chaining-thoughts-and-llms-to-learn-dna-structural-biophysics-tyler-d-ross-et-al-2024>(1/1 | 91/119) Chaining thoughts and LLMs to learn DNA structural biophysics (Tyler D. Ross et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tyler D. Ross, Ashwin Gopinath. (2024)<br><strong>Chaining thoughts and LLMs to learn DNA structural biophysics</strong><br><button class=copy-to-clipboard title="Chaining thoughts and LLMs to learn DNA structural biophysics" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01332v1.pdf filename=2403.01332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The future development of an AI scientist, a tool that is capable of integrating a variety of experimental data and generating testable hypotheses, holds immense potential. So far, bespoke machine learning models have been created to specialize in singular scientific tasks, but otherwise lack the flexibility of a general purpose model. Here, we show that a general purpose <b>large</b> <b>language</b> <b>model,</b> <b>chatGPT</b> 3.5-turbo, can be <b>fine-tuned</b> to learn the structural biophysics of DNA. We find that both <b>fine-tuning</b> models to return chain-of-thought responses and chaining together models <b>fine-tuned</b> for subtasks have an enhanced ability to analyze and design DNA sequences and their structures.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--92119-towards-full-authorship-with-ai-supporting-revision-with-ai-generated-views-jiho-kim-et-al-2024>(1/3 | 92/119) Towards Full Authorship with AI: Supporting Revision with AI-Generated Views (Jiho Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiho Kim, Ray C. Flanagan, Noelle E. Haviland, ZeAi Sun, Souad N. Yakubu, Edom A. Maru, Kenneth C. Arnold. (2024)<br><strong>Towards Full Authorship with AI: Supporting Revision with AI-Generated Views</strong><br><button class=copy-to-clipboard title="Towards Full Authorship with AI: Supporting Revision with AI-Generated Views" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2; I-7-1; I-2-7, cs-AI, cs-CY, cs-HC, cs.HC<br>Keyword Score: 50<br>Keywords: Generative AI, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01055v1.pdf filename=2403.01055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are shaping a new user interface (UI) paradigm in writing tools by enabling users to generate <b>text</b> <b>through</b> <b>prompts.</b> This paradigm shifts some creative control from the user to the system, thereby diminishing the user&rsquo;s authorship and autonomy in the writing process. To restore autonomy, we introduce Textfocals, a UI prototype designed to investigate a human-centered approach that emphasizes the user&rsquo;s role in writing. Textfocals supports the writing process by providing <b>LLM-generated</b> summaries, questions, and advice (i.e., <b>LLM</b> views) in a sidebar of a <b>text</b> <b>editor,</b> encouraging reflection and self-driven revision in writing without direct <b>text</b> <b>generation.</b> Textfocals&rsquo; UI affordances, including contextually adaptive views and scaffolding for <b>prompt</b> selection and customization, offer a novel way to interact with <b>LLMs</b> where users maintain full authorship of their writing. A formative user study with Textfocals showed promising evidence that this approach might help users develop underdeveloped ideas, cater to the rhetorical audience, and clarify their writing. However, the study also showed interaction design challenges related to document navigation and scoping, <b>prompt</b> engineering, and context management. Our work highlights the breadth of the design space of writing support interfaces powered by <b>generative</b> <b>AI</b> that maintain authorship integrity.</p></p class="citation"></blockquote><h3 id=23--93119-the-science-of-data-collection-insights-from-surveys-can-improve-machine-learning-models-stephanie-eckman-et-al-2024>(2/3 | 93/119) The Science of Data Collection: Insights from Surveys can Improve Machine Learning Models (Stephanie Eckman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephanie Eckman, Barbara Plank, Frauke Kreuter. (2024)<br><strong>The Science of Data Collection: Insights from Surveys can Improve Machine Learning Models</strong><br><button class=copy-to-clipboard title="The Science of Data Collection: Insights from Surveys can Improve Machine Learning Models" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: E-0, cs-HC, cs.HC, stat-ME<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01208v1.pdf filename=2403.01208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Whether future AI models make the world safer or less safe for humans rests in part on our ability to efficiently collect accurate data from people about what they want the models to do. However, collecting high quality data is difficult, and most AI/ML researchers are not trained in data collection methods. The growing emphasis on data-centric AI highlights the potential of data to enhance model performance. It also reveals an opportunity to gain insights from survey methodology, the science of collecting high-quality survey data. In this position paper, we <b>summarize</b> lessons from the survey methodology literature and discuss how they can improve the quality of training and feedback data, which in turn improve model performance. Based on the cognitive response process model, we formulate specific hypotheses about the aspects of label collection that may impact training data quality. We also suggest collaborative research ideas into how possible biases in data collection can be mitigated, making models more accurate and human-centric.</p></p class="citation"></blockquote><h3 id=33--94119-towards-rehabcoach-design-and-preliminary-evaluation-of-a-conversational-agent-supporting-unsupervised-therapy-after-stroke-giada-devittori-et-al-2024>(3/3 | 94/119) Towards RehabCoach: Design and Preliminary Evaluation of a Conversational Agent Supporting Unsupervised Therapy after Stroke (Giada Devittori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giada Devittori, Mehdi Akeddar, Alexandra Retevoi, Fabian Schneider, Viktoria Cvetkova, Daria Dinacci, Antonella Califfi, Paolo Rossi, Claudio Petrillo, Tobias Kowatsch, Olivier Lambercy. (2024)<br><strong>Towards RehabCoach: Design and Preliminary Evaluation of a Conversational Agent Supporting Unsupervised Therapy after Stroke</strong><br><button class=copy-to-clipboard title="Towards RehabCoach: Design and Preliminary Evaluation of a Conversational Agent Supporting Unsupervised Therapy after Stroke" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01127v1.pdf filename=2403.01127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> therapy after stroke is a promising way to boost therapy dose without significantly increasing the workload on healthcare professionals. However, it raises important challenges, such as lower adherence to therapy in the absence of social interaction with therapists. We present the initial prototype of RehabCoach, a novel smartphone-based app with conversational agent to support <b>unsupervised</b> therapy. RehabCoach is designed to increase patients engagement and adherence to therapy and to provide information (e.g., about stroke, health) in an interactive and user-friendly manner. We report on the design and usability evaluation of the first prototype of RehabCoach, assessed by four stroke patients and five healthcare professionals, who interacted with the app in a single testing session. Task completion time and success rates were measured for 15 representative tasks, and participants assessed usability via questionnaires and a semi-structured interview. Results show that it was feasible for stroke patients to successfully interact with RehabCoach (task success $\geq$ 93 $%$) without requiring extensive training. Participants positively rated the usability of RehabCoach (mean mHealth App Usability Questionnaire score: 1.3 for primary users, 1.4 for healthcare professionals, on a scale from 1 (positive evaluation) to 7). The feedback collected in this work opens the door to further enhance RehabCoach as an interactive digital tool to support <b>unsupervised</b> rehabilitation.</p></p class="citation"></blockquote><h2 id=csai-1>cs.AI (1)</h2><h3 id=11--95119-the-case-for-animal-friendly-ai-sankalpa-ghose-et-al-2024>(1/1 | 95/119) The Case for Animal-Friendly AI (Sankalpa Ghose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sankalpa Ghose, Yip Fai Tse, Kasra Rasaee, Jeff Sebo, Peter Singer. (2024)<br><strong>The Case for Animal-Friendly AI</strong><br><button class=copy-to-clipboard title="The Case for Animal-Friendly AI" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 43<br>Keywords: Benchmarking, ChatGPT, Claude, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01199v1.pdf filename=2403.01199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence is seen as increasingly important, and potentially profoundly so, but the fields of AI ethics and AI engineering have not fully recognized that these technologies, including <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> will have massive impacts on animals. We argue that this impact matters, because animals matter morally. As a first experiment in evaluating animal consideration in <b>LLMs,</b> we constructed a proof-of-concept Evaluation System, which assesses <b>LLM</b> responses and biases from multiple perspectives. This system evaluates <b>LLM</b> outputs by two criteria: their truthfulness, and the degree of consideration they give to the interests of animals. We tested OpenAI <b>ChatGPT</b> 4 and Anthropic <b>Claude</b> 2.1 using a set of structured queries and predefined normative perspectives. Preliminary results suggest that the outcomes of the tested models can be <b>benchmarked</b> regarding the consideration they give to animals, and that generated positions and biases might be addressed and mitigated with more developed and validated systems. Our research contributes one possible approach to integrating animal ethics in AI, opening pathways for future studies and practical applications in various fields, including education, public policy, and regulation, that involve or relate to animals and society. Overall, this study serves as a step towards more useful and responsible AI systems that better recognize and respect the vital interests and perspectives of all sentient beings.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--96119-experimental-evaluation-of-the-etsi-dcc-adaptive-approach-and-related-algorithms-oscar-amador-et-al-2024>(1/1 | 96/119) Experimental Evaluation of the ETSI DCC Adaptive Approach and Related Algorithms (Oscar Amador et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oscar Amador, Ignacio Soto, Maria Calderon, Manuel Urueña. (2024)<br><strong>Experimental Evaluation of the ETSI DCC Adaptive Approach and Related Algorithms</strong><br><button class=copy-to-clipboard title="Experimental Evaluation of the ETSI DCC Adaptive Approach and Related Algorithms" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01297v1.pdf filename=2403.01297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decentralized Congestion Control (DCC) mechanisms have been a core part of protocol stacks for vehicular networks since their inception and standardization. The ETSI ITS-G5 protocol stack for vehicular communications considers the usage of DCC not only in the network or access layers, but also as a part of the cross-layer architecture that influences how often messages are generated and transmitted. ETSI DCC mechanisms have evolved from a reactive approach based on a finite state machine, to an adaptive approach that relies on a linear control algorithm. This linear control algorithm, called LIMERIC, is the basis of the mechanism used in the ETSI DCC Adaptive Approach. The behavior of this algorithm depends on a set of parameters. Different values for these parameters have been proposed in the literature, including those defined in the ETSI specification. A recent proposal is Dual-$\alpha$, which chooses parameters to improve convergence and <b>fairness</b> when the algorithm has to react to fast changes in the use of the shared medium (transitory situations). This article evaluates, by means of <b>simulations,</b> the performance of the ETSI DCC Adaptive Approach and related algorithms, considering both steady state and transitory situations. Results show that a bad selection of parameters can make a DCC algorithm ineffective, that the ETSI DCC Adaptive algorithm performs well in steady state conditions, and that Dual-$\alpha$ performs as well in steady state conditions and outperforms the ETSI DCC Adaptive Approach in transitory scenarios.</p></p class="citation"></blockquote><h2 id=cscr-4>cs.CR (4)</h2><h3 id=14--97119-employing-llms-for-incident-response-planning-and-review-sam-hays-et-al-2024>(1/4 | 97/119) Employing LLMs for Incident Response Planning and Review (Sam Hays et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sam Hays, Dr. Jules White. (2024)<br><strong>Employing LLMs for Incident Response Planning and Review</strong><br><button class=copy-to-clipboard title="Employing LLMs for Incident Response Planning and Review" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01271v1.pdf filename=2403.01271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incident Response Planning (IRP) is essential for effective cybersecurity management, requiring detailed documentation (or playbooks) to guide security personnel during incidents. Yet, creating comprehensive IRPs is often hindered by challenges such as complex systems, high turnover rates, and legacy technologies lacking documentation. This paper argues that, despite these obstacles, the development, review, and refinement of IRPs can be significantly enhanced through the utilization of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT.</b> By leveraging <b>LLMs</b> for tasks such as drafting initial plans, suggesting best practices, and identifying documentation gaps, organizations can overcome resource constraints and improve their readiness for cybersecurity incidents. We discuss the potential of <b>LLMs</b> to streamline IRP processes, while also considering the limitations and the need for human oversight in ensuring the accuracy and relevance of generated content. Our findings contribute to the cybersecurity field by demonstrating a novel approach to enhancing IRP with AI technologies, offering practical insights for organizations seeking to bolster their incident response capabilities.</p></p class="citation"></blockquote><h3 id=24--98119-efficient-algorithm-level-error-detection-for-number-theoretic-transform-assessed-on-fpgas-kasra-ahmadi-et-al-2024>(2/4 | 98/119) Efficient Algorithm Level Error Detection for Number-Theoretic Transform Assessed on FPGAs (Kasra Ahmadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kasra Ahmadi, Saeed Aghapour, Mehran Mozaffari Kermani, Reza Azarderakhsh. (2024)<br><strong>Efficient Algorithm Level Error Detection for Number-Theoretic Transform Assessed on FPGAs</strong><br><button class=copy-to-clipboard title="Efficient Algorithm Level Error Detection for Number-Theoretic Transform Assessed on FPGAs" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01215v1.pdf filename=2403.01215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Polynomial multiplication stands out as a highly demanding arithmetic process in the development of post-quantum cryptosystems. The importance of number-theoretic transform (NTT) extends beyond post-quantum cryptosystems, proving valuable in enhancing existing security protocols such as digital signature schemes and hash functions. Due to the potential for errors to significantly disrupt the operation of secure, cryptographically-protected systems, compromising data integrity, and safeguarding against side-channel attacks initiated through faults it is essential to incorporate mitigating error detection schemes. This paper introduces algorithm level fault detection schemes in NTT multiplication, representing a significant enhancement compared to previous research. We evaluate this through the <b>simulation</b> of a fault model, ensuring that the conducted assessments accurately mirror the obtained results. Consequently, we attain a notably comprehensive coverage of errors. Finally, we assess the performance of our efficient error detection scheme on FPGAs to showcase its implementation and resource requirements. Through implementation of our error detection approach on Xilinx/AMD Zynq Ultrascale+ and Artix-7, we achieve a comparable throughput with just a 9% increase in area and 13% increase in latency compared to the original hardware implementations.</p></p class="citation"></blockquote><h3 id=34--99119-autoattacker-a-large-language-model-guided-system-to-implement-automatic-cyber-attacks-jiacen-xu-et-al-2024>(3/4 | 99/119) AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks (Jiacen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacen Xu, Jack W. Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swaminathan, Zhou Li. (2024)<br><strong>AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks</strong><br><button class=copy-to-clipboard title="AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01038v1.pdf filename=2403.01038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated impressive results on natural language tasks, and security researchers are beginning to employ them in both offensive and defensive systems. In cyber-security, there have been multiple research efforts that utilize <b>LLMs</b> focusing on the pre-breach stage of attacks like phishing and malware generation. However, so far there lacks a comprehensive study regarding whether <b>LLM-based</b> systems can be leveraged to simulate the post-breach stage of attacks that are typically human-operated, or &ldquo;hands-on-keyboard&rdquo; attacks, under various attack techniques and environments. As <b>LLMs</b> inevitably advance, they may be able to automate both the pre- and post-breach attack stages. This shift may transform organizational attacks from rare, expert-led events to frequent, automated operations requiring no expertise and executed at automation speed and scale. This risks fundamentally changing global computer security and correspondingly causing substantial economic impacts, and a goal of this work is to better understand these risks now so we can better prepare for these inevitable ever-more-capable <b>LLMs</b> on the horizon. On the immediate impact side, this research serves three purposes. First, an automated <b>LLM-based,</b> post-breach exploitation framework can help analysts quickly test and continually improve their organization&rsquo;s network security posture against previously unseen attacks. Second, an <b>LLM-based</b> penetration test system can extend the effectiveness of red teams with a limited number of human analysts. Finally, this research can help defensive systems and teams learn to detect novel attack behaviors preemptively before their use in the wild&mldr;.</p></p class="citation"></blockquote><h3 id=44--100119-characterizing-ethereum-upgradable-smart-contracts-and-their-security-implications-xiaofan-li-et-al-2024>(4/4 | 100/119) Characterizing Ethereum Upgradable Smart Contracts and Their Security Implications (Xiaofan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaofan Li, Jin Yang, Jiaqi Chen, Yuzhe Tang, Xing Gao. (2024)<br><strong>Characterizing Ethereum Upgradable Smart Contracts and Their Security Implications</strong><br><button class=copy-to-clipboard title="Characterizing Ethereum Upgradable Smart Contracts and Their Security Implications" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CE, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01290v1.pdf filename=2403.01290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Upgradeable smart contracts (USCs) have been widely adopted to enable modifying deployed smart contracts. While USCs bring great flexibility to developers, improper usage might introduce new security issues, potentially allowing attackers to hijack USCs and their users. In this paper, we conduct a large-scale measurement study to characterize USCs and their security implications in the wild. We <b>summarize</b> six commonly used USC patterns and develop a tool, USCDetector, to identify USCs without needing source code. Particularly, USCDetector collects various information such as bytecode and transaction information to construct upgrade chains for USCs and disclose potentially vulnerable ones. We evaluate USCDetector using verified smart contracts (i.e., with source code) as ground truth and show that USCDetector can achieve high accuracy with a precision of 96.26%. We then use USCDetector to conduct a large-scale study on Ethereum, covering a total of 60,251,064 smart contracts. USCDetecor constructs 10,218 upgrade chains and discloses multiple real-world USCs with potential security issues.</p></p class="citation"></blockquote><h2 id=cspf-1>cs.PF (1)</h2><h3 id=11--101119-hetegen-heterogeneous-parallel-inference-for-large-language-models-on-resource-constrained-devices-xuanlei-zhao-et-al-2024>(1/1 | 101/119) HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices (Xuanlei Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanlei Zhao, Bin Jia, Haotian Zhou, Ziming Liu, Shenggan Cheng, Yang You. (2024)<br><strong>HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices</strong><br><button class=copy-to-clipboard title="HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PF<br>Categories: cs-DC, cs-PF, cs.PF<br>Keyword Score: 30<br>Keywords: Low-Resource, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01164v1.pdf filename=2403.01164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent times, the emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has resulted in increasingly larger model size, posing challenges for inference on <b>low-resource</b> devices. Prior approaches have explored offloading to facilitate low-memory inference but often suffer from efficiency due to I/O bottlenecks. To achieve low-latency <b>LLMs</b> inference on resource-constrained devices, we introduce HeteGen, a novel approach that presents a principled framework for heterogeneous parallel computing using CPUs and GPUs. Based on this framework, HeteGen further employs heterogeneous parallel computing and asynchronous overlap for <b>LLMs</b> to mitigate I/O bottlenecks. Our experiments demonstrate a substantial improvement in inference speed, surpassing state-of-the-art methods by over 317% at most.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--102119-inevitable-metaverse-a-novel-twitter-dataset-for-public-sentiments-on-metaverse-kadhim-hayawi-et-al-2024>(1/2 | 102/119) Inevitable-Metaverse: A Novel Twitter Dataset for Public Sentiments on Metaverse (Kadhim Hayawi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kadhim Hayawi, Sakib Shahriar, Mohamed Adel Serhani, Eiman Alothali. (2024)<br><strong>Inevitable-Metaverse: A Novel Twitter Dataset for Public Sentiments on Metaverse</strong><br><button class=copy-to-clipboard title="Inevitable-Metaverse: A Novel Twitter Dataset for Public Sentiments on Metaverse" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 30<br>Keywords: BERT, Transformer, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01095v1.pdf filename=2403.01095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Metaverse has emerged as a novel technology with the objective to merge the physical world into the virtual world. This technology has seen a lot of interest and investment in recent times from prominent organizations including Facebook which has changed its company name to Meta with the goal of being the leader in developing this technology. Although people in general are excited about the prospects of metaverse due to potential use cases such as virtual meetings and virtual learning environments, there are also concerns due to potential negative consequences. For instance, people are concerned about their data privacy as well as spending a lot of their time on the metaverse leading to negative impacts in real life. Therefore, this research aims to further investigate the public <b>sentiments</b> <b>regarding</b> metaverse on social media. A total of 86565 metaverse-related tweets were used to perform lexicon-based <b>sentiment</b> <b>analysis.</b> Furthermore, various machine and deep learning models with various text features were utilized to predict the <b>sentiment</b> <b>class.</b> The <b>BERT</b> <b>transformer</b> model was demonstrated to be the best at predicting the <b>sentiment</b> <b>categories</b> with 92.6% accuracy and 0.91 F-measure on the test dataset. Finally, the implications and future research directions were also discussed.</p></p class="citation"></blockquote><h3 id=22--103119-autonomous-intelligent-systems-from-illusion-of-control-to-inescapable-delusion-stéphane-grumbach-et-al-2024>(2/2 | 103/119) Autonomous Intelligent Systems: From Illusion of Control to Inescapable Delusion (Stéphane Grumbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stéphane Grumbach, Giorgio Resta, Riccardo Torlone. (2024)<br><strong>Autonomous Intelligent Systems: From Illusion of Control to Inescapable Delusion</strong><br><button class=copy-to-clipboard title="Autonomous Intelligent Systems: From Illusion of Control to Inescapable Delusion" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: K-4; K-4-1; K-5, cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01292v1.pdf filename=2403.01292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous systems, including <b>generative</b> <b>AI,</b> have been adopted faster than previous digital innovations. Their impact on society might as well be more profound, with a radical restructuring of the economy of knowledge and dramatic consequences for social and institutional balances. Different attitudes to control these systems have emerged rooted in the classical pillars of legal systems, proprietary rights, and social responsibility. We show how an illusion of control might be guiding governments and regulators, while autonomous systems might be driving us to inescapable delusion.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=11--104119-high-dimensional-tail-index-regression-with-an-application-to-text-analyses-of-viral-posts-in-social-media-yuya-sasaki-et-al-2024>(1/1 | 104/119) High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media (Yuya Sasaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuya Sasaki, Jing Tao, Yulong Wang. (2024)<br><strong>High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media</strong><br><button class=copy-to-clipboard title="High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, econ-EM, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01318v1.pdf filename=2403.01318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the empirical power law of the distributions of credits (e.g., the number of &ldquo;likes&rdquo;) of viral posts in social media, we introduce the high-dimensional tail index regression and methods of estimation and inference for its parameters. We propose a regularized estimator, establish its consistency, and derive its convergence rate. To conduct inference, we propose to debias the regularized estimate, and establish the asymptotic normality of the debiased estimator. <b>Simulation</b> studies support our theory. These methods are applied to text analyses of viral posts in X (formerly Twitter) concerning LGBTQ+.</p></p class="citation"></blockquote><h2 id=csir-1>cs.IR (1)</h2><h3 id=11--105119-supplier-recommendation-in-online-procurement-victor-coscrato-et-al-2024>(1/1 | 105/119) Supplier Recommendation in Online Procurement (Victor Coscrato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Coscrato, Derek Bridge. (2024)<br><strong>Supplier Recommendation in Online Procurement</strong><br><button class=copy-to-clipboard title="Supplier Recommendation in Online Procurement" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01301v1.pdf filename=2403.01301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Supply chain optimization is key to a healthy and profitable business. Many companies use online procurement systems to agree contracts with suppliers. It is vital that the most competitive suppliers are invited to bid for such contracts. In this work, we propose a <b>recommender</b> <b>system</b> to assist with supplier discovery in road freight online procurement. Our system is able to provide personalized supplier <b>recommendations,</b> taking into account customer needs and preferences. This is a novel application of <b>recommender</b> <b>systems,</b> calling for design choices that fit the unique requirements of online procurement. Our preliminary results, using real-world data, are promising.</p></p class="citation"></blockquote><h2 id=csro-5>cs.RO (5)</h2><h3 id=15--106119-smooth-computation-without-input-delay-robust-tube-based-model-predictive-control-for-robot-manipulator-planning-qie-sima-et-al-2024>(1/5 | 106/119) Smooth Computation without Input Delay: Robust Tube-Based Model Predictive Control for Robot Manipulator Planning (Qie Sima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qie Sima, Yu Luo, Tianyin Ji, Fuchun Sun, Huaping Liu, Jianwei Zhang. (2024)<br><strong>Smooth Computation without Input Delay: Robust Tube-Based Model Predictive Control for Robot Manipulator Planning</strong><br><button class=copy-to-clipboard title="Smooth Computation without Input Delay: Robust Tube-Based Model Predictive Control for Robot Manipulator Planning" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01265v1.pdf filename=2403.01265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model Predictive Control (MPC) has exhibited remarkable capabilities in optimizing objectives and meeting constraints. However, the substantial computational burden associated with solving the Optimal Control Problem (OCP) at each triggering instant introduces significant delays between state sampling and control application. These delays limit the practicality of MPC in resource-constrained systems when engaging in complex tasks. The intuition to address this issue in this paper is that by predicting the successor state, the controller can solve the OCP one time step ahead of time thus avoiding the delay of the next action. To this end, we compute deviations between real and nominal system states, predicting forthcoming real states as initial conditions for the imminent OCP solution. Anticipatory computation stores optimal control based on current nominal states, thus mitigating the delay effects. Additionally, we establish an upper bound for linearization error, effectively linearizing the nonlinear system, reducing OCP complexity, and enhancing response speed. We provide empirical validation through two numerical <b>simulations</b> and corresponding real-world robot tasks, demonstrating significant performance improvements and augmented response speed (up to $90%$) resulting from the seamless integration of our proposed approach compared to conventional time-triggered MPC strategies.</p></p class="citation"></blockquote><h3 id=25--107119-shaping-multi-robot-patrol-performance-with-heterogeneity-in-individual-learning-behavior-connor-york-et-al-2024>(2/5 | 107/119) Shaping Multi-Robot Patrol Performance with Heterogeneity in Individual Learning Behavior (Connor York et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Connor York, Zachary R Madin, Paul O&rsquo;Dowd, Edmund R Hunt. (2024)<br><strong>Shaping Multi-Robot Patrol Performance with Heterogeneity in Individual Learning Behavior</strong><br><button class=copy-to-clipboard title="Shaping Multi-Robot Patrol Performance with Heterogeneity in Individual Learning Behavior" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01181v1.pdf filename=2403.01181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Individual differences in learning behavior within social groups, whether in humans, other animals, or among robots, can have significant effects on collective task performance. This is because it can affect individuals&rsquo; response to the environment and their interactions with each other. In recent years there has been rising interest in the question of how individual differences, whether in learning or other traits, affect collective outcomes: studied, for example, in social insect foraging behavior. Multi-robot, &lsquo;swarm&rsquo; systems have a heritage of bioinspiration from such examples, and here we consider whether heterogeneity in a learning behavior called latent inhibition (LI) may be useful for a team of patrolling robots tasked with environmental monitoring and <b>anomaly</b> <b>detection.</b> Individuals with high LI can be seen as better at learning to be inattentive to irrelevant or unrewarding stimuli, while low LI individuals might be seen as &lsquo;distractible&rsquo; and yet, more positively, more exploratory. We introduce a simple model of the effects of LI as the probability of re-searching a location for a reward (anomalous reading) where it has previously been found to be unrewarding (irrelevant). In simulated patrols, we find that a negatively skewed distribution of mostly high LI robots, and just a single low LI robot, is collectively most effective at monitoring dynamic environments. These results are an example of &lsquo;functional heterogeneity&rsquo; in &lsquo;swarm engineering&rsquo; and could inform predictions for ecological distributions of learning traits within social groups.</p></p class="citation"></blockquote><h3 id=35--108119-a-non-cubic-space-filling-modular-robot-tyler-hummer-et-al-2024>(3/5 | 108/119) A non-cubic space-filling modular robot (Tyler Hummer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tyler Hummer, Sam Kriegman. (2024)<br><strong>A non-cubic space-filling modular robot</strong><br><button class=copy-to-clipboard title="A non-cubic space-filling modular robot" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01323v1.pdf filename=2403.01323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Space-filling building blocks of diverse shape permeate nature at all levels of organization, from atoms to honeycombs, and have proven useful in artificial systems, from molecular containers to clay bricks. But, despite the wide variety of space-filling polyhedra known to mathematics, only the cube has been explored in robotics. Thus, here we roboticize a non-cubic space-filling shape: the rhombic dodecahedron. This <b>geometry</b> offers an appealing alternative to cubes as it greatly simplifies rotational motion of one cell about the edge of another, and increases the number of neighbors each cell can communicate with and hold on to. To better understand the challenges and opportunities of these and other space-filling machines, we manufactured 48 rhombic dodecahedral cells and used them to build various superstructures. We report locomotive ability of some of the structures we built, and discuss the dis/advantages of the different designs we tested. We also introduce a strategy for genderless passive docking of cells that generalizes to any polyhedra with radially symmetrical faces. Future work will allow the cells to freely roll/rotate about one another so that they may realize the full potential of their unique shape.</p></p class="citation"></blockquote><h3 id=45--109119-grid-based-fast-and-structural-visual-odometry-zhang-zhihe-2024>(4/5 | 109/119) Grid-based Fast and Structural Visual Odometry (Zhang Zhihe, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhang Zhihe. (2024)<br><strong>Grid-based Fast and Structural Visual Odometry</strong><br><button class=copy-to-clipboard title="Grid-based Fast and Structural Visual Odometry" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01110v1.pdf filename=2403.01110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of Simultaneous Localization and Mapping (SLAM), researchers have always pursued better performance in terms of accuracy and time cost. Traditional algorithms typically rely on fundamental geometric elements in images to establish connections between frames. However, these elements suffer from disadvantages such as uneven distribution and slow extraction. In addition, <b>geometry</b> elements like lines have not been fully utilized in the process of pose estimation. To address these challenges, we propose GFS-VO, a grid-based RGB-D visual odometry algorithm that maximizes the utilization of both point and line features. Our algorithm incorporates fast line extraction and a stable line homogenization scheme to improve feature processing. To fully leverage hidden elements in the scene, we introduce Manhattan Axes (MA) to provide constraints between local map and current frame. Additionally, we have designed an algorithm based on breadth-first search for extracting plane normal vectors. To evaluate the performance of GFS-VO, we conducted extensive experiments. The results demonstrate that our proposed algorithm exhibits significant improvements in both time cost and accuracy compared to existing approaches.</p></p class="citation"></blockquote><h3 id=55--110119-a-cost-effective-cooperative-exploration-and-inspection-strategy-for-heterogeneous-aerial-system-xinhang-xu-et-al-2024>(5/5 | 110/119) A Cost-Effective Cooperative Exploration and Inspection Strategy for Heterogeneous Aerial System (Xinhang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinhang Xu, Muqing Cao, Shenghai Yuan, Thien Hoang Nguyen, Thien-Minh Nguyen, Lihua Xie. (2024)<br><strong>A Cost-Effective Cooperative Exploration and Inspection Strategy for Heterogeneous Aerial System</strong><br><button class=copy-to-clipboard title="A Cost-Effective Cooperative Exploration and Inspection Strategy for Heterogeneous Aerial System" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01225v1.pdf filename=2403.01225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a cost-effective strategy for heterogeneous UAV swarm systems for cooperative aerial inspection. Unlike previous swarm inspection works, the proposed method does not rely on precise prior knowledge of the environment and can complete full 3D surface coverage of objects in any shape. In this work, agents are partitioned into teams, with each drone assign a different task, including mapping, exploration, and inspection. Task allocation is facilitated by assigning optimal inspection volumes to each team, following best-first rules. A voxel map-based representation of the environment is used for pathfinding, and a rule-based path-planning method is the core of this approach. We achieved the best performance in all challenging experiments with the proposed approach, surpassing all <b>benchmark</b> methods for similar tasks across multiple evaluation trials. The proposed method is open source at <a href=https://github.com/ntu-aris/caric_baseline>https://github.com/ntu-aris/caric_baseline</a> and used as the baseline of the Cooperative Aerial Robots Inspection Challenge at the 62nd IEEE Conference on Decision and Control 2023.</p></p class="citation"></blockquote><h2 id=eesssy-2>eess.SY (2)</h2><h3 id=12--111119-secure-and-scalable-network-slicing-with-plug-and-play-support-for-power-distribution-system-communication-networks-jian-zhong-et-al-2024>(1/2 | 111/119) Secure and Scalable Network Slicing with Plug-and-Play Support for Power Distribution System Communication Networks (Jian Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Zhong, Chen Chen, Yuqi Qian, Yiheng Bian, Yuxiong Huang, Zhaohong Bie. (2024)<br><strong>Secure and Scalable Network Slicing with Plug-and-Play Support for Power Distribution System Communication Networks</strong><br><button class=copy-to-clipboard title="Secure and Scalable Network Slicing with Plug-and-Play Support for Power Distribution System Communication Networks" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01257v1.pdf filename=2403.01257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid development of power distribution systems (PDSs), the number of terminal devices and the types of delivered services involved are constantly growing. These trends make the operations of PDSs highly dependent on the support of advanced communication networks, which face two related challenges. The first is to provide sufficient flexibility, resilience, and security to meet varying demands and ensure the proper operation of gradually diversifying network services. The second is to realize the automatic identification of terminal devices, thus reducing the network maintenance burden. To solve these problems, this paper presents a novel multiservice network integration and device authentication slice-based network slicing scheme. In this scheme, the integration of PDS communication networks enables network resource sharing, and recovery from communication interruption is achieved through network slicing in the integrated network. Authentication servers periodically poll terminal devices, adjusting network slice ranges based on authentication results, thereby facilitating dynamic network slicing. Additionally, secure plug-and-play support for PDS terminal devices and network protection are achieved through device identification and dynamic adjustment of network slices. On this basis, a network optimization and upgrading methodology for load balancing and robustness enhancement is further proposed. This approach is designed to improve the performance of PDS communication networks, adapting to ongoing PDS development and the evolution of PDS services. The <b>simulation</b> results show that the proposed schemes endow a PDS communication network with favorable resource utilization, fault recovery, terminal device plug-and-play support, load balancing, and improved network robustness.</p></p class="citation"></blockquote><h3 id=22--112119-real-time-hybrid-controls-of-energy-storage-and-load-shedding-for-integrated-power-and-energy-systems-of-ships-linh-vu-et-al-2024>(2/2 | 112/119) Real-time hybrid controls of energy storage and load shedding for integrated power and energy systems of ships (Linh Vu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linh Vu, Thai-Thanh Nguyen, Bang Le-Huy Nguyen, Md Isfakul Anam, Tuyen Vu. (2024)<br><strong>Real-time hybrid controls of energy storage and load shedding for integrated power and energy systems of ships</strong><br><button class=copy-to-clipboard title="Real-time hybrid controls of energy storage and load shedding for integrated power and energy systems of ships" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01102v1.pdf filename=2403.01102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an original energy management methodology to enhance the resilience of ship power systems. The integration of various energy storage systems (ESS), including battery energy storage systems (BESS) and super-capacitor energy storage systems (SCESS), in modern ship power systems poses challenges in designing an efficient energy management system (EMS). The EMS proposed in this paper aims to achieve multiple objectives. The primary objective is to minimize shed loads, while the secondary objective is to effectively manage different types of ESS. Considering the diverse ramp-rate characteristics of generators, SCESS, and BESS, the proposed EMS exploits these differences to determine an optimal long-term schedule for minimizing shed loads. Furthermore, the proposed EMS balances the state-of-charge (SoC) of ESS and prioritizes the SCESS&rsquo;s SoC levels to ensure the efficient operation of BESS and SCESS. For better computational efficiency, we introduce the receding horizon optimization method, enabling real-time EMS implementation. A comparison with the fixed horizon optimization (FHO) validates its effectiveness. <b>Simulation</b> studies and results demonstrate that the proposed EMS efficiently manages generators, BESS, and SCESS, ensuring system resilience under generation shortages. Additionally, the proposed methodology significantly reduces the computational burden compared to the FHO technique while maintaining acceptable resilience performance.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--113119-towards-accurate-lip-to-speech-synthesis-in-the-wild-sindhu-hegde-et-al-2024>(1/1 | 113/119) Towards Accurate Lip-to-Speech Synthesis in-the-Wild (Sindhu Hegde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sindhu Hegde, Rudrabha Mukhopadhyay, C. V. Jawahar, Vinay Namboodiri. (2024)<br><strong>Towards Accurate Lip-to-Speech Synthesis in-the-Wild</strong><br><button class=copy-to-clipboard title="Towards Accurate Lip-to-Speech Synthesis in-the-Wild" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-CV, cs-MM, cs-SD, cs.MM, eess-AS<br>Keyword Score: 13<br>Keywords: Benchmarking, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01087v1.pdf filename=2403.01087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel approach to address the task of synthesizing speech from silent videos of any in-the-wild speaker solely based on lip movements. The traditional approach of directly generating speech from lip videos faces the challenge of not being able to learn a robust language model from speech alone, resulting in unsatisfactory outcomes. To overcome this issue, we propose incorporating noisy text supervision using a state-of-the-art lip-to-text network that instills language information into our model. The noisy text is generated using a pre-trained lip-to-text model, enabling our approach to work without text annotations during inference. We design a visual <b>text-to-speech</b> network that utilizes the visual stream to generate accurate speech, which is in-sync with the silent input video. We perform extensive experiments and ablation studies, demonstrating our approach&rsquo;s superiority over the current state-of-the-art methods on various <b>benchmark</b> datasets. Further, we demonstrate an essential practical application of our method in assistive technology by generating speech for an ALS patient who has lost the voice but can make mouth movements. Our demo video, code, and additional details can be found at \url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw}.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--114119-performance-evaluation-of-acceleration-of-convolutional-layers-on-openedgecgra-nicolò-carpentieri-et-al-2024>(1/1 | 114/119) Performance evaluation of acceleration of convolutional layers on OpenEdgeCGRA (Nicolò Carpentieri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolò Carpentieri, Juan Sapriza, Davide Schiavone, Daniele Jahier Pagliari, David Atienza, Maurizio Martina, Alessio Burrello. (2024)<br><strong>Performance evaluation of acceleration of convolutional layers on OpenEdgeCGRA</strong><br><button class=copy-to-clipboard title="Performance evaluation of acceleration of convolutional layers on OpenEdgeCGRA" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01236v1.pdf filename=2403.01236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, efficiently deploying deep learning solutions on the edge has received increasing attention. New platforms are emerging to support the increasing demand for flexibility and high performance. In this work, we explore the efficient mapping of <b>convolutional</b> layers on an open-hardware, low-power Coarse-Grain Reconfigurable Array (CGRA), namely OpenEdgeCGRA. We explore both direct implementations of <b>convolution</b> and solutions that transform it into a matrix multiplication through an Im2col transformation, and experiment with various tensor parallelism axes. We show that for this hardware target, direct <b>convolution,</b> coupled with weight parallelism reaches the best latency and energy efficiency, outperforming a CPU implementation by 3.4x and 9.9x in terms of energy and latency, respectively.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--115119-a-bayesian-committee-machine-potential-for-oxygen-containing-organic-compounds-seungwon-kim-et-al-2024>(1/1 | 115/119) A Bayesian Committee Machine Potential for Oxygen-containing Organic Compounds (Seungwon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungwon Kim, D. ChangMo Yang, Soohaeng Yoo Willow, Chang Woo Myung. (2024)<br><strong>A Bayesian Committee Machine Potential for Oxygen-containing Organic Compounds</strong><br><button class=copy-to-clipboard title="A Bayesian Committee Machine Potential for Oxygen-containing Organic Compounds" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01158v1.pdf filename=2403.01158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the pivotal role of oxygen-containing organic compounds in serving as an energy source for living organisms and contributing to protein formation is crucial in the field of biochemistry. This study addresses the challenge of comprehending protein-protein interactions (PPI) and developing predicitive models for proteins and organic compounds, with a specific focus on quantifying their binding affinity. Here, we introduce the active Bayesian Committee Machine (BCM) potential, specifically designed to predict oxygen-containing organic compounds within eight groups of CHO. The BCM potential adopts a committee-based approach to tackle scalability issues associated with kernel regressors, particularly when dealing with large datasets. Its adaptable structure allows for efficient and cost-effective expansion, maintaing both transferability and scalability. Through systematic <b>benchmarking,</b> we position the sparse BCM potential as a promising contender in the pursuit of a universal machine learning potential.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--116119-graphmini-accelerating-graph-pattern-matching-using-auxiliary-graphs-juelin-liu-et-al-2024>(1/1 | 116/119) GraphMini: Accelerating Graph Pattern Matching Using Auxiliary Graphs (Juelin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juelin Liu, Sandeep Polisetty, Hui Guan, Marco Serafini. (2024)<br><strong>GraphMini: Accelerating Graph Pattern Matching Using Auxiliary Graphs</strong><br><button class=copy-to-clipboard title="GraphMini: Accelerating Graph Pattern Matching Using Auxiliary Graphs" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs-PF, cs.DB<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01050v1.pdf filename=2403.01050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> pattern matching is a fundamental problem encountered by many common <b>graph</b> mining tasks and the basic building block of several <b>graph</b> mining systems. This paper explores for the first time how to proactively prune <b>graphs</b> to speed up <b>graph</b> pattern matching by leveraging the structure of the query pattern and the input <b>graph.</b> We propose building auxiliary <b>graphs,</b> which are different pruned versions of the <b>graph,</b> during query execution. This requires careful balancing between the upfront cost of building and managing auxiliary <b>graphs</b> and the gains of faster set operations. To this end, we propose GraphMini, a new system that uses query compilation and a new cost model to minimize the cost of building and maintaining auxiliary <b>graphs</b> and maximize gains. Our evaluation shows that using GraphMini can achieve one order of magnitude speedup compared to state-of-the-art subgraph enumeration systems on commonly used <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--117119-network-analysis-using-krylov-subspace-trajectories-h-robert-frost-2024>(1/1 | 117/119) Network analysis using Krylov subspace trajectories (H. Robert Frost, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H. Robert Frost. (2024)<br><strong>Network analysis using Krylov subspace trajectories</strong><br><button class=copy-to-clipboard title="Network analysis using Krylov subspace trajectories" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-NA, cs-SI, math-NA, physics-soc-ph, physics.soc-ph<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01269v1.pdf filename=2403.01269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We describe a set of network analysis methods based on the rows of the Krylov subspace matrix computed from a network adjacency matrix via power iteration using a non-random initial vector. We refer to these node-specific row vectors as Krylov subspace trajectories. While power iteration using a random initial starting vector is commonly applied to the network adjacency matrix to compute eigenvector centrality values, this application only uses the final vector generated after numerical convergence. Importantly, use of a random initial vector means that the intermediate results of power iteration are also random and lack a clear interpretation. To the best of our knowledge, use of intermediate power iteration results for network analysis has been limited to techniques that leverage just a single pre-convergence solution, e.g., Power Iteration <b>Clustering.</b> In this paper, we explore methods that apply power iteration with a non-random inital vector to the network adjacency matrix to generate Krylov subspace trajectories for each node. These non-random trajectories provide important information regarding network structure, node importance, and response to perturbations. We have created this short preprint in part to generate feedback from others in the network analysis community who might be aware of similar existing work.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--118119-gsl-lpa-fast-label-propagation-algorithm-lpa-for-community-detection-with-no-internally-disconnected-communities-subhajit-sahu-2024>(1/1 | 118/119) GSL-LPA: Fast Label Propagation Algorithm (LPA) for Community Detection with no Internally-Disconnected Communities (Subhajit Sahu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhajit Sahu. (2024)<br><strong>GSL-LPA: Fast Label Propagation Algorithm (LPA) for Community Detection with no Internally-Disconnected Communities</strong><br><button class=copy-to-clipboard title="GSL-LPA: Fast Label Propagation Algorithm (LPA) for Community Detection with no Internally-Disconnected Communities" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: G-2-2; I-5-3, cs-DC, cs-SI, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01261v1.pdf filename=2403.01261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Community detection is the problem of identifying tightly connected clusters of nodes within a network. Efficient parallel algorithms for this play a crucial role in various applications, especially as datasets expand to significant sizes. The Label Propagation Algorithm (LPA) is commonly employed for this purpose due to its ease of parallelization, rapid execution, and scalability. However, it may yield internally disconnected communities. This technical report introduces GSL-LPA, derived from our parallelization of LPA, namely GVE-LPA. Our experiments on a system with two 16-core Intel Xeon Gold 6226R processors show that GSL-LPA not only mitigates this issue but also surpasses FLPA, igraph LPA, and NetworKit LPA by 55x, 10,300x, and 5.8x, respectively, achieving a processing rate of 844 M edges/s on a 3.8 B edge <b>graph.</b> Additionally, GSL-LPA scales at a rate of 1.6x for every doubling of threads.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--119119-a-strongly-subcubic-combinatorial-algorithm-for-triangle-detection-with-applications-adrian-dumitrescu-2024>(1/1 | 119/119) A Strongly Subcubic Combinatorial Algorithm for Triangle Detection with Applications (Adrian Dumitrescu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Dumitrescu. (2024)<br><strong>A Strongly Subcubic Combinatorial Algorithm for Triangle Detection with Applications</strong><br><button class=copy-to-clipboard title="A Strongly Subcubic Combinatorial Algorithm for Triangle Detection with Applications" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01085v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01085v2.pdf filename=2403.01085v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We revisit the algorithmic problem of finding a triangle in a <b>graph:</b> We give a randomized combinatorial algorithm for triangle detection in a given $n$-vertex <b>graph</b> with $m$ edges running in $O(n^{7/3})$ time, or alternatively in $O(m^{4/3})$ time. This may come as a surprise since it invalidates several conjectures in the literature. In particular, - the $O(n^{7/3})$ runtime surpasses the long-standing fastest algorithm for triangle detection based on matrix multiplication running in $O(n^\omega) = O(n^{2.372})$ time, due to Itai and Rodeh (1978). - the $O(m^{4/3})$ runtime surpasses the long-standing fastest algorithm for triangle detection in sparse <b>graphs</b> based on matrix multiplication running in $O(m^{2\omega/(\omega+1)})= O(m^{1.407})$ time due to Alon, Yuster, and Zwick (1997). - the $O(n^{7/3})$ time algorithm for triangle detection leads to a $O(n^{25/9} \log{n})$ time combinatorial algorithm for $n \times n$ Boolean matrix multiplication, by a reduction of V. V. Williams and R.~R.~Williams (2018).This invalidates a conjecture of A.~Abboud and V. V. Williams (FOCS 2014). - the $O(m^{4/3})$ runtime invalidates a conjecture of A.~Abboud and V. V. Williams (FOCS 2014) that any combinatorial algorithm for triangle detection requires $m^{3/2 - o(1)}$ time. - as a direct application of the triangle detection algorithm, we obtain a faster exact algorithm for the $k$-clique problem, surpassing an almost $40$ years old algorithm of Ne{\v{s}}et{\v{r}}il and Poljak (1985). This result strongly disproves the combinatorial $k$-clique conjecture. - as another direct application of the triangle detection algorithm, we obtain a faster exact algorithm for the \textsc{Max-Cut} problem, surpassing an almost $20$ years old algorithm of R.~R.~Williams (2005).</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.03</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.05</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-24>cs.CL (24)</a><ul><li><a href=#124--1119-distilling-text-style-transfer-with-self-explanation-from-llms-chiyu-zhang-et-al-2024>(1/24 | 1/119) Distilling Text Style Transfer With Self-Explanation From LLMs (Chiyu Zhang et al., 2024)</a></li><li><a href=#224--2119-vbart-the-turkish-llm-meliksah-turker-et-al-2024>(2/24 | 2/119) VBART: The Turkish LLM (Meliksah Turker et al., 2024)</a></li><li><a href=#324--3119-lm4opt-unveiling-the-potential-of-large-language-models-in-formulating-mathematical-optimization-problems-tasnim-ahmed-et-al-2024>(3/24 | 3/119) LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems (Tasnim Ahmed et al., 2024)</a></li><li><a href=#424--4119-ragged-edges-the-double-edged-sword-of-retrieval-augmented-chatbots-philip-feldman-james-r-foulds-et-al-2024>(4/24 | 4/119) RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots (Philip Feldman. James R. Foulds et al., 2024)</a></li><li><a href=#524--5119-star-constraint-lora-with-dynamic-active-learning-for-data-efficient-fine-tuning-of-large-language-models-linhai-zhang-et-al-2024>(5/24 | 5/119) STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models (Linhai Zhang et al., 2024)</a></li><li><a href=#624--6119-faima-feature-aware-in-context-learning-for-multi-domain-aspect-based-sentiment-analysis-songhua-yang-et-al-2024>(6/24 | 6/119) FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis (Songhua Yang et al., 2024)</a></li><li><a href=#724--7119-lab-large-scale-alignment-for-chatbots-shivchander-sudalairaj-et-al-2024>(7/24 | 7/119) LAB: Large-Scale Alignment for ChatBots (Shivchander Sudalairaj et al., 2024)</a></li><li><a href=#824--8119-improving-the-validity-of-automatically-generated-feedback-via-reinforcement-learning-alexander-scarlatos-et-al-2024>(8/24 | 8/119) Improving the Validity of Automatically Generated Feedback via Reinforcement Learning (Alexander Scarlatos et al., 2024)</a></li><li><a href=#924--9119-reading-subtext-evaluating-large-language-models-on-short-story-summarization-with-writers-melanie-subbiah-et-al-2024>(9/24 | 9/119) Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers (Melanie Subbiah et al., 2024)</a></li><li><a href=#1024--10119-vnlp-turkish-nlp-package-meliksah-turker-et-al-2024>(10/24 | 10/119) VNLP: Turkish NLP Package (Meliksah Turker et al., 2024)</a></li><li><a href=#1124--11119-diner-debiasing-aspect-based-sentiment-analysis-with-multi-variable-causal-inference-jialong-wu-et-al-2024>(11/24 | 11/119) DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference (Jialong Wu et al., 2024)</a></li><li><a href=#1224--12119-balancing-exploration-and-exploitation-in-llm-using-soft-rllf-for-enhanced-negation-understanding-ha-thanh-nguyen-et-al-2024>(12/24 | 12/119) Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding (Ha-Thanh Nguyen et al., 2024)</a></li><li><a href=#1324--13119-mitigating-catastrophic-forgetting-in-large-language-models-with-self-synthesized-rehearsal-jianheng-huang-et-al-2024>(13/24 | 13/119) Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal (Jianheng Huang et al., 2024)</a></li><li><a href=#1424--14119-intactkv-improving-large-language-model-quantization-by-keeping-pivot-tokens-intact-ruikang-liu-et-al-2024>(14/24 | 14/119) IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact (Ruikang Liu et al., 2024)</a></li><li><a href=#1524--15119-machine-translation-in-the-covid-domain-an-english-irish-case-study-for-loresmt-2021-séamus-lankford-et-al-2024>(15/24 | 15/119) Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021 (Séamus Lankford et al., 2024)</a></li><li><a href=#1624--16119-accelerating-greedy-coordinate-gradient-via-probe-sampling-yiran-zhao-et-al-2024>(16/24 | 16/119) Accelerating Greedy Coordinate Gradient via Probe Sampling (Yiran Zhao et al., 2024)</a></li><li><a href=#1724--17119-dmoerm-recipes-of-mixture-of-experts-for-effective-reward-modeling-shanghaoran-quan-2024>(17/24 | 17/119) DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling (Shanghaoran Quan, 2024)</a></li><li><a href=#1824--18119-a-comprehensive-cross-language-framework-for-harmful-content-detection-with-the-aid-of-sentiment-analysis-mohammad-dehghani-2024>(18/24 | 18/119) A comprehensive cross-language framework for harmful content detection with the aid of sentiment analysis (Mohammad Dehghani, 2024)</a></li><li><a href=#1924--19119-api-is-enough-conformal-prediction-for-large-language-models-without-logit-access-jiayuan-su-et-al-2024>(19/24 | 19/119) API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access (Jiayuan Su et al., 2024)</a></li><li><a href=#2024--20119-a-survey-of-ai-generated-text-forensic-systems-detection-attribution-and-characterization-tharindu-kumarage-et-al-2024>(20/24 | 20/119) A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization (Tharindu Kumarage et al., 2024)</a></li><li><a href=#2124--21119-llmcrit-teaching-large-language-models-to-use-criteria-weizhe-yuan-et-al-2024>(21/24 | 21/119) LLMCRIT: Teaching Large Language Models to Use Criteria (Weizhe Yuan et al., 2024)</a></li><li><a href=#2224--22119-parallelparc-a-scalable-pipeline-for-generating-natural-language-analogies-oren-sultan-et-al-2024>(22/24 | 22/119) ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies (Oren Sultan et al., 2024)</a></li><li><a href=#2324--23119-boottod-bootstrap-task-oriented-dialogue-representations-by-aligning-diverse-responses-weihao-zeng-et-al-2024>(23/24 | 23/119) BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses (Weihao Zeng et al., 2024)</a></li><li><a href=#2424--24119-mulcogbench-a-multi-modal-cognitive-benchmark-dataset-for-evaluating-chinese-and-english-computational-language-models-yunhao-zhang-et-al-2024>(24/24 | 24/119) MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating Chinese and English Computational Language Models (Yunhao Zhang et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--25119-llamoco-instruction-tuning-of-large-language-models-for-optimization-code-generation-zeyuan-ma-et-al-2024>(1/2 | 25/119) LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation (Zeyuan Ma et al., 2024)</a></li><li><a href=#22--26119-decentralized-implicit-differentiation-lucas-fuentes-valenzuela-et-al-2024>(2/2 | 26/119) Decentralized Implicit Differentiation (Lucas Fuentes Valenzuela et al., 2024)</a></li></ul></li><li><a href=#cslg-33>cs.LG (33)</a><ul><li><a href=#133--27119-opengraph-towards-open-graph-foundation-models-lianghao-xia-et-al-2024>(1/33 | 27/119) OpenGraph: Towards Open Graph Foundation Models (Lianghao Xia et al., 2024)</a></li><li><a href=#233--28119-evaluating-large-language-models-as-virtual-annotators-for-time-series-physical-sensing-data-aritra-hota-et-al-2024>(2/33 | 28/119) Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data (Aritra Hota et al., 2024)</a></li><li><a href=#333--29119-less-is-more-hop-wise-graph-attention-for-scalable-and-generalizable-learning-on-circuits-chenhui-deng-et-al-2024>(3/33 | 29/119) Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits (Chenhui Deng et al., 2024)</a></li><li><a href=#433--30119-teaching-mlp-more-graph-information-a-three-stage-multitask-knowledge-distillation-framework-junxian-li-et-al-2024>(4/33 | 30/119) Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework (Junxian Li et al., 2024)</a></li><li><a href=#533--31119-nomad-attention-efficient-llm-inference-on-cpus-through-multiply-add-free-attention-tianyi-zhang-et-al-2024>(5/33 | 31/119) NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention (Tianyi Zhang et al., 2024)</a></li><li><a href=#633--32119-polynormer-polynomial-expressive-graph-transformer-in-linear-time-chenhui-deng-et-al-2024>(6/33 | 32/119) Polynormer: Polynomial-Expressive Graph Transformer in Linear Time (Chenhui Deng et al., 2024)</a></li><li><a href=#733--33119-dissecting-language-models-machine-unlearning-via-selective-pruning-nicholas-pochinkov-et-al-2024>(7/33 | 33/119) Dissecting Language Models: Machine Unlearning via Selective Pruning (Nicholas Pochinkov et al., 2024)</a></li><li><a href=#833--34119-a-hybrid-model-for-traffic-incident-detection-based-on-generative-adversarial-networks-and-transformer-model-xinying-lu-et-al-2024>(8/33 | 34/119) A Hybrid Model for Traffic Incident Detection based on Generative Adversarial Networks and Transformer Model (Xinying Lu et al., 2024)</a></li><li><a href=#933--35119-llm-pq-serving-llm-on-heterogeneous-clusters-with-phase-aware-partition-and-adaptive-quantization-juntao-zhao-et-al-2024>(9/33 | 35/119) LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization (Juntao Zhao et al., 2024)</a></li><li><a href=#1033--36119-pairwise-alignment-improves-graph-domain-adaptation-shikun-liu-et-al-2024>(10/33 | 36/119) Pairwise Alignment Improves Graph Domain Adaptation (Shikun Liu et al., 2024)</a></li><li><a href=#1133--37119-cool-a-conjoint-perspective-on-spatio-temporal-graph-neural-network-for-traffic-forecasting-wei-ju-et-al-2024>(11/33 | 37/119) COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting (Wei Ju et al., 2024)</a></li><li><a href=#1233--38119-defending-against-data-reconstruction-attacks-in-federated-learning-an-information-theory-approach-qi-tan-et-al-2024>(12/33 | 38/119) Defending Against Data Reconstruction Attacks in Federated Learning: An Information Theory Approach (Qi Tan et al., 2024)</a></li><li><a href=#1333--39119-pseudo-label-calibration-semi-supervised-multi-modal-entity-alignment-luyao-wang-et-al-2024>(13/33 | 39/119) Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment (Luyao Wang et al., 2024)</a></li><li><a href=#1433--40119-improve-cost-efficiency-of-active-learning-over-noisy-dataset-zan-kai-chong-et-al-2024>(14/33 | 40/119) Improve Cost Efficiency of Active Learning over Noisy Dataset (Zan-Kai Chong et al., 2024)</a></li><li><a href=#1533--41119-bespoke-non-stationary-solvers-for-fast-sampling-of-diffusion-and-flow-models-neta-shaul-et-al-2024>(15/33 | 41/119) Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models (Neta Shaul et al., 2024)</a></li><li><a href=#1633--42119-active-deep-kernel-learning-of-molecular-functionalities-realizing-dynamic-structural-embeddings-ayana-ghosh-et-al-2024>(16/33 | 42/119) Active Deep Kernel Learning of Molecular Functionalities: Realizing Dynamic Structural Embeddings (Ayana Ghosh et al., 2024)</a></li><li><a href=#1733--43119-γ-vae-curvature-regularized-variational-autoencoders-for-uncovering-emergent-low-dimensional-geometric-structure-in-high-dimensional-data-jason-z-kim-et-al-2024>(17/33 | 43/119) $Γ$-VAE: Curvature regularized variational autoencoders for uncovering emergent low dimensional geometric structure in high dimensional data (Jason Z. Kim et al., 2024)</a></li><li><a href=#1833--44119-icc-quantifying-image-caption-concreteness-for-multimodal-dataset-curation-moran-yanuka-et-al-2024>(18/33 | 44/119) ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation (Moran Yanuka et al., 2024)</a></li><li><a href=#1933--45119-stochastic-gradient-descent-for-streaming-linear-and-rectified-linear-systems-with-massart-noise-halyun-jeong-et-al-2024>(19/33 | 45/119) Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise (Halyun Jeong et al., 2024)</a></li><li><a href=#2033--46119-feature-alignment-rethinking-efficient-active-learning-via-proxy-in-the-context-of-pre-trained-models-ziting-wen-et-al-2024>(20/33 | 46/119) Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models (Ziting Wen et al., 2024)</a></li><li><a href=#2133--47119-temporal-knowledge-graph-completion-with-time-sensitive-relations-in-hypercomplex-space-li-cai-et-al-2024>(21/33 | 47/119) Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space (Li Cai et al., 2024)</a></li><li><a href=#2233--48119-seeing-unseen-discover-novel-biomedical-concepts-via-geometry-constrained-probabilistic-modeling-jianan-fan-et-al-2024>(22/33 | 48/119) Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling (Jianan Fan et al., 2024)</a></li><li><a href=#2333--49119-a-two-stage-algorithm-for-cost-efficient-multi-instance-counterfactual-explanations-andré-artelt-et-al-2024>(23/33 | 49/119) A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations (André Artelt et al., 2024)</a></li><li><a href=#2433--50119-can-a-confident-prior-replace-a-cold-posterior-martin-marek-et-al-2024>(24/33 | 50/119) Can a Confident Prior Replace a Cold Posterior? (Martin Marek et al., 2024)</a></li><li><a href=#2533--51119-spatio-temporal-field-neural-networks-for-air-quality-inference-yutong-feng-et-al-2024>(25/33 | 51/119) Spatio-Temporal Field Neural Networks for Air Quality Inference (Yutong Feng et al., 2024)</a></li><li><a href=#2633--52119-near-optimal-per-action-regret-bounds-for-sleeping-bandits-quan-nguyen-et-al-2024>(26/33 | 52/119) Near-optimal Per-Action Regret Bounds for Sleeping Bandits (Quan Nguyen et al., 2024)</a></li><li><a href=#2733--53119-acme-ad-accelerated-model-explanations-for-anomaly-detection-valentina-zaccaria-et-al-2024>(27/33 | 53/119) AcME-AD: Accelerated Model Explanations for Anomaly Detection (Valentina Zaccaria et al., 2024)</a></li><li><a href=#2833--54119-inexact-unlearning-needs-more-careful-evaluations-to-avoid-a-false-sense-of-privacy-jamie-hayes-et-al-2024>(28/33 | 54/119) Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy (Jamie Hayes et al., 2024)</a></li><li><a href=#2933--55119-training-unbiased-diffusion-models-from-biased-dataset-yeongmin-kim-et-al-2024>(29/33 | 55/119) Training Unbiased Diffusion Models From Biased Dataset (Yeongmin Kim et al., 2024)</a></li><li><a href=#3033--56119-mpipn-a-multi-physics-informed-pointnet-for-solving-parametric-acoustic-structure-systems-chu-wang-et-al-2024>(30/33 | 56/119) MPIPN: A Multi Physics-Informed PointNet for solving parametric acoustic-structure systems (Chu Wang et al., 2024)</a></li><li><a href=#3133--57119-efficient-episodic-memory-utilization-of-cooperative-multi-agent-reinforcement-learning-hyungho-na-et-al-2024>(31/33 | 57/119) Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning (Hyungho Na et al., 2024)</a></li><li><a href=#3233--58119-continuous-mean-zero-disagreement-regularized-imitation-learning-cmz-dril-noah-ford-et-al-2024>(32/33 | 58/119) Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL) (Noah Ford et al., 2024)</a></li><li><a href=#3333--59119-graphrcg-self-conditioned-graph-generation-via-bootstrapped-representations-song-wang-et-al-2024>(33/33 | 59/119) GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations (Song Wang et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--60119-automatic-speech-recognition-using-advanced-deep-learning-approaches-a-survey-hamza-kheddar-et-al-2024>(1/2 | 60/119) Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey (Hamza Kheddar et al., 2024)</a></li><li><a href=#22--61119-enhancing-audio-generation-diversity-with-visual-information-zeyu-xie-et-al-2024>(2/2 | 61/119) Enhancing Audio Generation Diversity with Visual Information (Zeyu Xie et al., 2024)</a></li></ul></li><li><a href=#cscv-29>cs.CV (29)</a><ul><li><a href=#129--62119-dna-family-boosting-weight-sharing-nas-with-block-wise-supervisions-guangrun-wang-et-al-2024>(1/29 | 62/119) DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions (Guangrun Wang et al., 2024)</a></li><li><a href=#229--63119-data-free-multi-label-image-recognition-via-llm-powered-prompt-tuning-shuo-yang-et-al-2024>(2/29 | 63/119) Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning (Shuo Yang et al., 2024)</a></li><li><a href=#329--64119-scenecraft-an-llm-agent-for-synthesizing-3d-scene-as-blender-code-ziniu-hu-et-al-2024>(3/29 | 64/119) SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code (Ziniu Hu et al., 2024)</a></li><li><a href=#429--65119-learn-suspected-anomalies-from-event-prompts-for-video-anomaly-detection-chenchen-tao-et-al-2024>(4/29 | 65/119) Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection (Chenchen Tao et al., 2024)</a></li><li><a href=#529--66119-text-guided-explorable-image-super-resolution-kanchana-vaishnavi-gandikota-et-al-2024>(5/29 | 66/119) Text-guided Explorable Image Super-resolution (Kanchana Vaishnavi Gandikota et al., 2024)</a></li><li><a href=#629--67119-ela-efficient-local-attention-for-deep-convolutional-neural-networks-wei-xu-et-al-2024>(6/29 | 67/119) ELA: Efficient Local Attention for Deep Convolutional Neural Networks (Wei Xu et al., 2024)</a></li><li><a href=#729--68119-dual-graph-attention-based-disentanglement-multiple-instance-learning-for-brain-age-estimation-fanzhe-yan-et-al-2024>(7/29 | 68/119) Dual Graph Attention based Disentanglement Multiple Instance Learning for Brain Age Estimation (Fanzhe Yan et al., 2024)</a></li><li><a href=#829--69119-on-the-road-to-portability-compressing-end-to-end-motion-planner-for-autonomous-driving-kaituo-feng-et-al-2024>(8/29 | 69/119) On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving (Kaituo Feng et al., 2024)</a></li><li><a href=#929--70119-sar-ae-sfp-sar-imagery-adversarial-example-in-real-physics-domain-with-target-scattering-feature-parameters-jiahao-cui-et-al-2024>(9/29 | 70/119) SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with Target Scattering Feature Parameters (Jiahao Cui et al., 2024)</a></li><li><a href=#1029--71119-leveraging-self-supervised-learning-for-scene-recognition-in-child-sexual-abuse-imagery-pedro-h-v-valois-et-al-2024>(10/29 | 71/119) Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery (Pedro H. V. Valois et al., 2024)</a></li><li><a href=#1129--72119-face-swap-via-diffusion-model-feifei-wang-2024>(11/29 | 72/119) Face Swap via Diffusion Model (Feifei Wang, 2024)</a></li><li><a href=#1229--73119-auxiliary-tasks-enhanced-dual-affinity-learning-for-weakly-supervised-semantic-segmentation-lian-xu-et-al-2024>(12/29 | 73/119) Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised Semantic Segmentation (Lian Xu et al., 2024)</a></li><li><a href=#1329--74119-adversarial-testing-for-visual-grounding-via-image-aware-property-reduction-zhiyuan-chang-et-al-2024>(13/29 | 74/119) Adversarial Testing for Visual Grounding via Image-Aware Property Reduction (Zhiyuan Chang et al., 2024)</a></li><li><a href=#1429--75119-boosting-box-supervised-instance-segmentation-with-pseudo-depth-xinyi-yu-et-al-2024>(14/29 | 75/119) Boosting Box-supervised Instance Segmentation with Pseudo Depth (Xinyi Yu et al., 2024)</a></li><li><a href=#1529--76119-tcig-two-stage-controlled-image-generation-with-quality-enhancement-through-diffusion-salaheldin-mohamed-2024>(15/29 | 76/119) TCIG: Two-Stage Controlled Image Generation with Quality Enhancement through Diffusion (Salaheldin Mohamed, 2024)</a></li><li><a href=#1629--77119-neural-radiance-fields-based-holography-invited-minsung-kang-et-al-2024>(16/29 | 77/119) Neural radiance fields-based holography [Invited] (Minsung Kang et al., 2024)</a></li><li><a href=#1729--78119-dynamic-3d-point-cloud-sequences-as-2d-videos-yiming-zeng-et-al-2024>(17/29 | 78/119) Dynamic 3D Point Cloud Sequences as 2D Videos (Yiming Zeng et al., 2024)</a></li><li><a href=#1829--79119-extracting-usable-predictions-from-quantized-networks-through-uncertainty-quantification-for-ood-detection-rishi-singhal-et-al-2024>(18/29 | 79/119) Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection (Rishi Singhal et al., 2024)</a></li><li><a href=#1929--80119-diffsal-joint-audio-and-video-learning-for-diffusion-saliency-prediction-junwen-xiong-et-al-2024>(19/29 | 80/119) DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction (Junwen Xiong et al., 2024)</a></li><li><a href=#2029--81119-nerf-vpt-learning-novel-view-representations-with-neural-radiance-fields-via-view-prompt-tuning-linsheng-chen-et-al-2024>(20/29 | 81/119) NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning (Linsheng Chen et al., 2024)</a></li><li><a href=#2129--82119-tumtraf-v2x-cooperative-perception-dataset-walter-zimmer-et-al-2024>(21/29 | 82/119) TUMTraf V2X Cooperative Perception Dataset (Walter Zimmer et al., 2024)</a></li><li><a href=#2229--83119-shapeboost-boosting-human-shape-estimation-with-part-based-parameterization-and-clothing-preserving-augmentation-siyuan-bian-et-al-2024>(22/29 | 83/119) ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation (Siyuan Bian et al., 2024)</a></li><li><a href=#2329--84119-image-based-dietary-assessment-a-healthy-eating-plate-estimation-system-assylzhan-izbassar-et-al-2024>(23/29 | 84/119) Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System (Assylzhan Izbassar et al., 2024)</a></li><li><a href=#2429--85119-fast-low-parameter-video-activity-localization-in-collaborative-learning-environments-venkatesh-jatla-et-al-2024>(24/29 | 85/119) Fast Low-parameter Video Activity Localization in Collaborative Learning Environments (Venkatesh Jatla et al., 2024)</a></li><li><a href=#2529--86119-run-time-introspection-of-2d-object-detection-in-automated-driving-systems-using-learning-representations-hakan-yekta-yatbaz-et-al-2024>(25/29 | 86/119) Run-time Introspection of 2D Object Detection in Automated Driving Systems Using Learning Representations (Hakan Yekta Yatbaz et al., 2024)</a></li><li><a href=#2629--87119-beyond-night-visibility-adaptive-multi-scale-fusion-of-infrared-and-visible-images-shufan-pei-et-al-2024>(26/29 | 87/119) Beyond Night Visibility: Adaptive Multi-Scale Fusion of Infrared and Visible Images (Shufan Pei et al., 2024)</a></li><li><a href=#2729--88119-benchmarking-segmentation-models-with-mask-preserved-attribute-editing-zijin-yin-et-al-2024>(27/29 | 88/119) Benchmarking Segmentation Models with Mask-Preserved Attribute Editing (Zijin Yin et al., 2024)</a></li><li><a href=#2829--89119-rewind-dataset-privacy-preserving-speaking-status-segmentation-from-multimodal-body-movement-signals-in-the-wild-jose-vargas-quiros-et-al-2024>(28/29 | 89/119) REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild (Jose Vargas Quiros et al., 2024)</a></li><li><a href=#2929--90119-neural-field-classifiers-via-target-encoding-and-classification-loss-xindi-yang-et-al-2024>(29/29 | 90/119) Neural Field Classifiers via Target Encoding and Classification Loss (Xindi Yang et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--91119-chaining-thoughts-and-llms-to-learn-dna-structural-biophysics-tyler-d-ross-et-al-2024>(1/1 | 91/119) Chaining thoughts and LLMs to learn DNA structural biophysics (Tyler D. Ross et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--92119-towards-full-authorship-with-ai-supporting-revision-with-ai-generated-views-jiho-kim-et-al-2024>(1/3 | 92/119) Towards Full Authorship with AI: Supporting Revision with AI-Generated Views (Jiho Kim et al., 2024)</a></li><li><a href=#23--93119-the-science-of-data-collection-insights-from-surveys-can-improve-machine-learning-models-stephanie-eckman-et-al-2024>(2/3 | 93/119) The Science of Data Collection: Insights from Surveys can Improve Machine Learning Models (Stephanie Eckman et al., 2024)</a></li><li><a href=#33--94119-towards-rehabcoach-design-and-preliminary-evaluation-of-a-conversational-agent-supporting-unsupervised-therapy-after-stroke-giada-devittori-et-al-2024>(3/3 | 94/119) Towards RehabCoach: Design and Preliminary Evaluation of a Conversational Agent Supporting Unsupervised Therapy after Stroke (Giada Devittori et al., 2024)</a></li></ul></li><li><a href=#csai-1>cs.AI (1)</a><ul><li><a href=#11--95119-the-case-for-animal-friendly-ai-sankalpa-ghose-et-al-2024>(1/1 | 95/119) The Case for Animal-Friendly AI (Sankalpa Ghose et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--96119-experimental-evaluation-of-the-etsi-dcc-adaptive-approach-and-related-algorithms-oscar-amador-et-al-2024>(1/1 | 96/119) Experimental Evaluation of the ETSI DCC Adaptive Approach and Related Algorithms (Oscar Amador et al., 2024)</a></li></ul></li><li><a href=#cscr-4>cs.CR (4)</a><ul><li><a href=#14--97119-employing-llms-for-incident-response-planning-and-review-sam-hays-et-al-2024>(1/4 | 97/119) Employing LLMs for Incident Response Planning and Review (Sam Hays et al., 2024)</a></li><li><a href=#24--98119-efficient-algorithm-level-error-detection-for-number-theoretic-transform-assessed-on-fpgas-kasra-ahmadi-et-al-2024>(2/4 | 98/119) Efficient Algorithm Level Error Detection for Number-Theoretic Transform Assessed on FPGAs (Kasra Ahmadi et al., 2024)</a></li><li><a href=#34--99119-autoattacker-a-large-language-model-guided-system-to-implement-automatic-cyber-attacks-jiacen-xu-et-al-2024>(3/4 | 99/119) AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks (Jiacen Xu et al., 2024)</a></li><li><a href=#44--100119-characterizing-ethereum-upgradable-smart-contracts-and-their-security-implications-xiaofan-li-et-al-2024>(4/4 | 100/119) Characterizing Ethereum Upgradable Smart Contracts and Their Security Implications (Xiaofan Li et al., 2024)</a></li></ul></li><li><a href=#cspf-1>cs.PF (1)</a><ul><li><a href=#11--101119-hetegen-heterogeneous-parallel-inference-for-large-language-models-on-resource-constrained-devices-xuanlei-zhao-et-al-2024>(1/1 | 101/119) HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices (Xuanlei Zhao et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--102119-inevitable-metaverse-a-novel-twitter-dataset-for-public-sentiments-on-metaverse-kadhim-hayawi-et-al-2024>(1/2 | 102/119) Inevitable-Metaverse: A Novel Twitter Dataset for Public Sentiments on Metaverse (Kadhim Hayawi et al., 2024)</a></li><li><a href=#22--103119-autonomous-intelligent-systems-from-illusion-of-control-to-inescapable-delusion-stéphane-grumbach-et-al-2024>(2/2 | 103/119) Autonomous Intelligent Systems: From Illusion of Control to Inescapable Delusion (Stéphane Grumbach et al., 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#11--104119-high-dimensional-tail-index-regression-with-an-application-to-text-analyses-of-viral-posts-in-social-media-yuya-sasaki-et-al-2024>(1/1 | 104/119) High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media (Yuya Sasaki et al., 2024)</a></li></ul></li><li><a href=#csir-1>cs.IR (1)</a><ul><li><a href=#11--105119-supplier-recommendation-in-online-procurement-victor-coscrato-et-al-2024>(1/1 | 105/119) Supplier Recommendation in Online Procurement (Victor Coscrato et al., 2024)</a></li></ul></li><li><a href=#csro-5>cs.RO (5)</a><ul><li><a href=#15--106119-smooth-computation-without-input-delay-robust-tube-based-model-predictive-control-for-robot-manipulator-planning-qie-sima-et-al-2024>(1/5 | 106/119) Smooth Computation without Input Delay: Robust Tube-Based Model Predictive Control for Robot Manipulator Planning (Qie Sima et al., 2024)</a></li><li><a href=#25--107119-shaping-multi-robot-patrol-performance-with-heterogeneity-in-individual-learning-behavior-connor-york-et-al-2024>(2/5 | 107/119) Shaping Multi-Robot Patrol Performance with Heterogeneity in Individual Learning Behavior (Connor York et al., 2024)</a></li><li><a href=#35--108119-a-non-cubic-space-filling-modular-robot-tyler-hummer-et-al-2024>(3/5 | 108/119) A non-cubic space-filling modular robot (Tyler Hummer et al., 2024)</a></li><li><a href=#45--109119-grid-based-fast-and-structural-visual-odometry-zhang-zhihe-2024>(4/5 | 109/119) Grid-based Fast and Structural Visual Odometry (Zhang Zhihe, 2024)</a></li><li><a href=#55--110119-a-cost-effective-cooperative-exploration-and-inspection-strategy-for-heterogeneous-aerial-system-xinhang-xu-et-al-2024>(5/5 | 110/119) A Cost-Effective Cooperative Exploration and Inspection Strategy for Heterogeneous Aerial System (Xinhang Xu et al., 2024)</a></li></ul></li><li><a href=#eesssy-2>eess.SY (2)</a><ul><li><a href=#12--111119-secure-and-scalable-network-slicing-with-plug-and-play-support-for-power-distribution-system-communication-networks-jian-zhong-et-al-2024>(1/2 | 111/119) Secure and Scalable Network Slicing with Plug-and-Play Support for Power Distribution System Communication Networks (Jian Zhong et al., 2024)</a></li><li><a href=#22--112119-real-time-hybrid-controls-of-energy-storage-and-load-shedding-for-integrated-power-and-energy-systems-of-ships-linh-vu-et-al-2024>(2/2 | 112/119) Real-time hybrid controls of energy storage and load shedding for integrated power and energy systems of ships (Linh Vu et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--113119-towards-accurate-lip-to-speech-synthesis-in-the-wild-sindhu-hegde-et-al-2024>(1/1 | 113/119) Towards Accurate Lip-to-Speech Synthesis in-the-Wild (Sindhu Hegde et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--114119-performance-evaluation-of-acceleration-of-convolutional-layers-on-openedgecgra-nicolò-carpentieri-et-al-2024>(1/1 | 114/119) Performance evaluation of acceleration of convolutional layers on OpenEdgeCGRA (Nicolò Carpentieri et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--115119-a-bayesian-committee-machine-potential-for-oxygen-containing-organic-compounds-seungwon-kim-et-al-2024>(1/1 | 115/119) A Bayesian Committee Machine Potential for Oxygen-containing Organic Compounds (Seungwon Kim et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--116119-graphmini-accelerating-graph-pattern-matching-using-auxiliary-graphs-juelin-liu-et-al-2024>(1/1 | 116/119) GraphMini: Accelerating Graph Pattern Matching Using Auxiliary Graphs (Juelin Liu et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--117119-network-analysis-using-krylov-subspace-trajectories-h-robert-frost-2024>(1/1 | 117/119) Network analysis using Krylov subspace trajectories (H. Robert Frost, 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--118119-gsl-lpa-fast-label-propagation-algorithm-lpa-for-community-detection-with-no-internally-disconnected-communities-subhajit-sahu-2024>(1/1 | 118/119) GSL-LPA: Fast Label Propagation Algorithm (LPA) for Community Detection with no Internally-Disconnected Communities (Subhajit Sahu, 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--119119-a-strongly-subcubic-combinatorial-algorithm-for-triangle-detection-with-applications-adrian-dumitrescu-2024>(1/1 | 119/119) A Strongly Subcubic Combinatorial Algorithm for Triangle Detection with Applications (Adrian Dumitrescu, 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>