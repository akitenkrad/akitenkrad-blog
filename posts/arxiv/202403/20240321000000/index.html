<!doctype html><html><head><title>arXiv @ 2024.03.21</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.21"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cond-mat.stat-mech (1) cs.AI (7) cs.CE (1) cs.CG (1) cs.CL (45) cs.CR (8) cs.CV (104) cs.CY (3) cs.DB (4) cs.DC (3) cs.DM (2) cs.DS (2) cs.ET (1) cs.GR (1) cs.HC (4) cs.IR (5) cs.IT (4) cs.LG (59) cs.MA (2) cs.MM (2) cs.NE (3) cs.NI (3) cs.RO (24) cs.SD (2) cs.SE (3) cs.SI (3) eess.AS (1) eess.IV (8) eess.SP (1) eess.SY (3) math.AT (1) math.CO (1) math.NA (8) math."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240321000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-21T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-21T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.21"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240321000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Mar 21, 2024</p></div><div class=title><h1>arXiv @ 2024.03.21</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cond-matstat-mech-1>cond-mat.stat-mech (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csai-7>cs.AI (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cscl-45>cs.CL (45)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cscr-8>cs.CR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cscv-104>cs.CV (104)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csdb-4>cs.DB (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csdc-3>cs.DC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csdm-2>cs.DM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cset-1>cs.ET (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cslg-59>cs.LG (59)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csmm-2>cs.MM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csne-3>cs.NE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csro-24>cs.RO (24)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#cssi-3>cs.SI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#eessiv-8>eess.IV (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#eesssy-3>eess.SY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#mathat-1>math.AT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#mathna-8>math.NA (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#physicsacc-ph-1>physics.acc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>5</td><td>3</td><td></td></tr><tr><td>Autoencoder</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Automatic Evaluation</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td></td><td>1</td><td></td></tr><tr><td>BERT</td><td>3</td><td></td><td>2</td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Benchmarking</td><td>12</td><td>28</td><td>16</td><td>1</td></tr><tr><td>Black Box</td><td>2</td><td></td><td>2</td><td>1</td></tr><tr><td>ChatGPT</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Chatbot</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td>2</td><td>2</td></tr><tr><td>Contrastive Learning</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Convolution</td><td></td><td>14</td><td>7</td><td></td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td>11</td><td>9</td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>5</td><td>3</td><td></td></tr><tr><td>Dialogue Response Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Dialogue State Tracking</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>18</td><td>2</td><td>1</td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Distribution Shift</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Explainable AI</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Fake News Detection</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>4</td><td></td></tr><tr><td>Few-shot</td><td>5</td><td>7</td><td>3</td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>14</td><td>10</td><td>8</td><td>3</td></tr><tr><td>Foundation Model</td><td></td><td>8</td><td>1</td><td></td></tr><tr><td>GPT</td><td>9</td><td>2</td><td>1</td><td>1</td></tr><tr><td>GPT-3</td><td>3</td><td></td><td></td><td>1</td></tr><tr><td>GPT-3.5</td><td>2</td><td></td><td></td><td>1</td></tr><tr><td>GPT-4</td><td>6</td><td></td><td>1</td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Gemini</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>9</td><td>1</td><td>1</td></tr><tr><td>Graph</td><td>2</td><td>5</td><td>7</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>8</td><td></td></tr><tr><td>Grounding</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Hate Speech Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Heuristic Approach</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>4</td><td>4</td><td>3</td><td></td></tr><tr><td>Information Retrieval</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>1</td><td>7</td><td></td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>Knowledge Transfer</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>1</td><td></td><td>1</td><td>1</td></tr><tr><td>LSTM</td><td>1</td><td>3</td><td>3</td><td></td></tr><tr><td>Label Smoothing</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Large Language Model</td><td>39</td><td>16</td><td>8</td><td>8</td></tr><tr><td>Low-Resource</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Multi-modal</td><td>2</td><td>28</td><td>13</td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Mutual Information</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Neural Machine Translation</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>8</td><td>1</td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Optical Character Recognition</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>4</td><td></td><td>2</td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Prompt</td><td>15</td><td>10</td><td>4</td><td>2</td></tr><tr><td>Prompt Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td>4</td><td></td></tr><tr><td>Question Answering</td><td>4</td><td>1</td><td>2</td><td></td></tr><tr><td>Reasoning</td><td>9</td><td>5</td><td>2</td><td>1</td></tr><tr><td>Reconstruction Loss</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td>1</td><td>6</td><td>4</td></tr><tr><td>Relation Extraction</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>5</td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>9</td><td>5</td><td>1</td></tr><tr><td>Self-supervised Pre-training</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td></td><td>3</td><td>9</td></tr><tr><td>Simulator</td><td>1</td><td></td><td>3</td><td>9</td></tr><tr><td>Speech-to-Speech Translation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>6</td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>3</td><td>4</td><td>3</td><td>1</td></tr><tr><td>T5</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>6</td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>7</td><td>17</td><td>8</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>6</td><td>4</td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td>8</td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>12</td><td>1</td><td>1</td></tr><tr><td>Visual Question Answering</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Yolo</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td>14</td><td>2</td><td>2</td></tr><tr><td>Zero-shot Learning</td><td></td><td>2</td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cslg-59>cs.LG (59)</h2><h3 id=159--1329-vl-icl-bench-the-devil-in-the-details-of-benchmarking-multimodal-in-context-learning-yongshuo-zong-et-al-2024>(1/59 | 1/329) VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning (Yongshuo Zong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales. (2024)<br><strong>VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning</strong><br><button class=copy-to-clipboard title="VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 152<br>Keywords: Benchmarking, Benchmarking, Few-shot, Multi-modal, Multi-modal, GPT, GPT-4, Grounding, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13164v1.pdf filename=2403.13164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> famously exhibit emergent <b>in-context</b> <b>learning</b> <b>(ICL)</b> &ndash; the ability to rapidly adapt to new tasks using <b>few-shot</b> examples provided as a <b>prompt,</b> without updating the model&rsquo;s weights. Built on top of <b>LLMs,</b> vision <b>large</b> <b>language</b> <b>models</b> (VLLMs) have advanced significantly in areas such as recognition, <b>reasoning,</b> and <b>grounding.</b> However, investigations into \emph{multimodal ICL} have predominantly focused on <b>few-shot</b> <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA),</b> and image captioning, which we will show neither exploit the strengths of <b>ICL,</b> nor test its limitations. The broader capabilities and limitations of <b>multimodal</b> <b>ICL</b> remain under-explored. In this study, we introduce a comprehensive <b>benchmark</b> VL-ICL Bench for <b>multimodal</b> <b>in-context</b> <b>learning,</b> encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to <b>reasoning</b> and long context length}. We evaluate the abilities of state-of-the-art VLLMs against this <b>benchmark</b> suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as <b>GPT-4,</b> find the tasks challenging. By highlighting a range of new <b>ICL</b> tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the <b>in-context</b> <b>learning</b> capabilities of VLLMs, as well as inspire new applications that leverage VLLM <b>ICL.</b> The code and dataset are available at <a href=https://github.com/ys-zong/VL-ICL>https://github.com/ys-zong/VL-ICL</a>.</p></p class="citation"></blockquote><h3 id=259--2329-a-comparison-of-deep-learning-architectures-for-spacecraft-anomaly-detection-daniel-lakey-et-al-2024>(2/59 | 2/329) A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection (Daniel Lakey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Lakey, Tim Schlippe. (2024)<br><strong>A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection</strong><br><button class=copy-to-clipboard title="A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 100<br>Keywords: Anomaly Detection, Convolution, Convolutional Neural Network, Convolutional Neural Network, LSTM, LSTM, LSTM, Recurrent Neural Network, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12864v1.pdf filename=2403.12864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for <b>anomaly</b> <b>detection</b> in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs),</b> <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> networks, and <b>Transformer-based</b> architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and <b>anomaly</b> <b>types.</b> Initial results indicate that while <b>CNNs</b> excel in identifying spatial patterns and may be effective for some classes of spacecraft data, <b>LSTMs</b> and <b>RNNs</b> show a marked proficiency in capturing temporal anomalies seen in time-series spacecraft telemetry. The <b>Transformer-based</b> architectures, given their ability to focus on both local and global contexts, have showcased promising results, especially in scenarios where anomalies are subtle and span over longer durations. Additionally, considerations such as computational efficiency, ease of deployment, and real-time processing capabilities were evaluated. While <b>CNNs</b> and <b>LSTMs</b> demonstrated a balance between accuracy and computational demands, <b>Transformer</b> architectures, though highly accurate, require significant computational resources. In conclusion, the choice of deep learning architecture for spacecraft <b>anomaly</b> <b>detection</b> is highly contingent on the nature of the data, the type of anomalies, and operational constraints.</p></p class="citation"></blockquote><h3 id=359--3329-adapt-to-robustify-prompt-tuning-vision-transformers-masih-eskandar-et-al-2024>(3/59 | 3/329) ADAPT to Robustify Prompt Tuning Vision Transformers (Masih Eskandar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masih Eskandar, Tooba Imtiaz, Zifeng Wang, Jennifer Dy. (2024)<br><strong>ADAPT to Robustify Prompt Tuning Vision Transformers</strong><br><button class=copy-to-clipboard title="ADAPT to Robustify Prompt Tuning Vision Transformers" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 70<br>Keywords: Vision Transformer, Adversarial Learning, Fine-tuning, Transformer, Prompt, Vision Transformer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13196v1.pdf filename=2403.13196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of deep models, including <b>Vision</b> <b>Transformers,</b> is known to be vulnerable to <b>adversarial</b> <b>attacks.</b> Many existing defenses against these attacks, such as <b>adversarial</b> <b>training,</b> rely on full-model <b>fine-tuning</b> to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient <b>prompt</b> tuning is used to adapt large <b>transformer-based</b> models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient <b>prompt</b> tuning of <b>Vision</b> <b>Transformers</b> for downstream tasks under the lens of robustness. We show that previous <b>adversarial</b> <b>defense</b> methods, when applied to the <b>prompt</b> tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive <b>adversarial</b> <b>training</b> in the <b>prompt</b> tuning paradigm. Our method achieves competitive robust accuracy of ~40% w.r.t. SOTA robustness methods using full-model <b>fine-tuning,</b> by tuning only ~1% of the number of parameters.</p></p class="citation"></blockquote><h3 id=459--4329-wildfire-danger-prediction-optimization-with-transfer-learning-spiros-maggioros-et-al-2024>(4/59 | 4/329) Wildfire danger prediction optimization with transfer learning (Spiros Maggioros et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Spiros Maggioros, Nikos Tsalkitzis. (2024)<br><strong>Wildfire danger prediction optimization with transfer learning</strong><br><button class=copy-to-clipboard title="Wildfire danger prediction optimization with transfer learning" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Object Detection, Anomaly Detection, Convolution, Convolutional Neural Network, Convolutional Neural Network, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12871v1.pdf filename=2403.12871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> have proven instrumental across various computer science domains, enabling advancements in <b>object</b> <b>detection,</b> classification, and <b>anomaly</b> <b>detection.</b> This paper explores the application of <b>CNNs</b> to analyze geospatial data specifically for identifying wildfire-affected areas. Leveraging <b>transfer</b> <b>learning</b> techniques, we <b>fine-tuned</b> <b>CNN</b> hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess moisture conditions. The study establishes a methodology for computing wildfire risk levels on a scale of 0 to 5, dynamically linked to weather patterns. Notably, through the integration of <b>transfer</b> <b>learning,</b> the <b>CNN</b> model achieved an impressive accuracy of 95% in identifying burnt areas. This research sheds light on the inner workings of <b>CNNs</b> and their practical, real-time utility in predicting and mitigating wildfires. By combining <b>transfer</b> <b>learning</b> and <b>CNNs,</b> this study contributes a robust approach to assess burnt areas, facilitating timely interventions and preventative measures against conflagrations.</p></p class="citation"></blockquote><h3 id=559--5329-pretraining-codomain-attention-neural-operators-for-solving-multiphysics-pdes-md-ashiqur-rahman-et-al-2024>(5/59 | 5/329) Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs (Md Ashiqur Rahman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Ashiqur Rahman, Robert Joseph George, Mogab Elleithy, Daniel Leibovici, Zongyi Li, Boris Bonev, Colin White, Julius Berner, Raymond A. Yeh, Jean Kossaifi, Kamyar Azizzadenesheli, Anima Anandkumar. (2024)<br><strong>Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs</strong><br><button class=copy-to-clipboard title="Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, Self-supervised Learning, Self-supervised Learning, Simulation, Simulator, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12553v1.pdf filename=2403.12553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs), due to complex geometries, interactions between physical variables, and the lack of large amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling <b>self-supervised</b> <b>learning</b> or pretraining of multiple PDE systems. Specifically, we extend positional encoding, <b>self-attention,</b> and normalization layers to the function space. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO&rsquo;s potential as a backbone for learning multiphysics PDEs over multiple systems by considering <b>few-shot</b> <b>learning</b> settings. On complex downstream tasks with limited data, such as fluid flow <b>simulations</b> and fluid-structure interactions, we found CoDA-NO to outperform existing methods on the <b>few-shot</b> <b>learning</b> task by over $36%$. The code is available at <a href=https://github.com/ashiq24/CoDA-NO>https://github.com/ashiq24/CoDA-NO</a>.</p></p class="citation"></blockquote><h3 id=659--6329-melting-point-mobile-evaluation-of-language-transformers-stefanos-laskaridis-et-al-2024>(6/59 | 6/329) MELTing point: Mobile Evaluation of Language Transformers (Stefanos Laskaridis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi. (2024)<br><strong>MELTing point: Mobile Evaluation of Language Transformers</strong><br><button class=copy-to-clipboard title="MELTing point: Mobile Evaluation of Language Transformers" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Quantization, Transformer, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12844v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12844v2.pdf filename=2403.12844v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence&rsquo;&rsquo;. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and <b>prompt</b> privacy becomes an ever more pressing issue, we explore the current state of mobile execution of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and <b>benchmarking</b> of <b>LLMs</b> on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction <b>fine-tuned</b> <b>LLMs</b> and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way. Our analysis is the first systematic study of on-device <b>LLM</b> execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models. Results highlight the performance heterogeneity across targets and corroborates that <b>LLM</b> inference is largely memory-bound. <b>Quantization</b> drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost. Drawing from its energy footprint and thermal behavior, the continuous execution of <b>LLMs</b> remains elusive, as both factors negatively affect user experience. Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost. We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments.</p></p class="citation"></blockquote><h3 id=759--7329-affinequant-affine-transformation-quantization-for-large-language-models-yuexiao-ma-et-al-2024>(7/59 | 7/329) AffineQuant: Affine Transformation Quantization for Large Language Models (Yuexiao Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, Rongrong Ji. (2024)<br><strong>AffineQuant: Affine Transformation Quantization for Large Language Models</strong><br><button class=copy-to-clipboard title="AffineQuant: Affine Transformation Quantization for Large Language Models" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Quantization, Zero-shot, LLaMA, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12544v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12544v1.pdf filename=2403.12544v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The significant resource requirements associated with <b>Large-scale</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training <b>Quantization</b> (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. Existing PTQ methods for <b>LLMs</b> limit the optimization scope to scaling transformations between pre- and post-quantization weights. In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). This approach extends the optimization scope and thus significantly minimizing <b>quantization</b> errors. Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and generalization capabilities. To ensure the invertibility of the transformation during optimization, we further introduce a gradual mask optimization method. This method initially focuses on optimizing the diagonal elements and gradually extends to the other elements. Such an approach aligns with the Levy-Desplanques theorem, theoretically ensuring invertibility of the transformation. As a result, significant performance improvements are evident across different <b>LLMs</b> on diverse datasets. To illustrate, we attain a C4 <b>perplexity</b> of 15.76 (2.26 lower vs 18.02 in OmniQuant) on the LLaMA2-7B model of W4A4 <b>quantization</b> without overhead. On <b>zero-shot</b> tasks, AffineQuant achieves an average of 58.61 accuracy (1.98 lower vs 56.63 in OmniQuant) when using 4/4-bit <b>quantization</b> for <b>LLaMA-30B,</b> which setting a new state-of-the-art <b>benchmark</b> for PTQ in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=859--8329-seven-pruning-transformer-model-by-reserving-sentinels-jinying-xiao-et-al-2024>(8/59 | 8/329) SEVEN: Pruning Transformer Model by Reserving Sentinels (Jinying Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinying Xiao, Ping Li, Jie Nie, Zhe Tang. (2024)<br><strong>SEVEN: Pruning Transformer Model by Reserving Sentinels</strong><br><button class=copy-to-clipboard title="SEVEN: Pruning Transformer Model by Reserving Sentinels" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Fine-tuning, Pruning, Transformer, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12688v1.pdf filename=2403.12688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale <b>Transformer</b> models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to <b>Convolutional</b> <b>Neural</b> <b>Networks,</b> commonly used <b>pruning</b> methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and <b>fine-tuning</b> TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved by SEVEN. Extensive experiments on various TM in natural language, <b>question-answering,</b> <b>and</b> image classification domains are conducted to validate the effectiveness of SEVEN. The results demonstrate significant improvements of SEVEN in multiple <b>pruning</b> scenarios and across different sparsity levels. Additionally, SEVEN exhibits robust performance under various <b>fine-tuning</b> strategies. The code is publicly available at <a href=https://github.com/xiaojinying/SEVEN>https://github.com/xiaojinying/SEVEN</a>.</p></p class="citation"></blockquote><h3 id=959--9329-sun-teams-contribution-to-abaw-2024-competition-audio-visual-valence-arousal-estimation-and-expression-recognition-denis-dresvyanskiy-et-al-2024>(9/59 | 9/329) SUN Team&rsquo;s Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition (Denis Dresvyanskiy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Denis Dresvyanskiy, Maxim Markitantov, Jiawei Yu, Peitong Li, Heysem Kaya, Alexey Karpov. (2024)<br><strong>SUN Team&rsquo;s Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition</strong><br><button class=copy-to-clipboard title="SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-1; I-2-6; I-2-10, cs-LG, cs.LG<br>Keyword Score: 56<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Fine-tuning, Multi-modal, Multi-modal, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12609v1.pdf filename=2403.12609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>emotions</b> <b>play</b> a central role in human communication, automatic <b>emotion</b> <b>recognition</b> has attracted increasing attention in the last two decades. While <b>multimodal</b> systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely &lsquo;in-the-wild&rsquo; data. This work investigates audiovisual deep learning approaches for <b>emotion</b> <b>recognition</b> in-the-wild problem. We particularly explore the effectiveness of architectures based on <b>fine-tuned</b> <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNN)</b> and Public Dimensional <b>Emotion</b> <b>Model</b> (PDEM), for video and audio modality, respectively. We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN). We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol.</p></p class="citation"></blockquote><h3 id=1059--10329-contextualized-messages-boost-graph-representations-brian-godwin-lim-2024>(10/59 | 10/329) Contextualized Messages Boost Graph Representations (Brian Godwin Lim, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brian Godwin Lim. (2024)<br><strong>Contextualized Messages Boost Graph Representations</strong><br><button class=copy-to-clipboard title="Contextualized Messages Boost Graph Representations" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12529v1.pdf filename=2403.12529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have gained significant interest in recent years due to their ability to handle arbitrarily structured data represented as <b>graphs.</b> <b>GNNs</b> <b>generally</b> follow the <b>message-passing</b> scheme to locally update node feature representations. A <b>graph</b> <b>readout</b> <b>function</b> is then employed to create a representation for the entire <b>graph.</b> <b>Several</b> <b>studies</b> proposed different <b>GNNs</b> by modifying the aggregation and combination strategies of the <b>message-passing</b> framework, often inspired by heuristics. Nevertheless, several studies have begun exploring <b>GNNs</b> from a theoretical perspective based on the <b>graph</b> <b>isomorphism</b> <b>problem</b> which inherently assumes countable node feature representations. Yet, there are only a few theoretical works exploring <b>GNNs</b> with uncountable node feature representations. This paper presents a new perspective on the representational capabilities of <b>GNNs</b> across all levels - node-level, neighborhood-level, and <b>graph-level</b> <b>-</b> <b>when</b> the space of node feature representation is uncountable. From the results, a novel soft-isomorphic relational <b>graph</b> <b>convolution</b> <b>network</b> (SIR-GCN) is proposed that emphasizes non-linear and contextualized transformations of neighborhood feature representations. The mathematical relationship of SIR-GCN and three widely used <b>GNNs</b> is explored to highlight the contribution. Validation on synthetic datasets then demonstrates that SIR-GCN outperforms comparable models even in simple node and <b>graph</b> <b>property</b> <b>prediction</b> tasks.</p></p class="citation"></blockquote><h3 id=1159--11329-ntk-guided-few-shot-class-incremental-learning-jingren-liu-et-al-2024>(11/59 | 11/329) NTK-Guided Few-Shot Class Incremental Learning (Jingren Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingren Liu, Zhong Ji, Yanwei Pang, YunLong Yu. (2024)<br><strong>NTK-Guided Few-Shot Class Incremental Learning</strong><br><button class=copy-to-clipboard title="NTK-Guided Few-Shot Class Incremental Learning" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Benchmarking, Convolution, Few-shot, Meta Learning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12486v1.pdf filename=2403.12486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model&rsquo;s potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a <b>meta-learning</b> <b>mechanism</b> grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate <b>self-supervised</b> <b>pre-training</b> on the base session to shape the initial network weights. Then they are carefully refined through curricular alignment, followed by the application of dual NTK regularization tailored specifically for both <b>convolutional</b> and linear layers. Through the combined effects of these measures, our network acquires robust NTK properties, significantly enhancing its foundational generalization. On popular FSCIL <b>benchmark</b> datasets, our NTK-FSCIL surpasses contemporary state-of-the-art approaches, elevating end-session accuracy by 2.9% to 8.7%.</p></p class="citation"></blockquote><h3 id=1259--12329-flowerformer-empowering-neural-architecture-encoding-using-a-flow-aware-graph-transformer-dongyeong-hwang-et-al-2024>(12/59 | 12/329) FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer (Dongyeong Hwang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongyeong Hwang, Hyunju Kim, Sunwoo Kim, Kijung Shin. (2024)<br><strong>FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer</strong><br><button class=copy-to-clipboard title="FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Message-Passing, Graph, Graph Neural Network, Representation Learning, Transformer, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12821v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12821v2.pdf filename=2403.12821v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a <b>graph,</b> <b>have</b> <b>shown</b> prominent performance. For enhanced <b>representation</b> <b>learning</b> of neural architectures, we introduce FlowerFormer, a powerful <b>graph</b> <b>transformer</b> <b>that</b> incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include <b>graph</b> <b>neural</b> <b>networks</b> and auto <b>speech</b> <b>recognition</b> models. Our code is available at <a href=http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer>http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer</a>.</p></p class="citation"></blockquote><h3 id=1359--13329-do-generated-data-always-help-contrastive-learning-yifei-wang-et-al-2024>(13/59 | 13/329) Do Generated Data Always Help Contrastive Learning? (Yifei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Wang, Jizhe Zhang, Yisen Wang. (2024)<br><strong>Do Generated Data Always Help Contrastive Learning?</strong><br><button class=copy-to-clipboard title="Do Generated Data Always Help Contrastive Learning?" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 48<br>Keywords: Diffusion Model, Benchmarking, Contrastive Learning, Data Augmentation, Representation Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12448v1.pdf filename=2403.12448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>Learning</b> (CL) has emerged as one of the most successful paradigms for <b>unsupervised</b> visual <b>representation</b> <b>learning,</b> yet it often depends on intensive manual <b>data</b> <b>augmentations.</b> With the rise of generative models, especially <b>diffusion</b> <b>models,</b> the ability to generate realistic images close to the real <b>data</b> <b>distribution</b> has been well recognized. These generated high-equality images have been successfully applied to enhance <b>contrastive</b> <b>representation</b> <b>learning,</b> a technique termed ``data inflation&rsquo;&rsquo;. However, we find that the generated <b>data</b> <b>(even</b> from a good <b>diffusion</b> <b>model</b> like DDPM) may sometimes even harm <b>contrastive</b> <b>learning.</b> We investigate the causes behind this failure from the perspective of both <b>data</b> <b>inflation</b> and <b>data</b> <b>augmentation.</b> For the first time, we reveal the complementary roles that stronger <b>data</b> <b>inflation</b> should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanations for these phenomena via deriving its generalization bounds under <b>data</b> <b>inflation.</b> Drawing from these insights, we propose Adaptive Inflation (AdaInf), a purely <b>data-centric</b> <b>strategy</b> without introducing any extra computation cost. On <b>benchmark</b> datasets, AdaInf can bring significant improvements for various <b>contrastive</b> <b>learning</b> methods. Notably, without using external <b>data,</b> <b>AdaInf</b> obtains 94.70% linear accuracy on CIFAR-10 with SimCLR, setting a new record that surpasses many sophisticated methods. Code is available at <a href=https://github.com/PKU-ML/adainf>https://github.com/PKU-ML/adainf</a>.</p></p class="citation"></blockquote><h3 id=1459--14329-tt-blip-enhancing-fake-news-detection-using-blip-and-tri-transformer-eunjee-choi-et-al-2024>(14/59 | 14/329) TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer (Eunjee Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eunjee Choi, Jong-Kook Kim. (2024)<br><strong>TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer</strong><br><button class=copy-to-clipboard title="TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, BERT, Fake News Detection, Fake News Detection, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12481v1.pdf filename=2403.12481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting <b>fake</b> <b>news</b> <b>has</b> received a lot of attention. Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated <b>multimodal</b> information. Also, the absence of specialized feature extraction for text and images further limits these methods. This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified <b>vision-language</b> understanding and generation (BLIP) for three types of information: <b>BERT</b> and BLIP\textsubscript{Txt} for text, ResNet and BLIP\textsubscript{Img} for images, and bidirectional BLIP encoders for <b>multimodal</b> information. The <b>Multimodal</b> Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved <b>multimodal</b> data analysis. The experiments are performed using two <b>fake</b> <b>news</b> <b>datasets,</b> Weibo and Gossipcop. The results indicate TT-BLIP outperforms the state-of-the-art models.</p></p class="citation"></blockquote><h3 id=1559--15329-fairsin-achieving-fairness-in-graph-neural-networks-through-sensitive-information-neutralization-cheng-yang-et-al-2024>(15/59 | 15/329) FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization (Cheng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Yang, Jixi Liu, Yunhe Yan, Chuan Shi. (2024)<br><strong>FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization</strong><br><button class=copy-to-clipboard title="FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12474v1.pdf filename=2403.12474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the remarkable success of <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> in modeling <b>graph-structured</b> <b>data,</b> <b>like</b> other machine learning models, <b>GNNs</b> are also susceptible to making biased predictions based on sensitive attributes, such as race and gender. For <b>fairness</b> consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking. However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and <b>fairness.</b> To address this issue, we unveil an innovative neutralization-based paradigm, where additional <b>Fairness-facilitating</b> Features (F3) are incorporated into node features or representations before message passing. The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nonsensitive information. We also provide theoretical explanations for our rationale, concluding that F3 can be realized by emphasizing the features of each node&rsquo;s heterogeneous neighbors (neighbors with different sensitive attributes). We name our method as FairSIN, and present three implementation variants from both data-centric and model-centric perspectives. Experimental results on five <b>benchmark</b> datasets with three different <b>GNN</b> backbones show that FairSIN significantly improves <b>fairness</b> metrics while maintaining high prediction accuracies.</p></p class="citation"></blockquote><h3 id=1659--16329-finding-the-missing-data-a-bert-inspired-approach-against-package-loss-in-wireless-sensing-zijian-zhao-et-al-2024>(16/59 | 16/329) Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing (Zijian Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Zhao, Tingwei Chen, Fanyi Meng, Hang Li, Xiaoyang Li, Guangxu Zhu. (2024)<br><strong>Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing</strong><br><button class=copy-to-clipboard title="Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, BERT, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12400v1.pdf filename=2403.12400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the development of various deep learning methods for Wi-Fi sensing, package loss often results in noncontinuous estimation of the Channel State Information (CSI), which negatively impacts the performance of the learning models. To overcome this challenge, we propose a deep learning model based on Bidirectional Encoder Representations from <b>Transformers</b> <b>(BERT)</b> for CSI recovery, named CSI-BERT. CSI-BERT can be trained in an <b>self-supervised</b> manner on the target dataset without the need for additional data. Furthermore, unlike traditional interpolation methods that focus on one subcarrier at a time, CSI-BERT captures the sequential relationships across different subcarriers. Experimental results demonstrate that CSI-BERT achieves lower error rates and faster speed compared to traditional interpolation methods, even when facing with high loss rates. Moreover, by harnessing the recovered CSI obtained from CSI-BERT, other deep learning models like Residual Network and <b>Recurrent</b> <b>Neural</b> <b>Network</b> can achieve an average increase in accuracy of approximately 15% in Wi-Fi sensing tasks. The collected dataset WiGesture and code for our model are publicly available at <a href=https://github.com/RS2002/CSI-BERT>https://github.com/RS2002/CSI-BERT</a>.</p></p class="citation"></blockquote><h3 id=1759--17329-learning-transferable-time-series-classifier-with-cross-domain-pre-training-from-language-model-mingyue-cheng-et-al-2024>(17/59 | 17/329) Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model (Mingyue Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyue Cheng, Xiaoyu Tao, Qi Liu, Hao Zhang, Yiheng Chen, Chenyi Lei. (2024)<br><strong>Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model</strong><br><button class=copy-to-clipboard title="Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Self-supervised Pre-training, Tokenization, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12372v1.pdf filename=2403.12372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advancements in <b>self-supervised</b> <b>pre-training</b> (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task. Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains. The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales. To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task. One of the key characteristics of CrossTimeNet is the newly designed time series <b>tokenization</b> module, which could effectively convert the raw time series into a sequence of discrete tokens based on a reconstruction optimization process. Besides, we highlight that predicting a high proportion of corrupted tokens can be very helpful for extracting informative patterns across different domains during SSL pre-training, which has been largely overlooked in past years. Furthermore, unlike previous works, our work treats the pre-training language model <b>(PLM)</b> as the initialization of the encoder network, investigating the feasibility of transferring the knowledge learned by the <b>PLM</b> to the time series area. Through these efforts, the path to cross-domain pre-training of a generic time series model can be effectively paved. We conduct extensive experiments in a real-world scenario across various time series classification domains. The experimental results clearly confirm CrossTimeNet&rsquo;s superior performance.</p></p class="citation"></blockquote><h3 id=1859--18329-sim2real-in-reconstructive-spectroscopy-deep-learning-with-augmented-device-informed-data-simulation-jiyi-chen-et-al-2024>(18/59 | 18/329) Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation (Jiyi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyi Chen, Pengyu Li, Yutong Wang, Pei-Cheng Ku, Qing Qu. (2024)<br><strong>Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation</strong><br><button class=copy-to-clipboard title="Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 40<br>Keywords: Data Augmentation, Distribution Shift, Distribution Shift, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12354v1.pdf filename=2403.12354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work proposes a deep learning (DL)-based framework, namely Sim2Real, for spectral signal reconstruction in reconstructive spectroscopy, focusing on efficient <b>data</b> <b>sampling</b> and fast inference time. The work focuses on the challenge of reconstructing real-world spectral signals under the extreme setting where only device-informed simulated <b>data</b> <b>are</b> available for training. Such device-informed simulated <b>data</b> <b>are</b> much easier to collect than real-world <b>data</b> <b>but</b> exhibit large <b>distribution</b> <b>shifts</b> from their real-world counterparts. To leverage such simulated <b>data</b> <b>effectively,</b> a hierarchical <b>data</b> <b>augmentation</b> strategy is introduced to mitigate the adverse effects of this domain shift, and a corresponding neural network for the spectral signal reconstruction with our augmented <b>data</b> <b>is</b> designed. Experiments using a real dataset measured from our spectrometer device demonstrate that Sim2Real achieves significant speed-up during the inference while attaining on-par performance with the state-of-the-art optimization-based methods.</p></p class="citation"></blockquote><h3 id=1959--19329-advancing-time-series-classification-with-multimodal-language-modeling-mingyue-cheng-et-al-2024>(19/59 | 19/329) Advancing Time Series Classification with Multimodal Language Modeling (Mingyue Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyue Cheng, Yiheng Chen, Qi Liu, Zhiding Liu, Yucong Luo. (2024)<br><strong>Advancing Time Series Classification with Multimodal Language Modeling</strong><br><button class=copy-to-clipboard title="Advancing Time Series Classification with Multimodal Language Modeling" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 39<br>Keywords: Benchmarking, Continuous Time, Continuous Time, Foundation Model, Multi-modal, Multi-modal, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12371v1.pdf filename=2403.12371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For the advancements of time series classification, scrutinizing previous studies, most existing methods adopt a common learning-to-classify paradigm - a time series classifier model tries to learn the relation between sequence inputs and target label encoded by one-hot distribution. Although effective, this paradigm conceals two inherent limitations: (1) encoding target categories with one-hot distribution fails to reflect the comparability and similarity between labels, and (2) it is very difficult to learn transferable model across domains, which greatly hinder the development of universal serving paradigm. In this work, we propose InstructTime, a novel attempt to reshape time series classification as a learning-to-generate paradigm. Relying on the powerful generative capacity of the <b>pre-trained</b> <b>language</b> <b>model,</b> the core idea is to formulate the classification of time series as a <b>multimodal</b> understanding task, in which both task-specific instructions and raw time series are treated as <b>multimodal</b> inputs while the label information is represented by texts. To accomplish this goal, three distinct designs are developed in the InstructTime. Firstly, a time series discretization module is designed to convert <b>continuous</b> <b>time</b> series into a sequence of hard tokens to solve the inconsistency issue across modal inputs. To solve the modality representation gap issue, for one thing, we introduce an alignment projected layer before feeding the transformed token of time series into language models. For another, we highlight the necessity of auto-regressive pre-training across domains, which can facilitate the transferability of the language model and boost the generalization performance. Extensive experiments are conducted over <b>benchmark</b> datasets, whose results uncover the superior performance of InstructTime and the potential for a universal <b>foundation</b> <b>model</b> in time series classification.</p></p class="citation"></blockquote><h3 id=2059--20329-prompt-fused-framework-for-inductive-logical-query-answering-zezhong-xu-et-al-2024>(20/59 | 20/329) Prompt-fused framework for Inductive Logical Query Answering (Zezhong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zezhong Xu, Peng Ye, Lei Liang, Huajun Chen, Wen Zhang. (2024)<br><strong>Prompt-fused framework for Inductive Logical Query Answering</strong><br><button class=copy-to-clipboard title="Prompt-fused framework for Inductive Logical Query Answering" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Knowledge Graph, Knowledge Graph, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12646v1.pdf filename=2403.12646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Answering logical queries on <b>knowledge</b> <b>graphs</b> <b>(KG)</b> poses a significant challenge for machine <b>reasoning.</b> The primary obstacle in this task stems from the inherent incompleteness of <b>KGs.</b> Existing research has predominantly focused on addressing the issue of missing edges in <b>KGs,</b> thereby neglecting another aspect of incompleteness: the emergence of new entities. Furthermore, most of the existing methods tend to reason over each logical operator separately, rather than comprehensively analyzing the query as a whole during the <b>reasoning</b> process. In this paper, we propose a query-aware <b>prompt-fused</b> framework named Pro-QE, which could incorporate existing query embedding methods and address the embedding of emerging entities through contextual information aggregation. Additionally, a query <b>prompt,</b> which is generated by encoding the symbolic query, is introduced to gather information relevant to the query from a holistic perspective. To evaluate the efficacy of our model in the inductive setting, we introduce two new challenging <b>benchmarks.</b> Experimental results demonstrate that our model successfully handles the issue of unseen entities in logical queries. Furthermore, the ablation study confirms the efficacy of the aggregator and <b>prompt</b> components.</p></p class="citation"></blockquote><h3 id=2159--21329-automated-contrastive-learning-strategy-search-for-time-series-baoyu-jing-et-al-2024>(21/59 | 21/329) Automated Contrastive Learning Strategy Search for Time Series (Baoyu Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baoyu Jing, Yansen Wang, Guoxin Sui, Jing Hong, Jingrui He, Yuqing Yang, Dongsheng Li, Kan Ren. (2024)<br><strong>Automated Contrastive Learning Strategy Search for Time Series</strong><br><button class=copy-to-clipboard title="Automated Contrastive Learning Strategy Search for Time Series" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 35<br>Keywords: Contrastive Learning, Data Augmentation, Reinforcement Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12641v1.pdf filename=2403.12641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>Contrastive</b> <b>Learning</b> (CL) has become a predominant <b>representation</b> <b>learning</b> paradigm for time series. Most existing methods in the literature focus on manually building specific <b>Contrastive</b> <b>Learning</b> Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn <b>representations</b> <b>for</b> various time series datasets and tasks, namely Automated <b>Contrastive</b> <b>Learning</b> (AutoCL). We first construct a principled universal search space of size over 3x1012, covering <b>data</b> <b>augmentation,</b> embedding transformation, <b>contrastive</b> <b>pair</b> construction and <b>contrastive</b> <b>losses.</b> Further, we introduce an efficient <b>reinforcement</b> <b>learning</b> algorithm, which optimizes CLS from the performance on the validation tasks, to obtain more effective CLS within the space. Experimental results on various real-world tasks and datasets demonstrate that AutoCL could automatically find the suitable CLS for a given dataset and task. From the candidate CLS found by AutoCL on several public datasets/tasks, we compose a transferable Generally Good Strategy (GGS), which has a strong performance for other datasets. We also provide empirical analysis as a guidance for future design of CLS.</p></p class="citation"></blockquote><h3 id=2259--22329-non-negative-contrastive-learning-yifei-wang-et-al-2024>(22/59 | 22/329) Non-negative Contrastive Learning (Yifei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Wang, Qi Zhang, Yaoyu Guo, Yisen Wang. (2024)<br><strong>Non-negative Contrastive Learning</strong><br><button class=copy-to-clipboard title="Non-negative Contrastive Learning" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 35<br>Keywords: Black Box, Contrastive Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12459v1.pdf filename=2403.12459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep representations have shown promising performance when transferred to downstream tasks in a <b>black-box</b> <b>manner.</b> Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative <b>Contrastive</b> <b>Learning</b> (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF&rsquo;s capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF&rsquo;s interpretability attributes, resulting in a more sparse and disentangled representation compared to standard <b>contrastive</b> <b>learning</b> (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these advantages enable NCL to outperform CL significantly on feature disentanglement, feature selection, as well as downstream classification tasks. At last, we show that NCL can be easily extended to other learning scenarios and benefit <b>supervised</b> <b>learning</b> as well. Code is available at <a href=https://github.com/PKU-ML/non_neg>https://github.com/PKU-ML/non_neg</a>.</p></p class="citation"></blockquote><h3 id=2359--23329-adaptsfl-adaptive-split-federated-learning-in-resource-constrained-edge-networks-zheng-lin-et-al-2024>(23/59 | 23/329) AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks (Zheng Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Lin, Guanqiao Qu, Wei Wei, Xianhao Chen, Kin K. Leung. (2024)<br><strong>AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks</strong><br><button class=copy-to-clipboard title="AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13101v1.pdf filename=2403.13101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split <b>federated</b> <b>learning</b> (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance communication-computing latency and training convergence. Extensive <b>simulations</b> across various datasets validate that our proposed AdaptSFL framework takes considerably less time to achieve a target accuracy than <b>benchmarks,</b> demonstrating the effectiveness of the proposed strategies.</p></p class="citation"></blockquote><h3 id=2459--24329-towards-better-statistical-understanding-of-watermarking-llms-zhongze-cai-et-al-2024>(24/59 | 24/329) Towards Better Statistical Understanding of Watermarking LLMs (Zhongze Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongze Cai, Shang Liu, Hanzhao Wang, Huaiyang Zhong, Xiaocheng Li. (2024)<br><strong>Towards Better Statistical Understanding of Watermarking LLMs</strong><br><button class=copy-to-clipboard title="Towards Better Statistical Understanding of Watermarking LLMs" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13027v1.pdf filename=2403.13027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the problem of watermarking <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We consider the trade-off between model distortion and detection ability and formulate it as a constrained optimization problem based on the green-red algorithm of Kirchenbauer et al. (2023a). We show that the optimal solution to the optimization problem enjoys a nice analytical property which provides a better understanding and inspires the algorithm design for the watermarking process. We develop an online dual gradient ascent watermarking algorithm in light of this optimization formulation and prove its asymptotic Pareto optimality between model distortion and detection ability. Such a result guarantees an averaged increased green list probability and henceforth detection ability explicitly (in contrast to previous results). Moreover, we provide a systematic discussion on the choice of the model distortion metrics for the watermarking problem. We justify our choice of KL divergence and present issues with the existing criteria of ``distortion-free&rsquo;&rsquo; and <b>perplexity.</b> Finally, we empirically evaluate our algorithms on extensive datasets against <b>benchmark</b> algorithms.</p></p class="citation"></blockquote><h3 id=2559--25329-sample-complexity-of-offline-distributionally-robust-linear-markov-decision-processes-he-wang-et-al-2024>(25/59 | 25/329) Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes (He Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He Wang, Laixi Shi, Yuejie Chi. (2024)<br><strong>Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes</strong><br><button class=copy-to-clipboard title="Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-TH<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12946v1.pdf filename=2403.12946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>offline</b> <b>reinforcement</b> <b>learning</b> (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy. To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes <b>(MDPs)</b> with an uncertainty set characterized by the total variation distance using <b>offline</b> <b>data.</b> <b>We</b> develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\tilde{O}(d)$, where $d$ is the feature dimension. We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed variance estimator.</p></p class="citation"></blockquote><h3 id=2659--26329-jetfire-efficient-and-accurate-transformer-pretraining-with-int8-data-flow-and-per-block-quantization-haocheng-xi-et-al-2024>(26/59 | 26/329) Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization (Haocheng Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haocheng Xi, Yuxiang Chen, Kang Zhao, Kaijun Zheng, Jianfei Chen, Jun Zhu. (2024)<br><strong>Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization</strong><br><button class=copy-to-clipboard title="Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Quantization, Quantization, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12422v1.pdf filename=2403.12422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretraining <b>transformers</b> are generally time-consuming. Fully <b>quantized</b> training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a <b>quantize-compute-dequantize</b> procedure, which often leads to suboptimal speedup and significant performance degradation when used in <b>transformers</b> due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to <b>transformers.</b> Our method features an INT8 data flow to optimize memory access and a per-block <b>quantization</b> method to maintain the accuracy of pretrained <b>transformers.</b> Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for <b>transformers.</b> Moreover, for a standard <b>transformer</b> block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory reduction compared to the FP16 baseline.</p></p class="citation"></blockquote><h3 id=2759--27329-understanding-training-free-diffusion-guidance-mechanisms-and-limitations-yifei-shen-et-al-2024>(27/59 | 27/329) Understanding Training-free Diffusion Guidance: Mechanisms and Limitations (Yifei Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Shen, Xinyang Jiang, Yezhen Wang, Yifan Yang, Dongqi Han, Dongsheng Li. (2024)<br><strong>Understanding Training-free Diffusion Guidance: Mechanisms and Limitations</strong><br><button class=copy-to-clipboard title="Understanding Training-free Diffusion Guidance: Mechanisms and Limitations" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Diffusion Model, Reinforcement Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12404v1.pdf filename=2403.12404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adding additional control to pretrained <b>diffusion</b> <b>models</b> has become an increasingly popular research area, with extensive applications in computer vision, <b>reinforcement</b> <b>learning,</b> and AI for science. Recently, several studies have proposed training-free <b>diffusion</b> <b>guidance</b> by using off-the-shelf networks pretrained on clean images. This approach enables <b>zero-shot</b> conditional generation for universal control formats, which appears to offer a free lunch in <b>diffusion</b> <b>guidance.</b> In this paper, we aim to develop a deeper understanding of the operational mechanisms and fundamental limitations of training-free guidance. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free methods are more susceptible to adversarial gradients and exhibit slower convergence rates compared to classifier guidance. We then introduce a collection of techniques designed to overcome the limitations, accompanied by theoretical rationale and empirical evidence. Our experiments in image and motion generation confirm the efficacy of these techniques.</p></p class="citation"></blockquote><h3 id=2859--28329-stg-mamba-spatial-temporal-graph-learning-via-selective-state-space-model-lincan-li-et-al-2024>(28/59 | 28/329) STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model (Lincan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lincan Li, Hanchen Wang, Wenjie Zhang, Adelle Coster. (2024)<br><strong>STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model</strong><br><button class=copy-to-clipboard title="STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12418v1.pdf filename=2403.12418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatial-Temporal <b>Graph</b> <b>(STG)</b> <b>data</b> is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal <b>graph</b> <b>learning.</b> <b>In</b> the past few years, various <b>GNN-based</b> methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system&rsquo;s dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal <b>Graph</b> <b>Mamba</b> <b>(STG-Mamba)</b> as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the <b>Graph</b> <b>Selective</b> <b>State</b> Space Block (GS3B) to precisely characterize the dynamic evolution of STG networks. STG-Mamba is formulated as an Encoder-Decoder architecture, which takes GS3B as the basic module, for efficient sequential data modeling. Furthermore, to strengthen <b>GNN&rsquo;s</b> ability of modeling STG data under the setting of SSSMs, we propose Kalman Filtering <b>Graph</b> <b>Neural</b> <b>Networks</b> (KFGN) for adaptive <b>graph</b> <b>structure</b> <b>upgrading.</b> KFGN smoothly fits in the context of selective state space evolution, and at the same time keeps linear complexity. Extensive empirical studies are conducted on three <b>benchmark</b> STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba. It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale <b>graph</b> <b>networks</b> <b>in</b> reducing the computational cost of FLOPs and test inference time.</p></p class="citation"></blockquote><h3 id=2959--29329-simple-ingredients-for-offline-reinforcement-learning-edoardo-cetin-et-al-2024>(29/59 | 29/329) Simple Ingredients for Offline Reinforcement Learning (Edoardo Cetin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edoardo Cetin, Andrea Tirinzoni, Matteo Pirotta, Alessandro Lazaric, Yann Ollivier, Ahmed Touati. (2024)<br><strong>Simple Ingredients for Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Simple Ingredients for Offline Reinforcement Learning" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13097v1.pdf filename=2403.13097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>reinforcement</b> <b>learning</b> algorithms have proven effective on datasets highly connected to the target downstream task. Yet, leveraging a novel testbed (MOOD) in which trajectories come from heterogeneous sources, we show that existing methods struggle with diverse data: their performance considerably deteriorates as data collected for related but different tasks is simply added to the <b>offline</b> <b>buffer.</b> <b>In</b> light of this finding, we conduct a large empirical study where we formulate and test several hypotheses to explain this failure. Surprisingly, we find that scale, more than algorithmic considerations, is the key factor influencing performance. We show that simple methods like AWAC and IQL with increased network size overcome the paradoxical failure modes from the inclusion of additional data in MOOD, and notably outperform prior state-of-the-art algorithms on the canonical D4RL <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=3059--30329-fairstg-countering-performance-heterogeneity-via-collaborative-sample-level-optimization-gengyu-lin-et-al-2024>(30/59 | 30/329) FairSTG: Countering performance heterogeneity via collaborative sample-level optimization (Gengyu Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gengyu Lin, Zhengyang Zhou, Qihe Huang, Kuo Yang, Shifen Cheng, Yang Wang. (2024)<br><strong>FairSTG: Countering performance heterogeneity via collaborative sample-level optimization</strong><br><button class=copy-to-clipboard title="FairSTG: Countering performance heterogeneity via collaborative sample-level optimization" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Fairness, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12391v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12391v1.pdf filename=2403.12391v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatiotemporal learning plays a crucial role in mobile computing techniques to empower smart cites. While existing research has made great efforts to achieve accurate predictions on the overall dataset, they still neglect the significant performance heterogeneity across samples. In this work, we designate the performance heterogeneity as the reason for unfair spatiotemporal learning, which not only degrades the practical functions of models, but also brings serious potential risks to real-world urban applications. To fix this gap, we propose a model-independent <b>Fairness-aware</b> framework for SpatioTemporal <b>Graph</b> learning (FairSTG), which inherits the idea of exploiting advantages of well-learned samples to challenging ones with collaborative mix-up. Specifically, FairSTG consists of a spatiotemporal feature extractor for model initialization, a collaborative representation enhancement for <b>knowledge</b> <b>transfer</b> between well-learned samples and challenging ones, and <b>fairness</b> objectives for immediately suppressing sample-level performance heterogeneity. Experiments on four spatiotemporal datasets demonstrate that our FairSTG significantly improves the <b>fairness</b> quality while maintaining comparable forecasting accuracy. Case studies show FairSTG can counter both spatial and temporal performance heterogeneity by our sample-level retrieval and compensation, and our work can potentially alleviate the risks on spatiotemporal resource allocation for underrepresented urban regions.</p></p class="citation"></blockquote><h3 id=3159--31329-multi-fidelity-surrogate-with-heterogeneous-input-spaces-for-modeling-melt-pools-in-laser-directed-energy-deposition-nandana-menon-et-al-2024>(31/59 | 31/329) Multi-fidelity surrogate with heterogeneous input spaces for modeling melt pools in laser-directed energy deposition (Nandana Menon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nandana Menon, Amrita Basak. (2024)<br><strong>Multi-fidelity surrogate with heterogeneous input spaces for modeling melt pools in laser-directed energy deposition</strong><br><button class=copy-to-clipboard title="Multi-fidelity surrogate with heterogeneous input spaces for modeling melt pools in laser-directed energy deposition" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 21<br>Keywords: Gaussian Process, Geometry, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13136v1.pdf filename=2403.13136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-fidelity (MF) modeling is a powerful statistical approach that can intelligently blend data from varied fidelity sources. This approach finds a compelling application in predicting melt pool <b>geometry</b> for laser-directed energy deposition (L-DED). One major challenge in using MF surrogates to merge a hierarchy of melt pool models is the variability in input spaces. To address this challenge, this paper introduces a novel approach for constructing an MF surrogate for predicting melt pool <b>geometry</b> by integrating models of varying complexity, that operate on heterogeneous input spaces. The first thermal model incorporates five input parameters i.e., laser power, scan velocity, powder flow rate, carrier gas flow rate, and nozzle height. In contrast, the second thermal model can only handle laser power and scan velocity. A mapping is established between the heterogeneous input spaces so that the five-dimensional space can be morphed into a pseudo two-dimensional space. Predictions are then blended using a <b>Gaussian</b> <b>process-based</b> co-kriging method. The resulting heterogeneous multi-fidelity <b>Gaussian</b> <b>process</b> (Het-MFGP) surrogate not only improves predictive accuracy but also offers computational efficiency by reducing evaluations required from the high-dimensional, high-fidelity thermal model. The results underscore the benefits of employing Het-MFGP for modeling melt pool behavior in L-DED. The framework successfully demonstrates how to leverage <b>multimodal</b> data and handle scenarios where certain input parameters may be difficult to model or measure.</p></p class="citation"></blockquote><h3 id=3259--32329-most-likely-sequence-generation-for-n-grams-transformers-hmms-and-markov-chains-by-using-rollout-algorithms-yuchao-li-et-al-2024>(32/59 | 32/329) Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms (Yuchao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchao Li, Dimitri Bertsekas. (2024)<br><strong>Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms</strong><br><button class=copy-to-clipboard title="Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: ChatGPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15465v1.pdf filename=2403.15465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we consider a <b>transformer</b> with an $n$-gram structure, such as the one underlying <b>ChatGPT.</b> The <b>transformer</b> provides next word probabilities, which can be used to generate word sequences. We consider methods for computing word sequences that are highly likely, based on these probabilities. Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we propose methods to compute highly likely sequences of $N$ words in time that is a low order polynomial in $N$ and in the vocabulary size of the $n$-gram. These methods are based on the rollout approach from approximate dynamic programming, a form of single policy iteration, which can improve the performance of any given heuristic policy. In our case we use a greedy heuristic that generates as next word one that has the highest probability. We show with analysis, examples, and computational experimentation that our methods are capable of generating highly likely sequences with a modest increase in computation over the greedy heuristic. While our analysis and experiments are focused on Markov chains of the type arising in <b>transformer</b> and <b>ChatGPT-like</b> models, our methods apply to general finite-state Markov chains, and related inference applications of Hidden Markov Models (HMM), where Viterbi decoding is used extensively.</p></p class="citation"></blockquote><h3 id=3359--33329-probabilistic-circuits-with-constraints-via-convex-optimization-soroush-ghandi-et-al-2024>(33/59 | 33/329) Probabilistic Circuits with Constraints via Convex Optimization (Soroush Ghandi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soroush Ghandi, Benjamin Quost, Cassio de Campos. (2024)<br><strong>Probabilistic Circuits with Constraints via Convex Optimization</strong><br><button class=copy-to-clipboard title="Probabilistic Circuits with Constraints via Convex Optimization" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13125v1.pdf filename=2403.13125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work addresses integrating <b>probabilistic</b> <b>propositional</b> logic constraints into the distribution encoded by a <b>probabilistic</b> <b>circuit</b> (PC). PCs are a class of tractable models that allow efficient computations (such as conditional and marginal probabilities) while achieving state-of-the-art performance in some domains. The proposed approach takes both a PC and constraints as inputs, and outputs a new PC that satisfies the constraints. This is done efficiently via convex optimization without the need to retrain the entire model. Empirical evaluations indicate that the combination of constraints and PCs can have multiple use cases, including the improvement of model performance under scarce or incomplete data, as well as the enforcement of machine learning <b>fairness</b> measures into the model without compromising model fitness. We believe that these ideas will open possibilities for multiple other applications involving the combination of logics and deep <b>probabilistic</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=3459--34329-neural-parameter-regression-for-explicit-representations-of-pde-solution-operators-konrad-mundinger-et-al-2024>(34/59 | 34/329) Neural Parameter Regression for Explicit Representations of PDE Solution Operators (Konrad Mundinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konrad Mundinger, Max Zimmer, Sebastian Pokutta. (2024)<br><strong>Neural Parameter Regression for Explicit Representations of PDE Solution Operators</strong><br><button class=copy-to-clipboard title="Neural Parameter Regression for Explicit Representations of PDE Solution Operators" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 20<br>Keywords: Fine-tuning, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12764v1.pdf filename=2403.12764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs). Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters. By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces. Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability. The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid <b>fine-tuning</b> and inference, even in cases of <b>out-of-distribution</b> examples.</p></p class="citation"></blockquote><h3 id=3559--35329-bilora-a-bi-level-optimization-framework-for-overfitting-resilient-low-rank-adaptation-of-large-pre-trained-models-rushi-qiang-et-al-2024>(35/59 | 35/329) BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models (Rushi Qiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rushi Qiang, Ruiyi Zhang, Pengtao Xie. (2024)<br><strong>BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models</strong><br><button class=copy-to-clipboard title="BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Natural Language Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13037v1.pdf filename=2403.13037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-rank adaptation (LoRA) is a popular method for <b>fine-tuning</b> large-scale pre-trained models in downstream tasks by learning low-rank incremental matrices. Though LoRA and its variants effectively reduce the number of trainable parameters compared to full <b>fine-tuning</b> methods, they often overfit training data, resulting in sub-optimal generalization on test data. To address this problem, we introduce BiLoRA, an overfitting-alleviating <b>fine-tuning</b> approach based on bi-level optimization (BLO). BiLoRA employs pseudo singular value decomposition to parameterize low-rank incremental matrices and splits the training of pseudo singular vectors and values across two different subsets of training data. This division, embedded within separate levels of the BLO framework, mitigates the risk of overfitting to a single dataset. Tested on ten datasets covering <b>natural</b> <b>language</b> <b>understanding</b> and generation tasks and applied to various well-known large pre-trained models, BiLoRA significantly outperforms LoRA methods and other <b>fine-tuning</b> approaches, with similar amounts of trainable parameters.</p></p class="citation"></blockquote><h3 id=3659--36329-lnpt-label-free-network-pruning-and-training-jinying-xiao-et-al-2024>(36/59 | 36/329) LNPT: Label-free Network Pruning and Training (Jinying Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinying Xiao, Ping Li, Zhe Tang, Jie Nie. (2024)<br><strong>LNPT: Label-free Network Pruning and Training</strong><br><button class=copy-to-clipboard title="LNPT: Label-free Network Pruning and Training" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Pruning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12690v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12690v2.pdf filename=2403.12690v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pruning</b> before training enables the deployment of neural networks on smart devices. By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices. It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance. However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance. In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization. Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance. We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online guidance for network <b>pruning</b> and learning on smart devices with unlabeled data. Our results demonstrate the superiority of this approach over <b>supervised</b> training.</p></p class="citation"></blockquote><h3 id=3759--37329-hybrid-unsupervised-learning-strategy-for-monitoring-industrial-batch-processes-christian-w-frey-2024>(37/59 | 37/329) Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch Processes (Christian W. Frey, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian W. Frey. (2024)<br><strong>Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch Processes</strong><br><button class=copy-to-clipboard title="Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch Processes" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SP, eess-SY<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13032v1.pdf filename=2403.13032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industrial production processes, especially in the pharmaceutical industry, are complex systems that require continuous monitoring to ensure efficiency, product quality, and safety. This paper presents a hybrid <b>unsupervised</b> <b>learning</b> strategy (HULS) for monitoring complex industrial processes. Addressing the limitations of traditional Self-Organizing Maps (SOMs), especially in scenarios with unbalanced data sets and highly correlated process variables, HULS combines existing <b>unsupervised</b> <b>learning</b> techniques to address these challenges. To evaluate the performance of the HULS concept, comparative experiments are performed based on a laboratory batch</p></p class="citation"></blockquote><h3 id=3859--38329-electioneering-the-network-dynamic-multi-step-adversarial-attacks-for-community-canvassing-saurabh-sharma-et-al-2024>(38/59 | 38/329) Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for Community Canvassing (Saurabh Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saurabh Sharma, Ambuj SIngh. (2024)<br><strong>Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for Community Canvassing</strong><br><button class=copy-to-clipboard title="Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for Community Canvassing" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs-SI, cs.LG<br>Keyword Score: 20<br>Keywords: Graph Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12399v1.pdf filename=2403.12399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem of online social network manipulation for community canvassing is of real concern in today&rsquo;s world. Motivated by the study of voter models, opinion and polarization dynamics on networks, we model community canvassing as a dynamic process over a network enabled via gradient-based attacks on <b>GNNs.</b> Existing attacks on <b>GNNs</b> are all single-step and do not account for the dynamic cascading nature of information diffusion in networks. We consider the realistic scenario where an adversary uses a <b>GNN</b> as a proxy to predict and manipulate voter preferences, especially uncertain voters. Gradient-based attacks on the <b>GNN</b> inform the adversary of strategic manipulations that can be made to proselytize targeted voters. In particular, we explore $\textit{minimum budget attacks for community canvassing}$ (MBACC). We show that the MBACC problem is NP-Hard and propose Dynamic Multi-Step <b>Adversarial</b> <b>Community</b> Canvassing (MAC) to address it. MAC makes dynamic local decisions based on the heuristic of low budget and high second-order influence to convert and perturb target voters. MAC is a dynamic multi-step attack that discovers low-budget and high-influence targets from which efficient cascading attacks can happen. We evaluate MAC against single-step baselines on the MBACC problem with multiple underlying networks and <b>GNN</b> models. Our experiments show the superiority of MAC which is able to discover efficient multi-hop attacks for <b>adversarial</b> <b>community</b> canvassing. Our code implementation and data is available at <a href=https://github.com/saurabhsharma1993/mac>https://github.com/saurabhsharma1993/mac</a>.</p></p class="citation"></blockquote><h3 id=3959--39329-u-net-kalman-filter-unetkf-an-example-of-machine-learning-assisted-ensemble-data-assimilation-feiyu-lu-2024>(39/59 | 39/329) U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted Ensemble Data Assimilation (Feiyu Lu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feiyu Lu. (2024)<br><strong>U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted Ensemble Data Assimilation</strong><br><button class=copy-to-clipboard title="U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted Ensemble Data Assimilation" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-ao-ph<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12366v1.pdf filename=2403.12366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning techniques have seen a tremendous rise in popularity in weather and climate sciences. Data assimilation (DA), which combines observations and numerical models, has great potential to incorporate machine learning and artificial intelligence (ML/AI) techniques. In this paper, we use U-Net, a type of <b>convolutional</b> neutral network <b>(CNN),</b> to predict the localized ensemble covariances for the Ensemble Kalman Filter (EnKF) algorithm. Using a 2-layer quasi-geostrophic model, U-Nets are trained using data from EnKF DA experiments. The trained U-Nets are then used to predict the flow-dependent localized error covariance matrices in U-Net Kalman Filter (UNetKF) experiments, which are compared to traditional 3-dimensional variational (3DVar), ensemble 3DVar (En3DVar) and EnKF methods. The performance of UNetKF can match or exceed that of 3DVar, En3DVar or EnKF. We also demonstrate that trained U-Nets can be transferred to a higher-resolution model for UNetKF implementation, which again performs competitively to 3DVar and EnKF, particularly for small ensemble sizes.</p></p class="citation"></blockquote><h3 id=4059--40329-policy-bifurcation-in-safe-reinforcement-learning-wenjun-zou-et-al-2024>(40/59 | 40/329) Policy Bifurcation in Safe Reinforcement Learning (Wenjun Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjun Zou, Yao Lv, Jie Li, Yujie Yang, Shengbo Eben Li, Jingliang Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, Keqiang Li. (2024)<br><strong>Policy Bifurcation in Safe Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Policy Bifurcation in Safe Reinforcement Learning" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12847v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12847v2.pdf filename=2403.12847v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safe <b>reinforcement</b> <b>learning</b> (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifurcated policy, we propose a safe RL algorithm called <b>multimodal</b> policy optimization (MUPO), which utilizes a Gaussian mixture distribution as the policy output. The bifurcated behavior can be achieved by selecting the Gaussian component with the highest mixing coefficient. Besides, MUPO also integrates spectral normalization and forward KL divergence to enhance the policy&rsquo;s capability of exploring different modes. Experiments with vehicle control tasks show that our algorithm successfully learns the bifurcated policy and ensures satisfying safety, while a continuous policy suffers from inevitable constraint violations.</p></p class="citation"></blockquote><h3 id=4159--41329-has-approximate-machine-unlearning-been-evaluated-properly-from-auditing-to-side-effects-cheng-long-wang-et-al-2024>(41/59 | 41/329) Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects (Cheng-Long Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng-Long Wang, Qi Li, Zihang Xiang, Di Wang. (2024)<br><strong>Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects</strong><br><button class=copy-to-clipboard title="Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12830v1.pdf filename=2403.12830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing concerns surrounding data privacy and security have underscored the critical necessity for <b>machine</b> <b>unlearning&ndash;aimed</b> at fully removing data lineage from <b>machine</b> <b>learning</b> models. MLaaS providers expect this to be their ultimate safeguard for regulatory compliance. Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of <b>machine</b> <b>unlearning</b> has been disappointingly slow, with this vital area often receiving insufficient focus. This paper seeks to address this shortfall by introducing well-defined and effective metrics for <b>black-box</b> <b>unlearning</b> auditing tasks. We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing. By relying exclusively on the original and unlearned models&ndash;eliminating the need to train additional shadow models&ndash;our approach simplifies the evaluation of unlearning at the individual data point level. Utilizing these metrics, we conduct an in-depth analysis of current approximate <b>machine</b> <b>unlearning</b> algorithms, identifying three key directions where these approaches fall short: utility, resilience, and equity. Our aim is that this work will greatly improve our understanding of approximate <b>machine</b> <b>unlearning</b> methods, taking a significant stride towards converting the theoretical right to data erasure into a auditable reality.</p></p class="citation"></blockquote><h3 id=4259--42329-robust-nas-under-adversarial-training-benchmark-theory-and-beyond-yongtao-wu-et-al-2024>(42/59 | 42/329) Robust NAS under adversarial training: benchmark, theory, and beyond (Yongtao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongtao Wu, Fanghui Liu, Carl-Johann Simon-Gabriel, Grigorios G Chrysos, Volkan Cevher. (2024)<br><strong>Robust NAS under adversarial training: benchmark, theory, and beyond</strong><br><button class=copy-to-clipboard title="Robust NAS under adversarial training: benchmark, theory, and beyond" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Adversarial Learning, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13134v1.pdf filename=2403.13134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in neural architecture search (NAS) emphasize the significance of considering robust architectures against malicious data. However, there is a notable absence of <b>benchmark</b> evaluations and theoretical guarantees for searching these robust architectures, especially when <b>adversarial</b> <b>training</b> is considered. In this work, we aim to address these two challenges, making twofold contributions. First, we release a comprehensive data set that encompasses both clean accuracy and robust accuracy for a vast array of adversarially trained networks from the NAS-Bench-201 search space on image datasets. Then, leveraging the neural tangent kernel (NTK) tool from deep learning theory, we establish a generalization theory for searching architecture in terms of clean accuracy and robust accuracy under multi-objective <b>adversarial</b> <b>training.</b> We firmly believe that our <b>benchmark</b> and theoretical insights will significantly benefit the NAS community through reliable reproducibility, efficient assessment, and theoretical foundation, particularly in the pursuit of robust architectures.</p></p class="citation"></blockquote><h3 id=4359--43329-predictive-scalable-and-interpretable-knowledge-tracing-on-structured-domains-hanqi-zhou-et-al-2024>(43/59 | 43/329) Predictive, scalable and interpretable knowledge tracing on structured domains (Hanqi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanqi Zhou, Robert Bamler, Charley M. Wu, Álvaro Tejero-Cantero. (2024)<br><strong>Predictive, scalable and interpretable knowledge tracing on structured domains</strong><br><button class=copy-to-clipboard title="Predictive, scalable and interpretable knowledge tracing on structured domains" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13179v1.pdf filename=2403.13179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner&rsquo;s progress (&lsquo;&lsquo;knowledge tracing&rsquo;&rsquo;; KT), and the prerequisite structure of the learning domain (&lsquo;&lsquo;knowledge mapping&rsquo;&rsquo;). While recent deep learning models achieve high KT accuracy, they do so at the expense of the interpretability of psychologically-inspired models. In this work, we present a solution to this trade-off. PSI-KT is a hierarchical generative approach that explicitly models how both individual cognitive traits and the prerequisite structure of knowledge influence learning dynamics, thus achieving interpretability by design. Moreover, by using scalable Bayesian inference, PSI-KT targets the real-world need for efficient personalization even with a growing body of learners and learning histories. Evaluated on three datasets from online learning platforms, PSI-KT achieves superior multi-step predictive accuracy and scalable inference in <b>continual-learning</b> <b>settings,</b> all while providing interpretable representations of learner-specific traits and the prerequisite structure of knowledge that causally supports learning. In sum, predictive, scalable and interpretable knowledge tracing with solid knowledge mapping lays a key foundation for effective personalized learning to make education accessible to a broad, global audience.</p></p class="citation"></blockquote><h3 id=4459--44329-adafish-fast-low-rank-parameter-efficient-fine-tuning-by-using-second-order-information-jiang-hu-et-al-2024>(44/59 | 44/329) AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information (Jiang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiang Hu, Quanzheng Li. (2024)<br><strong>AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information</strong><br><button class=copy-to-clipboard title="AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13128v1.pdf filename=2403.13128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in large-scale pretrained models have significantly improved performance across a variety of tasks in natural language processing and computer vision. However, the extensive number of parameters in these models necessitates substantial memory and computational resources for full training. To adapt these models for downstream tasks or specific application-oriented datasets, parameter-efficient <b>fine-tuning</b> methods leveraging pretrained parameters have gained considerable attention. However, it can still be time-consuming due to lots of parameters and epochs. In this work, we introduce AdaFish, an efficient algorithm of the second-order type designed to expedite the training process within low-rank decomposition-based <b>fine-tuning</b> frameworks. Our key observation is that the associated generalized Fisher information matrix is either low-rank or extremely small-scaled. Such a generalized Fisher information matrix is shown to be equivalent to the Hessian matrix. Moreover, we prove the global convergence of AdaFish, along with its iteration/oracle complexity. Numerical experiments show that our algorithm is quite competitive with the state-of-the-art AdamW method.</p></p class="citation"></blockquote><h3 id=4559--45329-deep-learning-with-noisy-labels-in-medical-prediction-problems-a-scoping-review-yishu-wei-et-al-2024>(45/59 | 45/329) Deep learning with noisy labels in medical prediction problems: a scoping review (Yishu Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yishu Wei, Yu Deng, Cong Sun, Mingquan Lin, Hongmei Jiang, Yifan Peng. (2024)<br><strong>Deep learning with noisy labels in medical prediction problems: a scoping review</strong><br><button class=copy-to-clipboard title="Deep learning with noisy labels in medical prediction problems: a scoping review" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13111v1.pdf filename=2403.13111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Objectives: Medical research faces substantial challenges from noisy labels attributed to factors like inter-expert variability and machine-extracted labels. Despite this, the adoption of label noise management remains limited, and label noise is largely ignored. To this end, there is a critical need to conduct a scoping review focusing on the problem space. This scoping review aims to comprehensively review label noise management in deep learning-based medical prediction problems, which includes label noise detection, label noise handling, and evaluation. Research involving label uncertainty is also included. Methods: Our scoping review follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We searched 4 databases, including PubMed, IEEE Xplore, Google Scholar, and Semantic Scholar. Our search terms include &ldquo;noisy label AND medical / healthcare / clinical&rdquo;, &ldquo;un-certainty AND medical / healthcare / clinical&rdquo;, and &ldquo;noise AND medical / healthcare / clinical&rdquo;. Results: A total of 60 papers met inclusion criteria between 2016 and 2023. A series of practical questions in medical research are investigated. These include the sources of label noise, the impact of label noise, the detection of label noise, label noise handling techniques, and their evaluation. Categorization of both label noise detection methods and handling techniques are provided. Discussion: From a methodological perspective, we observe that the medical community has been up to date with the broader deep-learning community, given that most techniques have been evaluated on medical data. We recommend considering label noise as a standard element in medical research, even if it is not dedicated to handling noisy labels. Initial experiments can start with easy-to-implement methods, such as noise-robust loss functions, weighting, and <b>curriculum</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=4659--46329-analyzing-the-impact-of-partial-sharing-on-the-resilience-of-online-federated-learning-against-model-poisoning-attacks-ehsan-lari-et-al-2024>(46/59 | 46/329) Analyzing the Impact of Partial Sharing on the Resilience of Online Federated Learning Against Model Poisoning Attacks (Ehsan Lari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Lari, Vinay Chakravarthi Gogineni, Reza Arablouei, Stefan Werner. (2024)<br><strong>Analyzing the Impact of Partial Sharing on the Resilience of Online Federated Learning Against Model Poisoning Attacks</strong><br><button class=copy-to-clipboard title="Analyzing the Impact of Partial Sharing on the Resilience of Online Federated Learning Against Model Poisoning Attacks" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13108v1.pdf filename=2403.13108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We scrutinize the resilience of the partial-sharing online <b>federated</b> <b>learning</b> (PSO-Fed) algorithm against model-poisoning attacks. PSO-Fed reduces the communication load by enabling clients to exchange only a fraction of their model estimates with the server at each update round. Partial sharing of model estimates also enhances the robustness of the algorithm against model-poisoning attacks. To gain better insights into this phenomenon, we analyze the performance of the PSO-Fed algorithm in the presence of Byzantine clients, malicious actors who may subtly tamper with their local models by adding noise before sharing them with the server. Through our analysis, we demonstrate that PSO-Fed maintains convergence in both mean and mean-square senses, even under the strain of model-poisoning attacks. We further derive the theoretical mean square error (MSE) of PSO-Fed, linking it to various parameters such as stepsize, attack probability, number of Byzantine clients, client participation rate, partial-sharing ratio, and noise variance. We also show that there is a non-trivial optimal stepsize for PSO-Fed when faced with model-poisoning attacks. The results of our extensive numerical experiments affirm our theoretical assertions and highlight the superior ability of PSO-Fed to counteract Byzantine attacks, outperforming other related leading algorithms.</p></p class="citation"></blockquote><h3 id=4759--47329-knowing-your-nonlinearities-shapley-interactions-reveal-the-underlying-structure-of-data-divyansh-singhvi-et-al-2024>(47/59 | 47/329) Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data (Divyansh Singhvi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Divyansh Singhvi, Andrej Erkelens, Raghav Jain, Diganta Misra, Naomi Saphra. (2024)<br><strong>Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data</strong><br><button class=copy-to-clipboard title="Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13106v1.pdf filename=2403.13106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Measuring nonlinear feature interaction is an established approach to understanding complex patterns of attribution in many models. In this paper, we use Shapley Taylor interaction indices (STII) to analyze the impact of underlying data structure on model representations in a variety of modalities, tasks, and architectures. Considering linguistic structure in masked and auto-regressive language models <b>(MLMs</b> and ALMs), we find that STII increases within idiomatic expressions and that <b>MLMs</b> scale STII with syntactic distance, relying more on syntax in their nonlinear structure than ALMs do. Our speech model findings reflect the phonetic principal that the openness of the oral cavity determines how much a phoneme varies based on its context. Finally, we study image classifiers and illustrate that feature interactions intuitively reflect object boundaries. Our wide range of results illustrates the benefits of interdisciplinary work and domain expertise in interpretability research.</p></p class="citation"></blockquote><h3 id=4859--48329-jaxued-a-simple-and-useable-ued-library-in-jax-samuel-coward-et-al-2024>(48/59 | 48/329) JaxUED: A simple and useable UED library in Jax (Samuel Coward et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Coward, Michael Beukman, Jakob Foerster. (2024)<br><strong>JaxUED: A simple and useable UED library in Jax</strong><br><button class=copy-to-clipboard title="JaxUED: A simple and useable UED library in Jax" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13091v1.pdf filename=2403.13091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present JaxUED, an open-source library providing minimal dependency implementations of modern <b>Unsupervised</b> Environment Design (UED) algorithms in Jax. JaxUED leverages hardware acceleration to obtain on the order of 100x speedups compared to prior, CPU-based implementations. Inspired by CleanRL, we provide fast, clear, understandable, and easily modifiable implementations, with the aim of accelerating research into UED. This paper describes our library and contains baseline results. Code can be found at <a href=https://github.com/DramaCow/jaxued>https://github.com/DramaCow/jaxued</a>.</p></p class="citation"></blockquote><h3 id=4959--49329-optimal-and-adaptive-non-stationary-dueling-bandits-under-a-generalized-borda-criterion-joe-suk-et-al-2024>(49/59 | 49/329) Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion (Joe Suk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joe Suk, Arpit Agarwal. (2024)<br><strong>Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion</strong><br><button class=copy-to-clipboard title="Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12950v1.pdf filename=2403.12950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In dueling <b>bandits,</b> the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling <b>bandits,</b> where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change. The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling <b>bandits.</b> Surprisingly, our techniques for non-stationary Borda dueling <b>bandits</b> also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable. This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task. Such a generalization was not previously known and is likely to be of independent interest.</p></p class="citation"></blockquote><h3 id=5059--50329-on-safety-in-safe-bayesian-optimization-christian-fiedler-et-al-2024>(50/59 | 50/329) On Safety in Safe Bayesian Optimization (Christian Fiedler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Fiedler, Johanna Menn, Lukas Kreisköther, Sebastian Trimpe. (2024)<br><strong>On Safety in Safe Bayesian Optimization</strong><br><button class=copy-to-clipboard title="On Safety in Safe Bayesian Optimization" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12948v1.pdf filename=2403.12948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this. Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world. In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms. First, these algorithms critically rely on frequentist uncertainty bounds for <b>Gaussian</b> <b>Process</b> (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees. We provide a detailed analysis of this problem and introduce Real-\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees. Second, we identify assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of the target function, a key technical assumption in SafeOpt-like algorithms, as a central obstacle to real-world usage. To overcome this challenge, we introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm, which guarantees safety without an assumption on the RKHS bound, and empirically show that this algorithm is not only safe, but also exhibits superior performance compared to the state-of-the-art on several function classes. Third, SafeOpt and derived algorithms rely on a discrete search space, making them difficult to apply to higher-dimensional problems. To widen the applicability of these algorithms, we introduce Lipschitz-only GP-UCB (LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional problems, while retaining safety.</p></p class="citation"></blockquote><h3 id=5159--51329-equivariant-ensembles-and-regularization-for-reinforcement-learning-in-map-based-path-planning-mirco-theile-et-al-2024>(51/59 | 51/329) Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning (Mirco Theile et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mirco Theile, Hongpeng Cao, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli. (2024)<br><strong>Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning</strong><br><button class=copy-to-clipboard title="Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12856v1.pdf filename=2403.12856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>reinforcement</b> <b>learning</b> (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.</p></p class="citation"></blockquote><h3 id=5259--52329-improving-interpretability-of-scores-in-anomaly-detection-based-on-gaussian-bernoulli-restricted-boltzmann-machine-kaiji-sekimoto-et-al-2024>(52/59 | 52/329) Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine (Kaiji Sekimoto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiji Sekimoto, Muneki Yasuda. (2024)<br><strong>Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine</strong><br><button class=copy-to-clipboard title="Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12672v1.pdf filename=2403.12672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for semi-supervised <b>anomaly</b> <b>detection,</b> where they are trained using only normal data points. In GBRBM-based <b>anomaly</b> <b>detection,</b> normal and anomalous data are classified based on a score that is identical to an energy function of the marginal GBRBM. However, the classification threshold is difficult to set to an appropriate value, as this score cannot be interpreted. In this study, we propose a measure that improves score&rsquo;s interpretability based on its cumulative distribution, and establish a guideline for setting the threshold using the interpretable measure. The results of numerical experiments show that the guideline is reasonable when setting the threshold solely using normal data points. Moreover, because identifying the measure involves computationally infeasible evaluation of the minimum score value, we also propose an evaluation method for the minimum score based on simulated annealing, which is widely used for optimization problems. The proposed evaluation method was also validated using numerical experiments.</p></p class="citation"></blockquote><h3 id=5359--53329-fedsr-a-semi-decentralized-federated-learning-algorithm-for-non-iidness-in-iot-system-jianjun-huang-et-al-2024>(53/59 | 53/329) FedSR: A Semi-Decentralized Federated Learning Algorithm for Non-IIDness in IoT System (Jianjun Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianjun Huang, Lixin Ye, Li Kang. (2024)<br><strong>FedSR: A Semi-Decentralized Federated Learning Algorithm for Non-IIDness in IoT System</strong><br><button class=copy-to-clipboard title="FedSR: A Semi-Decentralized Federated Learning Algorithm for Non-IIDness in IoT System" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14718v1.pdf filename=2403.14718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the Industrial Internet of Things (IoT), a large amount of data will be generated every day. Due to privacy and security issues, it is difficult to collect all these data together to train deep learning models, thus the <b>federated</b> <b>learning,</b> a distributed machine learning paradigm that protects data privacy, has been widely used in IoT. However, in practical <b>federated</b> <b>learning,</b> the data distributions usually have large differences across devices, and the heterogeneity of data will deteriorate the performance of the model. Moreover, <b>federated</b> <b>learning</b> in IoT usually has a large number of devices involved in training, and the limited communication resource of cloud servers become a bottleneck for training. To address the above issues, in this paper, we combine centralized <b>federated</b> <b>learning</b> with decentralized <b>federated</b> <b>learning</b> to design a semi-decentralized cloud-edge-device hierarchical <b>federated</b> <b>learning</b> framework, which can mitigate the impact of data heterogeneity, and can be deployed at lage scale in IoT. To address the effect of data heterogeneity, we use an incremental subgradient optimization algorithm in each ring cluster to improve the generalization ability of the ring cluster models. Our extensive experiments show that our approach can effectively mitigate the impact of data heterogeneity and alleviate the communication bottleneck in cloud servers.</p></p class="citation"></blockquote><h3 id=5459--54329-understanding-why-label-smoothing-degrades-selective-classification-and-how-to-fix-it-guoxuan-xia-et-al-2024>(54/59 | 54/329) Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It (Guoxuan Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoxuan Xia, Olivier Laurent, Gianni Franchi, Christos-Savvas Bouganis. (2024)<br><strong>Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It</strong><br><button class=copy-to-clipboard title="Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Label Smoothing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14715v1.pdf filename=2403.14715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Label</b> <b>smoothing</b> (LS) is a popular regularisation method for training deep neural network classifiers due to its effectiveness in improving test accuracy and its simplicity in implementation. &ldquo;Hard&rdquo; one-hot <b>labels</b> <b>are</b> &ldquo;smoothed&rdquo; by uniformly distributing probability mass to other classes, reducing overfitting. In this work, we reveal that LS negatively affects selective classification (SC) - where the aim is to reject misclassifications using a model&rsquo;s predictive uncertainty. We first demonstrate empirically across a range of tasks and architectures that LS leads to a consistent degradation in SC. We then explain this by analysing logit-level gradients, showing that LS exacerbates overconfidence and underconfidence by regularising the max logit more when the probability of error is low, and less when the probability of error is high. This elucidates previously reported experimental results where strong classifiers underperform in SC. We then demonstrate the empirical effectiveness of logit normalisation for recovering lost SC performance caused by LS. Furthermore, based on our gradient analysis, we explain why such normalisation is effective. We will release our code shortly.</p></p class="citation"></blockquote><h3 id=5559--55329-transfer-in-sequential-multi-armed-bandits-via-reward-samples-rahul-n-r-et-al-2024>(55/59 | 55/329) Transfer in Sequential Multi-armed Bandits via Reward Samples (Rahul N R et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul N R, Vaibhav Katewa. (2024)<br><strong>Transfer in Sequential Multi-armed Bandits via Reward Samples</strong><br><button class=copy-to-clipboard title="Transfer in Sequential Multi-armed Bandits via Reward Samples" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12428v1.pdf filename=2403.12428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a sequential stochastic multi-armed <b>bandit</b> problem where the agent interacts with <b>bandit</b> over multiple episodes. The reward distribution of the arms remain constant throughout an episode but can change over different episodes. We propose an algorithm based on UCB to transfer the reward samples from the previous episodes and improve the cumulative regret performance over all the episodes. We provide regret analysis and empirical results for our algorithm, which show significant improvement over the standard UCB algorithm without transfer.</p></p class="citation"></blockquote><h3 id=5659--56329-temporally-consistent-koopman-autoencoders-for-forecasting-dynamical-systems-indranil-nayak-et-al-2024>(56/59 | 56/329) Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems (Indranil Nayak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Indranil Nayak, Debdipta Goswami, Mrinal Kumar, Fernando Teixeira. (2024)<br><strong>Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems</strong><br><button class=copy-to-clipboard title="Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12335v1.pdf filename=2403.12335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Absence of sufficiently high-quality data often poses a key challenge in data-driven modeling of high-dimensional spatio-temporal dynamical systems. Koopman <b>Autoencoders</b> (KAEs) harness the expressivity of deep neural networks (DNNs), the dimension reduction capabilities of <b>autoencoders,</b> and the spectral properties of the Koopman operator to learn a reduced-order feature space with simpler, linear dynamics. However, the effectiveness of KAEs is hindered by limited and noisy training datasets, leading to poor generalizability. To address this, we introduce the Temporally-Consistent Koopman <b>Autoencoder</b> (tcKAE), designed to generate accurate long-term predictions even with constrained and noisy training data. This is achieved through a consistency regularization term that enforces prediction coherence across different time steps, thus enhancing the robustness and generalizability of tcKAE over existing models. We provide analytical justification for this approach based on Koopman spectral theory and empirically demonstrate tcKAE&rsquo;s superior performance over state-of-the-art KAE models across a variety of test cases, including simple pendulum oscillations, kinetic plasmas, fluid flows, and sea surface temperature data.</p></p class="citation"></blockquote><h3 id=5759--57329-fedfisher-leveraging-fisher-information-for-one-shot-federated-learning-divyansh-jhunjhunwala-et-al-2024>(57/59 | 57/329) FedFisher: Leveraging Fisher Information for One-Shot Federated Learning (Divyansh Jhunjhunwala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Divyansh Jhunjhunwala, Shiqiang Wang, Gauri Joshi. (2024)<br><strong>FedFisher: Leveraging Fisher Information for One-Shot Federated Learning</strong><br><button class=copy-to-clipboard title="FedFisher: Leveraging Fisher Information for One-Shot Federated Learning" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12329v1.pdf filename=2403.12329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Standard <b>federated</b> <b>learning</b> (FL) algorithms typically require multiple rounds of communication between the server and the clients, which has several drawbacks, including requiring constant network connectivity, repeated investment of computational resources, and susceptibility to privacy attacks. One-Shot FL is a new paradigm that aims to address this challenge by enabling the server to train a global model in a single round of communication. In this work, we present FedFisher, a novel algorithm for one-shot FL that makes use of Fisher information matrices computed on local client models, motivated by a Bayesian perspective of FL. First, we theoretically analyze FedFisher for two-layer over-parameterized ReLU neural networks and show that the error of our one-shot FedFisher global model becomes vanishingly small as the width of the neural networks and amount of local training at clients increases. Next, we propose practical variants of FedFisher using the diagonal Fisher and K-FAC approximation for the full Fisher and highlight their communication and compute efficiency for FL. Finally, we conduct extensive experiments on various datasets, which show that these variants of FedFisher consistently improve over competing baselines.</p></p class="citation"></blockquote><h3 id=5859--58329-bilevel-hypergraph-networks-for-multi-modal-alzheimers-diagnosis-angelica-i-aviles-rivero-et-al-2024>(58/59 | 58/329) Bilevel Hypergraph Networks for Multi-Modal Alzheimer&rsquo;s Diagnosis (Angelica I. Aviles-Rivero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angelica I. Aviles-Rivero, Chun-Wun Cheng, Zhongying Deng, Zoe Kourtzi, Carola-Bibiane Schönlieb. (2024)<br><strong>Bilevel Hypergraph Networks for Multi-Modal Alzheimer&rsquo;s Diagnosis</strong><br><button class=copy-to-clipboard title="Bilevel Hypergraph Networks for Multi-Modal Alzheimer's Diagnosis" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12719v1.pdf filename=2403.12719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Early detection of Alzheimer&rsquo;s disease&rsquo;s precursor stages is imperative for significantly enhancing patient outcomes and quality of life. This challenge is tackled through a semi-supervised <b>multi-modal</b> diagnosis framework. In particular, we introduce a new hypergraph framework that enables higher-order relations between <b>multi-modal</b> data, while utilising minimal labels. We first introduce a bilevel hypergraph optimisation framework that jointly learns a <b>graph</b> augmentation policy and a semi-supervised classifier. This dual learning strategy is hypothesised to enhance the robustness and generalisation capabilities of the model by fostering new pathways for information propagation. Secondly, we introduce a novel strategy for generating pseudo-labels more effectively via a gradient-driven flow. Our experimental results demonstrate the superior performance of our framework over current techniques in diagnosing Alzheimer&rsquo;s disease.</p></p class="citation"></blockquote><h3 id=5959--59329-dynamic-survival-analysis-for-early-event-prediction-hugo-yèche-et-al-2024>(59/59 | 59/329) Dynamic Survival Analysis for Early Event Prediction (Hugo Yèche et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Yèche, Manuel Burger, Dinara Veshchezerova, Gunnar Rätsch. (2024)<br><strong>Dynamic Survival Analysis for Early Event Prediction</strong><br><button class=copy-to-clipboard title="Dynamic Survival Analysis for Early Event Prediction" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12818v1.pdf filename=2403.12818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics. By adapting and evaluating DSA models against traditional EEP <b>benchmarks,</b> our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference). This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management.</p></p class="citation"></blockquote><h2 id=cscl-45>cs.CL (45)</h2><h3 id=145--60329-automated-data-curation-for-robust-language-model-fine-tuning-jiuhai-chen-et-al-2024>(1/45 | 60/329) Automated Data Curation for Robust Language Model Fine-Tuning (Jiuhai Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiuhai Chen, Jonas Mueller. (2024)<br><strong>Automated Data Curation for Robust Language Model Fine-Tuning</strong><br><button class=copy-to-clipboard title="Automated Data Curation for Robust Language Model Fine-Tuning" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 120<br>Keywords: Fine-tuning, Fine-tuning, Supervised Learning, GPT, GPT-3, GPT-3.5, GPT-4, Text Generation, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12776v1.pdf filename=2403.12776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> have become the de facto approach to sequence-to-sequence <b>text</b> <b>generation</b> tasks, but for specialized tasks/domains, a pretrained <b>LLM</b> lacks specific capabilities to produce accurate or well-formatted responses. <b>Supervised</b> <b>fine-tuning</b> specializes a <b>LLM</b> by training it on dataset of example <b>prompts</b> with target responses, but real-world data tends to be noisy. While many <b>fine-tuning</b> algorithms exist, here we consider a \emph{data-centric AI} perspective on <b>LLM</b> <b>fine-tuning,</b> studying how to \emph{systematically} curate the training dataset to improve the <b>LLM</b> produced via \emph{any} <b>fine-tuning</b> algorithm. We introduce an automated data curation pipeline CLEAR (Confidence-based <b>LLM</b> Evaluation And Rectification) for <b>instruction</b> <b>tuning</b> datasets, that can be used with any <b>LLM</b> and <b>fine-tuning</b> procedure. CLEAR estimates which training data is low-quality and either filters or corrects it. Automatically identifying which data to filter or correct is done via <b>LLM-derived</b> confidence estimates, to ensure only confident modifications to the dataset. Unlike existing data curation techniques, CLEAR is a comprehensive framework that can improve a dataset (and trained model outputs) without additional <b>fine-tuning</b> computations. We don&rsquo;t assume access to a stronger <b>LLM</b> than the model being <b>fine-tuned</b> (e.g.\ relying on <b>GPT-4</b> when <b>fine-tuning</b> <b>GPT-3.5),</b> to see whether CLEAR can meaningfully improve the capabilities of any <b>LLM.</b> Experiments reveal that CLEAR consistently improves the performance of <b>fine-tuned</b> models across many datasets and models (like <b>GPT-3.5</b> and Llama2).</p></p class="citation"></blockquote><h3 id=245--61329-chart-based-reasoning-transferring-capabilities-from-llms-to-vlms-victor-carbune-et-al-2024>(2/45 | 61/329) Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs (Victor Carbune et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Carbune, Hassan Mansoor, Fangyu Liu, Rahul Aralikatte, Gilles Baechler, Jindong Chen, Abhanshu Sharma. (2024)<br><strong>Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs</strong><br><button class=copy-to-clipboard title="Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 96<br>Keywords: Optical Character Recognition, Fine-tuning, Multi-modal, Multi-modal, GPT, Gemini, Reasoning, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12596v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12596v1.pdf filename=2403.12596v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> models (VLMs) are achieving increasingly strong performance on <b>multimodal</b> tasks. However, <b>reasoning</b> capabilities remain limited particularly for smaller VLMs, while those of <b>large-language</b> <b>models</b> <b>(LLMs)</b> have seen numerous improvements. We propose a technique to transfer capabilities from <b>LLMs</b> to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-art performance when applied on the PaLI3-5B VLM by \citet{chen2023pali3}, while also enabling much better performance on PlotQA and FigureQA. We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by \citet{liu2023deplot}. We then propose constructing a 20x larger dataset than the original training set. To improve general <b>reasoning</b> capabilities and improve numerical operations, we synthesize <b>reasoning</b> traces using the table representation of charts. Lastly, our model is <b>fine-tuned</b> using the multitask loss introduced by \citet{hsieh2023distilling}. Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream <b>OCR</b> system, while keeping inference time constant compared to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought <b>prompt</b> \cite{chen2023program}, our model outperforms the recently introduced <b>Gemini</b> Ultra and <b>GPT-4V.</b></p></p class="citation"></blockquote><h3 id=345--62329-automatic-summarization-of-doctor-patient-encounter-dialogues-using-large-language-model-through-prompt-tuning-mengxian-lyu-et-al-2024>(3/45 | 62/329) Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning (Mengxian Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengxian Lyu, Cheng Peng, Xiaohan Li, Patrick Balian, Jiang Bian, Yonghui Wu. (2024)<br><strong>Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning</strong><br><button class=copy-to-clipboard title="Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Fine-tuning, T5, Neural Machine Translation, Text Summarization, Large Language Model, Large Language Model, Prompt, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13089v1.pdf filename=2403.13089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic <b>text</b> <b>summarization</b> (ATS) is an emerging technology to assist clinicians in providing continuous and coordinated care. This study presents an approach to <b>summarize</b> doctor-patient dialogues using generative <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We developed <b>prompt-tuning</b> algorithms to instruct generative <b>LLMs</b> to <b>summarize</b> clinical <b>text.</b> <b>We</b> examined the <b>prompt-tuning</b> strategies, the size of soft <b>prompts,</b> and the few-short learning ability of GatorTronGPT, a generative clinical <b>LLM</b> developed using 277 billion clinical and general English words with up to 20 billion parameters. We compared GatorTronGPT with a previous solution based on <b>fine-tuning</b> of a widely used <b>T5</b> model, using a clinical <b>benchmark</b> dataset <b>MTS-DIALOG.</b> The experimental results show that the GatorTronGPT- 20B model achieved the best performance on all evaluation metrics. The proposed solution has a low computing cost as the <b>LLM</b> parameters are not updated during <b>prompt-tuning.</b> This study demonstrates the efficiency of generative clinical <b>LLMs</b> for clinical ATS through <b>prompt</b> tuning.</p></p class="citation"></blockquote><h3 id=445--63329-rankprompt-step-by-step-comparisons-make-language-models-better-reasoners-chi-hu-et-al-2024>(4/45 | 63/329) RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners (Chi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chi Hu, Yuan Ge, Xiangnan Ma, Hang Cao, Qiang Li, Yonghua Yang, Tong Xiao, Jingbo Zhu. (2024)<br><strong>RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners</strong><br><button class=copy-to-clipboard title="RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Automatic Evaluation, ChatGPT, GPT, GPT-4, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12373v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12373v3.pdf filename=2403.12373v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved impressive performance across various <b>reasoning</b> tasks. However, even state-of-the-art <b>LLMs</b> such as <b>ChatGPT</b> are prone to logical errors during their <b>reasoning</b> processes. Existing solutions, such as deploying task-specific verifiers or voting over multiple <b>reasoning</b> paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new <b>prompting</b> method that enables <b>LLMs</b> to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of <b>LLMs</b> to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and <b>commonsense</b> <b>reasoning</b> tasks show that RankPrompt significantly enhances the <b>reasoning</b> performance of <b>ChatGPT</b> and <b>GPT-4,</b> with improvements of up to 13%. Moreover, RankPrompt excels in <b>LLM-based</b> <b>automatic</b> <b>evaluations</b> for open-ended tasks, aligning with human judgments 74% of the time in the AlpacaEval dataset. It also exhibits robustness to variations in response order and consistency. Collectively, our results validate RankPrompt as an effective method for eliciting high-quality feedback from language models.</p></p class="citation"></blockquote><h3 id=545--64329-crosstune-black-box-few-shot-classification-with-label-enhancement-danqing-luo-et-al-2024>(5/45 | 64/329) CrossTune: Black-Box Few-Shot Classification with Label Enhancement (Danqing Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danqing Luo, Chen Zhang, Yan Zhang, Haizhou Li. (2024)<br><strong>CrossTune: Black-Box Few-Shot Classification with Label Enhancement</strong><br><button class=copy-to-clipboard title="CrossTune: Black-Box Few-Shot Classification with Label Enhancement" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 88<br>Keywords: Benchmarking, Black Box, Few-shot, Fine-tuning, ChatGPT, Text Classification, In-context Learning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12468v1.pdf filename=2403.12468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training or <b>finetuning</b> large-scale language models <b>(LLMs)</b> requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as <b>black</b> <b>boxes</b> and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these <b>black-box</b> <b>models</b> to downstream tasks using gradient-free <b>prompt</b> optimization, but this often involves an expensive process of searching task-specific <b>prompts.</b> Therefore, we are motivated to study <b>black-box</b> <b>language</b> model adaptation without <b>prompt</b> search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input <b>text</b> <b>sequence</b> and task-specific label descriptions. Its effectiveness is examined in the context of <b>few-shot</b> <b>text</b> <b>classification.</b> To improve the generalization of CrossTune, we utilize <b>ChatGPT</b> to generate additional training data through <b>in-context</b> <b>learning.</b> A switch mechanism is implemented to exclude low-quality <b>ChatGPT-generated</b> data. Through extensive experiments on seven <b>benchmark</b> <b>text</b> <b>classification</b> datasets, we demonstrate that our proposed approach outperforms the previous state-of-the-art gradient-free <b>black-box</b> <b>tuning</b> method by 5.7% on average. Even without using <b>ChatGPT-augmented</b> data, CrossTune performs better or comparably than previous <b>black-box</b> <b>tuning</b> methods, suggesting the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=645--65329-pragmatic-competence-evaluation-of-large-language-models-for-korean-dojun-park-et-al-2024>(6/45 | 65/329) Pragmatic Competence Evaluation of Large Language Models for Korean (Dojun Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dojun Park, Jiwoo Lee, Hyeyun Jeong, Seohyun Park, Sungeun Lee. (2024)<br><strong>Pragmatic Competence Evaluation of Large Language Models for Korean</strong><br><button class=copy-to-clipboard title="Pragmatic Competence Evaluation of Large Language Models for Korean" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Automatic Evaluation, Benchmarking, Few-shot, Few-shot Learning, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12675v1.pdf filename=2403.12675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The current evaluation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> predominantly relies on <b>benchmarks</b> focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation. Our study extends this evaluation to explore <b>LLMs&rsquo;</b> pragmatic competence&ndash;a facet previously underexamined before the advent of sophisticated <b>LLMs,</b> specifically in the context of Korean. We employ two distinct evaluation setups: the conventional MCQ format, adapted for <b>automatic</b> <b>evaluation,</b> and Open-Ended Questions (OEQs), assessed by human experts, to examine <b>LLMs&rsquo;</b> narrative response capabilities without predefined options. Our findings reveal that <b>GPT-4</b> excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an <b>LLM</b> optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to <b>GPT-4.</b> Furthermore, while <b>few-shot</b> <b>learning</b> strategies generally enhance <b>LLM</b> performance, Chain-of-Thought (CoT) <b>prompting</b> introduces a bias toward literal interpretations, hindering accurate pragmatic inference. Considering the growing expectation for <b>LLMs</b> to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing <b>LLMs&rsquo;</b> abilities to grasp and convey sophisticated meanings beyond mere literal interpretations.</p></p class="citation"></blockquote><h3 id=745--66329-encode-once-and-decode-in-parallel-efficient-transformer-decoding-bo-ru-lu-et-al-2024>(7/45 | 66/329) Encode Once and Decode in Parallel: Efficient Transformer Decoding (Bo-Ru Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo-Ru Lu, Nikita Haduong, Chien-Yu Lin, Hao Cheng, Noah A. Smith, Mari Ostendorf. (2024)<br><strong>Encode Once and Decode in Parallel: Efficient Transformer Decoding</strong><br><button class=copy-to-clipboard title="Encode Once and Decode in Parallel: Efficient Transformer Decoding" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, GPT, GPT-4, Transformer, Dialogue State Tracking, Question Answering, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13112v1.pdf filename=2403.13112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> NLP models are powerful but have high computational costs that limit deployment scenarios. <b>Finetuned</b> encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as <b>GPT-4.</b> We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and <b>question-answering</b> <b>tasks</b> where multiple outputs are required of a single input. Our method, <b>prompt-in-decoder</b> (PiD), encodes the input once and decodes output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding, thereby reducing the decoder&rsquo;s memory footprint. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for <b>dialogue</b> <b>state</b> <b>tracking,</b> <b>summarization,</b> and <b>question-answering</b> <b>tasks</b> with comparable or better performance. We release our training/inference code and checkpoints.</p></p class="citation"></blockquote><h3 id=845--67329-towards-unsupervised-question-answering-system-with-multi-level-summarization-for-legal-text-m-manvith-prabhu-et-al-2024>(8/45 | 67/329) Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text (M Manvith Prabhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M Manvith Prabhu, Haricharana Srinivasa, Anand Kumar M. (2024)<br><strong>Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text</strong><br><button class=copy-to-clipboard title="Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Convolutional Neural Network, Unsupervised Learning, LSTM, T5, Question Answering, Reasoning, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13107v1.pdf filename=2403.13107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper <b>summarizes</b> Team SCaLAR&rsquo;s work on SemEval-2024 Task 5: Legal Argument <b>Reasoning</b> in Civil Procedure. To address this Binary Classification task, which was daunting due to the complexity of the Legal Texts involved, we propose a simple yet novel similarity and distance-based <b>unsupervised</b> approach to generate labels. Further, we explore the Multi-level fusion of Legal-Bert embeddings using ensemble features, including <b>CNN,</b> GRU, and <b>LSTM.</b> To address the lengthy nature of Legal explanation in the dataset, we introduce <b>T5-based</b> segment-wise <b>summarization,</b> which successfully retained crucial information, enhancing the model&rsquo;s performance. Our <b>unsupervised</b> system witnessed a 20-point increase in macro F1-score on the development set and a 10-point increase on the test set, which is promising given its uncomplicated architecture.</p></p class="citation"></blockquote><h3 id=945--68329-llms-based-few-shot-disease-predictions-using-ehr-a-novel-approach-combining-predictive-agent-reasoning-and-critical-agent-instruction-hejie-cui-et-al-2024>(9/45 | 68/329) LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction (Hejie Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hejie Cui, Zhuocheng Shen, Jieyu Zhang, Hui Shao, Lianhui Qin, Joyce C. Ho, Carl Yang. (2024)<br><strong>LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction</strong><br><button class=copy-to-clipboard title="LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: J-3; I-2-7, cs-AI, cs-CL, cs-LG, cs-MA, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Supervised Learning, Supervised Learning, Zero-shot, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15464v1.pdf filename=2403.15464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on <b>supervised</b> <b>learning</b> methods that require <b>large</b> <b>labeled</b> <b>datasets,</b> which can be expensive and challenging to obtain. In this study, we investigate the feasibility of applying <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to convert structured patient visit data (e.g., diagnoses, labs, prescriptions) into natural language narratives. We evaluate the <b>zero-shot</b> and <b>few-shot</b> performance of <b>LLMs</b> using various EHR-prediction-oriented <b>prompting</b> strategies. Furthermore, we propose a novel approach that utilizes <b>LLM</b> agents with different roles: a predictor agent that makes predictions and generates <b>reasoning</b> processes and a critic agent that analyzes incorrect predictions and provides guidance for improving the <b>reasoning</b> of the predictor agent. Our results demonstrate that with the proposed approach, <b>LLMs</b> can achieve decent <b>few-shot</b> performance compared to traditional <b>supervised</b> <b>learning</b> methods in EHR-based disease predictions, suggesting its potential for health-oriented applications.</p></p class="citation"></blockquote><h3 id=1045--69329-instructing-large-language-models-to-identify-and-ignore-irrelevant-conditions-zhenyu-wu-et-al-2024>(10/45 | 69/329) Instructing Large Language Models to Identify and Ignore Irrelevant Conditions (Zhenyu Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Wu, Chao Shen, Meng Jiang. (2024)<br><strong>Instructing Large Language Models to Identify and Ignore Irrelevant Conditions</strong><br><button class=copy-to-clipboard title="Instructing Large Language Models to Identify and Ignore Irrelevant Conditions" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, GPT, GPT-3, GPT-3.5, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12744v1.pdf filename=2403.12744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Math word problem (MWP) solving requires generating a <b>reasoning</b> path based on a given problem description that often contains irrelevant conditions. Existing chain-of-thought (CoT) <b>prompting</b> methods elicited multi-step <b>reasoning</b> abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to solve MWPs. However, they were seriously confused by the irrelevant conditions, resulting in low accuracy. In this paper, we propose a novel approach named I$^3$C that instructs <b>LLMs</b> to identify and ignore irrelevant conditions. It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question. Then it <b>prompts</b> <b>LLMs</b> to verify the irrelevant conditions. Lastly it instructs the <b>LLMs</b> with the verification on relevant and irrelevant conditions to avoid confusion and improve <b>reasoning</b> paths. Moreover, we propose to select (problem, <b>reasoning</b> paths) pairs as demonstrations to enhance I$^3$C with <b>few-shot</b> <b>reasoning.</b> We develop I$^3$C-Select that selects the most confusing problems based on the semantic relevance measurement. We conduct extensive experiments on eight MWP datasets. I$^3$C can be combined with any CoT <b>prompting</b> methods to improve the performance of solving MWPs. Notably, with <b>GPT-3.5-Turbo</b> and I$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art <b>few-shot</b> <b>prompting</b> method Complex-CoT by +11.7 and +11.1. Our implementation is made publicly available at <a href=https://wzy6642.github.io/I3C.github.io/>https://wzy6642.github.io/I3C.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1145--70329-alphafin-benchmarking-financial-analysis-with-retrieval-augmented-stock-chain-framework-xiang-li-et-al-2024>(11/45 | 70/329) AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework (Xiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Jun Huang, Wei Lin. (2024)<br><strong>AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework</strong><br><button class=copy-to-clipboard title="AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 76<br>Keywords: Benchmarking, Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12582v1.pdf filename=2403.12582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial <b>question</b> <b>answering.</b> Currently, machine learning and deep learning algorithms (ML&amp;DL) have been widely applied for stock trend predictions, leading to significant progress. However, these methods fail to provide reasons for predictions, lacking interpretability and <b>reasoning</b> processes. Also, they can not integrate textual information such as financial news or reports. Meanwhile, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have remarkable textual understanding and generation ability. But due to the scarcity of financial training datasets and limited integration with real-time knowledge, <b>LLMs</b> still suffer from hallucinations and are unable to keep up with the latest information. To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-thought (CoT) data. It has a positive impact on training <b>LLMs</b> for completing financial analysis. We then use AlphaFin datasets to <b>benchmark</b> a state-of-the-art method, called Stock-Chain, for effectively tackling the financial analysis task, which integrates <b>retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> techniques. Extensive experiments are conducted to demonstrate the effectiveness of our framework on financial analysis.</p></p class="citation"></blockquote><h3 id=1245--71329-improving-generalizability-of-extracting-social-determinants-of-health-using-large-language-models-through-prompt-tuning-cheng-peng-et-al-2024>(12/45 | 71/329) Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning (Cheng Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Peng, Zehao Yu, Kaleb E Smith, Wei-Hsuan Lo-Ciganic, Jiang Bian, Yonghui Wu. (2024)<br><strong>Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning</strong><br><button class=copy-to-clipboard title="Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Transfer Learning, Information Retrieval, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12374v1.pdf filename=2403.12374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The progress in natural language processing (NLP) using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has greatly improved patient <b>information</b> <b>extraction</b> from clinical narratives. However, most methods based on the <b>fine-tuning</b> strategy have limited <b>transfer</b> <b>learning</b> ability for cross-domain applications. This study proposed a novel approach that employs a soft <b>prompt-based</b> learning architecture, which introduces trainable <b>prompts</b> to guide <b>LLMs</b> toward desired outputs. We examined two types of <b>LLM</b> architectures, including encoder-only GatorTron and decoder-only GatorTronGPT, and evaluated their performance for the extraction of social determinants of health (SDoH) using a cross-institution dataset from the 2022 n2c2 challenge and a cross-disease dataset from the University of Florida (UF) Health. The results show that decoder-only <b>LLMs</b> with <b>prompt</b> tuning achieved better performance in cross-domain applications. GatorTronGPT achieved the best F1 scores for both datasets, outperforming traditional <b>fine-tuned</b> GatorTron by 8.9% and 21.8% in a cross-institution setting, and 5.5% and 14.5% in a cross-disease setting.</p></p class="citation"></blockquote><h3 id=1345--72329-llmlingua-2-data-distillation-for-efficient-and-faithful-task-agnostic-prompt-compression-zhuoshi-pan-et-al-2024>(13/45 | 72/329) LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression (Zhuoshi Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Dongmei Zhang. (2024)<br><strong>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</strong><br><button class=copy-to-clipboard title="LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Out-of-domain, LLaMA, Transformer, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12968v1.pdf filename=2403.12968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on task-agnostic <b>prompt</b> compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress <b>prompts</b> by removing tokens or lexical units according to their information entropy obtained from a causal language model such as <b>LLaMa-7B.</b> The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for <b>prompt</b> compression; (ii) it is not aligned with the <b>prompt</b> compression objective. To address these issues, we propose a data <b>distillation</b> procedure to derive knowledge from an <b>LLM</b> to compress <b>prompts</b> without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate <b>prompt</b> compression as a token classification problem to guarantee the faithfulness of the compressed <b>prompt</b> to the original one, and use a <b>Transformer</b> encoder as the base architecture to capture all essential information for <b>prompt</b> compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT. We evaluate our method on both in-domain and <b>out-of-domain</b> datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different <b>LLMs.</b> Additionally, our model is 3x-6x faster than existing <b>prompt</b> compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.</p></p class="citation"></blockquote><h3 id=1445--73329-lhmke-a-large-scale-holistic-multi-subject-knowledge-evaluation-benchmark-for-chinese-large-language-models-chuang-liu-et-al-2024>(14/45 | 73/329) LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models (Chuang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuang Liu, Renren Jin, Yuqi Ren, Deyi Xiong. (2024)<br><strong>LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models</strong><br><button class=copy-to-clipboard title="LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Zero-shot, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12601v1.pdf filename=2403.12601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chinese <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have recently demonstrated impressive capabilities across various NLP <b>benchmarks</b> and real-world applications. However, the existing <b>benchmarks</b> for comprehensively evaluating these <b>LLMs</b> are still insufficient, particularly in terms of measuring knowledge that <b>LLMs</b> capture. Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue. Yet, these <b>benchmarks</b> primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types. To tackle this problem, we propose LHMKE, a <b>Large-scale,</b> <b>Holistic,</b> <b>and</b> Multi-subject Knowledge Evaluation <b>benchmark</b> in this paper. LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese <b>LLMs.</b> It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional certification exams. Notably, LHMKE includes both objective and subjective questions, offering a more holistic evaluation of the knowledge level of <b>LLMs.</b> We have assessed 11 Chinese <b>LLMs</b> under the <b>zero-shot</b> setting, which aligns with real examinations, and compared their performance across different subjects. We also conduct an in-depth analysis to check whether <b>GPT-4</b> can automatically score subjective predictions. Our findings suggest that LHMKE is a challenging and advanced testbed for Chinese <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1545--74329-automatic-information-extraction-from-employment-tribunal-judgements-using-large-language-models-joana-ribeiro-de-faria-et-al-2024>(15/45 | 74/329) Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models (Joana Ribeiro de Faria et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joana Ribeiro de Faria, Huiyuan Xie, Felix Steffek. (2024)<br><strong>Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models</strong><br><button class=copy-to-clipboard title="Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12936v1.pdf filename=2403.12936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions. The extraction of key <b>information</b> <b>from</b> these documents provides a concise overview of a case, crucial for both legal experts and the public. With the advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> automatic <b>information</b> <b>extraction</b> has become increasingly feasible and efficient. This paper presents a comprehensive study on the application of <b>GPT-4,</b> a <b>large</b> <b>language</b> <b>model,</b> for automatic <b>information</b> <b>extraction</b> from UK Employment Tribunal (UKET) cases. We meticulously evaluated <b>GPT-4&rsquo;s</b> performance in extracting critical <b>information</b> <b>with</b> a manual verification process to ensure the accuracy and relevance of the extracted data. Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists and the general public, including the facts of the case, the claims made, references to legal statutes, references to precedents, general case outcomes and corresponding labels, detailed order and remedies and reasons for the decision. The second task is more focused, aimed at analysing three of those extracted features, namely facts, claims and outcomes, in order to facilitate the development of a tool capable of predicting the outcome of employment law disputes. Through our analysis, we demonstrate that <b>LLMs</b> like <b>GPT-4</b> can obtain high accuracy in legal <b>information</b> <b>extraction,</b> highlighting the potential of <b>LLMs</b> in revolutionising the way legal <b>information</b> <b>is</b> processed and utilised, offering significant implications for legal research and practice.</p></p class="citation"></blockquote><h3 id=1645--75329-fine-tuning-pre-trained-language-models-to-detect-in-game-trash-talks-daniel-fesalbon-et-al-2024>(16/45 | 75/329) Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks (Daniel Fesalbon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Fesalbon, Arvin De La Cruz, Marvin Mallari, Nelson Rodelas. (2024)<br><strong>Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks</strong><br><button class=copy-to-clipboard title="Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, BERT, GPT, GPT-3, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15458v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15458v1.pdf filename=2403.15458v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Common problems in playing online mobile and computer games were related to toxic behavior and abusive communication among players. Based on different reports and studies, the study also discusses the impact of online hate speech and toxicity on players&rsquo; in-game performance and overall well-being. This study investigates the capability of <b>pre-trained</b> <b>language</b> <b>models</b> to classify or detect trash talk or toxic in-game messages The study employs and evaluates the performance of <b>pre-trained</b> <b>BERT</b> <b>and</b> <b>GPT</b> language models in detecting toxicity within in-game chats. Using publicly available APIs, in-game chat data from DOTA 2 game matches were collected, processed, reviewed, and labeled as non-toxic, mild (toxicity), and toxic. The study was able to collect around two thousand in-game chats to train and test <b>BERT</b> (Base-uncased), <b>BERT</b> (Large-uncased), and <b>GPT-3</b> models. Based on the three models&rsquo; state-of-the-art performance, this study concludes <b>pre-trained</b> <b>language</b> <b>models&rsquo;</b> promising potential for addressing online hate speech and in-game insulting trash talk.</p></p class="citation"></blockquote><h3 id=1745--76329-cross-lingual-transfer-for-natural-language-inference-via-multilingual-prompt-translator-xiaoyu-qiu-et-al-2024>(17/45 | 76/329) Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator (Xiaoyu Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Qiu, Yuechen Wang, Jiaxin Shi, Wengang Zhou, Houqiang Li. (2024)<br><strong>Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator</strong><br><button class=copy-to-clipboard title="Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, Low-Resource, Natural Language Inference, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12407v1.pdf filename=2403.12407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Based on multilingual pre-trained models, cross-lingual transfer with <b>prompt</b> <b>learning</b> has shown promising effectiveness, where soft <b>prompt</b> <b>learned</b> in a source language is transferred to target languages for downstream tasks, particularly in the <b>low-resource</b> scenario. To efficiently transfer soft <b>prompt,</b> <b>we</b> propose a novel framework, Multilingual <b>Prompt</b> <b>Translator</b> (MPT), where a multilingual <b>prompt</b> <b>translator</b> is introduced to properly process crucial knowledge embedded in <b>prompt</b> <b>by</b> changing language knowledge while retaining task knowledge. Concretely, we first train <b>prompt</b> <b>in</b> source language and employ translator to translate it into target <b>prompt.</b> <b>Besides,</b> we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target <b>prompt</b> <b>with</b> multilingual knowledge. In <b>few-shot</b> settings on XNLI, MPT demonstrates superiority over baselines by remarkable improvements. MPT is more prominent compared with vanilla <b>prompting</b> <b>when</b> transferring to languages quite distinct from source language.</p></p class="citation"></blockquote><h3 id=1845--77329-agent-flan-designing-data-and-methods-of-effective-agent-tuning-for-large-language-models-zehui-chen-et-al-2024>(18/45 | 77/329) Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models (Zehui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, Feng Zhao. (2024)<br><strong>Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models</strong><br><button class=copy-to-clipboard title="Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12881v1.pdf filename=2403.12881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-sourced <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general <b>LLMs</b> becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent <b>reasoning,</b> which significantly shifts from the distribution of its pre-training data; (2) <b>LLMs</b> exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively <b>Fine-tune</b> LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation <b>benchmark.</b> Besides, it consistently improves the agent capability of <b>LLMs</b> when scaling model sizes while slightly enhancing the general capability of <b>LLMs.</b> The code will be available at <a href=https://github.com/InternLM/Agent-FLAN>https://github.com/InternLM/Agent-FLAN</a>.</p></p class="citation"></blockquote><h3 id=1945--78329-generalizable-and-stable-finetuning-of-pretrained-language-models-on-low-resource-texts-sai-ashish-somayajula-et-al-2024>(19/45 | 78/329) Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts (Sai Ashish Somayajula et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sai Ashish Somayajula, Youwei Liang, Abhishek Singh, Li Zhang, Pengtao Xie. (2024)<br><strong>Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts</strong><br><button class=copy-to-clipboard title="Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Low-Resource, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12918v1.pdf filename=2403.12918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pretrained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> have advanced Natural Language Processing (NLP) tasks significantly, but <b>finetuning</b> <b>PLMs</b> on <b>low-resource</b> datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by <b>finetuning</b> a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the <b>pretrained</b> <b>weights.</b> <b>However,</b> they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for <b>finetuning</b> <b>PLMs.</b> Our approach represents each network weight as a mixup of task-specific weight and <b>pretrained</b> <b>weight,</b> <b>controlled</b> by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting. We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of <b>finetuning</b> <b>PLMs</b> on <b>low-resource</b> datasets.</p></p class="citation"></blockquote><h3 id=2045--79329-investigating-text-shortening-strategy-in-bert-truncation-vs-summarization-mirza-alim-mutasodirin-et-al-2024>(20/45 | 79/329) Investigating Text Shortening Strategy in BERT: Truncation vs Summarization (Mirza Alim Mutasodirin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mirza Alim Mutasodirin, Radityo Eko Prasojo. (2024)<br><strong>Investigating Text Shortening Strategy in BERT: Truncation vs Summarization</strong><br><button class=copy-to-clipboard title="Investigating Text Shortening Strategy in BERT: Truncation vs Summarization" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: BERT, Transformer, Text Classification, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12799v1.pdf filename=2403.12799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The parallelism of <b>Transformer-based</b> models comes at the cost of their input max-length. Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of <b>summarization</b> as an alternative. In this study, we investigate the performance of document truncation and <b>summarization</b> in <b>text</b> <b>classification</b> tasks. Each of the two was investigated with several variations. This study also investigated how close their performances are to the performance of full-text. We used a dataset of <b>summarization</b> tasks based on Indonesian news articles (IndoSum) to do classification tests. This study shows how the summaries outperform the majority of truncation method variations and lose to only one. The best strategy obtained in this study is taking the head of the document. The second is extractive <b>summarization.</b> This study explains what happened to the result, leading to further research in order to exploit the potential of document <b>summarization</b> as a shortening alternative. The code and data used in this work are publicly available in <a href=https://github.com/mirzaalimm/TruncationVsSummarization>https://github.com/mirzaalimm/TruncationVsSummarization</a>.</p></p class="citation"></blockquote><h3 id=2145--80329-dr3-ask-large-language-models-not-to-give-off-topic-answers-in-open-domain-multi-hop-question-answering-yuan-gao-et-al-2024>(21/45 | 80/329) Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering (Yuan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Gao, Yiheng Zhu, Yuanbin Cao, Yinzhi Zhou, Zhen Wu, Yujie Chen, Shenglan Wu, Haoyuan Hu, Xinyu Dai. (2024)<br><strong>Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering</strong><br><button class=copy-to-clipboard title="Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12393v1.pdf filename=2403.12393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open Domain Multi-Hop <b>Question</b> <b>Answering</b> (ODMHQA) plays a crucial role in Natural Language Processing (NLP) by aiming to answer complex <b>questions</b> <b>through</b> multi-step <b>reasoning</b> over retrieved information from external knowledge sources. Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable performance in solving ODMHQA owing to their capabilities including planning, <b>reasoning,</b> and utilizing tools. However, <b>LLMs</b> may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original <b>questions.</b> <b>This</b> issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance. To alleviate this issue, we propose the Discriminate->Re-Compose->Re- Solve->Re-Decompose (Dr3) mechanism. Specifically, the Discriminator leverages the intrinsic capabilities of <b>LLMs</b> to judge whether the generated answers are off-topic. In cases where an off-topic answer is detected, the Corrector performs step-wise revisions along the reversed <b>reasoning</b> chain (Re-Compose->Re-Solve->Re-Decompose) until the final answer becomes on-topic. Experimental results on the HotpotQA and 2WikiMultiHopQA datasets demonstrate that our Dr3 mechanism considerably reduces the occurrence of off-topic answers in ODMHQA by nearly 13%, improving the performance in Exact Match (EM) by nearly 3% compared to the baseline method without the Dr3 mechanism.</p></p class="citation"></blockquote><h3 id=2245--81329-towards-interpretable-hate-speech-detection-using-large-language-model-extracted-rationales-ayushi-nirmal-et-al-2024>(22/45 | 81/329) Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales (Ayushi Nirmal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayushi Nirmal, Amrita Bhattacharjee, Paras Sheth, Huan Liu. (2024)<br><strong>Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales</strong><br><button class=copy-to-clipboard title="Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 35<br>Keywords: Black Box, Hate Speech Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12403v1.pdf filename=2403.12403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew <b>hate</b> <b>speech</b> <b>and</b> offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of <b>hate</b> <b>speech.</b> <b>Although</b> several <b>hate</b> <b>speech</b> <b>detection</b> methods exist, most of these <b>black-box</b> <b>methods</b> are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to extract features in the form of rationales from the input text, to train a base <b>hate</b> <b>speech</b> <b>classifier,</b> thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of <b>LLMs</b> and the discriminative power of state-of-the-art <b>hate</b> <b>speech</b> <b>classifiers</b> to make these classifiers faithfully interpretable. Our comprehensive evaluation on a variety of social media <b>hate</b> <b>speech</b> <b>datasets</b> demonstrate: (1) the goodness of the <b>LLM-extracted</b> rationales, and (2) the surprising retention of detector performance even after training to ensure interpretability.</p></p class="citation"></blockquote><h3 id=2345--82329-multi-dimensional-machine-translation-evaluation-model-evaluation-and-resource-for-korean-dojun-park-et-al-2024>(23/45 | 82/329) Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean (Dojun Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dojun Park, Sebastian Padó. (2024)<br><strong>Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean</strong><br><button class=copy-to-clipboard title="Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Automatic Evaluation, Benchmarking, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12666v1.pdf filename=2403.12666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Almost all frameworks for the manual or <b>automatic</b> <b>evaluation</b> of <b>machine</b> <b>translation</b> characterize the quality of an <b>MT</b> output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation <b>benchmark</b> for the language pair English-Korean and (b) reframing <b>MT</b> evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based <b>MT</b> evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperforms its counterpart in the style dimension while reference-based models retain an edge regarding accuracy. Overall, RemBERT emerges as the most promising model. Through our evaluation, we offer an insight into the translation quality in a more fine-grained, interpretable manner.</p></p class="citation"></blockquote><h3 id=2445--83329-characteristic-ai-agents-via-large-language-models-xi-wang-et-al-2024>(24/45 | 83/329) Characteristic AI Agents via Large Language Models (Xi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xi Wang, Hongliang Dai, Shen Gao, Piji Li. (2024)<br><strong>Characteristic AI Agents via Large Language Models</strong><br><button class=copy-to-clipboard title="Characteristic AI Agents via Large Language Models" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12368v1.pdf filename=2403.12368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has led to significant enhancements in the performance of <b>chatbot</b> systems. Many researchers have dedicated their efforts to the development of bringing characteristics to <b>chatbots.</b> While there have been commercial products for developing role-driven <b>chatbots</b> using <b>LLMs,</b> it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of <b>LLMs</b> in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a <b>benchmark</b> for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called ``Character100&rsquo;&rsquo; is built for this <b>benchmark,</b> comprising the most-visited people on Wikipedia for language models to role-play. With the constructed dataset, we conduct comprehensive assessment of <b>LLMs</b> across various settings. In addition, we devise a set of automatic metrics for quantitative performance evaluation. The experimental results underscore the potential directions for further improvement in the capabilities of <b>LLMs</b> in constructing characteristic AI agents. The <b>benchmark</b> is available at <a href=https://github.com/nuaa-nlp/Character100>https://github.com/nuaa-nlp/Character100</a>.</p></p class="citation"></blockquote><h3 id=2545--84329-self-generated-replay-memories-for-continual-neural-machine-translation-michele-resta-et-al-2024>(25/45 | 84/329) Self-generated Replay Memories for Continual Neural Machine Translation (Michele Resta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michele Resta, Davide Bacciu. (2024)<br><strong>Self-generated Replay Memories for Continual Neural Machine Translation</strong><br><button class=copy-to-clipboard title="Self-generated Replay Memories for Continual Neural Machine Translation" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Transformer, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13130v1.pdf filename=2403.13130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>Neural</b> <b>Machine</b> <b>Translation</b> systems exhibit strong performance in several different languages and are constantly improving. Their ability to learn continuously is, however, still severely limited by the catastrophic forgetting issue. In this work, we leverage a key property of encoder-decoder <b>Transformers,</b> i.e. their generative ability, to propose a novel approach to continually learning <b>Neural</b> <b>Machine</b> <b>Translation</b> systems. We show how this can effectively learn on a stream of experiences comprising different languages, by leveraging a replay memory populated by using the model itself as a generator of parallel sentences. We empirically demonstrate that our approach can counteract catastrophic forgetting without requiring explicit memorization of training data. Code will be publicly available upon publication. Code: <a href=https://github.com/m-resta/sg-rep>https://github.com/m-resta/sg-rep</a></p></p class="citation"></blockquote><h3 id=2645--85329-sebastian-basti-wastl-recognizing-named-entities-in-bavarian-dialectal-data-siyao-peng-et-al-2024>(26/45 | 85/329) Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data (Siyao Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyao Peng, Zihang Sun, Huangyan Shan, Marie Kolm, Verena Blaschke, Ekaterina Artemova, Barbara Plank. (2024)<br><strong>Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data</strong><br><button class=copy-to-clipboard title="Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12749v1.pdf filename=2403.12749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects. This paper introduces the first dialectal <b>NER</b> dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information. We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive <b>NER</b> results on Bavarian. Incorporating knowledge from the larger German <b>NER</b> (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet. Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus. Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five <b>NER</b> and two Bavarian-German dialect identification tasks and achieve <b>NER</b> SOTA on bar-wiki. We substantiate the necessity of our <b>low-resource</b> BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance.</p></p class="citation"></blockquote><h3 id=2745--86329-factorized-learning-assisted-with-large-language-model-for-gloss-free-sign-language-translation-zhigang-chen-et-al-2024>(27/45 | 86/329) Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation (Zhigang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhigang Chen, Benjia Zhou, Jun Li, Jun Wan, Zhen Lei, Ning Jiang, Quan Lu, Guoqing Zhao. (2024)<br><strong>Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation</strong><br><button class=copy-to-clipboard title="Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12556v1.pdf filename=2403.12556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous Sign Language Translation (SLT) methods achieve superior performance by relying on gloss annotations. However, labeling high-quality glosses is a labor-intensive task, which limits the further development of SLT. Although some approaches work towards gloss-free SLT through jointly training the visual encoder and translation network, these efforts still suffer from poor performance and inefficient use of the powerful <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM).</b> Most seriously, we find that directly introducing <b>LLM</b> into SLT will lead to insufficient learning of visual representations as <b>LLM</b> dominates the learning curve. To address these problems, we propose Factorized Learning assisted with <b>Large</b> <b>Language</b> <b>Model</b> (FLa-LLM) for gloss-free SLT. Concretely, we factorize the training process into two stages. In the visual initialing stage, we employ a lightweight translation model after the visual encoder to pre-train the visual encoder. In the <b>LLM</b> <b>fine-tuning</b> stage, we freeze the acquired knowledge in the visual encoder and integrate it with a pre-trained <b>LLM</b> to inspire the <b>LLM&rsquo;s</b> translation potential. This factorized training strategy proves to be highly effective as evidenced by significant improvements achieved across three SLT datasets which are all conducted under the gloss-free setting.</p></p class="citation"></blockquote><h3 id=2845--87329-an-empirical-study-of-speech-language-models-for-prompt-conditioned-speech-synthesis-yifan-peng-et-al-2024>(28/45 | 87/329) An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis (Yifan Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, Hongyu Gong. (2024)<br><strong>An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis</strong><br><button class=copy-to-clipboard title="An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12402v1.pdf filename=2403.12402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech language models (LMs) are promising for high-quality speech synthesis through <b>in-context</b> <b>learning.</b> A typical speech LM takes discrete semantic units as content and a short utterance as <b>prompt,</b> and synthesizes speech which preserves the content&rsquo;s semantics but mimics the <b>prompt&rsquo;s</b> style. However, there is no systematic understanding on how the synthesized audio is controlled by the <b>prompt</b> and content. In this work, we conduct an empirical study of the widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and provide insights into the <b>prompt</b> design and content semantic units. Our analysis reveals that heterogeneous and nonstationary <b>prompts</b> hurt the audio quality in contrast to the previous finding that longer <b>prompts</b> always lead to better synthesis. Moreover, we find that the speaker style of the synthesized audio is also affected by the content in addition to the <b>prompt.</b> We further show that semantic units carry rich acoustic information such as pitch, tempo, volume and speech emphasis, which might be leaked from the content to the synthesized audio.</p></p class="citation"></blockquote><h3 id=2945--88329-pipelined-biomedical-event-extraction-rivaling-joint-learning-pengchao-wu-et-al-2024>(29/45 | 88/329) Pipelined Biomedical Event Extraction Rivaling Joint Learning (Pengchao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengchao Wu, Xuefeng Li, Jinghang Gu, Longhua Qian, Guodong Zhou. (2024)<br><strong>Pipelined Biomedical Event Extraction Rivaling Joint Learning</strong><br><button class=copy-to-clipboard title="Pipelined Biomedical Event Extraction Rivaling Joint Learning" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BERT, Information Retrieval, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12386v1.pdf filename=2403.12386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Biomedical event extraction is an <b>information</b> <b>extraction</b> task to obtain events from biomedical text, whose targets include the type, the trigger, and the respective arguments involved in an event. Traditional biomedical event extraction usually adopts a pipelined approach, which contains trigger identification, argument role recognition, and finally event construction either using specific rules or by machine learning. In this paper, we propose an n-ary <b>relation</b> <b>extraction</b> method based on the <b>BERT</b> pre-training model to construct Binding events, in order to capture the semantic <b>information</b> <b>about</b> an event&rsquo;s context and its participants. The experimental results show that our method achieves promising results on the GE11 and GE13 corpora of the BioNLP shared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates that by significantly improving theperformance of Binding events, the overall performance of the pipelined event extraction approach or even exceeds those of current joint learning methods.</p></p class="citation"></blockquote><h3 id=3045--89329-assessing-effect-sizes-variability-and-power-in-the-on-line-study-of-language-production-bürki-audrey-et-al-2024>(30/45 | 89/329) Assessing effect sizes, variability, and power in the on-line study of language production (Bürki Audrey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bürki Audrey, Vasishth Shravan. (2024)<br><strong>Assessing effect sizes, variability, and power in the on-line study of language production</strong><br><button class=copy-to-clipboard title="Assessing effect sizes, variability, and power in the on-line study of language production" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Sample Size, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15459v1.pdf filename=2403.15459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the pandemic, many experimental psychologists and linguists have started to collect data over the internet (hereafter on-line data). The feasibility of such experiments and the <b>sample</b> <b>sizes</b> required to achieve sufficient statistical power in future experiments have to be assessed. This in turn requires information on effect sizes and variability. In a series of analyses, we compare response time data obtained in the same word production experiment conducted in the lab and on-line. These analyses allow us to determine whether the two settings differ in effect sizes, in the consistency of responses over the course of the experiment, in the variability of average response times across participants, in the magnitude of effect sizes across participants, or in the amount of unexplained variability. We assess the impact of these differences on the power of the design in a series of <b>simulations.</b> Our findings temper the enthusiasm raised by previous studies and suggest that on-line production studies might be feasible but at a non-negligible cost. The <b>sample</b> <b>sizes</b> required to achieve sufficient power in on-line language production studies come with a non-negligible increase in the amount of manual labour.</p></p class="citation"></blockquote><h3 id=3145--90329-graphere-jointly-multiple-event-event-relation-extraction-via-graph-enhanced-event-embeddings-haochen-li-et-al-2024>(31/45 | 90/329) GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings (Haochen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochen Li, Di Geng. (2024)<br><strong>GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings</strong><br><button class=copy-to-clipboard title="GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Graph, Transformer, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12523v1.pdf filename=2403.12523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Events describe the state changes of entities. In a document, multiple events are connected by various <b>relations</b> <b>(e.g.,</b> Coreference, Temporal, Causal, and Subevent). Therefore, obtaining the connections between events through Event-Event <b>Relation</b> <b>Extraction</b> (ERE) is critical to understand natural language. There are two main problems in the current ERE works: a. Only embeddings of the event triggers are used for event feature representation, ignoring event arguments (e.g., time, place, person, etc.) and their structure within the event. b. The interconnection between <b>relations</b> <b>(e.g.,</b> temporal and causal <b>relations</b> <b>usually</b> interact with each other ) is ignored. To solve the above problems, this paper proposes a jointly multiple ERE framework called GraphERE based on <b>Graph-enhanced</b> Event Embeddings. First, we enrich the event embeddings with event argument and structure features by using static AMR <b>graphs</b> and IE <b>graphs;</b> Then, to jointly extract multiple event <b>relations,</b> <b>we</b> use Node <b>Transformer</b> and construct Task-specific Dynamic Event <b>Graphs</b> for each type of <b>relation.</b> <b>Finally,</b> we used a multi-task learning strategy to train the whole framework. Experimental results on the latest MAVEN-ERE dataset validate that GraphERE significantly outperforms existing methods. Further analyses indicate the effectiveness of the <b>graph-enhanced</b> event embeddings and the joint extraction strategy.</p></p class="citation"></blockquote><h3 id=3245--91329-third-party-language-model-performance-prediction-from-instruction-rahul-nadkarni-et-al-2024>(32/45 | 91/329) Third-Party Language Model Performance Prediction from Instruction (Rahul Nadkarni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul Nadkarni, Yizhong Wang, Noah A. Smith. (2024)<br><strong>Third-Party Language Model Performance Prediction from Instruction</strong><br><button class=copy-to-clipboard title="Third-Party Language Model Performance Prediction from Instruction" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Instruction Following, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12413v1.pdf filename=2403.12413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language model-based <b>instruction-following</b> <b>systems</b> have lately shown increasing performance on many <b>benchmark</b> tasks, demonstrating the capability of adapting to a broad variety of <b>instructions.</b> <b>However,</b> such systems are often not designed to be transparent about their limitations; a user may easily <b>prompt</b> a model with an <b>instruction</b> <b>without</b> any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task. We propose a third party performance prediction framework, where a separate model is trained to predict the metric resulting from evaluating an <b>instruction-following</b> <b>system</b> on a task while assuming access only to its inputs and outputs at inference time. We perform this analysis with a variety of both open and closed <b>instruction-following</b> <b>models</b> as well as multiple performance predictors, and examine the effect of various factors such as model size, number of training tasks, and <b>prompt</b> format. Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations of modern <b>instruction-following</b> <b>natural</b> language processing systems.</p></p class="citation"></blockquote><h3 id=3345--92329-dated-data-tracing-knowledge-cutoffs-in-large-language-models-jeffrey-cheng-et-al-2024>(33/45 | 92/329) Dated Data: Tracing Knowledge Cutoffs in Large Language Models (Jeffrey Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme. (2024)<br><strong>Dated Data: Tracing Knowledge Cutoffs in Large Language Models</strong><br><button class=copy-to-clipboard title="Dated Data: Tracing Knowledge Cutoffs in Large Language Models" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12958v1.pdf filename=2403.12958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Released <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the <b>LLM</b> must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model&rsquo;s demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the <b>LLM</b> designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an <b>LLM</b> by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct <b>large-scale</b> <b>analysis</b> <b>on</b> open pre-training datasets. Our analysis reveals two reasons for these inconsistencies: (1) temporal biases of CommonCrawl data due to non-trivial amounts of old data in new dumps and (2) complications in <b>LLM</b> deduplication schemes involving semantic duplicates and lexical near-duplicates. Overall, our results show that knowledge cutoffs are not as simple as they have seemed and that care must be taken both by <b>LLM</b> dataset curators as well as practitioners who seek to use information from these models.</p></p class="citation"></blockquote><h3 id=3445--93329-supporting-energy-policy-research-with-large-language-models-grant-buster-et-al-2024>(34/45 | 93/329) Supporting Energy Policy Research with Large Language Models (Grant Buster et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Grant Buster, Pavlo Pinchuk, Jacob Barrons, Ryan McKeever, Aaron Levine, Anthony Lopez. (2024)<br><strong>Supporting Energy Policy Research with Large Language Models</strong><br><button class=copy-to-clipboard title="Supporting Energy Policy Research with Large Language Models" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12924v1.pdf filename=2403.12924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent growth in renewable energy development in the United States has been accompanied by a simultaneous surge in renewable energy siting ordinances. These zoning laws play a critical role in dictating the placement of wind and solar resources that are critical for achieving low-carbon energy futures. In this context, efficient access to and management of siting ordinance data becomes imperative. The National Renewable Energy Laboratory (NREL) recently introduced a public wind and solar siting database to fill this need. This paper presents a method for harnessing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to automate the extraction of these siting ordinances from legal documents, enabling this database to maintain accurate up-to-date information in the rapidly changing energy policy landscape. A novel contribution of this research is the integration of a decision tree framework with <b>LLMs.</b> Our results show that this approach is 85 to 90% accurate with outputs that can be used directly in downstream quantitative modeling. We discuss opportunities to use this work to support similar <b>large-scale</b> <b>policy</b> <b>research</b> in the energy sector. By unlocking new efficiencies in the extraction and analysis of legal documents using <b>LLMs,</b> this study enables a path forward for automated <b>large-scale</b> <b>energy</b> <b>policy</b> research.</p></p class="citation"></blockquote><h3 id=3545--94329-epistemology-of-language-models-do-language-models-have-holistic-knowledge-minsu-kim-et-al-2024>(35/45 | 94/329) Epistemology of Language Models: Do Language Models Have Holistic Knowledge? (Minsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsu Kim, James Thorne. (2024)<br><strong>Epistemology of Language Models: Do Language Models Have Holistic Knowledge?</strong><br><button class=copy-to-clipboard title="Epistemology of Language Models: Do Language Models Have Holistic Knowledge?" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12862v1.pdf filename=2403.12862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the inherent knowledge in language models from the perspective of epistemological holism. The purpose of this paper is to explore whether <b>LLMs</b> exhibit characteristics consistent with epistemological holism. These characteristics suggest that core knowledge, such as general scientific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise. To assess these traits related to holism, we created a scientific <b>reasoning</b> dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation. In the abduction task, the language models explained situations while avoiding revising the core knowledge. However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles.</p></p class="citation"></blockquote><h3 id=3645--95329-simple-hack-for-transformers-against-heavy-long-text-classification-on-a-time--and-memory-limited-gpu-service-mirza-alim-mutasodirin-et-al-2024>(36/45 | 95/329) Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service (Mirza Alim Mutasodirin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mirza Alim Mutasodirin, Radityo Eko Prasojo, Achmad F. Abka, Hanif Rasyidi. (2024)<br><strong>Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service</strong><br><button class=copy-to-clipboard title="Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12563v1.pdf filename=2403.12563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many NLP researchers rely on free computational services, such as Google Colab, to <b>fine-tune</b> their <b>Transformer</b> models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource. In Indonesian, only a few works were found on long-text classification using <b>Transformers.</b> Most only use a small amount of data and do not report any HPO. In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer. We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words. To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library. Using the best hack found, we then compare 512, 256, and 128 tokens length. We find that removing stopwords while keeping punctuation and low-frequency words is the best hack. Some of our setups manage to outperform taking 512 first tokens using a smaller 128 or 256 first tokens which manage to represent the same information while requiring less computational resources. The findings could help developers to efficiently pursue optimal performance of the models using limited resources.</p></p class="citation"></blockquote><h3 id=3745--96329-a-large-collection-of-model-generated-contradictory-responses-for-consistency-aware-dialogue-systems-shiki-sato-et-al-2024>(37/45 | 96/329) A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems (Shiki Sato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiki Sato, Reina Akama, Jun Suzuki, Kentaro Inui. (2024)<br><strong>A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems</strong><br><button class=copy-to-clipboard title="A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Dialogue Response Generation, Dialogue System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12500v1.pdf filename=2403.12500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mitigating the generation of contradictory responses poses a substantial challenge in <b>dialogue</b> <b>response</b> <b>generation.</b> The quality and quantity of available contradictory response data play a vital role in suppressing these contradictions, offering two significant benefits. First, having access to large contradiction data enables a comprehensive examination of their characteristics. Second, data-driven methods to mitigate contradictions may be enhanced with large-scale contradiction data for training. Nevertheless, no attempt has been made to build an extensive collection of model-generated contradictory responses. In this paper, we build a large dataset of response generation models&rsquo; contradictions for the first time. Then, we acquire valuable insights into the characteristics of model-generated contradictions through an extensive analysis of the collected responses. Lastly, we also demonstrate how this dataset substantially enhances the performance of data-driven contradiction suppression methods.</p></p class="citation"></blockquote><h3 id=3845--97329-arapoembert-a-pretrained-language-model-for-arabic-poetry-analysis-faisal-qarah-2024>(38/45 | 97/329) AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis (Faisal Qarah, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Faisal Qarah. (2024)<br><strong>AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis</strong><br><button class=copy-to-clipboard title="AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Sentiment Analysis, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12392v1.pdf filename=2403.12392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Arabic poetry, with its rich linguistic features and profound cultural significance, presents a unique challenge to the Natural Language Processing (NLP) field. The complexity of its structure and context necessitates advanced computational models for accurate analysis. In this paper, we introduce AraPoemBERT, an Arabic language model <b>pretrained</b> <b>exclusively</b> <b>on</b> Arabic poetry text. To demonstrate the effectiveness of the proposed model, we compared AraPoemBERT with 5 different Arabic language models on various NLP tasks related to Arabic poetry. The new model outperformed all other models and achieved state-of-the-art results in most of the downstream tasks. AraPoemBERT achieved unprecedented accuracy in two out of three novel tasks: poet&rsquo;s gender classification (99.34% accuracy), and poetry sub-meter classification (97.79% accuracy). In addition, the model achieved an accuracy score in poems&rsquo; rhyme classification (97.73% accuracy) which is almost equivalent to the best score reported in this study. Moreover, the proposed model significantly outperformed previous work and other comparative models in the tasks of poems&rsquo; <b>sentiment</b> <b>analysis,</b> achieving an accuracy of 78.95%, and poetry meter classification (99.03% accuracy), while significantly expanding the scope of these two problems. The dataset used in this study, contains more than 2.09 million verses collected from online sources, each associated with various attributes such as meter, sub-meter, poet, rhyme, and topic. The results demonstrate the effectiveness of the proposed model in understanding and analyzing Arabic poetry, achieving state-of-the-art results in several tasks and outperforming previous works and other language models included in the study. AraPoemBERT model is publicly available on \url{https://huggingface.co/faisalq}.</p></p class="citation"></blockquote><h3 id=3945--98329-prompt-based-graph-model-for-joint-liberal-event-extraction-and-event-schema-induction-haochen-li-et-al-2024>(39/45 | 98/329) Prompt-based Graph Model for Joint Liberal Event Extraction and Event Schema Induction (Haochen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochen Li, Di Geng. (2024)<br><strong>Prompt-based Graph Model for Joint Liberal Event Extraction and Event Schema Induction</strong><br><button class=copy-to-clipboard title="Prompt-based Graph Model for Joint Liberal Event Extraction and Event Schema Induction" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Graph, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12526v1.pdf filename=2403.12526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Events are essential components of speech and texts, describing the changes in the state of entities. The event extraction task aims to identify and classify events and find their participants according to event schemas. Manually predefined event schemas have limited coverage and are hard to migrate across domains. Therefore, the researchers propose Liberal Event Extraction (LEE), which aims to extract events and discover event schemas simultaneously. However, existing LEE models rely heavily on external language knowledge bases and require the manual development of numerous rules for noise removal and knowledge alignment, which is complex and laborious. To this end, we propose a <b>Prompt-based</b> <b>Graph</b> Model for Liberal Event Extraction (PGLEE). Specifically, we use a <b>prompt-based</b> model to obtain candidate triggers and arguments, and then build heterogeneous event <b>graphs</b> to encode the structures within and between events. Experimental results prove that our approach achieves excellent performance with or without predefined event schemas, while the automatically detected event schemas are proven high quality.</p></p class="citation"></blockquote><h3 id=4045--99329-comparing-explanation-faithfulness-between-multilingual-and-monolingual-fine-tuned-language-models-zhixue-zhao-et-al-2024>(40/45 | 99/329) Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models (Zhixue Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixue Zhao, Nikolaos Aletras. (2024)<br><strong>Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models</strong><br><button class=copy-to-clipboard title="Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12809v1.pdf filename=2403.12809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers. Our code is available: <a href=https://github.com/casszhao/multilingual-faith>https://github.com/casszhao/multilingual-faith</a>.</p></p class="citation"></blockquote><h3 id=4145--100329-classla-web-comparable-web-corpora-of-south-slavic-languages-enriched-with-linguistic-and-genre-annotation-nikola-ljubešić-et-al-2024>(41/45 | 100/329) CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation (Nikola Ljubešić et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikola Ljubešić, Taja Kuzman. (2024)<br><strong>CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation</strong><br><button class=copy-to-clipboard title="CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12721v1.pdf filename=2403.12721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space. The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents. The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology. All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the <b>Transformer-based</b> multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment. The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora, with variations in the most prominent genre categories being well-explained by the economic strength of each language community. A comparison of the distribution of genre categories across the corpora indicates that web corpora from less developed countries primarily consist of news articles. Conversely, web corpora from economically more developed countries exhibit a smaller proportion of news content, with a greater presence of promotional and opinionated texts.</p></p class="citation"></blockquote><h3 id=4245--101329-empowering-air-travelers-a-chatbot-for-canadian-air-passenger-rights-maksym-taranukhin-et-al-2024>(42/45 | 101/329) Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights (Maksym Taranukhin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maksym Taranukhin, Sahithya Ravi, Gabor Lukacs, Evangelos Milios, Vered Shwartz. (2024)<br><strong>Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights</strong><br><button class=copy-to-clipboard title="Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12678v1.pdf filename=2403.12678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights. Recognizing this demand, we present a <b>chatbot</b> to assist passengers and educate them about their rights. Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations. The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances. The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions. A user study comparing the <b>chatbot</b> to a Google search demonstrated the <b>chatbot&rsquo;s</b> usefulness and ease of use. Beyond the primary goal of providing accurate and timely information to air passengers regarding their rights, we hope that this system will also enable further research exploring the tradeoff between the user-friendly conversational interface of <b>chatbots</b> and the accuracy of retrieval systems.</p></p class="citation"></blockquote><h3 id=4345--102329-mslm-s2st-a-multitask-speech-language-model-for-textless-speech-to-speech-translation-with-speaker-style-preservation-yifan-peng-et-al-2024>(43/45 | 102/329) MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation (Yifan Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, Hongyu Gong. (2024)<br><strong>MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation</strong><br><button class=copy-to-clipboard title="MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 10<br>Keywords: Speech-to-Speech Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12408v1.pdf filename=2403.12408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There have been emerging research interest and advances in <b>speech-to-speech</b> translation (S2ST), translating utterances from one language to another. This work proposes Multitask Speech Language Model (MSLM), which is a decoder-only speech language model trained in a multitask setting. Without reliance on text training data, our model is able to support multilingual S2ST with speaker style preserved.</p></p class="citation"></blockquote><h3 id=4445--103329-wav2gloss-generating-interlinear-glossed-text-from-speech-taiqi-he-et-al-2024>(44/45 | 103/329) Wav2Gloss: Generating Interlinear Glossed Text from Speech (Taiqi He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taiqi He, Kwanghee Choi, Lindia Tjuatja, Nathaniel R. Robinson, Jiatong Shi, Shinji Watanabe, Graham Neubig, David R. Mortensen, Lori Levin. (2024)<br><strong>Wav2Gloss: Generating Interlinear Glossed Text from Speech</strong><br><button class=copy-to-clipboard title="Wav2Gloss: Generating Interlinear Glossed Text from Speech" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13169v1.pdf filename=2403.13169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thousands of the world&rsquo;s languages are in danger of extinction&ndash;a tremendous threat to cultural identities and human language diversity. Interlinear Glossed Text (IGT) is a form of linguistic annotation that can support documentation and resource creation for these languages&rsquo; communities. IGT typically consists of (1) transcriptions, (2) morphological segmentation, (3) glosses, and (4) free translations to a majority language. We propose Wav2Gloss: a task to extract these four annotation components automatically from speech, and introduce the first dataset to this end, Fieldwork: a corpus of speech with all these annotations covering 37 languages with standard formatting and train/dev/test splits. We compare end-to-end and cascaded Wav2Gloss methods, with analysis suggesting that pre-trained decoders assist with translation and glossing, that multi-task and multilingual approaches are underperformant, and that end-to-end systems perform better than cascaded systems, despite the text-only systems&rsquo; advantages. We provide <b>benchmarks</b> to lay the ground work for future research on IGT generation from speech.</p></p class="citation"></blockquote><h3 id=4545--104329-when-do-more-contexts-help-with-sarcasm-recognition-ojas-nimase-et-al-2024>(45/45 | 104/329) When Do &lsquo;More Contexts&rsquo; Help with Sarcasm Recognition? (Ojas Nimase et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ojas Nimase, Sanghyun Hong. (2024)<br><strong>When Do &lsquo;More Contexts&rsquo; Help with Sarcasm Recognition?</strong><br><button class=copy-to-clipboard title="When Do 'More Contexts' Help with Sarcasm Recognition?" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12469v1.pdf filename=2403.12469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sarcasm recognition is challenging because it needs an understanding of the true intention, which is opposite to or different from the literal meaning of the words. Prior work has addressed this challenge by developing a series of methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to models. While shown to be effective individually, no study has systematically evaluated their collective effectiveness. As a result, it remains unclear to what extent additional contexts can improve sarcasm recognition. In this work, we explore the improvements that existing methods bring by incorporating more contexts into a model. To this end, we develop a framework where we can integrate multiple contextual cues and test different approaches. In evaluation with four approaches on three sarcasm recognition <b>benchmarks,</b> we achieve existing state-of-the-art performances and also demonstrate the benefits of sequentially adding more contexts. We also identify inherent drawbacks of using more contexts, highlighting that in the pursuit of even better results, the model may need to adopt societal biases.</p></p class="citation"></blockquote><h2 id=cscv-104>cs.CV (104)</h2><h3 id=1104--105329-emotion-recognition-using-transformers-with-masked-learning-seongjae-min-et-al-2024>(1/104 | 105/329) Emotion Recognition Using Transformers with Masked Learning (Seongjae Min et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seongjae Min, Junseok Yang, Sangjun Lim, Junyong Lee, Sangwon Lee, Sejoon Lim. (2024)<br><strong>Emotion Recognition Using Transformers with Masked Learning</strong><br><button class=copy-to-clipboard title="Emotion Recognition Using Transformers with Masked Learning" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 100<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, LSTM, LSTM, LSTM, Transformer, Emotion Recognition, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13731v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13731v2.pdf filename=2403.13731v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, deep learning has achieved innovative advancements in various fields, including the analysis of human <b>emotions</b> <b>and</b> behaviors. Initiatives such as the Affective Behavior Analysis in-the-wild (ABAW) competition have been particularly instrumental in driving research in this area by providing diverse and challenging datasets that enable precise evaluation of complex <b>emotional</b> <b>states.</b> This study leverages the <b>Vision</b> <b>Transformer</b> (ViT) and <b>Transformer</b> models to focus on the estimation of Valence-Arousal (VA), which signifies the positivity and intensity of <b>emotions,</b> <b>recognition</b> of various facial expressions, and detection of Action Units (AU) representing fundamental muscle movements. This approach transcends traditional <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> based methods, proposing a new <b>Transformer-based</b> framework that maximizes the understanding of temporal and spatial features. The core contributions of this research include the introduction of a learning technique through random frame masking and the application of Focal loss adapted for imbalanced data, enhancing the accuracy and applicability of <b>emotion</b> <b>and</b> behavior analysis in real-world settings. This approach is expected to contribute to the advancement of <b>emotional</b> <b>computing</b> and deep learning methodologies.</p></p class="citation"></blockquote><h3 id=2104--106329-towards-multimodal-in-context-learning-for-vision--language-models-sivan-doveh-et-al-2024>(2/104 | 106/329) Towards Multimodal In-Context Learning for Vision & Language Models (Sivan Doveh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sivan Doveh, Shaked Perek, M. Jehanzeb Mirza, Amit Alfassy, Assaf Arbelle, Shimon Ullman, Leonid Karlinsky. (2024)<br><strong>Towards Multimodal In-Context Learning for Vision & Language Models</strong><br><button class=copy-to-clipboard title="Towards Multimodal In-Context Learning for Vision & Language Models" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 89<br>Keywords: Benchmarking, Few-shot, Multi-modal, Multi-modal, In-context Learning, In-context Learning, In-context Learning, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12736v1.pdf filename=2403.12736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by the emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that can truly understand human language, significant progress has been made in aligning other, non-language, modalities to be <code>understandable' by an &lt;b>LLM,&lt;/b> primarily via converting their samples into a sequence of embedded language-like tokens directly fed into the &lt;b>LLM&lt;/b> (decoder) input stream. However, so far limited attention has been given to transferring (and evaluating) one of the core &lt;b>LLM&lt;/b> capabilities to the emerging VLMs, namely the &lt;b>In-Context&lt;/b> &lt;b>Learning&lt;/b> &lt;b>(ICL)&lt;/b> ability, or in other words to guide VLMs to desired target downstream tasks or output structure using &lt;b>in-context&lt;/b> &lt;b>image+text&lt;/b> demonstrations. In this work, we dive deeper into analyzing the capabilities of some of the state-of-the-art VLMs to follow &lt;b>ICL&lt;/b> &lt;b>instructions,&lt;/b> &lt;b>discovering&lt;/b> them to be somewhat lacking. We discover that even models that underwent &lt;b>large-scale&lt;/b> &lt;b>mixed&lt;/b> &lt;b>modality&lt;/b> pre-training and were implicitly guided to make use of interleaved image and text information (intended to consume helpful context from multiple images) under-perform when &lt;b>prompted&lt;/b> with &lt;b>few-shot&lt;/b> &lt;b>(ICL)&lt;/b> demonstrations, likely due to their lack of </code>direct&rsquo; <b>ICL</b> <b>instruction</b> <b>tuning.</b> To test this conjecture, we propose a simple, yet surprisingly effective, strategy of extending a common VLM alignment framework with <b>ICL</b> support, methodology, and curriculum. We explore, analyze, and provide insights into effective data mixes, leading up to a significant 21.03% (and 11.3% on average) <b>ICL</b> performance boost over the strongest VLM baselines and a variety of <b>ICL</b> <b>benchmarks.</b> We also contribute new <b>benchmarks</b> for <b>ICL</b> evaluation in VLMs and discuss their advantages over the prior art.</p></p class="citation"></blockquote><h3 id=3104--107329-medbind-unifying-language-and-multimodal-medical-data-embeddings-yuan-gao-et-al-2024>(3/104 | 107/329) MEDBind: Unifying Language and Multimodal Medical Data Embeddings (Yuan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Gao, Sangwook Kim, David E Austin, Chris McIntosh. (2024)<br><strong>MEDBind: Unifying Language and Multimodal Medical Data Embeddings</strong><br><button class=copy-to-clipboard title="MEDBind: Unifying Language and Multimodal Medical Data Embeddings" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 79<br>Keywords: Benchmarking, Few-shot, Multi-modal, Multi-modal, Zero-shot, Image2text, Large Language Model, Prompt, Vision-and-Language, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12894v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12894v2.pdf filename=2403.12894v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical <b>vision-language</b> pretraining models (VLPM) have achieved remarkable progress in fusing chest X-rays (CXR) with clinical texts, introducing <b>image-text</b> data binding approaches that enable <b>zero-shot</b> <b>learning</b> and downstream clinical tasks. However, the current landscape lacks the holistic integration of additional medical modalities, such as electrocardiograms (ECG). We present MEDBind (Medical Electronic patient recorD), which learns joint embeddings across CXR, ECG, and medical text. Using text data as the central anchor, MEDBind features tri-modality binding, delivering competitive performance in top-K retrieval, <b>zero-shot,</b> <b>and</b> <b>few-shot</b> <b>benchmarks</b> against established VLPM, and the ability for CXR-to-ECG <b>zero-shot</b> <b>classification</b> and retrieval. This seamless integration is achieved through combination of contrastive loss on modality-text pairs with our proposed contrastive loss function, Edge-Modality Contrastive Loss, fostering a cohesive embedding space for CXR, ECG, and text. Finally, we demonstrate that MEDBind can improve downstream tasks by directly integrating CXR and ECG embeddings into a <b>large-language</b> <b>model</b> <b>for</b> <b>multimodal</b> <b>prompt</b> tuning.</p></p class="citation"></blockquote><h3 id=4104--108329-visiongpt-llm-assisted-real-time-anomaly-detection-for-safe-visual-navigation-hao-wang-et-al-2024>(4/104 | 108/329) VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Jiayou Qin, Ashish Bastola, Xiwen Chen, John Suchanek, Zihao Gong, Abolfazl Razi. (2024)<br><strong>VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation</strong><br><button class=copy-to-clipboard title="VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV<br>Keyword Score: 70<br>Keywords: Yolo, Object Detection, Anomaly Detection, Zero-shot, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12415v1.pdf filename=2403.12415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the potential of Large Language Models(LLMs) in <b>zero-shot</b> <b>anomaly</b> <b>detection</b> for safe visual navigation. With the assistance of the state-of-the-art real-time open-world <b>object</b> <b>detection</b> model <b>Yolo-World</b> and specialized <b>prompts,</b> the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of <b>LLMs</b> and the open-vocabulary <b>object</b> <b>detection</b> model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different <b>prompt</b> components, provided the vision for future improvement in visual accessibility, and paved the way for <b>LLMs</b> in video <b>anomaly</b> <b>detection</b> and <b>vision-language</b> understanding.</p></p class="citation"></blockquote><h3 id=5104--109329-dettoolchain-a-new-prompting-paradigm-to-unleash-detection-ability-of-mllm-yixuan-wu-et-al-2024>(5/104 | 109/329) DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM (Yixuan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, Philip Torr. (2024)<br><strong>DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM</strong><br><button class=copy-to-clipboard title="DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 69<br>Keywords: Object Detection, Graph, Multi-modal, Multi-modal, Zero-shot, GPT, Gemini, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12488v1.pdf filename=2403.12488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DetToolChain, a novel <b>prompting</b> paradigm, to unleash the <b>zero-shot</b> <b>object</b> <b>detection</b> ability of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs), such as <b>GPT-4V</b> and <b>Gemini.</b> Our approach consists of a detection <b>prompting</b> toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these <b>prompts.</b> Specifically, the <b>prompts</b> in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene <b>graphs).</b> Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements. The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases. Compared to existing state-of-the-art methods, <b>GPT-4V</b> with our DetToolChain improves state-of-the-art <b>object</b> <b>detectors</b> by +21.5% AP50 on MS COCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val set for <b>zero-shot</b> referring expression comprehension, +14.5% AP on D-cube describe <b>object</b> <b>detection</b> FULL setting.</p></p class="citation"></blockquote><h3 id=6104--110329-unibind-llm-augmented-unified-and-balanced-representation-space-to-bind-them-all-yuanhuiyi-lyu-et-al-2024>(6/104 | 110/329) UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All (Yuanhuiyi Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanhuiyi Lyu, Xu Zheng, Jiazhou Zhou, Lin Wang. (2024)<br><strong>UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All</strong><br><button class=copy-to-clipboard title="UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Contrastive Learning, Fine-tuning, Multi-modal, Zero-shot, Large Language Model, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12532v1.pdf filename=2403.12532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present UniBind, a flexible and efficient approach that learns a unified representation space for seven diverse modalities &ndash; images, <b>text,</b> <b>audio,</b> point cloud, thermal, video, and event data. Existing works, eg., ImageBind, treat the image as the central modality and build an image-centered representation space; however, the space may be sub-optimal as it leads to an unbalanced representation space among all modalities. Moreover, the category names are directly used to extract <b>text</b> <b>embeddings</b> for the downstream tasks, making it hardly possible to represent the semantics of <b>multi-modal</b> data. The &lsquo;out-of-the-box&rsquo; insight of our UniBind is to make the alignment center modality-agnostic and further learn a unified and balanced representation space, empowered by the <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> UniBind is superior in its flexible application to all CLIP-style models and delivers remarkable performance boosts. To make this possible, we 1) construct a knowledge base of <b>text</b> <b>embeddings</b> with the help of <b>LLMs</b> and <b>multi-modal</b> <b>LLMs;</b> 2) adaptively build <b>LLM-augmented</b> class-wise embedding center on top of the knowledge base and encoded visual embeddings; 3) align all the embeddings to the <b>LLM-augmented</b> embedding center via <b>contrastive</b> <b>learning</b> to achieve a unified and balanced representation space. UniBind shows strong <b>zero-shot</b> recognition performance gains over prior arts by an average of 6.36%. Finally, we achieve new state-of-the-art performance, eg., a 6.75% gain on ImageNet, on the <b>multi-modal</b> <b>fine-tuning</b> setting while reducing 90% of the learnable parameters.</p></p class="citation"></blockquote><h3 id=7104--111329-relationvlm-making-large-vision-language-models-understand-visual-relations-zhipeng-huang-et-al-2024>(7/104 | 111/329) RelationVLM: Making Large Vision-Language Models Understand Visual Relations (Zhipeng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhipeng Huang, Zhizheng Zhang, Zheng-Jun Zha, Yan Lu, Baining Guo. (2024)<br><strong>RelationVLM: Making Large Vision-Language Models Understand Visual Relations</strong><br><button class=copy-to-clipboard title="RelationVLM: Making Large Vision-Language Models Understand Visual Relations" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Few-shot, Reasoning, In-context Learning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12801v1.pdf filename=2403.12801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of <b>Large</b> <b>Vision-Language</b> <b>Models</b> (LVLMs) is striving to catch up with the success of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> yet it faces more challenges to be resolved. Very recent works enable LVLMs to localize object-level visual contents and ground text to them. Nonetheless, current LVLMs still struggle to precisely understand visual relations due to the lack of relevant data. In this work, we present RelationVLM, a <b>large</b> <b>vision-language</b> <b>model</b> capable of comprehending various levels and types of relations whether across multiple images or within a video. Specifically, we devise a multi-stage relation-aware training scheme and a series of corresponding data configuration strategies to bestow RelationVLM with the capabilities of understanding semantic relations, temporal associations and geometric transforms. Extensive case studies and quantitative evaluations show RelationVLM has strong capability in understanding such relations and emerges impressive <b>in-context</b> capability of <b>reasoning</b> from <b>few-shot</b> examples by comparison. This work fosters the advancements of LVLMs by enabling them to support a wider range of downstream applications toward artificial general intelligence.</p></p class="citation"></blockquote><h3 id=8104--112329-vitgaze-gaze-following-with-interaction-features-in-vision-transformers-yuehao-song-et-al-2024>(8/104 | 112/329) ViTGaze: Gaze Following with Interaction Features in Vision Transformers (Yuehao Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuehao Song, Xinggang Wang, Jingfeng Yao, Wenyu Liu, Jinglin Zhang, Xiangmin Xu. (2024)<br><strong>ViTGaze: Gaze Following with Interaction Features in Vision Transformers</strong><br><button class=copy-to-clipboard title="ViTGaze: Gaze Following with Interaction Features in Vision Transformers" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Self-supervised Learning, Self-supervised Pre-training, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12778v1.pdf filename=2403.12778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaze following aims to interpret human-scene interactions by predicting the person&rsquo;s focal point of gaze. Prevailing approaches often use multi-modality inputs, most of which adopt a two-stage framework. Hence their performance highly depends on the previous prediction accuracy. Others use a single-modality approach with complex decoders, increasing network computational load. Inspired by the remarkable success of pre-trained plain <b>Vision</b> <b>Transformers</b> (ViTs), we introduce a novel single-modality gaze following framework, ViTGaze. In contrast to previous methods, ViTGaze creates a brand new gaze following framework based mainly on powerful encoders (dec. param. less than 1%). Our principal insight lies in that the inter-token interactions within <b>self-attention</b> can be transferred to interactions between humans and scenes. Leveraging this presumption, we formulate a framework consisting of a 4D interaction encoder and a 2D spatial guidance module to extract human-scene interaction information from <b>self-attention</b> maps. Furthermore, our investigation reveals that ViT with <b>self-supervised</b> <b>pre-training</b> exhibits an enhanced ability to extract correlated information. A large number of experiments have been conducted to demonstrate the performance of the proposed method. Our method achieves state-of-the-art (SOTA) performance among all single-modality methods (3.4% improvement on AUC, 5.1% improvement on AP) and very comparable performance against multi-modality methods with 59% number of parameters less.</p></p class="citation"></blockquote><h3 id=9104--113329-as-firm-as-their-foundations-can-open-sourced-foundation-models-be-used-to-create-adversarial-examples-for-downstream-tasks-anjun-hu-et-al-2024>(9/104 | 113/329) As Firm As Their Foundations: Can open-sourced foundation models be used to create adversarial examples for downstream tasks? (Anjun Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anjun Hu, Jindong Gu, Francesco Pinto, Konstantinos Kamnitsas, Philip Torr. (2024)<br><strong>As Firm As Their Foundations: Can open-sourced foundation models be used to create adversarial examples for downstream tasks?</strong><br><button class=copy-to-clipboard title="As Firm As Their Foundations: Can open-sourced foundation models be used to create adversarial examples for downstream tasks?" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Object Detection, Foundation Model, Question Answering, Visual Question Answering, Vision-and-Language, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12693v1.pdf filename=2403.12693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> pre-trained on web-scale <b>vision-language</b> data, such as CLIP, are widely used as cornerstones of powerful machine learning systems. While pre-training offers clear advantages for downstream learning, it also endows downstream models with shared <b>adversarial</b> <b>vulnerabilities</b> that can be easily identified through the open-sourced <b>foundation</b> <b>model.</b> In this work, we expose such vulnerabilities in CLIP&rsquo;s downstream models and show that <b>foundation</b> <b>models</b> can serve as a basis for attacking their downstream systems. In particular, we propose a simple yet effective <b>adversarial</b> <b>attack</b> strategy termed Patch Representation Misalignment (PRM). Solely based on open-sourced CLIP vision encoders, this method produces adversaries that simultaneously fool more than 20 downstream models spanning 4 common <b>vision-language</b> tasks (semantic segmentation, <b>object</b> <b>detection,</b> image captioning and <b>visual</b> <b>question-answering).</b> <b>Our</b> findings highlight the concerning safety risks introduced by the extensive usage of public <b>foundational</b> <b>models</b> in the development of downstream systems, calling for extra caution in these scenarios.</p></p class="citation"></blockquote><h3 id=10104--114329-compound-expression-recognition-via-multi-model-ensemble-jun-yu-et-al-2024>(10/104 | 114/329) Compound Expression Recognition via Multi Model Ensemble (Jun Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Yu, Jichao Zhu, Wangyuan Zhu. (2024)<br><strong>Compound Expression Recognition via Multi Model Ensemble</strong><br><button class=copy-to-clipboard title="Compound Expression Recognition via Multi Model Ensemble" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Zero-shot, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12572v1.pdf filename=2403.12572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compound Expression Recognition (CER) plays a crucial role in interpersonal interactions. Due to the existence of Compound Expressions , human emotional expressions are complex, requiring consideration of both local and global facial expressions to make judgments. In this paper, to address this issue, we propose a solution based on ensemble learning methods for Compound Expression Recognition. Specifically, our task is classification, where we train three expression classification models based on <b>convolutional</b> <b>networks,</b> <b>Vision</b> <b>Transformers,</b> and multi-scale local attention networks. Then, through model ensemble using late fusion, we merge the outputs of multiple models to predict the final result. Our method achieves high accuracy on RAF-DB and is able to recognize expressions through <b>zero-shot</b> on certain portions of C-EXPR-DB.</p></p class="citation"></blockquote><h3 id=11104--115329-transformmix-learning-transformation-and-mixing-strategies-from-data-tsz-him-cheung-et-al-2024>(11/104 | 115/329) TransformMix: Learning Transformation and Mixing Strategies from Data (Tsz-Him Cheung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsz-Him Cheung, Dit-Yan Yeung. (2024)<br><strong>TransformMix: Learning Transformation and Mixing Strategies from Data</strong><br><button class=copy-to-clipboard title="TransformMix: Learning Transformation and Mixing Strategies from Data" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Object Detection, Data Augmentation, Heuristic Approach, Knowledge Distillation, Knowledge Distillation, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12429v1.pdf filename=2403.12429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> improves the generalization power of deep learning models by synthesizing more training samples. Sample-mixing is a popular <b>data</b> <b>augmentation</b> approach that creates additional <b>data</b> <b>by</b> combining existing samples. Recent sample-mixing methods, like Mixup and Cutmix, adopt simple mixing operations to blend multiple inputs. Although such a <b>heuristic</b> <b>approach</b> shows certain performance gains in some computer vision tasks, it mixes the images blindly and does not adapt to different datasets automatically. A mixing strategy that is effective for a particular dataset does not often generalize well to other datasets. If not properly configured, the methods may create misleading mixed images, which jeopardize the effectiveness of sample-mixing augmentations. In this work, we propose an automated approach, TransformMix, to learn better transformation and mixing augmentation strategies from <b>data.</b> <b>In</b> particular, TransformMix applies learned transformations and mixing masks to create compelling mixed images that contain correct and important information for the target tasks. We demonstrate the effectiveness of TransformMix on multiple datasets in <b>transfer</b> <b>learning,</b> classification, <b>object</b> <b>detection,</b> and <b>knowledge</b> <b>distillation</b> settings. Experimental results show that our method achieves better performance as well as efficiency when compared with strong sample-mixing baselines.</p></p class="citation"></blockquote><h3 id=12104--116329-mplug-docowl-15-unified-structure-learning-for-ocr-free-document-understanding-anwen-hu-et-al-2024>(12/104 | 116/329) mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding (Anwen Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, Jingren Zhou. (2024)<br><strong>mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding</strong><br><button class=copy-to-clipboard title="mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 59<br>Keywords: Optical Character Recognition, Benchmarking, Convolution, Multi-modal, Multi-modal, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12895v1.pdf filename=2403.12895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Existing <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs. Our Unified Structure Learning comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through <b>convolution,</b> enabling the <b>LLM</b> to understand high-resolution images more efficiently. Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning. Finally, we construct a small but high-quality <b>reasoning</b> tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain. Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding <b>benchmarks,</b> improving the SOTA performance of MLLMs with a 7B <b>LLM</b> by more than 10 points in 5/10 <b>benchmarks.</b> Our codes, models, and datasets are publicly available at <a href=https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5>https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5</a>.</p></p class="citation"></blockquote><h3 id=13104--117329-compositional-3d-scene-synthesis-with-scene-graph-guided-layout-shape-generation-yao-wei-et-al-2024>(13/104 | 117/329) Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation (Yao Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Wei, Martin Renqiang Min, George Vosselman, Li Erran Li, Michael Ying Yang. (2024)<br><strong>Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation</strong><br><button class=copy-to-clipboard title="Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Diffusion Model, Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12848v1.pdf filename=2403.12848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compositional 3D scene synthesis has diverse applications across a spectrum of industries such as robotics, films, and video games, as it closely mirrors the complexity of real-world multi-object environments. Early works typically employ shape retrieval based frameworks which naturally suffer from limited shape diversity. Recent progresses have been made in shape generation with powerful generative models, such as <b>diffusion</b> <b>models,</b> which increases the shape fidelity. However, these approaches separately treat 3D shape generation and layout generation. The synthesized scenes are usually hampered by layout collision, which implies that the scene-level fidelity is still under-explored. In this paper, we aim at generating realistic and reasonable 3D scenes from scene <b>graph.</b> To enrich the representation capability of the given scene <b>graph</b> inputs, <b>large</b> <b>language</b> <b>model</b> is utilized to explicitly aggregate the global <b>graph</b> features with local relationship features. With a unified <b>graph</b> <b>convolution</b> <b>network</b> <b>(GCN),</b> <b>graph</b> features are extracted from scene <b>graphs</b> updated via joint layout-shape distribution. During scene generation, an IoU-based regularization loss is introduced to constrain the predicted 3D layouts. <b>Benchmarked</b> on the SG-FRONT dataset, our method achieves better 3D scene synthesis, especially in terms of scene-level fidelity. The source code will be released after publication.</p></p class="citation"></blockquote><h3 id=14104--118329-better-call-sal-towards-learning-to-segment-anything-in-lidar-aljoša-ošep-et-al-2024>(14/104 | 118/329) Better Call SAL: Towards Learning to Segment Anything in Lidar (Aljoša Ošep et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aljoša Ošep, Tim Meinhardt, Francesco Ferroni, Neehar Peri, Deva Ramanan, Laura Leal-Taixé. (2024)<br><strong>Better Call SAL: Towards Learning to Segment Anything in Lidar</strong><br><button class=copy-to-clipboard title="Better Call SAL: Towards Learning to Segment Anything in Lidar" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 53<br>Keywords: Foundation Model, Knowledge Distillation, Multi-modal, Supervised Learning, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13129v1.pdf filename=2403.13129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose $\texttt{SAL}$ ($\texttt{S}$egment $\texttt{A}$nything in $\texttt{L}$idar) method consisting of a text-promptable <b>zero-shot</b> model for segmenting and classifying any object in Lidar, and a pseudo-labeling engine that facilitates model training without manual supervision. While the established paradigm for $\textit{Lidar Panoptic Segmentation}$ (LPS) relies on manual supervision for a handful of object classes defined a priori, we utilize 2D vision <b>foundation</b> <b>models</b> to generate 3D supervision &ldquo;for free&rdquo;. Our pseudo-labels consist of instance masks and corresponding CLIP tokens, which we lift to Lidar using calibrated <b>multi-modal</b> data. By training our model on these labels, we <b>distill</b> the 2D <b>foundation</b> <b>models</b> into our Lidar $\texttt{SAL}$ model. Even without manual labels, our model achieves $91%$ in terms of class-agnostic segmentation and $44%$ in terms of <b>zero-shot</b> LPS of the fully <b>supervised</b> state-of-the-art. Furthermore, we outperform several baselines that do not <b>distill</b> but only lift image features to 3D. More importantly, we demonstrate that $\texttt{SAL}$ supports arbitrary class <b>prompts,</b> can be easily extended to new datasets, and shows significant potential to improve with increasing amounts of self-labeled data.</p></p class="citation"></blockquote><h3 id=15104--119329-improved-eatformer-a-vision-transformer-for-medical-image-classification-yulong-shisu-et-al-2024>(15/104 | 119/329) Improved EATFormer: A Vision Transformer for Medical Image Classification (Yulong Shisu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulong Shisu, Susano Mingwin, Yongshuai Wanwag, Zengqiang Chenso, Sunshin Huing. (2024)<br><strong>Improved EATFormer: A Vision Transformer for Medical Image Classification</strong><br><button class=copy-to-clipboard title="Improved EATFormer: A Vision Transformer for Medical Image Classification" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13167v1.pdf filename=2403.13167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The accurate analysis of medical images is vital for diagnosing and predicting medical conditions. Traditional approaches relying on radiologists and clinicians suffer from inconsistencies and missed diagnoses. Computer-aided diagnosis systems can assist in achieving early, accurate, and efficient diagnoses. This paper presents an improved Evolutionary Algorithm-based <b>Transformer</b> architecture for medical image classification using <b>Vision</b> <b>Transformers.</b> The proposed EATFormer architecture combines the strengths of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> and <b>Vision</b> <b>Transformers,</b> leveraging their ability to identify patterns in data and adapt to specific characteristics. The architecture incorporates novel components, including the Enhanced EA-based <b>Transformer</b> block with Feed-Forward Network, Global and Local Interaction , and Multi-Scale Region Aggregation modules. It also introduces the Modulated Deformable MSA module for dynamic modeling of irregular locations. The paper discusses the <b>Vision</b> <b>Transformer</b> (ViT) model&rsquo;s key features, such as patch-based processing, positional context incorporation, and Multi-Head Attention mechanism. It introduces the Multi-Scale Region Aggregation module, which aggregates information from different receptive fields to provide an inductive bias. The Global and Local Interaction module enhances the MSA-based global module by introducing a local path for extracting discriminative local information. Experimental results on the Chest X-ray and Kvasir datasets demonstrate that the proposed EATFormer significantly improves prediction speed and accuracy compared to baseline models.</p></p class="citation"></blockquote><h3 id=16104--120329-just-shift-it-test-time-prototype-shifting-for-zero-shot-generalization-with-vision-language-models-elaine-sui-et-al-2024>(16/104 | 120/329) Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models (Elaine Sui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elaine Sui, Xiaohan Wang, Serena Yeung-Levy. (2024)<br><strong>Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models</strong><br><button class=copy-to-clipboard title="Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Distribution Shift, Distribution Shift, Zero-shot, Prompt, Vision-and-Language, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12952v1.pdf filename=2403.12952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advancements in <b>vision-language</b> models (VLMs) have propelled the field of computer vision, particularly in the <b>zero-shot</b> <b>learning</b> setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in <b>prompt</b> engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 datasets involving natural <b>distribution</b> <b>shifts</b> and cross-dataset generalization demonstrate TPS&rsquo;s superior performance, achieving state-of-the-art results while reducing resource requirements.</p></p class="citation"></blockquote><h3 id=17104--121329-you-only-sample-once-taming-one-step-text-to-image-synthesis-by-self-cooperative-diffusion-gans-yihong-luo-et-al-2024>(17/104 | 121/329) You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs (Yihong Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihong Luo, Xiaolong Chen, Jing Tang. (2024)<br><strong>You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs</strong><br><button class=copy-to-clipboard title="You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Generative Adversarial Network, Transformer, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12931v1.pdf filename=2403.12931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis. This is achieved by integrating the diffusion process with <b>GANs.</b> Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we show that our method can be extended to <b>finetune</b> pre-trained <b>text-to-image</b> diffusion for high-quality one-step <b>text-to-image</b> synthesis even with LoRA <b>fine-tuning.</b> In particular, we provide the first diffusion <b>transformer</b> that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training. Our code is provided at <a href=https://github.com/Luo-Yihong/YOSO>https://github.com/Luo-Yihong/YOSO</a>.</p></p class="citation"></blockquote><h3 id=18104--122329-hydra-a-hyper-agent-for-dynamic-compositional-visual-reasoning-fucai-ke-et-al-2024>(18/104 | 122/329) HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning (Fucai Ke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi. (2024)<br><strong>HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning</strong><br><button class=copy-to-clipboard title="HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Reasoning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12884v1.pdf filename=2403.12884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in visual <b>reasoning</b> (VR), particularly with the aid of <b>Large</b> <b>Vision-Language</b> <b>Models</b> (VLMs), show promise but require access to <b>large-scale</b> <b>datasets</b> <b>and</b> face challenges such as high computational costs and limited generalization capabilities. Compositional visual <b>reasoning</b> approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to perform planning, <b>reasoning,</b> or both, without considering the effect of their decisions on the visual <b>reasoning</b> process, which can lead to errors or failed procedures. To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual <b>reasoning</b> framework designed for reliable and incrementally progressive general <b>reasoning.</b> HYDRA integrates three essential modules: a planner, a <b>Reinforcement</b> <b>Learning</b> (RL) agent serving as a cognitive controller, and a reasoner. The planner and reasoner modules utilize an <b>LLM</b> to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop. This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the <b>reasoning</b> process, leading to more reliable <b>reasoning</b> outputs and ultimately enhancing its overall effectiveness. Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets.</p></p class="citation"></blockquote><h3 id=19104--123329-learning-cross-view-visual-geo-localization-without-ground-truth-haoyuan-li-et-al-2024>(19/104 | 123/329) Learning Cross-view Visual Geo-localization without Ground Truth (Haoyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyuan Li, Chang Xu, Wen Yang, Huai Yu, Gui-Song Xia. (2024)<br><strong>Learning Cross-view Visual Geo-localization without Ground Truth</strong><br><button class=copy-to-clipboard title="Learning Cross-view Visual Geo-localization without Ground Truth" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Foundation Model, Reconstruction Loss, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12702v1.pdf filename=2403.12702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-View Geo-Localization (CVGL) involves determining the geographical location of a query image by matching it with a corresponding GPS-tagged reference image. Current state-of-the-art methods predominantly rely on training models with labeled paired images, incurring substantial annotation costs and training burdens. In this study, we investigate the adaptation of frozen models for CVGL without requiring ground truth pair labels. We observe that training on unlabeled cross-view images presents significant challenges, including the need to establish relationships within unlabeled data and reconcile view discrepancies between uncertain queries and references. To address these challenges, we propose a <b>self-supervised</b> <b>learning</b> framework to train a learnable adapter for a frozen <b>Foundation</b> <b>Model</b> (FM). This adapter is designed to map feature distributions from diverse views into a uniform space using unlabeled data exclusively. To establish relationships within unlabeled data, we introduce an Expectation-Maximization-based Pseudo-labeling module, which iteratively estimates associations between cross-view features and optimizes the adapter. To maintain the robustness of the FM&rsquo;s representation, we incorporate an information consistency module with a <b>reconstruction</b> <b>loss,</b> ensuring that adapted features retain strong discriminative ability across views. Experimental results demonstrate that our proposed method achieves significant improvements over vanilla FMs and competitive accuracy compared to <b>supervised</b> methods, while necessitating fewer training parameters and relying solely on unlabeled data. Evaluation of our adaptation for task-specific models further highlights its broad applicability.</p></p class="citation"></blockquote><h3 id=20104--124329-boosting-transferability-in-vision-language-attacks-via-diversification-along-the-intersection-region-of-adversarial-trajectory-sensen-gao-et-al-2024>(20/104 | 124/329) Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory (Sensen Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sensen Gao, Xiaojun Jia, Xuhong Ren, Ivor Tsang, Qing Guo. (2024)<br><strong>Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory</strong><br><button class=copy-to-clipboard title="Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Image2text, Vision-and-Language, Vision-and-Language, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12445v1.pdf filename=2403.12445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> pre-training (VLP) models exhibit remarkable capabilities in comprehending both images and text, yet they remain susceptible to <b>multimodal</b> <b>adversarial</b> <b>examples</b> (AEs). Strengthening <b>adversarial</b> <b>attacks</b> and uncovering vulnerabilities, especially common issues in VLP models (e.g., high transferable AEs), can stimulate further research on constructing reliable and practical VLP models. A recent work (i.e., Set-level guidance attack) indicates that augmenting <b>image-text</b> pairs to increase AE diversity along the optimization path enhances the transferability of <b>adversarial</b> <b>examples</b> significantly. However, this approach predominantly emphasizes diversity around the online <b>adversarial</b> <b>examples</b> (i.e., AEs in the optimization period), leading to the risk of overfitting the victim model and affecting the transferability. In this study, we posit that the diversity of <b>adversarial</b> <b>examples</b> towards the clean input and online AEs are both pivotal for enhancing transferability across VLP models. Consequently, we propose using diversification along the intersection region of <b>adversarial</b> <b>trajectory</b> to expand the diversity of AEs. To fully leverage the interaction between modalities, we introduce text-guided <b>adversarial</b> <b>example</b> selection during optimization. Furthermore, to further mitigate the potential overfitting, we direct the <b>adversarial</b> <b>text</b> deviating from the last intersection region along the optimization path, rather than <b>adversarial</b> <b>images</b> as in existing methods. Extensive experiments affirm the effectiveness of our method in improving transferability across various VLP models and downstream <b>vision-and-language</b> tasks (e.g., <b>Image-Text</b> Retrieval(ITR), Visual Grounding(VG), Image Captioning(IC)).</p></p class="citation"></blockquote><h3 id=21104--125329-diffusion-driven-self-supervised-learning-for-shape-reconstruction-and-pose-estimation-jingtao-sun-et-al-2024>(21/104 | 125/329) Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation (Jingtao Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingtao Sun, Yaonan Wang, Mingtao Feng, Chao Ding, Mike Zheng Shou, Ajmal Saeed Mian. (2024)<br><strong>Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation</strong><br><button class=copy-to-clipboard title="Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Graph, Convolution, Self-supervised Learning, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12728v1.pdf filename=2403.12728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive mannual labeling costs. Recently, various <b>self-supervised</b> <b>category-level</b> pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model for <b>self-supervised</b> <b>training,</b> and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven <b>self-supervised</b> <b>network</b> for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point <b>Transformer</b> in our network. This module adopts a point <b>convolutional</b> layer with radial-kernels for pose-aware learning and a 3D scale-invariant <b>graph</b> <b>convolution</b> layer for object-level shape representation, respectively. Furthermore, we introduce a pretrain-to-refine <b>self-supervised</b> <b>training</b> paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art <b>self-supervised</b> <b>category-level</b> baselines and even surpasses some fully-supervised instance-level and category-level methods.</p></p class="citation"></blockquote><h3 id=22104--126329-videobadminton-a-video-dataset-for-badminton-action-recognition-qi-li-et-al-2024>(22/104 | 126/329) VideoBadminton: A Video Dataset for Badminton Action Recognition (Qi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Li, Tzu-Chen Chiu, Hsiang-Wei Huang, Min-Te Sun, Wei-Shinn Ku. (2024)<br><strong>VideoBadminton: A Video Dataset for Badminton Action Recognition</strong><br><button class=copy-to-clipboard title="VideoBadminton: A Video Dataset for Badminton Action Recognition" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12385v1.pdf filename=2403.12385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the dynamic and evolving field of computer vision, action recognition has become a key focus, especially with the advent of sophisticated methodologies like <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> <b>Convolutional</b> <b>3D,</b> <b>Transformer,</b> and spatial-temporal feature fusion. These technologies have shown promising results on well-established <b>benchmarks</b> but face unique challenges in real-world applications, particularly in sports analysis, where the precise decomposition of activities and the distinction of subtly different actions are crucial. Existing datasets like UCF101, HMDB51, and Kinetics have offered a diverse range of video data for various scenarios. However, there&rsquo;s an increasing need for fine-grained video datasets that capture detailed categorizations and nuances within broader action categories. In this paper, we introduce the VideoBadminton dataset derived from high-quality badminton footage. Through an exhaustive evaluation of leading methodologies on this dataset, this study aims to advance the field of action recognition, particularly in badminton sports. The introduction of VideoBadminton could not only serve for badminton action recognition but also provide a dataset for recognizing fine-grained actions. The insights gained from these evaluations are expected to catalyze further research in action comprehension, especially within sports contexts.</p></p class="citation"></blockquote><h3 id=23104--127329-deblurdinat-a-lightweight-and-effective-transformer-for-image-deblurring-hanzhou-liu-et-al-2024>(23/104 | 127/329) DeblurDiNAT: A Lightweight and Effective Transformer for Image Deblurring (Hanzhou Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzhou Liu, Binghan Li, Chengkai Liu, Mi Lu. (2024)<br><strong>DeblurDiNAT: A Lightweight and Effective Transformer for Image Deblurring</strong><br><button class=copy-to-clipboard title="DeblurDiNAT: A Lightweight and Effective Transformer for Image Deblurring" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Graph Attention Networks, Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13163v1.pdf filename=2403.13163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blurry images may contain local and global non-uniform artifacts, which complicate the deblurring process and make it more challenging to achieve satisfactory results. Recently, <b>Transformers</b> generate improved deblurring outcomes than existing <b>CNN</b> architectures. However, the large model size and long inference time are still two bothersome issues which have not been fully explored. To this end, we propose DeblurDiNAT, a compact encoder-decoder <b>Transformer</b> which efficiently restores clean images from real-world blurry ones. We adopt an alternating dilation factor structure with the aim of global-local feature learning. Also, we observe that simply using <b>self-attention</b> layers in networks does not always produce good deblurred results. To solve this problem, we propose a channel modulation <b>self-attention</b> (CMSA) block, where a cross-channel learner (CCL) is utilized to capture channel relationships. In addition, we present a divide and multiply feed-forward network (DMFN) allowing fast feature propagation. Moreover, we design a lightweight <b>gated</b> feature fusion (LGFF) module, which performs controlled feature merging. Comprehensive experimental results show that the proposed model, named DeblurDiNAT, provides a favorable performance boost without introducing noticeable computational costs over the baseline, and achieves state-of-the-art (SOTA) performance on several image deblurring datasets. Compared to nearest competitors, our space-efficient and time-saving method demonstrates a stronger generalization ability with 3%-68% fewer parameters and produces deblurred images that are visually closer to the ground truth.</p></p class="citation"></blockquote><h3 id=24104--128329-negative-yields-positive-unified-dual-path-adapter-for-vision-language-models-ce-zhang-et-al-2024>(24/104 | 128/329) Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models (Ce Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie. (2024)<br><strong>Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models</strong><br><button class=copy-to-clipboard title="Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12964v1.pdf filename=2403.12964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, large-scale pre-trained <b>Vision-Language</b> Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient <b>fine-tuning.</b> In this work, we innovatively introduce the concept of dual learning into <b>fine-tuning</b> VLMs, i.e., we not only learn what an image is, but also what an image isn&rsquo;t. Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples. In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks. Our extensive experimental results across 15 datasets validate that the proposed DualAdapter outperforms existing state-of-the-art methods on both <b>few-shot</b> <b>learning</b> and domain generalization tasks while achieving competitive computational efficiency. Code is available at <a href=https://github.com/zhangce01/DualAdapter>https://github.com/zhangce01/DualAdapter</a>.</p></p class="citation"></blockquote><h3 id=25104--129329-tuning-free-image-customization-with-image-and-text-guidance-pengzhi-li-et-al-2024>(25/104 | 129/329) Tuning-Free Image Customization with Image and Text Guidance (Pengzhi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengzhi Li, Qiang Nie, Ying Chen, Xi Jiang, Kai Wu, Yuhuan Lin, Yong Liu, Jinlong Peng, Chengjie Wang, Feng Zheng. (2024)<br><strong>Tuning-Free Image Customization with Image and Text Guidance</strong><br><button class=copy-to-clipboard title="Tuning-Free Image Customization with Image and Text Guidance" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Text2image, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12658v1.pdf filename=2403.12658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite significant advancements in image customization with <b>diffusion</b> <b>models,</b> current methods still have several limitations: 1) unintended changes in non-target areas when regenerating the entire image; 2) guidance solely by a reference image or text descriptions; and 3) time-consuming <b>fine-tuning,</b> which limits their practical application. In response, we introduce a tuning-free framework for simultaneous <b>text-image-guided</b> image customization, enabling precise editing of specific image regions within seconds. Our approach preserves the semantic features of the reference image subject while allowing modification of detailed attributes based on text descriptions. To achieve this, we propose an innovative attention blending strategy that blends <b>self-attention</b> features in the UNet decoder during the denoising process. To our knowledge, this is the first tuning-free method that concurrently utilizes text and image guidance for image customization in specific regions. Our approach outperforms previous methods in both human and quantitative evaluations, providing an efficient solution for various practical applications, such as image synthesis, design, and creative photography.</p></p class="citation"></blockquote><h3 id=26104--130329-chain-of-spot-interactive-reasoning-improves-large-vision-language-models-zuyan-liu-et-al-2024>(26/104 | 130/329) Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models (Zuyan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, Jiwen Lu. (2024)<br><strong>Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Instruction Following, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12966v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12966v2.pdf filename=2403.12966v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of <b>vision-language</b> understanding, the proficiency of models in interpreting and <b>reasoning</b> over visual content has become a cornerstone for numerous applications. However, it is challenging for the visual encoder in Large <b>Vision-Language</b> Models (LVLMs) to extract useful features tailored to questions that aid the language model&rsquo;s response. Furthermore, a common practice among existing LVLMs is to utilize lower-resolution images, which restricts the ability for visual recognition. Our work introduces the Chain-of-Spot (CoS) method, which we describe as Interactive <b>Reasoning,</b> a novel approach that enhances feature extraction by focusing on key regions of interest (ROI) within the image, corresponding to the posed questions or <b>instructions.</b> <b>This</b> technique allows LVLMs to access more detailed visual information without altering the original image resolution, thereby offering multi-granularity image features. By integrating Chain-of-Spot with instruct-following LLaVA-1.5 models, the process of image <b>reasoning</b> consistently improves performance across a wide range of <b>multimodal</b> datasets and <b>benchmarks</b> without bells and whistles and achieves new state-of-the-art results. Our empirical findings demonstrate a significant improvement in LVLMs&rsquo; ability to understand and reason about visual content, paving the way for more sophisticated visual <b>instruction-following</b> <b>applications.</b> Code and models are available at <a href=https://github.com/dongyh20/Chain-of-Spot>https://github.com/dongyh20/Chain-of-Spot</a></p></p class="citation"></blockquote><h3 id=27104--131329-texdreamer-towards-zero-shot-high-fidelity-3d-human-texture-generation-yufei-liu-et-al-2024>(27/104 | 131/329) TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation (Yufei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Liu, Junwei Zhu, Junshu Tang, Shijie Zhang, Jiangning Zhang, Weijian Cao, Chengjie Wang, Yunsheng Wu, Dongjin Huang. (2024)<br><strong>TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation</strong><br><button class=copy-to-clipboard title="TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Zero-shot, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12906v1.pdf filename=2403.12906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Texturing 3D humans with semantic UV maps remains a challenge due to the difficulty of acquiring reasonably unfolded UV. Despite recent text-to-3D advancements in supervising multi-view renderings using large <b>text-to-image</b> (T2I) models, issues persist with generation speed, text consistency, and texture quality, resulting in data scarcity among existing datasets. We present TexDreamer, the first <b>zero-shot</b> <b>multimodal</b> high-fidelity 3D human texture generation model. Utilizing an efficient texture adaptation <b>finetuning</b> strategy, we adapt large T2I model to a semantic UV structure while preserving its original generalization capability. Leveraging a novel feature translator module, the trained model is capable of generating high-fidelity 3D human textures from either text or image within seconds. Furthermore, we introduce ArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024) 3D human texture dataset which contains 50k high-fidelity textures with text descriptions.</p></p class="citation"></blockquote><h3 id=28104--132329-dynamic-spatial-temporal-aggregation-for-skeleton-aware-sign-language-recognition-lianyu-hu-et-al-2024>(28/104 | 132/329) Dynamic Spatial-Temporal Aggregation for Skeleton-Aware Sign Language Recognition (Lianyu Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lianyu Hu, Liqing Gao, Zekang Liu, Wei Feng. (2024)<br><strong>Dynamic Spatial-Temporal Aggregation for Skeleton-Aware Sign Language Recognition</strong><br><button class=copy-to-clipboard title="Dynamic Spatial-Temporal Aggregation for Skeleton-Aware Sign Language Recognition" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12519v1.pdf filename=2403.12519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skeleton-aware sign language recognition (SLR) has gained popularity due to its ability to remain unaffected by background information and its lower computational requirements. Current methods utilize spatial <b>graph</b> <b>modules</b> <b>and</b> temporal modules to capture spatial and temporal features, respectively. However, their spatial <b>graph</b> <b>modules</b> <b>are</b> typically built on fixed <b>graph</b> <b>structures</b> <b>such</b> as <b>graph</b> <b>convolutional</b> <b>networks</b> or a single learnable <b>graph,</b> <b>which</b> <b>only</b> partially explore joint relationships. Additionally, a simple temporal <b>convolution</b> kernel is used to capture temporal information, which may not fully capture the complex movement patterns of different signers. To overcome these limitations, we propose a new spatial architecture consisting of two concurrent branches, which build input-sensitive joint relationships and incorporates specific domain knowledge for recognition, respectively. These two branches are followed by an aggregation process to distinguishe important joint connections. We then propose a new temporal module to model multi-scale temporal information to capture complex human dynamics. Our method achieves state-of-the-art accuracy compared to previous skeleton-aware methods on four large-scale SLR <b>benchmarks.</b> Moreover, our method demonstrates superior accuracy compared to RGB-based methods in most cases while requiring much fewer computational resources, bringing better accuracy-computation trade-off. Code is available at <a href=https://github.com/hulianyuyy/DSTA-SLR>https://github.com/hulianyuyy/DSTA-SLR</a>.</p></p class="citation"></blockquote><h3 id=29104--133329-multimodal-fusion-method-with-spatiotemporal-sequences-and-relationship-learning-for-valence-arousal-estimation-jun-yu-et-al-2024>(29/104 | 133/329) Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation (Jun Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Yu, Gongpeng Zhao, Yongqi Wang, Zhihong Wei, Yang Zheng, Zerui Zhang, Zhongpeng Cai, Guochen Xie, Jichao Zhu, Wangyuan Zhu. (2024)<br><strong>Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation</strong><br><button class=copy-to-clipboard title="Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-SD, cs.CV, eess-AS<br>Keyword Score: 36<br>Keywords: Convolution, Convolutional Neural Network, Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12425v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12425v2.pdf filename=2403.12425v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents our approach for the VA (Valence-Arousal) estimation task in the ABAW6 competition. We devised a comprehensive model by preprocessing video frames and audio segments to extract visual and audio features. Through the utilization of Temporal <b>Convolutional</b> <b>Network</b> (TCN) modules, we effectively captured the temporal and spatial correlations between these features. Subsequently, we employed a <b>Transformer</b> encoder structure to learn long-range dependencies, thereby enhancing the model&rsquo;s performance and generalization ability. Our method leverages a <b>multimodal</b> data fusion approach, integrating pre-trained audio and video backbones for feature extraction, followed by TCN-based spatiotemporal encoding and <b>Transformer-based</b> temporal information capture. Experimental results demonstrate the effectiveness of our approach, achieving competitive performance in VA estimation on the AffWild2 dataset.</p></p class="citation"></blockquote><h3 id=30104--134329-adapting-visual-language-models-for-generalizable-anomaly-detection-in-medical-images-chaoqin-huang-et-al-2024>(30/104 | 134/329) Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images (Chaoqin Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoqin Huang, Aofan Jiang, Jinghao Feng, Ya Zhang, Xinchao Wang, Yanfeng Wang. (2024)<br><strong>Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images</strong><br><button class=copy-to-clipboard title="Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Anomaly Detection, Benchmarking, Few-shot, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12570v1.pdf filename=2403.12570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in large-scale visual-language pre-trained models have led to significant progress in zero-/few-shot <b>anomaly</b> <b>detection</b> within natural image domains. However, the substantial domain divergence between natural and medical images limits the effectiveness of these methodologies in medical <b>anomaly</b> <b>detection.</b> This paper introduces a novel lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical <b>anomaly</b> <b>detection.</b> Our approach integrates multiple residual adapters into the pre-trained visual encoder, enabling a stepwise enhancement of visual features across different levels. This multi-level adaptation is guided by multi-level, pixel-wise visual-language feature alignment loss functions, which recalibrate the model&rsquo;s focus from object semantics in natural imagery to <b>anomaly</b> <b>identification</b> in medical images. The adapted features exhibit improved generalization across various medical data types, even in <b>zero-shot</b> scenarios where the model encounters unseen medical modalities and anatomical regions during training. Our experiments on medical <b>anomaly</b> <b>detection</b> <b>benchmarks</b> demonstrate that our method significantly surpasses current state-of-the-art models, with an average AUC improvement of 6.24% and 7.33% for <b>anomaly</b> <b>classification,</b> 2.03% and 2.37% for <b>anomaly</b> <b>segmentation,</b> under the <b>zero-shot</b> and <b>few-shot</b> settings, respectively. Source code is available at: <a href=https://github.com/MediaBrain-SJTU/MVFA-AD>https://github.com/MediaBrain-SJTU/MVFA-AD</a></p></p class="citation"></blockquote><h3 id=31104--135329-confidence-self-calibration-for-multi-label-class-incremental-learning-kaile-du-et-al-2024>(31/104 | 135/329) Confidence Self-Calibration for Multi-Label Class-Incremental Learning (Kaile Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaile Du, Yifan Zhou, Fan Lyu, Yuyang Li, Chen Lu, Guangcan Liu. (2024)<br><strong>Confidence Self-Calibration for Multi-Label Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="Confidence Self-Calibration for Multi-Label Class-Incremental Learning" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12559v1.pdf filename=2403.12559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL) arises when only the new classes are labeled during training, while past and future labels remain unavailable. This issue leads to a proliferation of false-positive errors due to erroneously high confidence multi-label predictions, exacerbating catastrophic forgetting within the disjoint label space. In this paper, we aim to refine multi-label confidence calibration in MLCIL and propose a Confidence Self-Calibration (CSC) approach. Firstly, for label relationship calibration, we introduce a class-incremental <b>graph</b> <b>convolutional</b> <b>network</b> that bridges the isolated label spaces by constructing learnable, dynamically extended label relationship <b>graph.</b> <b>Then,</b> <b>for</b> confidence calibration, we present a max-entropy regularization for each multi-label increment, facilitating confidence self-calibration through the penalization of over-confident output distributions. Our approach attains new state-of-the-art results in MLCIL tasks on both MS-COCO and PASCAL VOC datasets, with the calibration of label confidences confirmed through our methodology.</p></p class="citation"></blockquote><h3 id=32104--136329-semantics-distortion-and-style-matter-towards-source-free-uda-for-panoramic-segmentation-xu-zheng-et-al-2024>(32/104 | 136/329) Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation (Xu Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Zheng, Pengyuan Zhou, Athanasios V. Vasilakos, Lin Wang. (2024)<br><strong>Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation</strong><br><button class=copy-to-clipboard title="Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Knowledge Transfer, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12505v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12505v2.pdf filename=2403.12505v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses an interesting yet challenging problem &ndash; source-free <b>unsupervised</b> <b>domain</b> <b>adaptation</b> (SFUDA) for pinhole-to-panoramic semantic segmentation &ndash; given only a pinhole image-trained model (i.e., source) and unlabeled panoramic images (i.e., target). Tackling this problem is nontrivial due to the semantic mismatches, style discrepancies, and inevitable distortion of panoramic images. To this end, we propose a novel method that utilizes Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) with a fixed FoV to mimic the pinhole images. Both projections are shown effective in extracting <b>knowledge</b> <b>from</b> the source model. However, the distinct projection discrepancies between source and target <b>domains</b> <b>impede</b> the direct <b>knowledge</b> <b>transfer;</b> thus, we propose a panoramic prototype adaptation module (PPAM) to integrate panoramic prototypes from the extracted <b>knowledge</b> <b>for</b> adaptation. We then impose the loss constraints on both predictions and prototypes and propose a cross-dual attention module (CDAM) at the feature level to better align the spatial and channel characteristics across the <b>domains</b> <b>and</b> projections. Both <b>knowledge</b> <b>extraction</b> and transfer processes are synchronously updated to reach the best performance. Extensive experiments on the synthetic and real-world <b>benchmarks,</b> including outdoor and indoor scenarios, demonstrate that our method achieves significantly better performance than prior SFUDA methods for pinhole-to-panoramic adaptation.</p></p class="citation"></blockquote><h3 id=33104--137329-task-customized-mixture-of-adapters-for-general-image-fusion-pengfei-zhu-et-al-2024>(33/104 | 137/329) Task-Customized Mixture of Adapters for General Image Fusion (Pengfei Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengfei Zhu, Yang Sun, Bing Cao, Qinghua Hu. (2024)<br><strong>Task-Customized Mixture of Adapters for General Image Fusion</strong><br><button class=copy-to-clipboard title="Task-Customized Mixture of Adapters for General Image Fusion" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Foundation Model, Multi-modal, Mutual Information, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12494v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12494v2.pdf filename=2403.12494v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>General image fusion aims at integrating important information from multi-source images. However, due to the significant cross-task gap, the respective fusion mechanism varies considerably in practice, resulting in limited performance across subtasks. To handle this problem, we propose a novel task-customized mixture of adapters (TC-MoA) for general image fusion, adaptively <b>prompting</b> various fusion tasks in a unified model. We borrow the insight from the mixture of experts (MoE), taking the experts as efficient tuning adapters to <b>prompt</b> a pre-trained <b>foundation</b> <b>model.</b> These adapters are shared across different tasks and constrained by <b>mutual</b> <b>information</b> regularization, ensuring compatibility with different tasks while complementarity for multi-source images. The task-specific routing networks customize these adapters to extract task-specific information from different sources with dynamic dominant intensity, performing adaptive visual feature <b>prompt</b> fusion. Notably, our TC-MoA controls the dominant intensity bias for different fusion tasks, successfully unifying multiple fusion tasks in a single model. Extensive experiments show that TC-MoA outperforms the competing approaches in learning commonalities while retaining compatibility for general image fusion <b>(multi-modal,</b> multi-exposure, and multi-focus), and also demonstrating striking controllability on more generalization experiments. The code is available at <a href=https://github.com/YangSun22/TC-MoA>https://github.com/YangSun22/TC-MoA</a> .</p></p class="citation"></blockquote><h3 id=34104--138329-scenescript-reconstructing-scenes-with-an-autoregressive-structured-language-model-armen-avetisyan-et-al-2024>(34/104 | 138/329) SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model (Armen Avetisyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, Jakob Engel, Edward Miller, Richard Newcombe, Vasileios Balntas. (2024)<br><strong>SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model</strong><br><button class=copy-to-clipboard title="SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13064v1.pdf filename=2403.13064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce SceneScript, a method that directly produces full scene models as a sequence of structured language commands using an autoregressive, token-based approach. Our proposed scene representation is inspired by recent successes in <b>transformers</b> & <b>LLMs,</b> and departs from more traditional methods which commonly describe scenes as meshes, voxel grids, point clouds or radiance fields. Our method infers the set of structured language commands directly from encoded visual data using a scene language encoder-decoder architecture. To train SceneScript, we generate and release a large-scale synthetic dataset called Aria Synthetic Environments consisting of 100k high-quality in-door scenes, with photorealistic and ground-truth annotated renders of egocentric scene walkthroughs. Our method gives state-of-the art results in architectural layout estimation, and competitive results in 3D <b>object</b> <b>detection.</b> Lastly, we explore an advantage for SceneScript, which is the ability to readily adapt to new commands via simple additions to the structured language, which we illustrate for tasks such as coarse 3D <b>object</b> <b>part</b> reconstruction.</p></p class="citation"></blockquote><h3 id=35104--139329-fouriscale-a-frequency-perspective-on-training-free-high-resolution-image-synthesis-linjiang-huang-et-al-2024>(35/104 | 139/329) FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis (Linjiang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, Hongsheng Li. (2024)<br><strong>FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis</strong><br><button class=copy-to-clipboard title="FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Convolution, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12963v1.pdf filename=2403.12963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we delve into the generation of high-resolution images from pre-trained <b>diffusion</b> <b>models,</b> addressing persistent challenges, such as repetitive patterns and structural distortions, that emerge when models are applied beyond their trained resolutions. To address this issue, we introduce an innovative, training-free approach FouriScale from the perspective of frequency domain analysis. We replace the original <b>convolutional</b> layers in pre-trained <b>diffusion</b> <b>models</b> by incorporating a dilation technique along with a low-pass operation, intending to achieve structural consistency and scale consistency across resolutions, respectively. Further enhanced by a padding-then-crop strategy, our method can flexibly handle <b>text-to-image</b> generation of various aspect ratios. By using the FouriScale as guidance, our method successfully balances the structural integrity and fidelity of generated images, achieving an astonishing capacity of arbitrary-size, high-resolution, and high-quality generation. With its simplicity and compatibility, our method can provide valuable insights for future explorations into the synthesis of ultra-high-resolution images. The code will be released at <a href=https://github.com/LeonHLJ/FouriScale>https://github.com/LeonHLJ/FouriScale</a>.</p></p class="citation"></blockquote><h3 id=36104--140329-fresco-spatial-temporal-correspondence-for-zero-shot-video-translation-shuai-yang-et-al-2024>(36/104 | 140/329) FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation (Shuai Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy. (2024)<br><strong>FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</strong><br><button class=copy-to-clipboard title="FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Zero-shot, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12962v1.pdf filename=2403.12962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The remarkable efficacy of <b>text-to-image</b> <b>diffusion</b> <b>models</b> has motivated extensive exploration of their potential application in video domains. <b>Zero-shot</b> methods seek to extend image <b>diffusion</b> <b>models</b> to videos without necessitating model training. Recent methods mainly focus on incorporating inter-frame correspondence into attention mechanisms. However, the soft constraint imposed on determining where to attend to valid features can sometimes be insufficient, resulting in temporal inconsistency. In this paper, we introduce FRESCO, intra-frame correspondence alongside inter-frame correspondence to establish a more robust spatial-temporal constraint. This enhancement ensures a more consistent transformation of semantically similar content across frames. Beyond mere attention guidance, our approach involves an explicit update of features to achieve high spatial-temporal consistency with the input video, significantly improving the visual coherence of the resulting translated videos. Extensive experiments demonstrate the effectiveness of our proposed framework in producing high-quality, coherent videos, marking a notable improvement over existing <b>zero-shot</b> methods.</p></p class="citation"></blockquote><h3 id=37104--141329-taptr-tracking-any-point-with-transformers-as-detection-hongyang-li-et-al-2024>(37/104 | 141/329) TAPTR: Tracking Any Point with Transformers as Detection (Hongyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Lei Zhang. (2024)<br><strong>TAPTR: Tracking Any Point with Transformers as Detection</strong><br><button class=copy-to-clipboard title="TAPTR: Tracking Any Point with Transformers as Detection" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13042v1.pdf filename=2403.13042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a simple and strong framework for Tracking Any Point with <b>TRansformers</b> (TAPTR). Based on the observation that point tracking bears a great resemblance to <b>object</b> <b>detection</b> and tracking, we borrow designs from DETR-like algorithms to address the task of TAP. In the proposed framework, in each video frame, each tracking point is represented as a point query, which consists of a positional part and a content part. As in DETR, each query (its position and content feature) is naturally updated layer by layer. Its visibility is predicted by its updated content feature. Queries belonging to the same tracking point can exchange information through <b>self-attention</b> along the temporal dimension. As all such operations are well-designed in DETR-like algorithms, the model is conceptually very simple. We also adopt some useful designs such as cost volume from optical flow models and develop simple designs to provide long temporal information while mitigating the feature drifting issue. Our framework demonstrates strong performance with state-of-the-art performance on various TAP datasets with faster inference speed.</p></p class="citation"></blockquote><h3 id=38104--142329-emotic-masked-autoencoder-with-attention-fusion-for-facial-expression-recognition-bach-nguyen-xuan-et-al-2024>(38/104 | 142/329) Emotic Masked Autoencoder with Attention Fusion for Facial Expression Recognition (Bach Nguyen-Xuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bach Nguyen-Xuan, Thien Nguyen-Hoang, Nhu Tai-Do. (2024)<br><strong>Emotic Masked Autoencoder with Attention Fusion for Facial Expression Recognition</strong><br><button class=copy-to-clipboard title="Emotic Masked Autoencoder with Attention Fusion for Facial Expression Recognition" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Autoencoder, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13039v1.pdf filename=2403.13039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial Expression Recognition (FER) is a critical task within computer vision with diverse applications across various domains. Addressing the challenge of limited FER datasets, which hampers the generalization capability of expression recognition models, is imperative for enhancing performance. Our paper presents an innovative approach integrating the MAE-Face <b>self-supervised</b> <b>learning</b> (SSL) method and Fusion Attention mechanism for expression classification, particularly showcased in the 6th Affective Behavior Analysis in-the-wild (ABAW) competition. Additionally, we propose preprocessing techniques to emphasize essential facial features, thereby enhancing model performance on both training and validation sets, notably demonstrated on the Aff-wild2 dataset.</p></p class="citation"></blockquote><h3 id=39104--143329-animatediff-lightning-cross-model-diffusion-distillation-shanchuan-lin-et-al-2024>(39/104 | 143/329) AnimateDiff-Lightning: Cross-Model Diffusion Distillation (Shanchuan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanchuan Lin, Xiao Yang. (2024)<br><strong>AnimateDiff-Lightning: Cross-Model Diffusion Distillation</strong><br><button class=copy-to-clipboard title="AnimateDiff-Lightning: Cross-Model Diffusion Distillation" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12706v1.pdf filename=2403.12706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present AnimateDiff-Lightning for lightning-fast video generation. Our model uses progressive adversarial <b>diffusion</b> <b>distillation</b> to achieve new state-of-the-art in few-step video generation. We discuss our modifications to adapt it for the video modality. Furthermore, we propose to simultaneously <b>distill</b> the probability flow of multiple base <b>diffusion</b> <b>models,</b> resulting in a single <b>distilled</b> motion module with broader style compatibility. We are pleased to release our <b>distilled</b> AnimateDiff-Lightning model for the community&rsquo;s use.</p></p class="citation"></blockquote><h3 id=40104--144329-laspa-latent-spatial-alignment-for-fast-training-free-single-image-editing-yazeed-alharbi-et-al-2024>(40/104 | 144/329) LASPA: Latent Spatial Alignment for Fast Training-free Single Image Editing (Yazeed Alharbi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yazeed Alharbi, Peter Wonka. (2024)<br><strong>LASPA: Latent Spatial Alignment for Fast Training-free Single Image Editing</strong><br><button class=copy-to-clipboard title="LASPA: Latent Spatial Alignment for Fast Training-free Single Image Editing" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12585v1.pdf filename=2403.12585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel, training-free approach for textual editing of real images using <b>diffusion</b> <b>models.</b> Unlike prior methods that rely on computationally expensive <b>finetuning,</b> our approach leverages LAtent SPatial Alignment (LASPA) to efficiently preserve image details. We demonstrate how the <b>diffusion</b> <b>process</b> is amenable to spatial guidance using a reference image, leading to semantically coherent edits. This eliminates the need for complex optimization and costly model <b>finetuning,</b> resulting in significantly faster editing compared to previous methods. Additionally, our method avoids the storage requirements associated with large <b>finetuned</b> models. These advantages make our approach particularly well-suited for editing on mobile devices and applications demanding rapid response times. While simple and fast, our method achieves 62-71% preference in a user-study and significantly better model-based editing strength and image preservation scores.</p></p class="citation"></blockquote><h3 id=41104--145329-pct-perspective-cue-training-framework-for-multi-camera-bev-segmentation-haruya-ishikawa-et-al-2024>(41/104 | 145/329) PCT: Perspective Cue Training Framework for Multi-Camera BEV Segmentation (Haruya Ishikawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haruya Ishikawa, Takumi Iida, Yoshinori Konishi, Yoshimitsu Aoki. (2024)<br><strong>PCT: Perspective Cue Training Framework for Multi-Camera BEV Segmentation</strong><br><button class=copy-to-clipboard title="PCT: Perspective Cue Training Framework for Multi-Camera BEV Segmentation" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Semi-Supervised Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12530v1.pdf filename=2403.12530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating annotations for bird&rsquo;s-eye-view (BEV) segmentation presents significant challenges due to the scenes&rsquo; complexity and the high manual annotation cost. In this work, we address these challenges by leveraging the abundance of unlabeled data available. We propose the Perspective Cue Training (PCT) framework, a novel training framework that utilizes pseudo-labels generated from unlabeled perspective images using publicly available semantic segmentation models trained on large street-view datasets. PCT applies a perspective view task head to the image encoder shared with the BEV segmentation head, effectively utilizing the unlabeled data to be trained with the generated pseudo-labels. Since image encoders are present in nearly all camera-based BEV segmentation architectures, PCT is flexible and applicable to various existing BEV architectures. PCT can be applied to various settings where unlabeled data is available. In this paper, we applied PCT for <b>semi-supervised</b> <b>learning</b> (SSL) and <b>unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA). Additionally, we introduce strong input perturbation through Camera Dropout (CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are crucial for enhancing SSL capabilities using our teacher-student framework. Our comprehensive approach is simple and flexible but yields significant improvements over various baselines for SSL and UDA, achieving competitive performances even against the current state-of-the-art.</p></p class="citation"></blockquote><h3 id=42104--146329-clip-vis-adapting-clip-for-open-vocabulary-video-instance-segmentation-wenqi-zhu-et-al-2024>(42/104 | 146/329) CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation (Wenqi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqi Zhu, Jiale Cao, Jin Xie, Shuangming Yang, Yanwei Pang. (2024)<br><strong>CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation</strong><br><button class=copy-to-clipboard title="CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Zero-shot, Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12455v1.pdf filename=2403.12455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-vocabulary video instance segmentation strives to segment and track instances belonging to an open set of categories in a video. The <b>vision-language</b> model Contrastive Language-Image Pre-training (CLIP) has shown strong <b>zero-shot</b> classification ability in image-level open-vocabulary task. In this paper, we propose a simple encoder-decoder network, called CLIP-VIS, to adapt CLIP for open-vocabulary video instance segmentation. Our CLIP-VIS adopts frozen CLIP image encoder and introduces three modules, including class-agnostic mask generation, temporal topK-enhanced matching, and weighted open-vocabulary classification. Given a set of initial queries, class-agnostic mask generation employs a <b>transformer</b> decoder to predict query masks and corresponding object scores and mask IoU scores. Then, temporal topK-enhanced matching performs query matching across frames by using K mostly matched frames. Finally, weighted open-vocabulary classification first generates query visual features with mask pooling, and second performs weighted classification using object scores and mask IoU scores. Our CLIP-VIS does not require the annotations of instance categories and identities. The experiments are performed on various video instance segmentation datasets, which demonstrate the effectiveness of our proposed method, especially on novel categories. When using ConvNeXt-B as backbone, our CLIP-VIS achieves the AP and APn scores of 32.1% and 40.3% on validation set of LV-VIS dataset, which outperforms OV2Seg by 11.0% and 24.0% respectively. We will release the source code and models at <a href=https://github.com/zwq456/CLIP-VIS.git>https://github.com/zwq456/CLIP-VIS.git</a>.</p></p class="citation"></blockquote><h3 id=43104--147329-ov9d-open-vocabulary-category-level-9d-object-pose-and-size-estimation-junhao-cai-et-al-2024>(43/104 | 147/329) OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation (Junhao Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhao Cai, Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qifeng Chen. (2024)<br><strong>OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation</strong><br><button class=copy-to-clipboard title="OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Foundation Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12396v1.pdf filename=2403.12396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies a new open-set problem, the open-vocabulary category-level object pose and size estimation. Given human text descriptions of arbitrary novel object categories, the robot agent seeks to predict the position, orientation, and size of the target object in the observed scene image. To enable such generalizability, we first introduce OO3D-9D, a large-scale photorealistic dataset for this task. Derived from OmniObject3D, OO3D-9D is the largest and most diverse dataset in the field of category-level object pose and size estimation. It includes additional annotations for the symmetry axis of each category, which help resolve symmetric ambiguity. Apart from the large-scale dataset, we find another key to enabling such generalizability is leveraging the strong prior knowledge in pre-trained visual-language <b>foundation</b> <b>models.</b> We then propose a framework built on pre-trained DinoV2 and <b>text-to-image</b> stable <b>diffusion</b> <b>models</b> to infer the normalized object coordinate space (NOCS) maps of the target instances. This framework fully leverages the visual semantic prior from DinoV2 and the aligned visual and language knowledge within the <b>text-to-image</b> <b>diffusion</b> <b>model,</b> which enables generalization to various text descriptions of novel categories. Comprehensive quantitative and qualitative experiments demonstrate that the proposed open-vocabulary method, trained on our large-scale synthesized data, significantly outperforms the baseline and can effectively generalize to real-world images of unseen categories. The project page is at <a href=https://ov9d.github.io>https://ov9d.github.io</a>.</p></p class="citation"></blockquote><h3 id=44104--148329-when-do-we-not-need-larger-vision-models-baifeng-shi-et-al-2024>(44/104 | 148/329) When Do We Not Need Larger Vision Models? (Baifeng Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, Trevor Darrell. (2024)<br><strong>When Do We Not Need Larger Vision Models?</strong><br><button class=copy-to-clipboard title="When Do We Not Need Larger Vision Models?" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13043v1.pdf filename=2403.13043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scaling up the size of vision models has been the de facto standard to obtain more powerful visual representations. In this work, we discuss the point beyond which larger vision models are not necessary. First, we demonstrate the power of Scaling on Scales (S$^2$), whereby a pre-trained and frozen smaller vision model (e.g., ViT-B or ViT-L), run over multiple image scales, can outperform larger models (e.g., ViT-H or ViT-G) on classification, segmentation, depth estimation, <b>Multimodal</b> <b>LLM</b> (MLLM) <b>benchmarks,</b> and robotic manipulation. Notably, S$^2$ achieves state-of-the-art performance in detailed understanding of MLLM on the V* <b>benchmark,</b> surpassing models such as <b>GPT-4V.</b> We examine the conditions under which S$^2$ is a preferred scaling approach compared to scaling on model size. While larger models have the advantage of better generalization on hard examples, we show that features of larger vision models can be well approximated by those of multi-scale smaller models. This suggests most, if not all, of the representations learned by current large pre-trained models can also be obtained from multi-scale smaller models. Our results show that a multi-scale smaller model has comparable learning capacity to a larger model, and pre-training smaller models with S$^2$ can match or even exceed the advantage of larger models. We release a Python package that can apply S$^2$ on any vision model with one line of code: <a href=https://github.com/bfshi/scaling_on_scales>https://github.com/bfshi/scaling_on_scales</a>.</p></p class="citation"></blockquote><h3 id=45104--149329-real-iad-a-real-world-multi-view-dataset-for-benchmarking-versatile-industrial-anomaly-detection-chengjie-wang-et-al-2024>(45/104 | 149/329) Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection (Chengjie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengjie Wang, Wenbing Zhu, Bin-Bin Gao, Zhenye Gan, Jianning Zhang, Zhihao Gu, Shuguang Qian, Mingang Chen, Lizhuang Ma. (2024)<br><strong>Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection</strong><br><button class=copy-to-clipboard title="Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Anomaly Detection, Benchmarking, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12580v1.pdf filename=2403.12580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industrial <b>anomaly</b> <b>detection</b> (IAD) has garnered significant attention and experienced rapid development. However, the recent development of IAD approach has encountered certain difficulties due to dataset limitations. On the one hand, most of the state-of-the-art methods have achieved saturation (over 99% in AUROC) on mainstream datasets such as MVTec, and the differences of methods cannot be well distinguished, leading to a significant gap between public datasets and actual application scenarios. On the other hand, the research on various new practical <b>anomaly</b> <b>detection</b> settings is limited by the scale of the dataset, posing a risk of overfitting in evaluation results. Therefore, we propose a large-scale, Real-world, and multi-view Industrial <b>Anomaly</b> <b>Detection</b> dataset, named Real-IAD, which contains 150K high-resolution images of 30 different objects, an order of magnitude larger than existing datasets. It has a larger range of defect area and ratio proportions, making it more challenging than previous datasets. To make the dataset closer to real application scenarios, we adopted a multi-view shooting method and proposed sample-level evaluation metrics. In addition, beyond the general <b>unsupervised</b> <b>anomaly</b> <b>detection</b> setting, we propose a new setting for Fully <b>Unsupervised</b> Industrial <b>Anomaly</b> <b>Detection</b> (FUIAD) based on the observation that the yield rate in industrial production is usually greater than 60%, which has more practical application value. Finally, we report the results of popular IAD methods on the Real-IAD dataset, providing a highly challenging <b>benchmark</b> to promote the development of the IAD field.</p></p class="citation"></blockquote><h3 id=46104--150329-self-learning-canonical-space-for-multi-view-3d-human-pose-estimation-xiaoben-li-et-al-2024>(46/104 | 150/329) Self-learning Canonical Space for Multi-view 3D Human Pose Estimation (Xiaoben Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoben Li, Mancheng Meng, Ziyan Wu, Terrence Chen, Fan Yang, Dinggang Shen. (2024)<br><strong>Self-learning Canonical Space for Multi-view 3D Human Pose Estimation</strong><br><button class=copy-to-clipboard title="Self-learning Canonical Space for Multi-view 3D Human Pose Estimation" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12440v1.pdf filename=2403.12440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-view 3D human pose estimation is naturally superior to single view one, benefiting from more comprehensive information provided by images of multiple views. The information includes camera poses, 2D/3D human poses, and 3D <b>geometry.</b> However, the accurate annotation of these information is hard to obtain, making it challenging to predict accurate 3D human pose from multi-view images. To deal with this issue, we propose a fully <b>self-supervised</b> framework, named cascaded multi-view aggregating network (CMANet), to construct a canonical parameter space to holistically integrate and exploit multi-view information. In our framework, the multi-view information is grouped into two categories: 1) intra-view information , 2) inter-view information. Accordingly, CMANet consists of two components: intra-view module (IRV) and inter-view module (IEV). IRV is used for extracting initial camera pose and 3D human pose of each view; IEV is to fuse complementary pose information and cross-view 3D <b>geometry</b> for a final 3D human pose. To facilitate the aggregation of the intra- and inter-view, we define a canonical parameter space, depicted by per-view camera pose and human pose and shape parameters ($\theta$ and $\beta$) of SMPL model, and propose a two-stage learning procedure. At first stage, IRV learns to estimate camera pose and view-dependent 3D human pose <b>supervised</b> by confident output of an off-the-shelf 2D keypoint detector. At second stage, IRV is frozen and IEV further refines the camera pose and optimizes the 3D human pose by implicitly encoding the cross-view complement and 3D <b>geometry</b> constraint, achieved by jointly fitting predicted multi-view 2D keypoints. The proposed framework, modules, and learning strategy are demonstrated to be effective by comprehensive experiments and CMANet is superior to state-of-the-art methods in extensive quantitative and qualitative analysis.</p></p class="citation"></blockquote><h3 id=47104--151329-precise-physics-driven-text-to-3d-generation-qingshan-xu-et-al-2024>(47/104 | 151/329) Precise-Physics Driven Text-to-3D Generation (Qingshan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingshan Xu, Jiao Liu, Melvin Wong, Caishun Chen, Yew-Soon Ong. (2024)<br><strong>Precise-Physics Driven Text-to-3D Generation</strong><br><button class=copy-to-clipboard title="Precise-Physics Driven Text-to-3D Generation" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Geometry, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12438v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12438v1.pdf filename=2403.12438v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-3D generation has shown great promise in generating novel 3D content based on given text <b>prompts.</b> However, existing generative methods mostly focus on geometric or visual plausibility while ignoring precise physics perception for the generated 3D shapes. This greatly hinders the practicality of generated 3D shapes in real-world applications. In this work, we propose Phy3DGen, a precise-physics-driven text-to-3D generation method. By analyzing the solid mechanics of generated 3D shapes, we reveal that the 3D shapes generated by existing text-to-3D generation methods are impractical for real-world applications as the generated 3D shapes do not conform to the laws of physics. To this end, we leverage 3D <b>diffusion</b> <b>models</b> to provide 3D shape priors and design a data-driven differentiable physics layer to optimize 3D shape priors with solid mechanics. This allows us to optimize <b>geometry</b> efficiently and learn precise physics information about 3D shapes at the same time. Experimental results demonstrate that our method can consider both geometric plausibility and precise physics perception, further bridging 3D virtual modeling and precise physical worlds.</p></p class="citation"></blockquote><h3 id=48104--152329-dmad-dual-memory-bank-for-real-world-anomaly-detection-jianlong-hu-et-al-2024>(48/104 | 152/329) DMAD: Dual Memory Bank for Real-World Anomaly Detection (Jianlong Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianlong Hu, Xu Chen, Zhenye Gan, Jinlong Peng, Shengchuan Zhang, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Liujuan Cao, Rongrong Ji. (2024)<br><strong>DMAD: Dual Memory Bank for Real-World Anomaly Detection</strong><br><button class=copy-to-clipboard title="DMAD: Dual Memory Bank for Real-World Anomaly Detection" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 25<br>Keywords: Anomaly Detection, Representation Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12362v1.pdf filename=2403.12362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training a unified model is considered to be more suitable for practical industrial <b>anomaly</b> <b>detection</b> scenarios due to its generalization ability and storage efficiency. However, this multi-class setting, which exclusively uses normal data, overlooks the few but important accessible annotated anomalies in the real world. To address the challenge of real-world <b>anomaly</b> <b>detection,</b> we propose a new framework named Dual Memory bank enhanced <b>representation</b> <b>learning</b> for <b>Anomaly</b> <b>Detection</b> (DMAD). This framework handles both <b>unsupervised</b> and semi-supervised scenarios in a unified (multi-class) setting. DMAD employs a dual memory bank to calculate feature distance and feature attention between normal and abnormal patterns, thereby encapsulating knowledge about normal and abnormal instances. This knowledge is then used to construct an enhanced <b>representation</b> <b>for</b> <b>anomaly</b> <b>score</b> learning. We evaluated DMAD on the MVTec-AD and VisA datasets. The results show that DMAD surpasses current state-of-the-art methods, highlighting DMAD&rsquo;s capability in handling the complexities of real-world <b>anomaly</b> <b>detection</b> scenarios.</p></p class="citation"></blockquote><h3 id=49104--153329-unveiling-the-anomalies-in-an-ever-changing-world-a-benchmark-for-pixel-level-anomaly-detection-in-continual-learning-nikola-bugarin-et-al-2024>(49/104 | 153/329) Unveiling the Anomalies in an Ever-Changing World: A Benchmark for Pixel-Level Anomaly Detection in Continual Learning (Nikola Bugarin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikola Bugarin, Jovana Bugaric, Manuel Barusco, Davide Dalle Pezze, Gian Antonio Susto. (2024)<br><strong>Unveiling the Anomalies in an Ever-Changing World: A Benchmark for Pixel-Level Anomaly Detection in Continual Learning</strong><br><button class=copy-to-clipboard title="Unveiling the Anomalies in an Ever-Changing World: A Benchmark for Pixel-Level Anomaly Detection in Continual Learning" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15463v1.pdf filename=2403.15463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>Detection</b> is a relevant problem in numerous real-world applications, especially when dealing with images. However, little attention has been paid to the issue of changes over time in the input data distribution, which may cause a significant decrease in performance. In this study, we investigate the problem of Pixel-Level <b>Anomaly</b> <b>Detection</b> in the <b>Continual</b> <b>Learning</b> setting, where new data arrives over time and the goal is to perform well on new and old data. We implement several state-of-the-art techniques to solve the <b>Anomaly</b> <b>Detection</b> problem in the classic setting and adapt them to work in the <b>Continual</b> <b>Learning</b> setting. To validate the approaches, we use a real-world dataset of images with pixel-based anomalies to provide a reliable <b>benchmark</b> and serve as a foundation for further advancements in the field. We provide a comprehensive analysis, discussing which <b>Anomaly</b> <b>Detection</b> methods and which families of approaches seem more suitable for the <b>Continual</b> <b>Learning</b> setting.</p></p class="citation"></blockquote><h3 id=50104--154329-learning-neural-volumetric-pose-features-for-camera-localization-jingyu-lin-et-al-2024>(50/104 | 154/329) Learning Neural Volumetric Pose Features for Camera Localization (Jingyu Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyu Lin, Jiaqi Gu, Bojian Wu, Lubin Fan, Renjie Chen, Ligang Liu, Jieping Ye. (2024)<br><strong>Learning Neural Volumetric Pose Features for Camera Localization</strong><br><button class=copy-to-clipboard title="Learning Neural Volumetric Pose Features for Camera Localization" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12800v1.pdf filename=2403.12800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel neural volumetric pose feature, termed PoseMap, designed to enhance camera localization by encapsulating the information between images and the associated camera poses. Our framework leverages an Absolute Pose Regression (APR) architecture, together with an augmented NeRF module. This integration not only facilitates the generation of novel views to enrich the training dataset but also enables the learning of effective pose features. Additionally, we extend our architecture for <b>self-supervised</b> online alignment, allowing our method to be used and <b>fine-tuned</b> for unlabelled images within a unified framework. Experiments demonstrate that our method achieves 14.28% and 20.51% performance gain on average in indoor and outdoor <b>benchmark</b> scenes, outperforming existing APR methods with state-of-the-art accuracy.</p></p class="citation"></blockquote><h3 id=51104--155329-watervg-waterway-visual-grounding-based-on-text-guided-vision-and-mmwave-radar-runwei-guan-et-al-2024>(51/104 | 155/329) WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar (Runwei Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runwei Guan, Liye Jia, Fengyufan Yang, Shanliang Yao, Erick Purwanto, Xiaohui Zhu, Eng Gee Lim, Jeremy Smith, Ka Lok Man, Yutao Yue. (2024)<br><strong>WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar</strong><br><button class=copy-to-clipboard title="WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Multi-modal, Grounding, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12686v1.pdf filename=2403.12686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The perception of waterways based on human intent holds significant importance for autonomous navigation and operations of Unmanned Surface Vehicles (USVs) in water environments. Inspired by visual <b>grounding,</b> in this paper, we introduce WaterVG, the first visual <b>grounding</b> dataset designed for USV-based waterway perception based on human intention <b>prompts.</b> WaterVG encompasses <b>prompts</b> describing multiple targets, with annotations at the instance level including bounding boxes and masks. Notably, WaterVG includes 11,568 samples with 34,950 referred targets, which integrates both visual and radar characteristics captured by monocular camera and millimeter-wave (mmWave) radar, enabling a finer granularity of text <b>prompts.</b> Furthermore, we propose a novel <b>multi-modal</b> visual <b>grounding</b> model, Potamoi, which is a <b>multi-modal</b> and multi-task model based on the one-stage paradigm with a designed Phased Heterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). In specific, MHSCA is a low-cost and efficient fusion module with a remarkably small parameter count and FLOPs, elegantly aligning and fusing scenario context information captured by two sensors with linguistic features, which can effectively address tasks of referring expression comprehension and segmentation based on fine-grained <b>prompts.</b> Comprehensive experiments and evaluations have been conducted on WaterVG, where our Potamoi archives state-of-the-art performances compared with counterparts.</p></p class="citation"></blockquote><h3 id=52104--156329-prompt-guided-adaptive-model-transformation-for-whole-slide-image-classification-yi-lin-et-al-2024>(52/104 | 156/329) Prompt-Guided Adaptive Model Transformation for Whole Slide Image Classification (Yi Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Lin, Zhengjie Zhu, Kwang-Ting Cheng, Hao Chen. (2024)<br><strong>Prompt-Guided Adaptive Model Transformation for Whole Slide Image Classification</strong><br><button class=copy-to-clipboard title="Prompt-Guided Adaptive Model Transformation for Whole Slide Image Classification" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Multiple Instance Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12537v1.pdf filename=2403.12537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multiple</b> <b>instance</b> <b>learning</b> (MIL) has emerged as a popular method for classifying histopathology whole slide images (WSIs). Existing approaches typically rely on frozen pre-trained models to extract instance features, neglecting the substantial domain shift between pre-training natural and histopathological images. To address this issue, we propose PAMT, a novel <b>Prompt-guided</b> Adaptive Model Transformation framework that enhances MIL classification performance by seamlessly adapting pre-trained models to the specific characteristics of histopathology data. To capture the intricate histopathology distribution, we introduce Representative Patch Sampling (RPS) and Prototypical Visual <b>Prompt</b> (PVP) to reform the input data, building a compact while informative representation. Furthermore, to narrow the domain gap, we introduce Adaptive Model Transformation (AMT) that integrates adapter blocks within the feature extraction pipeline, enabling the pre-trained models to learn domain-specific features. We rigorously evaluate our approach on two publicly available datasets, Camelyon16 and TCGA-NSCLC, showcasing substantial improvements across various MIL models. Our findings affirm the potential of PAMT to set a new <b>benchmark</b> in WSI classification, underscoring the value of a targeted reprogramming approach.</p></p class="citation"></blockquote><h3 id=53104--157329-entity6k-a-large-open-domain-evaluation-dataset-for-real-world-entity-recognition-jielin-qiu-et-al-2024>(53/104 | 157/329) Entity6K: A Large Open-Domain Evaluation Dataset for Real-World Entity Recognition (Jielin Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jielin Qiu, William Han, Winfred Wang, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Christos Faloutsos, Lei Li, Lijuan Wang. (2024)<br><strong>Entity6K: A Large Open-Domain Evaluation Dataset for Real-World Entity Recognition</strong><br><button class=copy-to-clipboard title="Entity6K: A Large Open-Domain Evaluation Dataset for Real-World Entity Recognition" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12339v1.pdf filename=2403.12339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-domain real-world entity recognition is essential yet challenging, involving identifying various entities in diverse environments. The lack of a suitable evaluation dataset has been a major obstacle in this field due to the vast number of entities and the extensive human effort required for data curation. We introduce Entity6K, a comprehensive dataset for real-world entity recognition, featuring 5,700 entities across 26 categories, each supported by 5 human-verified images with annotations. Entity6K offers a diverse range of entity names and categorizations, addressing a gap in existing datasets. We conducted <b>benchmarks</b> with existing models on tasks like image captioning, <b>object</b> <b>detection,</b> <b>zero-shot</b> classification, and dense captioning to demonstrate Entity6K&rsquo;s effectiveness in evaluating models&rsquo; entity recognition capabilities. We believe Entity6K will be a valuable resource for advancing accurate entity recognition in open-domain settings.</p></p class="citation"></blockquote><h3 id=54104--158329-luwa-dataset-learning-lithic-use-wear-analysis-on-microscopic-images-jing-zhang-et-al-2024>(54/104 | 158/329) LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images (Jing Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Zhang, Irving Fang, Juexiao Zhang, Hao Wu, Akshat Kaushik, Alice Rodriguez, Hanwen Zhao Zhuo Zheng, Radu Iovita, Chen Feng. (2024)<br><strong>LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images</strong><br><button class=copy-to-clipboard title="LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13171v1.pdf filename=2403.13171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lithic Use-Wear Analysis (LUWA) using microscopic images is an underexplored vision-for-science research area. It seeks to distinguish the worked material, which is critical for understanding archaeological artifacts, material interactions, tool functionalities, and dental records. However, this challenging task goes beyond the well-studied image classification problem for common objects. It is affected by many confounders owing to the complex wear mechanism and microscopic imaging, which makes it difficult even for human experts to identify the worked material successfully. In this paper, we investigate the following three questions on this unique vision task for the first time:(i) How well can state-of-the-art pre-trained models (like DINOv2) generalize to the rarely seen domain? (ii) How can <b>few-shot</b> <b>learning</b> be exploited for scarce microscopic images? (iii) How do the ambiguous magnification and sensing modality influence the classification accuracy? To study these, we collaborated with archaeologists and built the first open-source and the largest LUWA dataset containing 23,130 microscopic images with different magnifications and sensing modalities. Extensive experiments show that existing pre-trained models notably outperform human experts but still leave a large gap for improvements. Most importantly, the LUWA dataset provides an underexplored opportunity for vision and learning communities and complements existing image classification problems on common objects.</p></p class="citation"></blockquote><h3 id=55104--159329-ultra-high-resolution-image-synthesis-with-pyramid-diffusion-model-jiajie-yang-2024>(55/104 | 159/329) Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model (Jiajie Yang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajie Yang. (2024)<br><strong>Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model</strong><br><button class=copy-to-clipboard title="Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12915v1.pdf filename=2403.12915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the Pyramid <b>Diffusion</b> <b>Model</b> (PDM), a novel architecture designed for ultra-high-resolution image synthesis. PDM utilizes a pyramid latent representation, providing a broader design space that enables more flexible, structured, and efficient perceptual compression which enable <b>AutoEncoder</b> and Network of <b>Diffusion</b> <b>to</b> equip branches and deeper layers. To enhance PDM&rsquo;s capabilities for generative tasks, we propose the integration of Spatial-Channel Attention and Res-Skip Connection, along with the utilization of Spectral Norm and Decreasing Dropout Strategy for the <b>Diffusion</b> <b>Network</b> and <b>AutoEncoder.</b> In summary, PDM achieves the synthesis of images with a 2K resolution for the first time, demonstrated on two new datasets comprising images of sizes 2048x2048 pixels and 2048x1024 pixels respectively. We believe that this work offers an alternative approach to designing scalable image generative models, while also providing incremental reinforcement for existing frameworks.</p></p class="citation"></blockquote><h3 id=56104--160329-confusing-pair-correction-based-on-category-prototype-for-domain-adaptation-under-noisy-environments-churan-zhi-et-al-2024>(56/104 | 160/329) Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments (Churan Zhi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Churan Zhi, Junbao Zhuo, Shuhui Wang. (2024)<br><strong>Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments</strong><br><button class=copy-to-clipboard title="Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12883v1.pdf filename=2403.12883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address <b>unsupervised</b> <b>domain</b> <b>adaptation</b> under noisy environments, which is more challenging and practical than traditional <b>domain</b> <b>adaptation.</b> In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced <b>domain</b> <b>shift</b> and a notable decline in the overall model performance. Previous methods employed prototype methods for <b>domain</b> <b>adaptation</b> on robust feature spaces. However, these approaches struggle to effectively classify classes with similar features under noisy environments. To address this issue, we propose a new method to detect and correct confusing class pair. We first divide classes into easy and hard classes based on the small loss criterion. We then leverage the top-2 predictions for each sample after aligning the source and target <b>domain</b> <b>to</b> find the confusing pair in the hard classes. We apply label correction to the noisy samples within the confusing pair. With the proposed label correction method, we can train our model with more accurate labels. Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods. Our codes are publicly available at <a href=https://github.com/Hehxcf/CPC/>https://github.com/Hehxcf/CPC/</a>.</p></p class="citation"></blockquote><h3 id=57104--161329-dreamda-generative-data-augmentation-with-diffusion-models-yunxiang-fu-et-al-2024>(57/104 | 161/329) DreamDA: Generative Data Augmentation with Diffusion Models (Yunxiang Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunxiang Fu, Chaoqi Chen, Yu Qiao, Yizhou Yu. (2024)<br><strong>DreamDA: Generative Data Augmentation with Diffusion Models</strong><br><button class=copy-to-clipboard title="DreamDA: Generative Data Augmentation with Diffusion Models" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12803v1.pdf filename=2403.12803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The acquisition of large-scale, high-quality <b>data</b> <b>is</b> a resource-intensive and time-consuming endeavor. Compared to conventional <b>Data</b> <b>Augmentation</b> (DA) techniques (e.g. cropping and rotation), exploiting prevailing <b>diffusion</b> <b>models</b> for <b>data</b> <b>generation</b> has received scant attention in classification tasks. Existing generative DA methods either inadequately bridge the domain gap between real-world and synthesized images, or inherently suffer from a lack of diversity. To solve these issues, this paper proposes a new classification-oriented framework DreamDA, which enables <b>data</b> <b>synthesis</b> and label generation by way of <b>diffusion</b> <b>models.</b> DreamDA generates diverse samples that adhere to the original <b>data</b> <b>distribution</b> by considering training images in the original <b>data</b> <b>as</b> seeds and perturbing their reverse <b>diffusion</b> <b>process.</b> In addition, since the labels of the generated <b>data</b> <b>may</b> not align with the labels of their corresponding seed images, we introduce a self-training paradigm for generating pseudo labels and training classifiers using the synthesized <b>data.</b> <b>Extensive</b> experiments across four tasks and five datasets demonstrate consistent improvements over strong baselines, revealing the efficacy of DreamDA in synthesizing high-quality and diverse images with accurate labels. Our code will be available at <a href=https://github.com/yunxiangfu2001/DreamDA>https://github.com/yunxiangfu2001/DreamDA</a>.</p></p class="citation"></blockquote><h3 id=58104--162329-discover-and-mitigate-multiple-biased-subgroups-in-image-classifiers-zeliang-zhang-et-al-2024>(58/104 | 162/329) Discover and Mitigate Multiple Biased Subgroups in Image Classifiers (Zeliang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeliang Zhang, Mingqian Feng, Zhiheng Li, Chenliang Xu. (2024)<br><strong>Discover and Mitigate Multiple Biased Subgroups in Image Classifiers</strong><br><button class=copy-to-clipboard title="Discover and Mitigate Multiple Biased Subgroups in Image Classifiers" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12777v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12777v2.pdf filename=2403.12777v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications. Such subgroups are typically unknown due to the absence of subgroup labels. Discovering biased subgroups is the key to understanding models&rsquo; failure modes and further improving models&rsquo; robustness. Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist. In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers. Our approach decomposes the image features into multiple components that represent multiple subgroups. This decomposition is achieved via a bilinear dimension reduction method, Partial Least Square (PLS), guided by useful supervision from the image classifier. We further interpret the semantic meaning of each subgroup component by generating natural language descriptions using <b>vision-language</b> <b>foundation</b> <b>models.</b> Finally, DIM mitigates multiple biased subgroups simultaneously via two strategies, including the data- and model-centric strategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate the effectiveness of DIM in discovering and mitigating multiple biased subgroups. Furthermore, DIM uncovers the failure modes of the classifier on Hard ImageNet, showcasing its broader applicability to understanding model bias in image classifiers. The code is available at <a href=https://github.com/ZhangAIPI/DIM>https://github.com/ZhangAIPI/DIM</a>.</p></p class="citation"></blockquote><h3 id=59104--163329-towards-controllable-face-generation-with-semantic-latent-diffusion-models-alex-ergasti-et-al-2024>(59/104 | 163/329) Towards Controllable Face Generation with Semantic Latent Diffusion Models (Alex Ergasti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Ergasti, Claudio Ferrari, Tomaso Fontanini, Massimo Bertozzi, Andrea Prati. (2024)<br><strong>Towards Controllable Face Generation with Semantic Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="Towards Controllable Face Generation with Semantic Latent Diffusion Models" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12743v1.pdf filename=2403.12743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic Image Synthesis (SIS) is among the most popular and effective techniques in the field of face generation and editing, thanks to its good generation quality and the versatility is brings along. Recent works attempted to go beyond the standard <b>GAN-based</b> framework, and started to explore <b>Diffusion</b> <b>Models</b> (DMs) for this task as these stand out with respect to <b>GANs</b> in terms of both quality and diversity. On the other hand, DMs lack in fine-grained controllability and reproducibility. To address that, in this paper we propose a SIS framework based on a novel Latent <b>Diffusion</b> <b>Model</b> architecture for human face generation and editing that is both able to reproduce and manipulate a real reference image and generate diversity-driven results. The proposed system utilizes both SPADE normalization and cross-attention layers to merge shape and style information and, by doing so, allows for a precise control over each of the semantic parts of the human face. This was not possible with previous methods in the state of the art. Finally, we performed an extensive set of experiments to prove that our model surpasses current state of the art, both qualitatively and quantitatively.</p></p class="citation"></blockquote><h3 id=60104--164329-eas-snn-end-to-end-adaptive-sampling-and-representation-for-event-based-detection-with-recurrent-spiking-neural-networks-ziming-wang-et-al-2024>(60/104 | 164/329) EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks (Ziming Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziming Wang, Ziling Wang, Huaning Li, Lang Qin, Runhao Jiang, De Ma, Huajin Tang. (2024)<br><strong>EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks</strong><br><button class=copy-to-clipboard title="EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-NE, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12574v1.pdf filename=2403.12574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event cameras, with their high dynamic range and temporal resolution, are ideally suited for <b>object</b> <b>detection,</b> especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent <b>convolutional</b> SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detection. Additionally, we introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to regulate potential distribution and address performance degradation encountered in spike-based sampling modules. Through rigorous testing on neuromorphic datasets for event-based detection, our approach demonstrably surpasses existing state-of-the-art spike-based methods, achieving superior performance with significantly fewer parameters and time steps. For instance, our method achieves a 4.4% mAP improvement on the Gen1 dataset, while requiring 38% fewer parameters and three time steps. Moreover, the applicability and effectiveness of our adaptive sampling methodology extend beyond SNNs, as demonstrated through further validation on conventional non-spiking detection models.</p></p class="citation"></blockquote><h3 id=61104--165329-a-hybrid-transformer-sequencer-approach-for-age-and-gender-classification-from-in-wild-facial-images-aakash-singh-et-al-2024>(61/104 | 165/329) A Hybrid Transformer-Sequencer approach for Age and Gender classification from in-wild facial images (Aakash Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aakash Singh, Vivek Kumar Singh. (2024)<br><strong>A Hybrid Transformer-Sequencer approach for Age and Gender classification from in-wild facial images</strong><br><button class=copy-to-clipboard title="A Hybrid Transformer-Sequencer approach for Age and Gender classification from in-wild facial images" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12483v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12483v2.pdf filename=2403.12483v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancements in computer vision and image processing techniques have led to emergence of new application in the domain of visual surveillance, targeted advertisement, content-based searching, and human-computer interaction etc. Out of the various techniques in computer vision, face analysis, in particular, has gained much attention. Several previous studies have tried to explore different applications of facial feature processing for a variety of tasks, including age and gender classification. However, despite several previous studies having explored the problem, the age and gender classification of in-wild human faces is still far from the achieving the desired levels of accuracy required for real-world applications. This paper, therefore, attempts to bridge this gap by proposing a hybrid model that combines <b>self-attention</b> and BiLSTM approaches for age and gender classification problems. The proposed models performance is compared with several state-of-the-art model proposed so far. An improvement of approximately 10percent and 6percent over the state-of-the-art implementations for age and gender classification, respectively, are noted for the proposed model. The proposed model is thus found to achieve superior performance and is found to provide a more generalized learning. The model can, therefore, be applied as a core classification component in various image processing and computer vision problems.</p></p class="citation"></blockquote><h3 id=62104--166329-privacy-preserving-face-recognition-using-trainable-feature-subtraction-yuxi-mi-et-al-2024>(62/104 | 166/329) Privacy-Preserving Face Recognition Using Trainable Feature Subtraction (Yuxi Mi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxi Mi, Zhizhou Zhong, Yuge Huang, Jiazhen Ji, Jianqing Xu, Jun Wang, Shaoming Wang, Shouhong Ding, Shuigeng Zhou. (2024)<br><strong>Privacy-Preserving Face Recognition Using Trainable Feature Subtraction</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Face Recognition Using Trainable Feature Subtraction" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Face Recognition, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12457v1.pdf filename=2403.12457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread adoption of <b>face</b> <b>recognition</b> has led to increasing privacy concerns, as unauthorized access to <b>face</b> <b>images</b> can expose sensitive personal information. This paper explores <b>face</b> <b>image</b> protection against viewing and recovery attacks. Inspired by image compression, we propose creating a visually uninformative <b>face</b> <b>image</b> through feature subtraction between an original <b>face</b> <b>and</b> its model-produced regeneration. Recognizable identity features within the image are encouraged by co-training a recognition model on its high-dimensional feature representation. To enhance privacy, the high-dimensional representation is crafted through random channel shuffling, resulting in randomized recognizable images devoid of attacker-leverageable texture details. We <b>distill</b> our methodologies into a novel privacy-preserving <b>face</b> <b>recognition</b> method, MinusFace. Experiments demonstrate its high recognition accuracy and effective privacy protection. Its code is available at <a href=https://github.com/Tencent/TFace>https://github.com/Tencent/TFace</a>.</p></p class="citation"></blockquote><h3 id=63104--167329-human-mesh-recovery-from-arbitrary-multi-view-images-xiaoben-li-et-al-2024>(63/104 | 167/329) Human Mesh Recovery from Arbitrary Multi-view Images (Xiaoben Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoben Li, Mancheng Meng, Ziyan Wu, Terrence Chen, Fan Yang, Dinggang Shen. (2024)<br><strong>Human Mesh Recovery from Arbitrary Multi-view Images</strong><br><button class=copy-to-clipboard title="Human Mesh Recovery from Arbitrary Multi-view Images" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12434v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12434v2.pdf filename=2403.12434v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human mesh recovery from arbitrary multi-view images involves two characteristics: the arbitrary camera poses and arbitrary number of camera views. Because of the variability, designing a unified framework to tackle this task is challenging. The challenges can be <b>summarized</b> as the dilemma of being able to simultaneously estimate arbitrary camera poses and recover human mesh from arbitrary multi-view images while maintaining flexibility. To solve this dilemma, we propose a divide and conquer framework for Unified Human Mesh Recovery (U-HMR) from arbitrary multi-view images. In particular, U-HMR consists of a decoupled structure and two main components: camera and body decoupling (CBD), camera pose estimation (CPE), and arbitrary view fusion (AVF). As camera poses and human body mesh are independent of each other, CBD splits the estimation of them into two sub-tasks for two individual sub-networks (\ie, CPE and AVF) to handle respectively, thus the two sub-tasks are disentangled. In CPE, since each camera pose is unrelated to the others, we adopt a shared MLP to process all views in a parallel way. In AVF, in order to fuse multi-view information and make the fusion operation independent of the number of views, we introduce a <b>transformer</b> decoder with a SMPL parameters query token to extract cross-view features for mesh recovery. To demonstrate the efficacy and flexibility of the proposed framework and effect of each component, we conduct extensive experiments on three public datasets: Human3.6M, MPI-INF-3DHP, and TotalCapture.</p></p class="citation"></blockquote><h3 id=64104--168329-comboverse-compositional-3d-assets-creation-using-spatially-aware-diffusion-guidance-yongwei-chen-et-al-2024>(64/104 | 168/329) ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance (Yongwei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, Ziwei Liu. (2024)<br><strong>ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance</strong><br><button class=copy-to-clipboard title="ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12409v1.pdf filename=2403.12409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating high-quality 3D assets from a given image is highly desirable in various applications such as AR/VR. Recent advances in single-image 3D generation explore feed-forward models that learn to infer the 3D model of an object without optimization. Though promising results have been achieved in single object generation, these methods often struggle to model complex 3D assets that inherently contain multiple objects. In this work, we present ComboVerse, a 3D generation framework that produces high-quality 3D assets with complex compositions by learning to combine multiple models. 1) We first perform an in-depth analysis of this ``multi-object gap&rsquo;&rsquo; from both model and data perspectives. 2) Next, with reconstructed 3D models of different objects, we seek to adjust their sizes, rotation angles, and locations to create a 3D asset that matches the given image. 3) To automate this process, we apply spatially-aware score <b>distillation</b> sampling (SSDS) from pretrained <b>diffusion</b> <b>models</b> to guide the positioning of objects. Our proposed framework emphasizes spatial alignment of objects, compared with standard score <b>distillation</b> sampling, and thus achieves more accurate results. Extensive experiments validate ComboVerse achieves clear improvements over existing methods in generating compositional 3D assets.</p></p class="citation"></blockquote><h3 id=65104--169329-xpose-explainable-human-pose-estimation-luyu-qiu-et-al-2024>(65/104 | 169/329) XPose: eXplainable Human Pose Estimation (Luyu Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luyu Qiu, Jianing Li, Lei Wen, Chi Su, Fei Hao, Chen Jason Zhang, Lei Chen. (2024)<br><strong>XPose: eXplainable Human Pose Estimation</strong><br><button class=copy-to-clipboard title="XPose: eXplainable Human Pose Estimation" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12370v1.pdf filename=2403.12370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current approaches in pose estimation primarily concentrate on enhancing model architectures, often overlooking the importance of comprehensively understanding the rationale behind model decisions. In this paper, we propose XPose, a novel framework that incorporates <b>Explainable</b> <b>AI</b> (XAI) principles into pose estimation. This integration aims to elucidate the individual contribution of each keypoint to final prediction, thereby elevating the model&rsquo;s transparency and interpretability. Conventional XAI techniques have predominantly addressed tasks with single-target tasks like classification. Additionally, the application of Shapley value, a common measure in XAI, to pose estimation has been hindered by prohibitive computational demands. To address these challenges, this work introduces an innovative concept called Group Shapley Value (GSV). This approach strategically organizes keypoints into clusters based on their interdependencies. Within these clusters, GSV meticulously calculates Shapley value for keypoints, while for inter-cluster keypoints, it opts for a more holistic group-level valuation. This dual-level computation framework meticulously assesses keypoint contributions to the final outcome, optimizing computational efficiency. Building on the insights into keypoint interactions, we devise a novel <b>data</b> <b>augmentation</b> technique known as Group-based Keypoint Removal (GKR). This method ingeniously removes individual keypoints during training phases, deliberately preserving those with strong mutual connections, thereby refining the model&rsquo;s predictive prowess for non-visible keypoints. The empirical validation of GKR across a spectrum of standard approaches attests to its efficacy. GKR&rsquo;s success demonstrates how using <b>Explainable</b> <b>AI</b> (XAI) can directly enhance pose estimation models.</p></p class="citation"></blockquote><h3 id=66104--170329-contextual-ad-narration-with-interleaved-multimodal-sequence-hanlin-wang-et-al-2024>(66/104 | 170/329) Contextual AD Narration with Interleaved Multimodal Sequence (Hanlin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanlin Wang, Zhan Tong, Kecheng Zheng, Yujun Shen, Limin Wang. (2024)<br><strong>Contextual AD Narration with Interleaved Multimodal Sequence</strong><br><button class=copy-to-clipboard title="Contextual AD Narration with Interleaved Multimodal Sequence" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Foundation Model, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12922v1.pdf filename=2403.12922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Audio Description (AD) task aims to generate descriptions of visual elements for visually impaired individuals to help them access long-form video contents, like movie. With video feature, text, character bank and context information as inputs, the generated ADs are able to correspond to the characters by name and provide reasonable, contextual descriptions to help audience understand the storyline of movie. To achieve this goal, we propose to leverage pre-trained <b>foundation</b> <b>models</b> through a simple and unified framework to generate ADs with interleaved <b>multimodal</b> sequence as input, termed as Uni-AD. To enhance the alignment of features across various modalities with finer granularity, we introduce a simple and lightweight module that maps video features into the textual feature space. Moreover, we also propose a character-refinement module to provide more precise information by identifying the main characters who play more significant role in the video context. With these unique designs, we further incorporate contextual information and a contrastive loss into our architecture to generate more smooth and contextual ADs. Experiments on the MAD-eval dataset show that Uni-AD can achieve state-of-the-art performance on AD generation, which demonstrates the effectiveness of our approach. Code will be available at <a href=https://github.com/MCG-NJU/Uni-AD>https://github.com/MCG-NJU/Uni-AD</a>.</p></p class="citation"></blockquote><h3 id=67104--171329-visualcritic-making-lmms-perceive-visual-quality-like-humans-zhipeng-huang-et-al-2024>(67/104 | 171/329) VisualCritic: Making LMMs Perceive Visual Quality Like Humans (Zhipeng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhipeng Huang, Zhizheng Zhang, Yiting Lu, Zheng-Jun Zha, Zhibo Chen, Baining Guo. (2024)<br><strong>VisualCritic: Making LMMs Perceive Visual Quality Like Humans</strong><br><button class=copy-to-clipboard title="VisualCritic: Making LMMs Perceive Visual Quality Like Humans" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Instruction Following<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12806v1.pdf filename=2403.12806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>At present, large <b>multimodal</b> models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals. However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception. Can LMMs achieve this and show the same degree of generalization in this regard? If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed. In this paper, we explore this question and provide the answer &ldquo;Yes!&rdquo;. As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment. VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models. As an <b>instruction-following</b> <b>LMM,</b> VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic. Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images.</p></p class="citation"></blockquote><h3 id=68104--172329-m2da-multi-modal-fusion-transformer-incorporating-driver-attention-for-autonomous-driving-dongyang-xu-et-al-2024>(68/104 | 172/329) M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving (Dongyang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongyang Xu, Haokun Li, Qingfan Wang, Ziying Song, Lei Chen, Hanming Deng. (2024)<br><strong>M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving</strong><br><button class=copy-to-clipboard title="M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12552v1.pdf filename=2403.12552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-end autonomous driving has witnessed remarkable progress. However, the extensive deployment of autonomous vehicles has yet to be realized, primarily due to 1) inefficient <b>multi-modal</b> environment perception: how to integrate data from <b>multi-modal</b> sensors more efficiently; 2) non-human-like scene understanding: how to effectively locate and predict critical risky agents in traffic scenarios like an experienced driver. To overcome these challenges, in this paper, we propose a <b>Multi-Modal</b> fusion <b>transformer</b> incorporating Driver Attention (M2DA) for autonomous driving. To better fuse <b>multi-modal</b> data and achieve higher alignment between different modalities, a novel Lidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By incorporating driver attention, we empower the human-like scene understanding ability to autonomous vehicles to identify crucial areas within complex scenarios precisely and ensure safety. We conduct experiments on the CARLA simulator and achieve state-of-the-art performance with less data in closed-loop <b>benchmarks.</b> Source codes are available at <a href=https://anonymous.4open.science/r/M2DA-4772>https://anonymous.4open.science/r/M2DA-4772</a>.</p></p class="citation"></blockquote><h3 id=69104--173329-reflectivity-is-all-you-need-advancing-lidar-semantic-segmentation-kasi-viswanath-et-al-2024>(69/104 | 173/329) Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation (Kasi Viswanath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kasi Viswanath, Peng Jiang, Srikanth Saripalli. (2024)<br><strong>Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV, eess-IV<br>Keyword Score: 15<br>Keywords: Geometry, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13188v1.pdf filename=2403.13188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LiDAR semantic segmentation frameworks predominantly leverage <b>geometry-based</b> features to differentiate objects within a scan. While these methods excel in scenarios with clear boundaries and distinct shapes, their performance declines in environments where boundaries are blurred, particularly in off-road contexts. To address this, recent strides in 3D segmentation algorithms have focused on harnessing raw LiDAR intensity measurements to improve prediction accuracy. Despite these efforts, current learning-based models struggle to correlate the intricate connections between raw intensity and factors such as distance, incidence angle, material reflectivity, and atmospheric conditions. Building upon our prior work, this paper delves into the advantages of employing calibrated intensity (also referred to as reflectivity) within learning-based LiDAR semantic segmentation frameworks. We initially establish that incorporating reflectivity as an input enhances the existing LiDAR semantic segmentation model. Furthermore, we present findings that enable the model to learn to calibrate intensity can boost its performance. Through extensive experimentation on the off-road dataset Rellis-3D, we demonstrate notable improvements. Specifically, converting intensity to reflectivity results in a 4% increase in mean Intersection over Union (mIoU) when compared to using raw intensity in Off-road scenarios. Additionally, we also investigate the possible benefits of using calibrated intensity in semantic segmentation in urban environments (SemanticKITTI) and cross-sensor <b>domain</b> <b>adaptation.</b></p></p class="citation"></blockquote><h3 id=70104--174329-gvgen-text-to-3d-generation-with-volumetric-representation-xianglong-he-et-al-2024>(70/104 | 174/329) GVGEN: Text-to-3D Generation with Volumetric Representation (Xianglong He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, Tong He. (2024)<br><strong>GVGEN: Text-to-3D Generation with Volumetric Representation</strong><br><button class=copy-to-clipboard title="GVGEN: Text-to-3D Generation with Volumetric Representation" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12957v1.pdf filename=2403.12957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques:(1) Structured Volumetric Representation. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique <b>pruning</b> and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D <b>geometry,</b> we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed ($\sim$7 seconds), effectively striking a balance between quality and efficiency.</p></p class="citation"></blockquote><h3 id=71104--175329-ponq-a-neural-qem-based-mesh-representation-nissim-maruani-et-al-2024>(71/104 | 175/329) PoNQ: a Neural QEM-based Mesh Representation (Nissim Maruani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nissim Maruani, Maks Ovsjanikov, Pierre Alliez, Mathieu Desbrun. (2024)<br><strong>PoNQ: a Neural QEM-based Mesh Representation</strong><br><button class=copy-to-clipboard title="PoNQ: a Neural QEM-based Mesh Representation" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12870v1.pdf filename=2403.12870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although polygon meshes have been a standard representation in <b>geometry</b> processing, their irregular and combinatorial nature hinders their suitability for learning-based applications. In this work, we introduce a novel learnable mesh representation through a set of local 3D sample Points and their associated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape, which we denote PoNQ. A global mesh is directly derived from PoNQ by efficiently leveraging the knowledge of the local quadric errors. Besides marking the first use of QEM within a neural shape representation, our contribution guarantees both topological and geometrical properties by ensuring that a PoNQ mesh does not self-intersect and is always the boundary of a volume. Notably, our representation does not rely on a regular grid, is <b>supervised</b> directly by the target surface alone, and also handles open surfaces with boundaries and/or sharp features. We demonstrate the efficacy of PoNQ through a learning-based mesh prediction from SDF grids and show that our method surpasses recent state-of-the-art techniques in terms of both surface and edge-based metrics.</p></p class="citation"></blockquote><h3 id=72104--176329-global-guided-focal-neural-radiance-field-for-large-scale-scene-rendering-mingqi-shao-et-al-2024>(72/104 | 176/329) Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering (Mingqi Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang. (2024)<br><strong>Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering</strong><br><button class=copy-to-clipboard title="Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Fine-tuning, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12839v1.pdf filename=2403.12839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in <b>geometry</b> and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need <b>fine-tuning</b> based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: <a href=https://shaomq2187.github.io/GF-NeRF/>https://shaomq2187.github.io/GF-NeRF/</a></p></p class="citation"></blockquote><h3 id=73104--177329-facexformer-a-unified-transformer-for-facial-analysis-kartik-narayan-et-al-2024>(73/104 | 177/329) FaceXFormer: A Unified Transformer for Facial Analysis (Kartik Narayan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kartik Narayan, Vibashan VS, Rama Chellappa, Vishal M. Patel. (2024)<br><strong>FaceXFormer: A Unified Transformer for Facial Analysis</strong><br><button class=copy-to-clipboard title="FaceXFormer: A Unified Transformer for Facial Analysis" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12960v1.pdf filename=2403.12960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce FaceXformer, an end-to-end unified <b>transformer</b> model for a comprehensive range of facial analysis tasks such as face parsing, landmark detection, head pose estimation, attributes recognition, and estimation of age, gender, race, and landmarks visibility. Conventional methods in face analysis have often relied on task-specific designs and preprocessing techniques, which limit their approach to a unified architecture. Unlike these conventional methods, our FaceXformer leverages a <b>transformer-based</b> encoder-decoder architecture where each task is treated as a learnable token, enabling the integration of multiple tasks within a single framework. Moreover, we propose a parameter-efficient decoder, FaceX, which jointly processes face and task tokens, thereby learning generalized and robust face representations across different tasks. To the best of our knowledge, this is the first work to propose a single model capable of handling all these facial analysis tasks using <b>transformers.</b> We conducted a comprehensive analysis of effective backbones for unified face task processing and evaluated different task queries and the synergy between them. We conduct experiments against state-of-the-art specialized models and previous multi-task models in both intra-dataset and cross-dataset evaluations across multiple <b>benchmarks.</b> Additionally, our model effectively handles images &ldquo;in-the-wild,&rdquo; demonstrating its robustness and generalizability across eight different tasks, all while maintaining the real-time performance of 37 FPS.</p></p class="citation"></blockquote><h3 id=74104--178329-waveface-authentic-face-restoration-with-efficient-frequency-recovery-yunqi-miao-et-al-2024>(74/104 | 178/329) WaveFace: Authentic Face Restoration with Efficient Frequency Recovery (Yunqi Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunqi Miao, Jiankang Deng, Jungong Han. (2024)<br><strong>WaveFace: Authentic Face Restoration with Efficient Frequency Recovery</strong><br><button class=copy-to-clipboard title="WaveFace: Authentic Face Restoration with Efficient Frequency Recovery" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12760v1.pdf filename=2403.12760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>diffusion</b> <b>models</b> are rising as a powerful solution for blind face restoration, they are criticized for two problems: 1) slow training and inference speed, and 2) failure in preserving identity and recovering fine-grained facial details. In this work, we propose WaveFace to solve the problems in the frequency domain, where low- and high-frequency components decomposed by wavelet transformation are considered individually to maximize authenticity as well as efficiency. The <b>diffusion</b> <b>model</b> is applied to recover the low-frequency component only, which presents general information of the original image but 1/16 in size. To preserve the original identity, the generation is conditioned on the low-frequency component of low-quality images at each denoising step. Meanwhile, high-frequency components at multiple decomposition levels are handled by a unified network, which recovers complex facial details in a single step. Evaluations on four <b>benchmark</b> datasets show that: 1) WaveFace outperforms state-of-the-art methods in authenticity, especially in terms of identity preservation, and 2) authentic images are restored with the efficiency 10x faster than existing <b>diffusion</b> <b>model-based</b> BFR methods.</p></p class="citation"></blockquote><h3 id=75104--179329-postometro-pose-token-enhanced-mesh-transformer-for-robust-3d-human-mesh-recovery-wendi-yang-et-al-2024>(75/104 | 179/329) PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery (Wendi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wendi Yang, Zihang Jiang, Shang Zhao, S. Kevin Zhou. (2024)<br><strong>PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery</strong><br><button class=copy-to-clipboard title="PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12473v1.pdf filename=2403.12473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the recent advancements in single-image-based human mesh recovery, there is a growing interest in enhancing its performance in certain extreme scenarios, such as occlusion, while maintaining overall model accuracy. Although obtaining accurately annotated 3D human poses under occlusion is challenging, there is still a wealth of rich and precise 2D pose annotations that can be leveraged. However, existing works mostly focus on directly leveraging 2D pose coordinates to estimate 3D pose and mesh. In this paper, we present PostoMETRO($\textbf{Pos}$e $\textbf{to}$ken enhanced $\textbf{ME}$sh $\textbf{TR}$ansf$\textbf{O}$rmer), which integrates occlusion-resilient 2D pose representation into <b>transformers</b> in a token-wise manner. Utilizing a specialized pose tokenizer, we efficiently condense 2D pose data to a compact sequence of pose tokens and feed them to the <b>transformer</b> together with the image tokens. This process not only ensures a rich depiction of texture from the image but also fosters a robust integration of pose and image information. Subsequently, these combined tokens are queried by vertex and joint tokens to decode 3D coordinates of mesh vertices and human joints. Facilitated by the robust pose token representation and the effective combination, we are able to produce more precise 3D coordinates, even under extreme scenarios like occlusion. Experiments on both standard and occlusion-specific <b>benchmarks</b> demonstrate the effectiveness of PostoMETRO. Qualitative results further illustrate the clarity of how 2D pose can help 3D reconstruction. Code will be made available.</p></p class="citation"></blockquote><h3 id=76104--180329-sc-diff-3d-shape-completion-with-latent-diffusion-models-juan-d-galvis-et-al-2024>(76/104 | 180/329) SC-Diff: 3D Shape Completion with Latent Diffusion Models (Juan D. Galvis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan D. Galvis, Xingxing Zuo, Simon Schaefer, Stefan Leutengger. (2024)<br><strong>SC-Diff: 3D Shape Completion with Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="SC-Diff: 3D Shape Completion with Latent Diffusion Models" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12470v1.pdf filename=2403.12470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a 3D shape completion approach using a 3D latent <b>diffusion</b> <b>model</b> optimized for completing shapes, represented as Truncated Signed Distance Functions (TSDFs), from partial 3D scans. Our method combines image-based conditioning through cross-attention and spatial conditioning through the integration of 3D features from captured partial scans. This dual guidance enables high-fidelity, realistic shape completions at superior resolutions. At the core of our approach is the compression of 3D data into a low-dimensional latent space using an auto-encoder inspired by 2D latent <b>diffusion</b> <b>models.</b> This compression facilitates the processing of higher-resolution shapes and allows us to apply our model across multiple object classes, a significant improvement over other existing <b>diffusion-based</b> <b>shape</b> completion methods, which often require a separate <b>diffusion</b> <b>model</b> for each class. We validated our approach against two common <b>benchmarks</b> in the field of shape completion, demonstrating competitive performance in terms of accuracy and realism and performing on par with state-of-the-art methods despite operating at a higher resolution with a single model for all object classes. We present a comprehensive evaluation of our model, showcasing its efficacy in handling diverse shape completion challenges, even on unseen object classes. The code will be released upon acceptance.</p></p class="citation"></blockquote><h3 id=77104--181329-few-shot-object-localization-yunhan-ren-et-al-2024>(77/104 | 181/329) Few-shot Object Localization (Yunhan Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunhan Ren, Bo Li, Chengyang Zhang, Yong Zhang, Baocai Yin. (2024)<br><strong>Few-shot Object Localization</strong><br><button class=copy-to-clipboard title="Few-shot Object Localization" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12466v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12466v2.pdf filename=2403.12466v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing object localization methods are tailored to locate a specific class of objects, relying on abundant labeled data for model optimization. However, in numerous real-world scenarios, acquiring large labeled data can be arduous, significantly constraining the broader application of localization models. To bridge this research gap, this paper proposes the novel task of <b>Few-Shot</b> Object Localization (FSOL), which seeks to achieve precise localization with limited samples available. This task achieves generalized object localization by leveraging a small number of labeled support samples to query the positional information of objects within corresponding images. To advance this research field, we propose an innovative high-performance baseline model. Our model integrates a dual-path feature augmentation module to enhance shape association and gradient differences between supports and query images, alongside a self query module designed to explore the association between feature maps and query images. Experimental results demonstrate a significant performance improvement of our approach in the FSOL task, establishing an efficient <b>benchmark</b> for further research. All codes and data are available at <a href=https://github.com/Ryh1218/FSOL>https://github.com/Ryh1218/FSOL</a>.</p></p class="citation"></blockquote><h3 id=78104--182329-eye-gaze-guided-multi-modal-alignment-framework-for-radiology-chong-ma-et-al-2024>(78/104 | 182/329) Eye-gaze Guided Multi-modal Alignment Framework for Radiology (Chong Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chong Ma, Hanqi Jiang, Wenting Chen, Zihao Wu, Xiaowei Yu, Fang Zeng, Lei Guo, Dajiang Zhu, Tuo Zhang, Dinggang Shen, Tianming Liu, Xiang Li. (2024)<br><strong>Eye-gaze Guided Multi-modal Alignment Framework for Radiology</strong><br><button class=copy-to-clipboard title="Eye-gaze Guided Multi-modal Alignment Framework for Radiology" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T07, I-2-0; I-4-0; I-5-4; I-7-0, cs-CL, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12416v1.pdf filename=2403.12416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>multi-modal</b> frameworks, the alignment of cross-modal features presents a significant challenge. The predominant approach in <b>multi-modal</b> pre-training emphasizes either global or local alignment between modalities, utilizing extensive datasets. This bottom-up driven method often suffers from a lack of interpretability, a critical concern in radiology. Previous studies have integrated high-level labels in medical images or text, but these still rely on manual annotation, a costly and labor-intensive process. Our work introduces a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. This data, indicating radiologists&rsquo; focus areas, naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze Guided <b>Multi-modal</b> Alignment (EGMA) framework to harness eye-gaze data for better alignment of image and text features, aiming to reduce reliance on manual annotations and thus cut training costs. Our model demonstrates robust performance, outperforming other state-of-the-art methods in <b>zero-shot</b> classification and retrieval tasks. The incorporation of easily-obtained eye-gaze data during routine radiological diagnoses signifies a step towards minimizing manual annotation dependency. Additionally, we explore the impact of varying amounts of eye-gaze data on model performance, highlighting the feasibility and utility of integrating this auxiliary data into <b>multi-modal</b> pre-training.</p></p class="citation"></blockquote><h3 id=79104--183329-depth-guided-nerf-training-via-earth-movers-distance-anita-rau-et-al-2024>(79/104 | 183/329) Depth-guided NeRF Training via Earth Mover&rsquo;s Distance (Anita Rau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy. (2024)<br><strong>Depth-guided NeRF Training via Earth Mover&rsquo;s Distance</strong><br><button class=copy-to-clipboard title="Depth-guided NeRF Training via Earth Mover's Distance" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13206v1.pdf filename=2403.13206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of predicted viewpoints. However, the photometric loss often does not provide enough information to disambiguate between different possible geometries yielding the same image. Previous work has thus incorporated depth supervision during NeRF training, leveraging dense predictions from pre-trained depth networks as pseudo-ground truth. While these depth priors are assumed to be perfect once filtered for noise, in practice, their accuracy is more challenging to capture. This work proposes a novel approach to uncertainty in depth priors for NeRF supervision. Instead of using custom-trained depth or uncertainty priors, we use off-the-shelf pretrained <b>diffusion</b> <b>models</b> to predict depth and capture uncertainty during the denoising process. Because we know that depth priors are prone to errors, we propose to supervise the ray termination distance distribution with Earth Mover&rsquo;s Distance instead of enforcing the rendered depth to replicate the depth prior exactly through L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth metrics by a large margin while maintaining performance on photometric measures.</p></p class="citation"></blockquote><h3 id=80104--184329-hermite-coordinate-interpolation-kernels-application-to-image-zooming-konstantinos-k-delibasis-et-al-2024>(80/104 | 184/329) Hermite coordinate interpolation kernels: application to image zooming (Konstantinos K. Delibasis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konstantinos K. Delibasis, Iro Oikonomou, Aristides I. Kechriniotis, Georgios N. Tsigaridas. (2024)<br><strong>Hermite coordinate interpolation kernels: application to image zooming</strong><br><button class=copy-to-clipboard title="Hermite coordinate interpolation kernels: application to image zooming" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13195v1.pdf filename=2403.13195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A number of basic image processing tasks, such as any geometric transformation require interpolation at subpixel image values. In this work we utilize the multidimensional coordinate Hermite spline interpolation defined on non-equal spaced, rectilinear grids and apply it to a very common image processing task, image zooming. Since Hermite interpolation utilizes function values, as well as partial derivative values, it is natural to apply it to image processing tasks as a special case of equi-spaced grid, using numerical approximations of the image partial derivatives at each pixel. Furthermore, the task of image interpolation requires the calculation of image values at positions with nono-zero fractional part. Thus, any spline interpolation can be written as <b>convolution</b> with an appropriate kernel. In this context we generate the Hermite kernels according to the derived $n-$dimensional interpolant of Theorem 2 in [1]. We show that despite the increased complexity of the interpolant, once the kernels are constructed, the Hermite spline interpolation can be applied to images as efficiently as any other less complicated method. Finally, we perform illustrative numerical examples to showcase the applicability and high accuracy of the proposed Hermite kernels for image zooming, compared to other interpolation methods, both traditional <b>convolution-based,</b> as well as employing deep learning, in terms of PSNR, as well as SSIM error metrics. The proposed Hermite spline kernels outperform all other methods in the majority of the test images, in experiments using many cascaded repetitions of the zoom operation. Interesting conclusions can be drawn considering all methods under comparison.</p></p class="citation"></blockquote><h3 id=81104--185329-3d-semantic-mapnet-building-maps-for-multi-object-re-identification-in-3d-vincent-cartillier-et-al-2024>(81/104 | 185/329) 3D Semantic MapNet: Building Maps for Multi-Object Re-Identification in 3D (Vincent Cartillier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Cartillier, Neha Jain, Irfan Essa. (2024)<br><strong>3D Semantic MapNet: Building Maps for Multi-Object Re-Identification in 3D</strong><br><button class=copy-to-clipboard title="3D Semantic MapNet: Building Maps for Multi-Object Re-Identification in 3D" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13190v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13190v1.pdf filename=2403.13190v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the task of 3D multi-object re-identification from embodied tours. Specifically, an agent is given two tours of an environment (e.g. an apartment) under two different layouts (e.g. arrangements of furniture). Its task is to detect and re-identify objects in 3D - e.g. a &ldquo;sofa&rdquo; moved from location A to B, a new &ldquo;chair&rdquo; in the second layout at location C, or a &ldquo;lamp&rdquo; from location D in the first layout missing in the second. To support this task, we create an automated infrastructure to generate paired egocentric tours of initial/modified layouts in the Habitat simulator using Matterport3D scenes, YCB and Google-scanned objects. We present 3D Semantic MapNet (3D-SMNet) - a two-stage re-identification model consisting of (1) a 3D object detector that operates on RGB-D videos with known pose, and (2) a differentiable object matching module that solves correspondence estimation between two sets of 3D bounding boxes. Overall, 3D-SMNet builds object-based maps of each layout and then uses a differentiable matcher to re-identify objects across the tours. After training 3D-SMNet on our generated episodes, we demonstrate <b>zero-shot</b> transfer to real-world rearrangement scenarios by instantiating our task in Replica, Active Vision, and RIO environments depicting rearrangements. On all datasets, we find 3D-SMNet outperforms competitive baselines. Further, we show jointly training on real and generated episodes can lead to significant improvements over training on real data alone.</p></p class="citation"></blockquote><h3 id=82104--186329-train-ego-path-detection-on-railway-tracks-using-end-to-end-deep-learning-thomas-laurent-2024>(82/104 | 186/329) Train Ego-Path Detection on Railway Tracks Using End-to-End Deep Learning (Thomas Laurent, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Laurent. (2024)<br><strong>Train Ego-Path Detection on Railway Tracks Using End-to-End Deep Learning</strong><br><button class=copy-to-clipboard title="Train Ego-Path Detection on Railway Tracks Using End-to-End Deep Learning" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13094v1.pdf filename=2403.13094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the task of &ldquo;train ego-path detection&rdquo;, a refined approach to railway track detection designed for intelligent onboard vision systems. Whereas existing research lacks precision and often considers all tracks within the visual field uniformly, our proposed task specifically aims to identify the train&rsquo;s immediate path, or &ldquo;ego-path&rdquo;, within potentially complex and dynamic railway environments. Building on this, we extend the RailSem19 dataset with ego-path annotations, facilitating further research in this direction. At the heart of our study lies TEP-Net, an end-to-end deep learning framework tailored for ego-path detection, featuring a configurable model architecture, a dynamic <b>data</b> <b>augmentation</b> strategy, and a domain-specific loss function. Leveraging a regression-based approach, TEP-Net outperforms SOTA: while addressing the track detection problem in a more nuanced way than previously, our model achieves 97.5% IoU on the test set and is faster than all existing methods. Further comparative analysis highlights the relevance of the conceptual choices behind TEP-Net, demonstrating its inherent propensity for robustness across diverse environmental conditions and operational dynamics. This work opens promising avenues for the development of intelligent driver assistance systems and autonomous train operations, paving the way toward safer and more efficient railway transportation.</p></p class="citation"></blockquote><h3 id=83104--187329-hulp-human-in-the-loop-for-prognosis-muhammad-ridzuan-et-al-2024>(83/104 | 187/329) HuLP: Human-in-the-Loop for Prognosis (Muhammad Ridzuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Ridzuan, Mai Kassem, Numan Saeed, Ikboljon Sobirov, Mohammad Yaqub. (2024)<br><strong>HuLP: Human-in-the-Loop for Prognosis</strong><br><button class=copy-to-clipboard title="HuLP: Human-in-the-Loop for Prognosis" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-HC, cs.CV<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13078v1.pdf filename=2403.13078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces HuLP, a <b>Human-in-the-Loop</b> for Prognosis model designed to enhance the reliability and interpretability of prognostic models in clinical contexts, especially when faced with the complexities of missing covariates and outcomes. HuLP offers an innovative approach that enables human expert intervention, empowering clinicians to interact with and correct models&rsquo; predictions, thus fostering collaboration between humans and AI models to produce more accurate prognosis. Additionally, HuLP addresses the challenges of missing data by utilizing neural networks and providing a tailored methodology that effectively handles missing data. Traditional methods often struggle to capture the nuanced variations within patient populations, leading to compromised prognostic predictions. HuLP imputes missing covariates based on imaging features, aligning more closely with clinician workflows and enhancing reliability. We conduct our experiments on two real-world, publicly available medical datasets to demonstrate the superiority of HuLP.</p></p class="citation"></blockquote><h3 id=84104--188329-magic-fixup-streamlining-photo-editing-by-watching-dynamic-videos-hadi-alzayer-et-al-2024>(84/104 | 188/329) Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos (Hadi Alzayer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hadi Alzayer, Zhihao Xia, Xuaner Zhang, Eli Shechtman, Jia-Bin Huang, Michael Gharbi. (2024)<br><strong>Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos</strong><br><button class=copy-to-clipboard title="Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13044v1.pdf filename=2403.13044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a generative model that, given a coarsely edited image, synthesizes a photorealistic output that follows the prescribed layout. Our method transfers fine details from the original image and preserves the identity of its parts. Yet, it adapts it to the lighting and context defined by the new layout. Our key insight is that videos are a powerful source of supervision for this task: objects and camera motions provide many observations of how the world changes with viewpoint, lighting, and physical interactions. We construct an image dataset in which each sample is a pair of source and target frames extracted from the same video at randomly chosen time intervals. We warp the source frame toward the target using two motion models that mimic the expected test-time user edits. We supervise our model to translate the warped image into the ground truth, starting from a pretrained <b>diffusion</b> <b>model.</b> Our model design explicitly enables fine detail transfer from the source frame to the generated image, while closely following the user-specified layout. We show that by using simple segmentations and coarse 2D manipulations, we can synthesize a photorealistic edit faithful to the user&rsquo;s input while addressing second-order effects like harmonizing the lighting and physical interactions between edited objects.</p></p class="citation"></blockquote><h3 id=85104--189329-textile-a-differentiable-metric-for-texture-tileability-carlos-rodriguez-pardo-et-al-2024>(85/104 | 189/329) TexTile: A Differentiable Metric for Texture Tileability (Carlos Rodriguez-Pardo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Rodriguez-Pardo, Dan Casas, Elena Garces, Jorge Lopez-Moreno. (2024)<br><strong>TexTile: A Differentiable Metric for Texture Tileability</strong><br><button class=copy-to-clipboard title="TexTile: A Differentiable Metric for Texture Tileability" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T07 (Primary) 68T45, 68U05 (Secondary), I-2-6; I-4-10; I-3-3; I-5-4; I-5-1; I-3-7; I-3-8; I-2-10, cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12961v1.pdf filename=2403.12961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability). Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture. In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures. Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom <b>data</b> <b>augmentation</b> and training regime aimed at increasing robustness and accuracy. We demonstrate that TexTile can be plugged into different state-of-the-art texture synthesis methods, including diffusion-based strategies, and generate tileable textures while keeping or even improving the overall texture quality. Furthermore, we show that TexTile can objectively evaluate any tileable texture synthesis method, whereas the current mix of existing metrics produces uncorrelated scores which heavily hinders progress in the field.</p></p class="citation"></blockquote><h3 id=86104--190329-segment-anything-for-comprehensive-analysis-of-grapevine-cluster-architecture-and-berry-properties-efrain-torres-lomas-et-al-2024>(86/104 | 190/329) Segment Anything for comprehensive analysis of grapevine cluster architecture and berry properties (Efrain Torres-Lomas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Efrain Torres-Lomas, Jimena Lado-Jimena, Guillermo Garcia-Zamora, Luis Diaz-Garcia. (2024)<br><strong>Segment Anything for comprehensive analysis of grapevine cluster architecture and berry properties</strong><br><button class=copy-to-clipboard title="Segment Anything for comprehensive analysis of grapevine cluster architecture and berry properties" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12935v1.pdf filename=2403.12935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Grape cluster architecture and compactness are complex traits influencing disease susceptibility, fruit quality, and yield. Evaluation methods for these traits include visual scoring, manual methodologies, and computer vision, with the latter being the most scalable approach. Most of the existing computer vision approaches for processing cluster images often rely on conventional segmentation or machine learning with extensive training and limited generalization. The Segment Anything Model (SAM), a novel <b>foundation</b> <b>model</b> trained on a massive image dataset, enables automated object segmentation without additional training. This study demonstrates out-of-the-box SAM&rsquo;s high accuracy in identifying individual berries in 2D cluster images. Using this model, we managed to segment approximately 3,500 cluster images, generating over 150,000 berry masks, each linked with spatial coordinates within their clusters. The correlation between human-identified berries and SAM predictions was very strong (Pearson r2=0.96). Although the visible berry count in images typically underestimates the actual cluster berry count due to visibility issues, we demonstrated that this discrepancy could be adjusted using a linear regression model (adjusted R2=0.87). We emphasized the critical importance of the angle at which the cluster is imaged, noting its substantial effect on berry counts and architecture. We proposed different approaches in which berry location information facilitated the calculation of complex features related to cluster architecture and compactness. Finally, we discussed SAM&rsquo;s potential integration into currently available pipelines for image generation and processing in vineyard conditions.</p></p class="citation"></blockquote><h3 id=87104--191329-zero-reference-low-light-enhancement-via-physical-quadruple-priors-wenjing-wang-et-al-2024>(87/104 | 191/329) Zero-Reference Low-Light Enhancement via Physical Quadruple Priors (Wenjing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjing Wang, Huan Yang, Jianlong Fu, Jiaying Liu. (2024)<br><strong>Zero-Reference Low-Light Enhancement via Physical Quadruple Priors</strong><br><button class=copy-to-clipboard title="Zero-Reference Low-Light Enhancement via Physical Quadruple Priors" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12933v1.pdf filename=2403.12933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding illumination and reducing the need for supervision pose a significant challenge in low-light enhancement. Current approaches are highly sensitive to data usage during training and illumination-specific hyper-parameters, limiting their ability to handle unseen scenarios. In this paper, we propose a new zero-reference low-light enhancement framework trainable solely with normal light images. To accomplish this, we devise an illumination-invariant prior inspired by the theory of physical light transfer. This prior serves as the bridge between normal and low-light images. Then, we develop a prior-to-image framework trained without low-light data. During testing, this framework is able to restore our illumination-invariant prior back to images, automatically achieving low-light enhancement. Within this framework, we leverage a pretrained generative <b>diffusion</b> <b>model</b> for model ability, introduce a bypass decoder to handle detail distortion, as well as offer a lightweight version for practicality. Extensive experiments demonstrate our framework&rsquo;s superiority in various scenarios as well as good interpretability, robustness, and efficiency. Code is available on our project homepage: <a href=http://daooshee.github.io/QuadPrior-Website/>http://daooshee.github.io/QuadPrior-Website/</a></p></p class="citation"></blockquote><h3 id=88104--192329-ddsb-an-unsupervised-and-training-free-method-for-phase-detection-in-echocardiography-zhenyu-bu-et-al-2024>(88/104 | 192/329) DDSB: An Unsupervised and Training-free Method for Phase Detection in Echocardiography (Zhenyu Bu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Bu, Yang Liu, Jiayu Huo, Jingjing Peng, Kaini Wang, Guangquan Zhou, Rachel Sparks, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin. (2024)<br><strong>DDSB: An Unsupervised and Training-free Method for Phase Detection in Echocardiography</strong><br><button class=copy-to-clipboard title="DDSB: An Unsupervised and Training-free Method for Phase Detection in Echocardiography" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12787v1.pdf filename=2403.12787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate identification of End-Diastolic (ED) and End-Systolic (ES) frames is key for cardiac function assessment through echocardiography. However, traditional methods face several limitations: they require extensive amounts of data, extensive annotations by medical experts, significant training resources, and often lack robustness. Addressing these challenges, we proposed an <b>unsupervised</b> and training-free method, our novel approach leverages <b>unsupervised</b> segmentation to enhance fault tolerance against segmentation inaccuracies. By identifying anchor points and analyzing directional deformation, we effectively reduce dependence on the accuracy of initial segmentation images and enhance fault tolerance, all while improving robustness. Tested on Echo-dynamic and CAMUS datasets, our method achieves comparable accuracy to learning-based models without their associated drawbacks. The code is available at <a href=https://github.com/MRUIL/DDSB>https://github.com/MRUIL/DDSB</a></p></p class="citation"></blockquote><h3 id=89104--193329-inter--and-intra-uncertainty-based-feature-aggregation-model-for-semi-supervised-histopathology-image-segmentation-qiangguo-jin-et-al-2024>(89/104 | 193/329) Inter- and intra-uncertainty based feature aggregation model for semi-supervised histopathology image segmentation (Qiangguo Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiangguo Jin, Hui Cui, Changming Sun, Yang Song, Jiangbin Zheng, Leilei Cao, Leyi Wei, Ran Su. (2024)<br><strong>Inter- and intra-uncertainty based feature aggregation model for semi-supervised histopathology image segmentation</strong><br><button class=copy-to-clipboard title="Inter- and intra-uncertainty based feature aggregation model for semi-supervised histopathology image segmentation" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12767v1.pdf filename=2403.12767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acquiring pixel-level annotations is often limited in applications such as histology studies that require domain expertise. Various <b>semi-supervised</b> <b>learning</b> approaches have been developed to work with limited ground truth annotations, such as the popular teacher-student models. However, hierarchical prediction uncertainty within the student model (intra-uncertainty) and image prediction uncertainty (inter-uncertainty) have not been fully utilized by existing methods. To address these issues, we first propose a novel inter- and intra-uncertainty regularization method to measure and constrain both inter- and intra-inconsistencies in the teacher-student architecture. We also propose a new two-stage network with pseudo-mask guided feature aggregation (PG-FANet) as the segmentation model. The two-stage structure complements with the uncertainty regularization strategy to avoid introducing extra modules in solving uncertainties and the aggregation mechanisms enable multi-scale and multi-stage feature integration. Comprehensive experimental results over the MoNuSeg and CRAG datasets show that our PG-FANet outperforms other state-of-the-art methods and our <b>semi-supervised</b> <b>learning</b> framework yields competitive performance with a limited amount of labeled data.</p></p class="citation"></blockquote><h3 id=90104--194329-building-brain-tumor-segmentation-networks-with-user-assisted-filter-estimation-and-selection-matheus-a-cerqueira-et-al-2024>(90/104 | 194/329) Building Brain Tumor Segmentation Networks with User-Assisted Filter Estimation and Selection (Matheus A. Cerqueira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matheus A. Cerqueira, Flávia Sprenger, Bernardo C. A. Teixeira, Alexandre X. Falcão. (2024)<br><strong>Building Brain Tumor Segmentation Networks with User-Assisted Filter Estimation and Selection</strong><br><button class=copy-to-clipboard title="Building Brain Tumor Segmentation Networks with User-Assisted Filter Estimation and Selection" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T07, 68T45, cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12748v1.pdf filename=2403.12748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brain tumor image segmentation is a challenging research topic in which deep-learning models have presented the best results. However, the traditional way of training those models from many pre-annotated images leaves several unanswered questions. Hence methodologies, such as Feature Learning from Image Markers (FLIM), have involved an expert in the learning loop to reduce human effort in data annotation and build models sufficiently deep for a given problem. FLIM has been successfully used to create encoders, estimating the filters of all <b>convolutional</b> layers from patches centered at marker voxels. In this work, we present Multi-Step (MS) FLIM - a user-assisted approach to estimating and selecting the most relevant filters from multiple FLIM executions. MS-FLIM is used only for the first <b>convolutional</b> layer, and the results already indicate improvement over FLIM. For evaluation, we build a simple U-shaped encoder-decoder network, named sU-Net, for glioblastoma segmentation using T1Gd and FLAIR MRI scans, varying the encoder&rsquo;s training method, using FLIM, MS-FLIM, and backpropagation algorithm. Also, we compared these sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two datasets. The results show that the sU-Net based on MS-FLIM outperforms the other training methods and achieves effectiveness within the standard deviations of the SOTA models.</p></p class="citation"></blockquote><h3 id=91104--195329-addressing-source-scale-bias-via-image-warping-for-domain-adaptation-shen-zheng-et-al-2024>(91/104 | 195/329) Addressing Source Scale Bias via Image Warping for Domain Adaptation (Shen Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shen Zheng, Anurag Ghosh, Srinivasa G. Narasimhan. (2024)<br><strong>Addressing Source Scale Bias via Image Warping for Domain Adaptation</strong><br><button class=copy-to-clipboard title="Addressing Source Scale Bias via Image Warping for Domain Adaptation" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12712v1.pdf filename=2403.12712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing &ndash; oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in <b>domain</b> <b>adaptation.</b> Our approach improves adaptation across geographies, lighting and weather conditions, is agnostic to the task, <b>domain</b> <b>adaptation</b> algorithm, saliency guidance, and underlying model architecture. Highlights include +6.1 mAP50 for BDD100K Clear $\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear $\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\rightarrow$ ACDC. Our approach adds minimal memory during training and has no additional latency at inference time. Please see Appendix for more results and analysis.</p></p class="citation"></blockquote><h3 id=92104--196329-audio-visual-compound-expression-recognition-method-based-on-late-modality-fusion-and-rule-based-decision-elena-ryumina-et-al-2024>(92/104 | 196/329) Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision (Elena Ryumina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elena Ryumina, Maxim Markitantov, Dmitry Ryumin, Heysem Kaya, Alexey Karpov. (2024)<br><strong>Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision</strong><br><button class=copy-to-clipboard title="Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12687v1.pdf filename=2403.12687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition. We propose a novel audio-visual method for compound expression recognition. Our method relies on <b>emotion</b> <b>recognition</b> models that fuse modalities at the <b>emotion</b> <b>probability</b> level, while decisions regarding the prediction of compound expressions are based on predefined rules. Notably, our method does not use any training data specific to the target task. The method is evaluated in multi-corpus training and cross-corpus validation setups. Our findings from the challenge demonstrate that the proposed method can potentially form a basis for development of intelligent tools for annotating audio-visual data in the context of human&rsquo;s basic and compound <b>emotions.</b> <b>The</b> source code is publicly available.</p></p class="citation"></blockquote><h3 id=93104--197329-hcpm-hierarchical-candidates-pruning-for-efficient-detector-free-matching-ying-chen-et-al-2024>(93/104 | 197/329) HCPM: Hierarchical Candidates Pruning for Efficient Detector-Free Matching (Ying Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Chen, Yong Liu, Kai Wu, Qiang Nie, Shang Xu, Huifang Ma, Bing Wang, Chengjie Wang. (2024)<br><strong>HCPM: Hierarchical Candidates Pruning for Efficient Detector-Free Matching</strong><br><button class=copy-to-clipboard title="HCPM: Hierarchical Candidates Pruning for Efficient Detector-Free Matching" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12543v1.pdf filename=2403.12543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based image matching methods play a crucial role in computer vision, yet they often suffer from substantial computational demands. To tackle this challenge, we present HCPM, an efficient and detector-free local feature-matching method that employs hierarchical <b>pruning</b> to optimize the matching pipeline. In contrast to recent detector-free methods that depend on an exhaustive set of coarse-level candidates for matching, HCPM selectively concentrates on a concise subset of informative candidates, resulting in fewer computational candidates and enhanced matching efficiency. The method comprises a self-pruning stage for selecting reliable candidates and an interactive-pruning stage that identifies correlated patches at the coarse level. Our results reveal that HCPM significantly surpasses existing methods in terms of speed while maintaining high accuracy. The source code will be made available upon publication.</p></p class="citation"></blockquote><h3 id=94104--198329-exact-language-guided-conceptual-reasoning-and-uncertainty-estimation-for-event-based-action-recognition-and-more-jiazhou-zhou-et-al-2024>(94/104 | 198/329) ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More (Jiazhou Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, Lin Wang. (2024)<br><strong>ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More</strong><br><button class=copy-to-clipboard title="ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12534v1.pdf filename=2403.12534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event cameras have recently been shown beneficial for practical vision tasks, such as action recognition, thanks to their high temporal resolution, power efficiency, and reduced privacy concerns. However, current research is hindered by 1) the difficulty in processing events because of their prolonged duration and dynamic actions with complex and ambiguous semantics and 2) the redundant action depiction of the event frame representation with fixed stacks. We find language naturally conveys abundant semantic information, rendering it stunningly superior in reducing semantic uncertainty. In light of this, we propose ExACT, a novel approach that, for the first time, tackles event-based action recognition from a cross-modal conceptualizing perspective. Our ExACT brings two technical contributions. Firstly, we propose an adaptive fine-grained event (AFE) representation to adaptively filter out the repeated events for the stationary objects while preserving dynamic ones. This subtly enhances the performance of ExACT without extra computational cost. Then, we propose a conceptual <b>reasoning-based</b> uncertainty estimation module, which simulates the recognition process to enrich the semantic representation. In particular, conceptual <b>reasoning</b> builds the temporal relation based on the action semantics, and uncertainty estimation tackles the semantic uncertainty of actions based on the distributional representation. Experiments show that our ExACT achieves superior recognition accuracy of 94.83%(+2.23%), 90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively.</p></p class="citation"></blockquote><h3 id=95104--199329-generalized-consistency-trajectory-models-for-image-manipulation-beomsu-kim-et-al-2024>(95/104 | 199/329) Generalized Consistency Trajectory Models for Image Manipulation (Beomsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beomsu Kim, Jaemin Kim, Jeongsol Kim, Jong Chul Ye. (2024)<br><strong>Generalized Consistency Trajectory Models for Image Manipulation</strong><br><button class=copy-to-clipboard title="Generalized Consistency Trajectory Models for Image Manipulation" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12510v1.pdf filename=2403.12510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion-based</b> <b>generative</b> models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of <b>diffusion</b> <b>models</b> lies in the iterative nature of <b>diffusion:</b> <b>diffusion</b> <b>breaks</b> down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbitrary distributions via ODEs. We discuss the design space of GCTMs and demonstrate their efficacy in various image manipulation tasks such as image-to-image translation, restoration, and editing. Code: \url{https://github.com/1202kbs/GCTM}</p></p class="citation"></blockquote><h3 id=96104--200329-vq-nerv-a-vector-quantized-neural-representation-for-videos-yunjie-xu-et-al-2024>(96/104 | 200/329) VQ-NeRV: A Vector Quantized Neural Representation for Videos (Yunjie Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunjie Xu, Xiang Feng, Feiwei Qin, Ruiquan Ge, Yong Peng, Changmiao Wang. (2024)<br><strong>VQ-NeRV: A Vector Quantized Neural Representation for Videos</strong><br><button class=copy-to-clipboard title="VQ-NeRV: A Vector Quantized Neural Representation for Videos" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12401v1.pdf filename=2403.12401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit neural representations (INR) excel in encoding videos within neural networks, showcasing promise in computer vision tasks like video compression and denoising. INR-based approaches reconstruct video frames from content-agnostic embeddings, which hampers their efficacy in video frame regression and restricts their generalization ability for video interpolation. To address these deficiencies, Hybrid Neural Representation for Videos (HNeRV) was introduced with content-adaptive embeddings. Nevertheless, HNeRV&rsquo;s compression ratios remain relatively low, attributable to an oversight in leveraging the network&rsquo;s shallow features and inter-frame residual information. In this work, we introduce an advanced U-shaped architecture, Vector <b>Quantized-NeRV</b> (VQ-NeRV), which integrates a novel component&ndash;the VQ-NeRV Block. This block incorporates a codebook mechanism to discretize the network&rsquo;s shallow residual features and inter-frame residual information effectively. This approach proves particularly advantageous in video compression, as it results in smaller size compared to <b>quantized</b> features. Furthermore, we introduce an original codebook optimization technique, termed shallow codebook optimization, designed to refine the utility and efficiency of the codebook. The experimental evaluations indicate that VQ-NeRV outperforms HNeRV on video regression tasks, delivering superior reconstruction quality (with an increase of 1-2 dB in Peak Signal-to-Noise Ratio (PSNR)), better bit per pixel (bpp) efficiency, and improved video inpainting outcomes.</p></p class="citation"></blockquote><h3 id=97104--201329-embarrassingly-simple-scribble-supervision-for-3d-medical-segmentation-karol-gotkowski-et-al-2024>(97/104 | 201/329) Embarrassingly Simple Scribble Supervision for 3D Medical Segmentation (Karol Gotkowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karol Gotkowski, Carsten Lüth, Paul F. Jäger, Sebastian Ziegler, Lars Krämer, Stefan Denner, Shuhan Xiao, Nico Disch, Klaus H. Maier-Hein, Fabian Isensee. (2024)<br><strong>Embarrassingly Simple Scribble Supervision for 3D Medical Segmentation</strong><br><button class=copy-to-clipboard title="Embarrassingly Simple Scribble Supervision for 3D Medical Segmentation" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12834v1.pdf filename=2403.12834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditionally, segmentation algorithms require dense annotations for training, demanding significant annotation efforts, particularly within the 3D medical imaging field. Scribble-supervised learning emerges as a possible solution to this challenge, promising a reduction in annotation efforts when creating large-scale datasets. Recently, a plethora of methods for optimized learning from scribbles have been proposed, but have so far failed to position scribble annotation as a beneficial alternative. We relate this shortcoming to two major issues: 1) the complex nature of many methods which deeply ties them to the underlying segmentation model, thus preventing a migration to more powerful state-of-the-art models as the field progresses and 2) the lack of a systematic evaluation to validate consistent performance across the broader medical domain, resulting in a lack of trust when applying these methods to new segmentation problems. To address these issues, we propose a comprehensive scribble supervision <b>benchmark</b> consisting of seven datasets covering a diverse set of anatomies and pathologies imaged with varying modalities. We furthermore propose the systematic use of partial losses, i.e. losses that are only computed on annotated voxels. Contrary to most existing methods, these losses can be seamlessly integrated into state-of-the-art segmentation methods, enabling them to learn from scribble annotations while preserving their original loss formulations. Our evaluation using nnU-Net reveals that while most existing methods suffer from a lack of generalization, the proposed approach consistently delivers state-of-the-art performance. Thanks to its simplicity, our approach presents an embarrassingly simple yet effective solution to the challenges of scribble supervision. Source code as well as our extensive scribble <b>benchmarking</b> suite will be made publicly available upon publication.</p></p class="citation"></blockquote><h3 id=98104--202329-hugs-holistic-urban-3d-scene-understanding-via-gaussian-splatting-hongyu-zhou-et-al-2024>(98/104 | 202/329) HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting (Hongyu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao. (2024)<br><strong>HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting</strong><br><button class=copy-to-clipboard title="HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12722v1.pdf filename=2403.12722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Holistic understanding of urban scenes based on RGB images is a challenging yet important problem. It encompasses understanding both the <b>geometry</b> and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects. Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding. Our main idea involves the joint optimization of <b>geometry,</b> appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints. Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=99104--203329-vox-fusion-voxel-based-neural-implicit-dense-tracking-and-mapping-with-multi-maps-hongjia-zhai-et-al-2024>(99/104 | 203/329) Vox-Fusion++: Voxel-based Neural Implicit Dense Tracking and Mapping with Multi-maps (Hongjia Zhai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongjia Zhai, Hai Li, Xingrui Yang, Gan Huang, Yuhang Ming, Hujun Bao, Guofeng Zhang. (2024)<br><strong>Vox-Fusion++: Voxel-based Neural Implicit Dense Tracking and Mapping with Multi-maps</strong><br><button class=copy-to-clipboard title="Vox-Fusion++: Voxel-based Neural Implicit Dense Tracking and Mapping with Multi-maps" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12536v1.pdf filename=2403.12536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce Vox-Fusion++, a multi-maps-based robust dense tracking and mapping system that seamlessly fuses neural implicit representations with traditional volumetric fusion techniques. Building upon the concept of implicit mapping and positioning systems, our approach extends its applicability to real-world scenarios. Our system employs a voxel-based neural implicit surface representation, enabling efficient encoding and optimization of the scene within each voxel. To handle diverse environments without prior knowledge, we incorporate an octree-based structure for scene division and dynamic expansion. To achieve real-time performance, we propose a high-performance multi-process framework. This ensures the system&rsquo;s suitability for applications with stringent time constraints. Additionally, we adopt the idea of multi-maps to handle large-scale scenes, and leverage loop detection and hierarchical pose optimization strategies to reduce long-term pose drift and remove duplicate <b>geometry.</b> Through comprehensive evaluations, we demonstrate that our method outperforms previous methods in terms of reconstruction quality and accuracy across various scenarios. We also show that our Vox-Fusion++ can be used in augmented reality and collaborative mapping applications. Our source code will be publicly available at \url{https://github.com/zju3dv/Vox-Fusion_Plus_Plus}</p></p class="citation"></blockquote><h3 id=100104--204329-geometric-constraints-in-deep-learning-frameworks-a-survey-vibhas-k-vats-et-al-2024>(100/104 | 204/329) Geometric Constraints in Deep Learning Frameworks: A Survey (Vibhas K Vats et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vibhas K Vats, David J Crandall. (2024)<br><strong>Geometric Constraints in Deep Learning Frameworks: A Survey</strong><br><button class=copy-to-clipboard title="Geometric Constraints in Deep Learning Frameworks: A Survey" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12431v1.pdf filename=2403.12431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stereophotogrammetry is an emerging technique of scene understanding. Its origins go back to at least the 1800s when people first started to investigate using photographs to measure the physical properties of the world. Since then, thousands of approaches have been explored. The classic geometric techniques of Shape from Stereo is built on using <b>geometry</b> to define constraints on scene and camera <b>geometry</b> and then solving the non-linear systems of equations. More recent work has taken an entirely different approach, using end-to-end deep learning without any attempt to explicitly model the <b>geometry.</b> In this survey, we explore the overlap for geometric-based and deep learning-based frameworks. We compare and contrast <b>geometry</b> enforcing constraints integrated into a deep learning framework for depth estimation or other closely related problems. We present a new taxonomy for prevalent <b>geometry</b> enforcing constraints used in modern deep learning frameworks. We also present insightful observations and potential future research directions.</p></p class="citation"></blockquote><h3 id=101104--205329-whac-world-grounded-humans-and-cameras-wanqi-yin-et-al-2024>(101/104 | 205/329) WHAC: World-grounded Humans and Cameras (Wanqi Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Ziwei Liu, Lei Yang. (2024)<br><strong>WHAC: World-grounded Humans and Cameras</strong><br><button class=copy-to-clipboard title="WHAC: World-grounded Humans and Cameras" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12959v1.pdf filename=2403.12959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories. Extensive experiments on both standard and newly established <b>benchmarks</b> highlight the superiority and efficacy of our framework. We will make the code and dataset publicly available.</p></p class="citation"></blockquote><h3 id=102104--206329-futuredepth-learning-to-predict-the-future-improves-video-depth-estimation-rajeev-yasarla-et-al-2024>(102/104 | 206/329) FutureDepth: Learning to Predict the Future Improves Video Depth Estimation (Rajeev Yasarla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rajeev Yasarla, Manish Kumar Singh, Hong Cai, Yunxiao Shi, Jisoo Jeong, Yinhao Zhu, Shizhong Han, Risheek Garrepalli, Fatih Porikli. (2024)<br><strong>FutureDepth: Learning to Predict the Future Improves Video Depth Estimation</strong><br><button class=copy-to-clipboard title="FutureDepth: Learning to Predict the Future Improves Video Depth Estimation" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12953v1.pdf filename=2403.12953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel video depth estimation approach, FutureDepth, which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training. More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively. In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process. Additionally, to enrich the learning of multiframe correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multiframe feature volumes. At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network. Through extensive experiments on several <b>benchmarks,</b> i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that FutureDepth significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models</p></p class="citation"></blockquote><h3 id=103104--207329-selective-domain-invariant-feature-for-generalizable-deepfake-detection-yingxin-lai-et-al-2024>(103/104 | 207/329) Selective Domain-Invariant Feature for Generalizable Deepfake Detection (Yingxin Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingxin Lai, Guoqing Yang Yifan He, Zhiming Luo, Shaozi Li. (2024)<br><strong>Selective Domain-Invariant Feature for Generalizable Deepfake Detection</strong><br><button class=copy-to-clipboard title="Selective Domain-Invariant Feature for Generalizable Deepfake Detection" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12707v1.pdf filename=2403.12707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With diverse presentation forgery methods emerging continually, detecting the authenticity of images has drawn growing attention. Although existing methods have achieved impressive accuracy in training dataset detection, they still perform poorly in the unseen domain and suffer from forgery of irrelevant information such as background and identity, affecting generalizability. To solve this problem, we proposed a novel framework Selective Domain-Invariant Feature (SDIF), which reduces the sensitivity to face forgery by fusing content features and styles. Specifically, we first use a Farthest-Point Sampling (FPS) training strategy to construct a task-relevant style sample representation space for fusing with content features. Then, we propose a dynamic feature extraction module to generate features with diverse styles to improve the performance and effectiveness of the feature extractor. Finally, a domain separation strategy is used to retain domain-related features to help distinguish between real and fake faces. Both qualitative and quantitative results in existing <b>benchmarks</b> and proposals demonstrate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=104104--208329-class-and-region-adaptive-constraints-for-network-calibration-balamurali-murugesan-et-al-2024>(104/104 | 208/329) Class and Region-Adaptive Constraints for Network Calibration (Balamurali Murugesan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Balamurali Murugesan, Julio Silva-Rodriguez, Ismail Ben Ayed, Jose Dolz. (2024)<br><strong>Class and Region-Adaptive Constraints for Network Calibration</strong><br><button class=copy-to-clipboard title="Class and Region-Adaptive Constraints for Network Calibration" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12364v1.pdf filename=2403.12364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present a novel approach to calibrate segmentation networks that considers the inherent challenges posed by different categories and object regions. In particular, we present a formulation that integrates class and region-wise constraints into the learning objective, with multiple penalty weights to account for class and region differences. Finding the optimal penalty weights manually, however, might be unfeasible, and potentially hinder the optimization process. To overcome this limitation, we propose an approach based on Class and Region-Adaptive constraints (CRaC), which allows to learn the class and region-wise penalty weights during training. CRaC is based on a general Augmented Lagrangian method, a well-established technique in constrained optimization. Experimental results on two popular segmentation <b>benchmarks,</b> and two well-known segmentation networks, demonstrate the superiority of CRaC compared to existing approaches. The code is available at: <a href=https://github.com/Bala93/CRac/>https://github.com/Bala93/CRac/</a></p></p class="citation"></blockquote><h2 id=physicsacc-ph-1>physics.acc-ph (1)</h2><h3 id=11--209329-a-conditional-latent-autoregressive-recurrent-model-for-generation-and-forecasting-of-beam-dynamics-in-particle-accelerators-mahindra-rautela-et-al-2024>(1/1 | 209/329) A conditional latent autoregressive recurrent model for generation and forecasting of beam dynamics in particle accelerators (Mahindra Rautela et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahindra Rautela, Alan Williams, Alexander Scheinker. (2024)<br><strong>A conditional latent autoregressive recurrent model for generation and forecasting of beam dynamics in particle accelerators</strong><br><button class=copy-to-clipboard title="A conditional latent autoregressive recurrent model for generation and forecasting of beam dynamics in particle accelerators" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.acc-ph<br>Categories: cs-CV, cs-LG, physics-acc-ph, physics.acc-ph<br>Keyword Score: 80<br>Keywords: Autoencoder, Simulation, Simulator, Unsupervised Learning, Variational Autoencoder, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13858v1.pdf filename=2403.13858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Particle accelerators are complex systems that focus, guide, and accelerate intense charged particle beams to high energy. Beam diagnostics present a challenging problem due to limited non-destructive measurements, computationally demanding <b>simulations,</b> and inherent uncertainties in the system. We propose a two-step <b>unsupervised</b> deep learning framework named as Conditional Latent Autoregressive Recurrent Model (CLARM) for learning the spatiotemporal dynamics of charged particles in accelerators. CLARM consists of a Conditional <b>Variational</b> <b>Autoencoder</b> (CVAE) transforming six-dimensional phase space into a lower-dimensional latent distribution and a <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> network capturing temporal dynamics in an autoregressive manner. The CLARM can generate projections at various accelerator modules by sampling and decoding the latent space representation. The model also forecasts future states (downstream locations) of charged particles from past states (upstream locations). The results demonstrate that the generative and forecasting ability of the proposed approach is promising when tested against a variety of evaluation metrics.</p></p class="citation"></blockquote><h2 id=csro-24>cs.RO (24)</h2><h3 id=124--210329-btgenbot-behavior-tree-generation-for-robotic-tasks-with-lightweight-llms-riccardo-andrea-izzo-et-al-2024>(1/24 | 210/329) BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs (Riccardo Andrea Izzo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Andrea Izzo, Gianluca Bardaro, Matteo Matteucci. (2024)<br><strong>BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs</strong><br><button class=copy-to-clipboard title="BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12761v1.pdf filename=2403.12761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach to generating behavior trees for robots using lightweight <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with a maximum of 7 billion parameters. The study demonstrates that it is possible to achieve satisfying results with compact <b>LLMs</b> when <b>fine-tuned</b> on a specific dataset. The key contributions of this research include the creation of a <b>fine-tuning</b> dataset based on existing behavior trees using <b>GPT-3.5</b> and a comprehensive comparison of multiple <b>LLMs</b> (namely llama2, <b>llama-chat,</b> and code-llama) across nine distinct tasks. To be thorough, we evaluated the generated behavior trees using static syntactical analysis, a validation system, a simulated environment, and a real robot. Furthermore, this work opens the possibility of deploying such solutions directly on the robot, enhancing its practical applicability. Findings from this study demonstrate the potential of <b>LLMs</b> with a limited number of parameters in generating effective and efficient robot behaviors.</p></p class="citation"></blockquote><h3 id=224--211329-towards-robots-that-know-when-they-need-help-affordance-based-uncertainty-for-large-language-model-planners-james-f-mullen-jr-et-al-2024>(2/24 | 211/329) Towards Robots That Know When They Need Help: Affordance-Based Uncertainty for Large Language Model Planners (James F. Mullen Jr. et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James F. Mullen Jr., Dinesh Manocha. (2024)<br><strong>Towards Robots That Know When They Need Help: Affordance-Based Uncertainty for Large Language Model Planners</strong><br><button class=copy-to-clipboard title="Towards Robots That Know When They Need Help: Affordance-Based Uncertainty for Large Language Model Planners" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Human Intervention, Simulation, Simulator, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13198v1.pdf filename=2403.13198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> showcase many desirable traits for intelligent and helpful robots. However, they are also known to hallucinate predictions. This issue is exacerbated in consumer robotics where <b>LLM</b> hallucinations may result in robots confidently executing plans that are contrary to user goals, relying more frequently on <b>human</b> <b>assistance,</b> or preventing the robot from asking for help at all. In this work, we present LAP, a novel approach for utilizing off-the-shelf <b>LLM&rsquo;s,</b> alongside scene and object Affordances, in robotic Planners that minimize harmful hallucinations and know when to ask for help. Our key finding is that calculating and leveraging a scene affordance score, a measure of whether a given action is possible in the provided scene, helps to mitigate hallucinations in <b>LLM</b> predictions and better align the <b>LLM&rsquo;s</b> confidence measure with the probability of success. We specifically propose and test three different affordance scores, which can be used independently or in tandem to improve performance across different use cases. The most successful of these individual scores involves <b>prompting</b> an <b>LLM</b> to determine if a given action is possible and safe in the given scene and uses the <b>LLM&rsquo;s</b> response to compute the score. Through experiments in both <b>simulation</b> and the real world, on tasks with a variety of ambiguities, we show that LAP significantly increases success rate and decreases the amount of <b>human</b> <b>intervention</b> required relative to prior art. For example, in our real-world testing paradigm, LAP decreases the <b>human</b> <b>help</b> rate of previous methods by over 33% at a success rate of 70%.</p></p class="citation"></blockquote><h3 id=324--212329-to-help-or-not-to-help-llm-based-attentive-support-for-human-robot-group-interactions-daniel-tanneberg-et-al-2024>(3/24 | 212/329) To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions (Daniel Tanneberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Tanneberg, Felix Ocker, Stephan Hasler, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Heiko Wersing, Bernhard Sendhoff, Michael Gienger. (2024)<br><strong>To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions</strong><br><button class=copy-to-clipboard title="To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-8; I-2-9, cs-AI, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12533v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12533v1.pdf filename=2403.12533v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the <b>common-sense</b> <b>reasoning</b> capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot&rsquo;s attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.</p></p class="citation"></blockquote><h3 id=424--213329-cadre-controllable-and-diverse-generation-of-safety-critical-driving-scenarios-using-real-world-trajectories-peide-huang-et-al-2024>(4/24 | 213/329) CaDRE: Controllable and Diverse Generation of Safety-Critical Driving Scenarios using Real-World Trajectories (Peide Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peide Huang, Wenhao Ding, Jonathan Francis, Bingqing Chen, Ding Zhao. (2024)<br><strong>CaDRE: Controllable and Diverse Generation of Safety-Critical Driving Scenarios using Real-World Trajectories</strong><br><button class=copy-to-clipboard title="CaDRE: Controllable and Diverse Generation of Safety-Critical Driving Scenarios using Real-World Trajectories" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 35<br>Keywords: Black Box, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13208v1.pdf filename=2403.13208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Simulation</b> is an indispensable tool in the development and testing of autonomous vehicles (AVs), offering an efficient and safe alternative to road testing by allowing the exploration of a wide range of scenarios. Despite its advantages, a significant challenge within <b>simulation-based</b> testing is the generation of safety-critical scenarios, which are essential to ensure that AVs can handle rare but potentially fatal situations. This paper addresses this challenge by introducing a novel generative framework, CaDRE, which is specifically designed for generating diverse and controllable safety-critical scenarios using real-world trajectories. Our approach optimizes for both the quality and diversity of scenarios by employing a unique formulation and algorithm that integrates real-world data, domain knowledge, and <b>black-box</b> <b>optimization</b> techniques. We validate the effectiveness of our framework through extensive testing in three representative types of traffic scenarios. The results demonstrate superior performance in generating diverse and high-quality scenarios with greater sample efficiency than existing <b>reinforcement</b> <b>learning</b> and sampling-based methods.</p></p class="citation"></blockquote><h3 id=524--214329-d-cubed-latent-diffusion-trajectory-optimisation-for-dexterous-deformable-manipulation-jun-yamada-et-al-2024>(5/24 | 214/329) D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation (Jun Yamada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Yamada, Shaohong Zhong, Jack Collins, Ingmar Posner. (2024)<br><strong>D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation</strong><br><button class=copy-to-clipboard title="D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12861v1.pdf filename=2403.12861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mastering dexterous robotic manipulation of deformable objects is vital for overcoming the limitations of parallel grippers in real-world applications. Current trajectory optimisation approaches often struggle to solve such tasks due to the large search space and the limited task information available from a cost function. In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent <b>diffusion</b> <b>model</b> (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions in the play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory in the dataset. To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse <b>diffusion</b> <b>process.</b> In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in <b>simulation.</b> Then, D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process. This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse <b>diffusion</b> <b>process.</b> Through empirical evaluation on a public <b>benchmark</b> of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin. We further demonstrate that trajectories found by D-Cubed readily transfer to a real-world LEAP hand on a folding task.</p></p class="citation"></blockquote><h3 id=624--215329-yell-at-your-robot-improving-on-the-fly-from-language-corrections-lucy-xiaoyang-shi-et-al-2024>(6/24 | 215/329) Yell At Your Robot: Improving On-the-Fly from Language Corrections (Lucy Xiaoyang Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn. (2024)<br><strong>Yell At Your Robot: Improving On-the-Fly from Language Corrections</strong><br><button class=copy-to-clipboard title="Yell At Your Robot: Improving On-the-Fly from Language Corrections" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Supervised Learning, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12910v1.pdf filename=2403.12910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either <b>zero-shot</b> high-level planners like pretrained language and <b>vision-language</b> models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge &ndash; the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily <b>supervised</b> with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (&ldquo;move a bit to the left&rdquo;), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy&rsquo;s ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at <a href=https://yay-robot.github.io/>https://yay-robot.github.io/</a>.</p></p class="citation"></blockquote><h3 id=724--216329-footstepnet-an-efficient-actor-critic-method-for-fast-on-line-bipedal-footstep-planning-and-forecasting-clément-gaspard-et-al-2024>(7/24 | 216/329) FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting (Clément Gaspard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clément Gaspard, Grégoire Passault, Mélodie Daniel, Olivier Ly. (2024)<br><strong>FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting</strong><br><button class=copy-to-clipboard title="FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12589v1.pdf filename=2403.12589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Designing a humanoid locomotion controller is challenging and classically split up in sub-problems. Footstep planning is one of those, where the sequence of footsteps is defined. Even in simpler environments, finding a minimal sequence, or even a feasible sequence, yields a complex optimization problem. In the literature, this problem is usually addressed by search-based algorithms (e.g. variants of A*). However, such approaches are either computationally expensive or rely on hand-crafted tuning of several parameters. In this work, at first, we propose an efficient footstep planning method to navigate in local environments with obstacles, based on state-of-the art Deep <b>Reinforcement</b> <b>Learning</b> (DRL) techniques, with very low computational requirements for on-line inference. Our approach is heuristic-free and relies on a continuous set of actions to generate feasible footsteps. In contrast, other methods necessitate the selection of a relevant discrete set of actions. Second, we propose a forecasting method, allowing to quickly estimate the number of footsteps required to reach different candidates of local targets. This approach relies on inherent computations made by the actor-critic DRL architecture. We demonstrate the validity of our approach with <b>simulation</b> results, and by a deployment on a kid-size humanoid robot during the RoboCup 2023 competition.</p></p class="citation"></blockquote><h3 id=824--217329-the-interplay-between-symmetries-and-impact-effects-on-hybrid-mechanical-systems-william-clark-et-al-2024>(8/24 | 217/329) The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical Systems (William Clark et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Clark, Leonardo Colombo, Anthony Bloch. (2024)<br><strong>The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical Systems</strong><br><button class=copy-to-clipboard title="The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical Systems" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO, math-DG<br>Keyword Score: 25<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12842v1.pdf filename=2403.12842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hybrid systems are dynamical systems with <b>continuous-time</b> <b>and</b> <b>discrete-time</b> <b>components</b> in their dynamics. When hybrid systems are defined on a principal bundle we are able to define two classes of impacts for the <b>discrete-time</b> <b>transition</b> of the dynamics: interior impacts and exterior impacts. In this paper we define hybrid systems on principal bundles, study the underlying <b>geometry</b> on the switching surface where impacts occur and we find conditions for which both exterior and interior impacts are preserved by the mechanical connection induced in the principal bundle.</p></p class="citation"></blockquote><h3 id=924--218329-meta-learning-for-fast-adaptation-in-intent-inferral-on-a-robotic-hand-orthosis-for-stroke-pedro-leandro-la-rotta-et-al-2024>(9/24 | 218/329) Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand Orthosis for Stroke (Pedro Leandro La Rotta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pedro Leandro La Rotta, Jingxi Xu, Ava Chen, Lauren Winterbottom, Wenxi Chen, Dawn Nilsen, Joel Stein, Matei Ciocarlie. (2024)<br><strong>Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand Orthosis for Stroke</strong><br><button class=copy-to-clipboard title="Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand Orthosis for Stroke" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Fine-tuning, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13147v1.pdf filename=2403.13147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose MetaEMG, a <b>meta-learning</b> <b>approach</b> for fast adaptation in intent inferral on a robotic hand orthosis for stroke. One key challenge in machine learning for assistive and rehabilitative robotics with disabled-bodied subjects is the difficulty of collecting labeled training data. Muscle tone and spasticity often vary significantly among stroke subjects, and hand function can even change across different use sessions of the device for the same subject. We investigate the use of <b>meta-learning</b> <b>to</b> mitigate the burden of data collection needed to adapt high-capacity neural networks to a new session or subject. Our experiments on real clinical data collected from five stroke subjects show that MetaEMG can improve the intent inferral accuracy with a small session- or subject-specific dataset and very few <b>fine-tuning</b> epochs. To the best of our knowledge, we are the first to formulate intent inferral on stroke subjects as a <b>meta-learning</b> <b>problem</b> and demonstrate fast adaptation to a new session or subject for controlling a robotic hand orthosis with EMG signals.</p></p class="citation"></blockquote><h3 id=1024--219329-vid2robot-end-to-end-video-conditioned-policy-learning-with-cross-attention-transformers-vidhi-jain-et-al-2024>(10/24 | 219/329) Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers (Vidhi Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta Dwibedi. (2024)<br><strong>Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers</strong><br><button class=copy-to-clipboard title="Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12943v1.pdf filename=2403.12943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot&rsquo;s ability to decode human intent and translate it into executable actions within its physical constraints and environment. We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse <b>prompt</b> video features to the robot&rsquo;s current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment between human and robot video representations. We evaluate Vid2Robot on real-world robots, demonstrating a 20% improvement in performance compared to other video-conditioned policies when using human demonstration videos. Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions from one object to another, and long-horizon composition, thus showcasing its potential for real-world applications. Project website: vid2robot.github.io</p></p class="citation"></blockquote><h3 id=1124--220329-semantic-layering-in-room-segmentation-via-llms-taehyeon-kim-et-al-2024>(11/24 | 220/329) Semantic Layering in Room Segmentation via LLMs (Taehyeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taehyeon Kim, Byung-Cheol Min. (2024)<br><strong>Semantic Layering in Room Segmentation via LLMs</strong><br><button class=copy-to-clipboard title="Semantic Layering in Room Segmentation via LLMs" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12920v1.pdf filename=2403.12920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce Semantic Layering in Room Segmentation via <b>LLMs</b> (SeLRoS), an advanced method for semantic room segmentation by integrating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with traditional 2D map-based segmentation. Unlike previous approaches that solely focus on the geometric segmentation of indoor environments, our work enriches segmented maps with semantic data, including object identification and spatial relationships, to enhance robotic navigation. By leveraging <b>LLMs,</b> we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation. Furthermore, SeLRoS overcomes the limitations of existing algorithms by using a semantic evaluation method to accurately distinguish true room divisions from those erroneously generated by furniture and segmentation inaccuracies. The effectiveness of SeLRoS is verified through its application across 30 different 3D environments. Source code and experiment videos for this work are available at: <a href=https://sites.google.com/view/selros>https://sites.google.com/view/selros</a>.</p></p class="citation"></blockquote><h3 id=1224--221329-pe-planner-a-performance-enhanced-quadrotor-motion-planner-for-autonomous-flight-in-complex-and-dynamic-environments-jiaxin-qiu-et-al-2024>(12/24 | 221/329) PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for Autonomous Flight in Complex and Dynamic Environments (Jiaxin Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxin Qiu, Qingchen Liu, Jiahu Qin, Dewang Cheng, Yawei Tian, Qichao Ma. (2024)<br><strong>PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for Autonomous Flight in Complex and Dynamic Environments</strong><br><button class=copy-to-clipboard title="PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for Autonomous Flight in Complex and Dynamic Environments" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12865v1.pdf filename=2403.12865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The role of a motion planner is pivotal in quadrotor applications, yet existing methods often struggle to adapt to complex environments, limiting their ability to achieve fast, safe, and robust flight. In this letter, we introduce a performance-enhanced quadrotor motion planner designed for autonomous flight in complex environments including dense obstacles, dynamic obstacles, and unknown disturbances. The global planner generates an initial trajectory through kinodynamic path searching and refines it using B-spline trajectory optimization. Subsequently, the local planner takes into account the quadrotor dynamics, estimated disturbance, global reference trajectory, control cost, time cost, and safety constraints to generate real-time control inputs, utilizing the framework of model predictive contouring control. Both <b>simulations</b> and real-world experiments corroborate the heightened robustness, safety, and speed of the proposed motion planner. Additionally, our motion planner achieves flights at more than 6.8 m/s in a challenging and complex racing scenario.</p></p class="citation"></blockquote><h3 id=1324--222329-shared-autonomy-via-variable-impedance-control-and-virtual-potential-fields-for-encoding-human-demonstration-shail-jadav-et-al-2024>(13/24 | 222/329) Shared Autonomy via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstration (Shail Jadav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shail Jadav, Johannes Heidersberger, Christian Ott, Dongheui Lee. (2024)<br><strong>Shared Autonomy via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstration</strong><br><button class=copy-to-clipboard title="Shared Autonomy via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstration" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12720v1.pdf filename=2403.12720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture. For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner. Therefore, two key components are addressed in this work: motion generation and shared autonomy. We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance. Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority. Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces. System passivity is ensured by an energy-tank based task passivation strategy. The framework&rsquo;s efficacy is validated through <b>simulations</b> and an experimental study employing a Franka Emika Research 3 robot.</p></p class="citation"></blockquote><h3 id=1424--223329-dynamic-manipulation-of-deformable-objects-using-imitation-learning-with-adaptation-to-hardware-constraints-eric-hannus-et-al-2024>(14/24 | 223/329) Dynamic Manipulation of Deformable Objects using Imitation Learning with Adaptation to Hardware Constraints (Eric Hannus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Hannus, Tran Nguyen Le, David Blanco-Mulero, Ville Kyrki. (2024)<br><strong>Dynamic Manipulation of Deformable Objects using Imitation Learning with Adaptation to Hardware Constraints</strong><br><button class=copy-to-clipboard title="Dynamic Manipulation of Deformable Objects using Imitation Learning with Adaptation to Hardware Constraints" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12685v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12685v1.pdf filename=2403.12685v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation Learning (IL) is a promising paradigm for learning dynamic manipulation of deformable objects since it does not depend on difficult-to-create accurate <b>simulations</b> of such objects. However, the translation of motions demonstrated by a human to a robot is a challenge for IL, due to differences in the embodiments and the robot&rsquo;s physical limits. These limits are especially relevant in dynamic manipulation where high velocities and accelerations are typical. To address this problem, we propose a framework that first maps a dynamic demonstration into a motion that respects the robot&rsquo;s constraints using a constrained Dynamic Movement Primitive. Second, the resulting object state is further optimized by quasi-static refinement motions to optimize task performance metrics. This allows both efficiently altering the object state by dynamic motions and stable small-scale refinements. We evaluate the framework in the challenging task of bag opening, designing the system BILBO: Bimanual dynamic manipulation using Imitation Learning for Bag Opening. Our results show that BILBO can successfully open a wide range of crumpled bags, using a demonstration with a single bag. See supplementary material at <a href=https://sites.google.com/view/bilbo-bag>https://sites.google.com/view/bilbo-bag</a>.</p></p class="citation"></blockquote><h3 id=1524--224329-ton-vio-online-time-offset-modeling-networks-for-robust-temporal-alignment-in-high-dynamic-motion-vio-chaoran-xiong-et-al-2024>(15/24 | 224/329) TON-VIO: Online Time Offset Modeling Networks for Robust Temporal Alignment in High Dynamic Motion VIO (Chaoran Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoran Xiong, Guoqing Liu, Qi Wu, Songpengcheng Xia, Tong Hua, Kehui Ma, Zhen Sun, Yan Xiang, Ling Pei. (2024)<br><strong>TON-VIO: Online Time Offset Modeling Networks for Robust Temporal Alignment in High Dynamic Motion VIO</strong><br><button class=copy-to-clipboard title="TON-VIO: Online Time Offset Modeling Networks for Robust Temporal Alignment in High Dynamic Motion VIO" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12504v1.pdf filename=2403.12504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal misalignment (time offset) between sensors is common in low cost visual-inertial odometry (VIO) systems. Such temporal misalignment introduces inconsistent constraints for state estimation, leading to a significant positioning drift especially in high dynamic motion scenarios. In this article, we focus on online temporal calibration to reduce the positioning drift caused by the time offset for high dynamic motion VIO. For the time offset observation model, most existing methods rely on accurate state estimation or stable visual tracking. For the prediction model, current methods oversimplify the time offset as a constant value with white Gaussian noise. However, these ideal conditions are seldom satisfied in real high dynamic scenarios, resulting in the poor performance. In this paper, we introduce online time offset modeling networks (TON) to enhance real-time temporal calibration. TON improves the accuracy of time offset observation and prediction modeling. Specifically, for observation modeling, we propose feature velocity observation networks to enhance velocity computation for features in unstable visual tracking conditions. For prediction modeling, we present time offset prediction networks to learn its evolution pattern. To highlight the effectiveness of our method, we integrate the proposed TON into both optimization-based and filter-based VIO systems. <b>Simulation</b> and real-world experiments are conducted to demonstrate the enhanced performance of our approach. Additionally, to contribute to the VIO community, we will open-source the code of our method on: <a href=https://github.com/Franky-X/FVON-TPN>https://github.com/Franky-X/FVON-TPN</a>.</p></p class="citation"></blockquote><h3 id=1624--225329-theoretical-modeling-and-bio-inspired-trajectory-optimization-of-a-multiple-locomotion-origami-robot-keqi-zhu-et-al-2024>(16/24 | 225/329) Theoretical Modeling and Bio-inspired Trajectory Optimization of A Multiple-locomotion Origami Robot (Keqi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keqi Zhu, Haotian Guo, Wei Yu, Hassen Nigatu, Tong Li, Huixu Dong. (2024)<br><strong>Theoretical Modeling and Bio-inspired Trajectory Optimization of A Multiple-locomotion Origami Robot</strong><br><button class=copy-to-clipboard title="Theoretical Modeling and Bio-inspired Trajectory Optimization of A Multiple-locomotion Origami Robot" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12471v1.pdf filename=2403.12471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research on mobile robots has focused on increasing their adaptability to unpredictable and unstructured environments using soft materials and structures. However, the determination of key design parameters and control over these compliant robots are predominantly iterated through experiments, lacking a solid theoretical foundation. To improve their efficiency, this paper aims to provide mathematics modeling over two locomotion, crawling and swimming. Specifically, a dynamic model is first devised to reveal the influence of the contact surfaces&rsquo; frictional coefficients on displacements in different motion phases. Besides, a swimming kinematics model is provided using coordinate transformation, based on which, we further develop an algorithm that systematically plans human-like swimming gaits, with maximum thrust obtained. The proposed algorithm is highly generalizable and has the potential to be applied in other soft robots with multiple joints. <b>Simulation</b> experiments have been conducted to illustrate the effectiveness of the proposed modeling.</p></p class="citation"></blockquote><h3 id=1724--226329-multi-object-ransac-efficient-plane-clustering-method-in-a-clutter-seunghyeon-lim-et-al-2024>(17/24 | 226/329) Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter (Seunghyeon Lim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seunghyeon Lim, Youngjae Yoo, Jun Ki Lee, Byoung-Tak Zhang. (2024)<br><strong>Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter</strong><br><button class=copy-to-clipboard title="Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Clustering, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12449v1.pdf filename=2403.12449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel method for plane <b>clustering</b> specialized in cluttered scenes using an RGB-D camera and validate its effectiveness through robot grasping experiments. Unlike existing methods, which focus on large-scale indoor structures, our approach &ndash; Multi-Object RANSAC emphasizes cluttered environments that contain a wide range of objects with different scales. It enhances plane segmentation by generating subplanes in Deep Plane <b>Clustering</b> (DPC) module, which are then merged with the final planes by post-processing. DPC rearranges the point cloud by voting layers to make subplane clusters, trained in a <b>self-supervised</b> manner using pseudo-labels generated from RANSAC. Multi-Object RANSAC demonstrates superior plane instance segmentation performances over other recent RANSAC applications. We conducted an experiment on robot suction-based grasping, comparing our method with vision-based grasping network and RANSAC applications. The results from this real-world scenario showed its remarkable performance surpassing the baseline methods, highlighting its potential for advanced scene understanding and manipulation.</p></p class="citation"></blockquote><h3 id=1824--227329-interactive-robot-environment-self-calibration-via-compliant-exploratory-actions-podshara-chanrungmaneekul-et-al-2024>(18/24 | 227/329) Interactive Robot-Environment Self-Calibration via Compliant Exploratory Actions (Podshara Chanrungmaneekul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Podshara Chanrungmaneekul, Kejia Ren, Joshua T. Grace, Aaron M. Dollar, Kaiyu Hang. (2024)<br><strong>Interactive Robot-Environment Self-Calibration via Compliant Exploratory Actions</strong><br><button class=copy-to-clipboard title="Interactive Robot-Environment Self-Calibration via Compliant Exploratory Actions" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13144v1.pdf filename=2403.13144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Calibrating robots into their workspaces is crucial for manipulation tasks. Existing calibration techniques often rely on sensors external to the robot (cameras, laser scanners, etc.) or specialized tools. This reliance complicates the calibration process and increases the costs and time requirements. Furthermore, the associated setup and measurement procedures require significant <b>human</b> <b>intervention,</b> which makes them more challenging to operate. Using the built-in force-torque sensors, which are nowadays a default component in collaborative robots, this work proposes a self-calibration framework where robot-environmental spatial relations are automatically estimated through compliant exploratory actions by the robot itself. The self-calibration approach converges, verifies its own accuracy, and terminates upon completion, autonomously purely through interactive exploration of the environment&rsquo;s geometries. Extensive experiments validate the effectiveness of our self-calibration approach in accurately establishing the robot-environment spatial relationships without the need for additional sensing equipment or any <b>human</b> <b>intervention.</b></p></p class="citation"></blockquote><h3 id=1924--228329-digital-twin-driven-reinforcement-learning-for-obstacle-avoidance-in-robot-manipulators-a-self-improving-online-training-framework-yuzhu-sun-et-al-2024>(19/24 | 228/329) Digital Twin-Driven Reinforcement Learning for Obstacle Avoidance in Robot Manipulators: A Self-Improving Online Training Framework (Yuzhu Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuzhu Sun, Mien Van, Stephen McIlvanna, Nguyen Minh Nhat, Kabirat Olayemi, Jack Close, Seán McLoone. (2024)<br><strong>Digital Twin-Driven Reinforcement Learning for Obstacle Avoidance in Robot Manipulators: A Self-Improving Online Training Framework</strong><br><button class=copy-to-clipboard title="Digital Twin-Driven Reinforcement Learning for Obstacle Avoidance in Robot Manipulators: A Self-Improving Online Training Framework" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13090v1.pdf filename=2403.13090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution and growing automation of collaborative robots introduce more complexity and unpredictability to systems, highlighting the crucial need for robot&rsquo;s adaptability and flexibility to address the increasing complexities of their environment. In typical industrial production scenarios, robots are often required to be re-programmed when facing a more demanding task or even a few changes in workspace conditions. To increase productivity, efficiency and reduce human effort in the design process, this paper explores the potential of using digital twin combined with <b>Reinforcement</b> <b>Learning</b> (RL) to enable robots to generate self-improving collision-free trajectories in real time. The digital twin, acting as a virtual counterpart of the physical system, serves as a &lsquo;forward run&rsquo; for monitoring, controlling, and optimizing the physical system in a safe and cost-effective manner. The physical system sends data to synchronize the digital system through the video feeds from cameras, which allows the virtual robot to update its observation and policy based on real scenarios. The bidirectional communication between digital and physical systems provides a promising platform for hardware-in-the-loop RL training through trial and error until the robot successfully adapts to its new environment. The proposed online training framework is demonstrated on the Unfactory Xarm5 collaborative robot, where the robot end-effector aims to reach the target position while avoiding obstacles. The experiment suggest that proposed framework is capable of performing policy online training, and that there remains significant room for improvement.</p></p class="citation"></blockquote><h3 id=2024--229329-adaptive-visual-imitation-learning-for-robotic-assisted-feeding-across-varied-bowl-configurations-and-food-types-rui-liu-et-al-2024>(20/24 | 229/329) Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types (Rui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Liu, Amisha Bhaskar, Pratap Tokekar. (2024)<br><strong>Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types</strong><br><button class=copy-to-clipboard title="Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12891v1.pdf filename=2403.12891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF). The goal is to acquire (i.e., scoop) food items from a bowl. However, achieving robust and adaptive food manipulation is particularly challenging. To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping. Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors. We validate the effectiveness of our approach by conducting experiments on a real robot. We also compare its performance with a baseline. The results demonstrate improvement over the baseline across all scenarios, with an enhancement of up to 2.5 times in terms of a success metric. Notably, our model, trained solely on data from a transparent glass bowl containing granular cereals, showcases generalization ability when tested <b>zero-shot</b> on other bowl configurations with different types of food.</p></p class="citation"></blockquote><h3 id=2124--230329-opti-acoustic-semantic-slam-with-unknown-objects-in-underwater-environments-kurran-singh-et-al-2024>(21/24 | 230/329) Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments (Kurran Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kurran Singh, Jungseok Hong, Nicholas R. Rypkema, John J. Leonard. (2024)<br><strong>Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments</strong><br><button class=copy-to-clipboard title="Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12837v1.pdf filename=2403.12837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent advances in semantic Simultaneous Localization and Mapping (SLAM) for terrestrial and aerial applications, underwater semantic SLAM remains an open and largely unaddressed research problem due to the unique sensing modalities and the object classes found underwater. This paper presents an object-based semantic SLAM method for underwater environments that can identify, localize, classify, and map a wide variety of marine objects without a priori knowledge of the object classes present in the scene. The method performs <b>unsupervised</b> object segmentation and object-level feature aggregation, and then uses opti-acoustic sensor fusion for object localization. Probabilistic data association is used to determine observation to landmark correspondences. Given such correspondences, the method then jointly optimizes landmark and vehicle position estimates. Indoor and outdoor underwater datasets with a wide variety of objects and challenging acoustic and lighting conditions are collected for evaluation and made publicly available. Quantitative and qualitative results show the proposed method achieves reduced trajectory error compared to baseline methods, and is able to obtain comparable map accuracy to a baseline closed-set method that requires hand-labeled data of all objects in the scene.</p></p class="citation"></blockquote><h3 id=2224--231329-bin-packing-optimization-via-deep-reinforcement-learning-baoying-wang-et-al-2024>(22/24 | 231/329) Bin Packing Optimization via Deep Reinforcement Learning (Baoying Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baoying Wang, Huixu Dong. (2024)<br><strong>Bin Packing Optimization via Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Bin Packing Optimization via Deep Reinforcement Learning" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12420v1.pdf filename=2403.12420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Bin Packing Problem (BPP) has attracted enthusiastic research interest recently, owing to widespread applications in logistics and warehousing environments. It is truly essential to optimize the bin packing to enable more objects to be packed into boxes. Object packing order and placement strategy are the two crucial optimization objectives of the BPP. However, existing optimization methods for BPP, such as the genetic algorithm (GA), emerge as the main issues in highly computational cost and relatively low accuracy, making it difficult to implement in realistic scenarios. To well relieve the research gaps, we present a novel optimization methodology of two-dimensional (2D)-BPP and three-dimensional (3D)-BPP for objects with regular shapes via deep <b>reinforcement</b> <b>learning</b> (DRL), maximizing the space utilization and minimizing the usage number of boxes. First, an end-to-end DRL neural network constructed by a modified Pointer Network consisting of an encoder, a decoder and an attention module is proposed to achieve the optimal object packing order. Second, conforming to the top-down operation mode, the placement strategy based on a height map is used to arrange the ordered objects in the boxes, preventing the objects from colliding with boxes and other objects in boxes. Third, the reward and loss functions are defined as the indicators of the compactness, pyramid, and usage number of boxes to conduct the training of the DRL neural network based on an on-policy actor-critic framework. Finally, a series of experiments are implemented to compare our method with conventional packing methods, from which we conclude that our method outperforms these packing methods in both packing accuracy and efficiency.</p></p class="citation"></blockquote><h3 id=2324--232329-cooperative-modular-manipulation-with-numerous-cable-driven-robots-for-assistive-construction-and-gap-crossing-kevin-murphy-et-al-2024>(23/24 | 232/329) Cooperative Modular Manipulation with Numerous Cable-Driven Robots for Assistive Construction and Gap Crossing (Kevin Murphy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Murphy, Joao C. V. Soares, Justin K. Yim, Dustin Nottage, Ahmet Soylemezoglu, Joao Ramos. (2024)<br><strong>Cooperative Modular Manipulation with Numerous Cable-Driven Robots for Assistive Construction and Gap Crossing</strong><br><button class=copy-to-clipboard title="Cooperative Modular Manipulation with Numerous Cable-Driven Robots for Assistive Construction and Gap Crossing" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13124v1.pdf filename=2403.13124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Soldiers in the field often need to cross negative obstacles, such as rivers or canyons, to reach goals or safety. Military gap crossing involves on-site temporary bridges construction. However, this procedure is conducted with dangerous, time and labor intensive operations, and specialized machinery. We envision a scalable robotic solution inspired by advancements in force-controlled and Cable Driven Parallel Robots (CDPRs); this solution can address the challenges inherent in this transportation problem, achieving fast, efficient, and safe deployment and field operations. We introduce the embodied vision in Co3MaNDR, a solution to the military gap crossing problem, a distributed robot consisting of several modules simultaneously pulling on a central payload, controlling the cables&rsquo; tensions to achieve complex objectives, such as precise trajectory tracking or force amplification. Hardware experiments demonstrate teleoperation of a payload, trajectory following, and the sensing and amplification of operators&rsquo; applied physical forces during slow operations. An operator was shown to manipulate a 27.2 <b>kg</b> (60 lb) payload with an average force utilization of 14.5% of its weight. Results indicate that the system can be scaled up to heavier payloads without compromising performance or introducing superfluous complexity. This research lays a foundation to expand CDPR technology to uncoordinated and unstable mobile platforms in unknown environments.</p></p class="citation"></blockquote><h3 id=2424--233329-on-designing-consistent-covariance-recovery-from-a-deep-learning-visual-odometry-engine-jagatpreet-singh-nir-et-al-2024>(24/24 | 233/329) On Designing Consistent Covariance Recovery from a Deep Learning Visual Odometry Engine (Jagatpreet Singh Nir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jagatpreet Singh Nir, Dennis Giaya, Hanumant Singh. (2024)<br><strong>On Designing Consistent Covariance Recovery from a Deep Learning Visual Odometry Engine</strong><br><button class=copy-to-clipboard title="On Designing Consistent Covariance Recovery from a Deep Learning Visual Odometry Engine" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13170v1.pdf filename=2403.13170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning techniques have significantly advanced in providing accurate visual odometry solutions by leveraging large datasets. However, generating uncertainty estimates for these methods remains a challenge. Traditional sensor fusion approaches in a Bayesian framework are well-established, but deep learning techniques with millions of parameters lack efficient methods for uncertainty estimation. This paper addresses the issue of uncertainty estimation for pre-trained deep-learning models in monocular visual odometry. We propose formulating a factor <b>graph</b> on an implicit layer of the deep learning network to recover relative covariance estimates, which allows us to determine the covariance of the Visual Odometry (VO) solution. We showcase the consistency of the deep learning engine&rsquo;s covariance approximation with an empirical analysis of the covariance model on the EUROC datasets to demonstrate the correctness of our formulation.</p></p class="citation"></blockquote><h2 id=eessiv-8>eess.IV (8)</h2><h3 id=18--234329-federated-semi-supervised-learning-for-medical-image-segmentation-with-intra-client-and-inter-client-consistency-yubin-zheng-et-al-2024>(1/8 | 234/329) Federated Semi-supervised Learning for Medical Image Segmentation with intra-client and inter-client Consistency (Yubin Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yubin Zheng, Peng Tang, Tianjie Ju, Weidong Qiu, Bo Yan. (2024)<br><strong>Federated Semi-supervised Learning for Medical Image Segmentation with intra-client and inter-client Consistency</strong><br><button class=copy-to-clipboard title="Federated Semi-supervised Learning for Medical Image Segmentation with intra-client and inter-client Consistency" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 80<br>Keywords: Autoencoder, Data Augmentation, Federated Learning, Knowledge Distillation, Self-supervised Learning, Self-supervised Learning, Semi-Supervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12695v1.pdf filename=2403.12695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation plays a vital role in clinic disease diagnosis and medical image analysis. However, labeling medical images for segmentation task is tough due to the indispensable domain expertise of radiologists. Furthermore, considering the privacy and sensitivity of medical images, it is impractical to build a centralized segmentation dataset from different medical institutions. <b>Federated</b> <b>learning</b> aims to train a shared model of isolated clients without local <b>data</b> <b>exchange</b> which aligns well with the scarcity and privacy characteristics of medical <b>data.</b> <b>To</b> solve the problem of labeling hard, many advanced <b>semi-supervised</b> <b>methods</b> have been proposed in a centralized <b>data</b> <b>setting.</b> As for <b>federated</b> <b>learning,</b> how to conduct <b>semi-supervised</b> <b>learning</b> under this distributed scenario is worth investigating. In this work, we propose a novel <b>federated</b> <b>semi-supervised</b> <b>learning</b> framework for medical image segmentation. The intra-client and inter-client consistency learning are introduced to smooth predictions at the <b>data</b> <b>level</b> and avoid confirmation bias of local models. They are achieved with the assistance of a <b>Variational</b> <b>Autoencoder</b> (VAE) trained collaboratively by clients. The added VAE model plays three roles: 1) extracting latent low-dimensional features of all labeled and unlabeled <b>data;</b> <b>2)</b> performing a novel type of <b>data</b> <b>augmentation</b> in calculating intra-client consistency loss; 3) utilizing the generative ability of itself to conduct inter-client consistency <b>distillation.</b> The proposed framework is compared with other <b>federated</b> <b>semi-supervised</b> <b>or</b> <b>self-supervised</b> <b>learning</b> methods. The experimental results illustrate that our method outperforms the state-of-the-art method while avoiding a lot of computation and communication overhead.</p></p class="citation"></blockquote><h3 id=28--235329-low-trace-adaptation-of-zero-shot-self-supervised-blind-image-denoising-jintong-hu-et-al-2024>(2/8 | 235/329) Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising (Jintong Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jintong Hu, Bin Xia, Bingchen Li, Wenming Yang. (2024)<br><strong>Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising</strong><br><button class=copy-to-clipboard title="Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 50<br>Keywords: Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12382v1.pdf filename=2403.12382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based denoiser has been the focus of recent development on image denoising. In the past few years, there has been increasing interest in developing <b>self-supervised</b> <b>denoising</b> networks that only require noisy images, without the need for clean ground truth for training. However, a performance gap remains between current <b>self-supervised</b> <b>methods</b> and their <b>supervised</b> <b>counterparts.</b> Additionally, these methods commonly depend on assumptions about noise characteristics, thereby constraining their applicability in real-world scenarios. Inspired by the properties of the Frobenius norm expansion, we discover that incorporating a trace term reduces the optimization goal disparity between <b>self-supervised</b> <b>and</b> <b>supervised</b> <b>methods,</b> thereby enhancing the performance of <b>self-supervised</b> <b>learning.</b> To exploit this insight, we propose a trace-constraint loss function and design the low-trace adaptation Noise2Noise (LoTA-N2N) model that bridges the gap between <b>self-supervised</b> <b>and</b> <b>supervised</b> <b>learning.</b> Furthermore, we have discovered that several existing <b>self-supervised</b> <b>denoising</b> frameworks naturally fall within the proposed trace-constraint loss as subcases. Extensive experiments conducted on natural and confocal image datasets indicate that our method achieves state-of-the-art performance within the realm of <b>zero-shot</b> <b>self-supervised</b> <b>image</b> denoising approaches, without relying on any assumptions regarding the noise.</p></p class="citation"></blockquote><h3 id=38--236329-trustworthiness-of-pretrained-transformers-for-lung-cancer-segmentation-aneesh-rangnekar-et-al-2024>(3/8 | 236/329) Trustworthiness of Pretrained Transformers for Lung Cancer Segmentation (Aneesh Rangnekar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aneesh Rangnekar, Nishant Nadkarni, Jue Jiang, Harini Veeraraghavan. (2024)<br><strong>Trustworthiness of Pretrained Transformers for Lung Cancer Segmentation</strong><br><button class=copy-to-clipboard title="Trustworthiness of Pretrained Transformers for Lung Cancer Segmentation" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13113v1.pdf filename=2403.13113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We assessed the trustworthiness of two self-supervision pretrained <b>transformer</b> models, Swin UNETR and SMIT, for <b>fine-tuned</b> lung (LC) tumor segmentation using 670 CT and MRI scans. We measured segmentation accuracy on two public 3D-CT datasets, robustness on CT scans of patients with COVID-19, CT scans of patients with ovarian cancer and T2-weighted MRI of men with prostate cancer, and <b>zero-shot</b> generalization of LC for T2-weighted MRIs. Both models demonstrated high accuracy on in-distribution data (Dice 0.80 for SMIT and 0.78 for Swin UNETR). SMIT showed similar near-out-of-distribution performance on CT scans (AUROC 89.85% vs. 89.19%) but significantly better far-out-of-distribution accuracy on CT (AUROC 97.2% vs. 87.1%) and MRI (92.15% vs. 73.8%). SMIT outperformed Swin UNETR in <b>zero-shot</b> segmentation on MRI (Dice 0.78 vs. 0.69). We expect these findings to guide the safe development and deployment of current and future pretrained models in routine clinical use.</p></p class="citation"></blockquote><h3 id=48--237329-sift-dbt-self-supervised-initialization-and-fine-tuning-for-imbalanced-digital-breast-tomosynthesis-image-classification-yuexi-du-et-al-2024>(4/8 | 237/329) SIFT-DBT: Self-supervised Initialization and Fine-Tuning for Imbalanced Digital Breast Tomosynthesis Image Classification (Yuexi Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuexi Du, Regina J. Hooley, John Lewin, Nicha C. Dvornek. (2024)<br><strong>SIFT-DBT: Self-supervised Initialization and Fine-Tuning for Imbalanced Digital Breast Tomosynthesis Image Classification</strong><br><button class=copy-to-clipboard title="SIFT-DBT: Self-supervised Initialization and Fine-Tuning for Imbalanced Digital Breast Tomosynthesis Image Classification" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13148v1.pdf filename=2403.13148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital Breast Tomosynthesis (DBT) is a widely used medical imaging modality for breast cancer screening and diagnosis, offering higher spatial resolution and greater detail through its 3D-like breast volume imaging capability. However, the increased data volume also introduces pronounced data imbalance challenges, where only a small fraction of the volume contains suspicious tissue. This further exacerbates the data imbalance due to the case-level distribution in real-world data and leads to learning a trivial classification model that only predicts the majority class. To address this, we propose a novel method using view-level contrastive <b>Self-supervised</b> Initialization and <b>Fine-Tuning</b> for identifying abnormal DBT images, namely SIFT-DBT. We further introduce a patch-level multi-instance learning method to preserve spatial resolution. The proposed method achieves 92.69% volume-wise AUC on an evaluation of 970 unique studies.</p></p class="citation"></blockquote><h3 id=58--238329-generative-enhancement-for-3d-medical-images-lingting-zhu-et-al-2024>(5/8 | 238/329) Generative Enhancement for 3D Medical Images (Lingting Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingting Zhu, Noel Codella, Dongdong Chen, Zhenchao Jin, Lu Yuan, Lequan Yu. (2024)<br><strong>Generative Enhancement for 3D Medical Images</strong><br><button class=copy-to-clipboard title="Generative Enhancement for 3D Medical Images" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12852v1.pdf filename=2403.12852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The limited availability of 3D medical image datasets, due to privacy concerns and high collection or annotation costs, poses significant challenges in the field of medical imaging. While a promising alternative is the use of synthesized medical data, there are few solutions for realistic 3D medical image synthesis due to difficulties in backbone design and fewer 3D training samples compared to 2D counterparts. In this paper, we propose GEM-3D, a novel generative approach to the synthesis of 3D medical images and the enhancement of existing datasets using conditional <b>diffusion</b> <b>models.</b> Our method begins with a 2D slice, noted as the informed slice to serve the patient prior, and propagates the generation process using a 3D segmentation mask. By decomposing the 3D medical images into masks and patient prior information, GEM-3D offers a flexible yet effective solution for generating versatile 3D images from existing datasets. GEM-3D can enable dataset enhancement by combining informed slice selection and generation at random positions, along with editable mask volumes to introduce large variations in <b>diffusion</b> <b>sampling.</b> Moreover, as the informed slice contains patient-wise information, GEM-3D can also facilitate <b>counterfactual</b> image synthesis and dataset-level de-enhancement with desired control. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D is capable of synthesizing high-quality 3D medical images with volumetric consistency, offering a straightforward solution for dataset enhancement during inference. The code is available at <a href=https://github.com/HKU-MedAI/GEM-3D>https://github.com/HKU-MedAI/GEM-3D</a>.</p></p class="citation"></blockquote><h3 id=68--239329-fuelvision-a-multimodal-data-fusion-and-multimodel-ensemble-algorithm-for-wildfire-fuels-mapping-riyaaz-uddien-shaik-et-al-2024>(6/8 | 239/329) FUELVISION: A Multimodal Data Fusion and Multimodel Ensemble Algorithm for Wildfire Fuels Mapping (Riyaaz Uddien Shaik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riyaaz Uddien Shaik, Mohamad Alipour, Eric Rowell, Bharathan Balaji, Adam Watts, Ertugrul Taciroglu. (2024)<br><strong>FUELVISION: A Multimodal Data Fusion and Multimodel Ensemble Algorithm for Wildfire Fuels Mapping</strong><br><button class=copy-to-clipboard title="FUELVISION: A Multimodal Data Fusion and Multimodel Ensemble Algorithm for Wildfire Fuels Mapping" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: I-4-9, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 16<br>Keywords: Generative AI, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15462v1.pdf filename=2403.15462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate assessment of fuel conditions is a prerequisite for fire ignition and behavior prediction, and risk management. The method proposed herein leverages diverse data sources including Landsat-8 optical imagery, Sentinel-1 (C-band) Synthetic Aperture Radar (SAR) imagery, PALSAR (L-band) SAR imagery, and terrain features to capture comprehensive information about fuel types and distributions. An ensemble model was trained to predict landscape-scale fuels such as the &lsquo;Scott and Burgan 40&rsquo; using the as-received Forest Inventory and Analysis (FIA) field survey plot data obtained from the USDA Forest Service. However, this basic approach yielded relatively poor results due to the inadequate amount of training data. Pseudo-labeled and fully synthetic datasets were developed using <b>generative</b> <b>AI</b> approaches to address the limitations of ground truth data availability. These synthetic datasets were used for augmenting the FIA data from California to enhance the robustness and coverage of model training. The use of an ensemble of methods including deep learning neural networks, decision trees, and gradient boosting offered a fuel mapping accuracy of nearly 80%. Through extensive experimentation and evaluation, the effectiveness of the proposed approach was validated for regions of the 2021 Dixie and Caldor fires. Comparative analyses against high-resolution data from the National Agriculture Imagery Program (NAIP) and timber harvest maps affirmed the robustness and reliability of the proposed approach, which is capable of near-real-time fuel mapping.</p></p class="citation"></blockquote><h3 id=78--240329-physics-guided-neural-networks-for-intraventricular-vector-flow-mapping-hang-jung-ling-et-al-2024>(7/8 | 240/329) Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping (Hang Jung Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Jung Ling, Salomé Bru, Julia Puig, Florian Vixège, Simon Mendez, Franck Nicoud, Pierre-Yves Courand, Olivier Bernard, Damien Garcia. (2024)<br><strong>Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping</strong><br><button class=copy-to-clipboard title="Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13040v1.pdf filename=2403.13040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify color Doppler in cardiac imaging. In this study, we propose novel alternatives to the traditional iVFM optimization scheme by utilizing physics-informed neural networks (PINNs) and a physics-guided nnU-Net-based <b>supervised</b> approach. Through rigorous evaluation on simulated color Doppler images derived from a patient-specific computational fluid dynamics model and in vivo Doppler acquisitions, both approaches demonstrate comparable reconstruction performance to the original iVFM algorithm. The efficiency of PINNs is boosted through dual-stage optimization and pre-optimized weights. On the other hand, the nnU-Net method excels in generalizability and real time capabilities. Notably, nnU-Net shows superior robustness on sparse and truncated Doppler data while maintaining independence from explicit boundary conditions. Overall, our results highlight the effectiveness of these methods in reconstructing intraventricular vector blood flow. The study also suggests potential applications of PINNs in ultrafast color Doppler imaging and the incorporation of fluid dynamics equations to derive biomarkers for cardiovascular diseases based on blood flow.</p></p class="citation"></blockquote><h3 id=88--241329-super-high-fidelity-image-compression-via-hierarchical-roi-and-adaptive-quantization-jixiang-luo-et-al-2024>(8/8 | 241/329) Super-High-Fidelity Image Compression via Hierarchical-ROI and Adaptive Quantization (Jixiang Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jixiang Luo, Yan Wang, Hongwei Qin. (2024)<br><strong>Super-High-Fidelity Image Compression via Hierarchical-ROI and Adaptive Quantization</strong><br><button class=copy-to-clipboard title="Super-High-Fidelity Image Compression via Hierarchical-ROI and Adaptive Quantization" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13030v1.pdf filename=2403.13030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learned Image Compression (LIC) has achieved dramatic progress regarding objective and subjective metrics. MSE-based models aim to improve objective metrics while generative models are leveraged to improve visual quality measured by subjective metrics. However, they all suffer from blurring or deformation at low bit rates, especially at below $0.2bpp$. Besides, deformation on human faces and text is unacceptable for visual quality assessment, and the problem becomes more prominent on small faces and text. To solve this problem, we combine the advantage of MSE-based models and generative models by utilizing region of interest (ROI). We propose Hierarchical-ROI (H-ROI), to split images into several foreground regions and one background region to improve the reconstruction of regions containing faces, text, and complex textures. Further, we propose adaptive <b>quantization</b> by non-linear mapping within the channel dimension to constrain the bit rate while maintaining the visual quality. Exhaustive experiments demonstrate that our methods achieve better visual quality on small faces and text with lower bit rates, e.g., $0.7X$ bits of HiFiC and $0.5X$ bits of BPG.</p></p class="citation"></blockquote><h2 id=csai-7>cs.AI (7)</h2><h3 id=17--242329-wolf-large-language-model-framework-for-cxr-understanding-seil-kang-et-al-2024>(1/7 | 242/329) WoLF: Large Language Model Framework for CXR Understanding (Seil Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seil Kang, Donghyun Kim, Junhyeok Kim, Hyo Kyung Lee, Seong Jae Hwang. (2024)<br><strong>WoLF: Large Language Model Framework for CXR Understanding</strong><br><button class=copy-to-clipboard title="WoLF: Large Language Model Framework for CXR Understanding" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 80<br>Keywords: Instruction Following, Question Answering, Visual Question Answering, Visual Question Answering, BLEU, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15456v1.pdf filename=2403.15456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Significant methodological strides have been made toward Chest X-ray (CXR) understanding via modern <b>vision-language</b> models (VLMs), demonstrating impressive <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> and CXR report generation abilities. However, existing CXR understanding frameworks still possess several procedural caveats. (1) Previous methods solely use CXR reports, which are insufficient for comprehensive <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA),</b> especially when additional health-related data like medication history and prior diagnoses are needed. (2) Previous methods use raw CXR reports, which are often arbitrarily structured. While modern language models can understand various text formats, restructuring reports for clearer, organized anatomy-based information could enhance their usefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize linguistic correctness, lacking the capability to offer nuanced assessments of the generated answers. In this work, to address the aforementioned caveats, we introduce WoLF, a Wide-scope <b>Large</b> <b>Language</b> <b>Model</b> Framework for CXR understanding. To resolve (1), we capture multi-faceted records of patients, which are utilized for accurate diagnoses in real-world clinical scenarios. Specifically, we adopt the Electronic Health Records (EHR) to generate <b>instruction-following</b> <b>data</b> suited for CXR understanding. Regarding (2), we enhance report generation performance by decoupling knowledge in CXR reports based on anatomical structure even within the attention step via masked attention. To address (3), we introduce an AI-evaluation protocol optimized for assessing the capabilities of <b>LLM.</b> Through extensive experimental validations, WoLF demonstrates superior performance over other models on MIMIC-CXR in the AI-evaluation arena about <b>VQA</b> (up to +9.47%p mean score) and by metrics about report generation (+7.3%p <b>BLEU-1).</b></p></p class="citation"></blockquote><h3 id=27--243329-enhancing-formal-theorem-proving-a-comprehensive-dataset-for-training-ai-models-on-coq-code-andreas-florath-2024>(2/7 | 243/329) Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code (Andreas Florath, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Florath. (2024)<br><strong>Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code</strong><br><button class=copy-to-clipboard title="Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LO, cs.AI<br>Keyword Score: 40<br>Keywords: Fine-tuning, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12627v1.pdf filename=2403.12627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness. Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Addressing this gap, we present a comprehensive dataset specifically designed to enhance <b>LLMs&rsquo;</b> proficiency in interpreting and generating Coq <b>code.</b> <b>This</b> dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information. Our primary aim is to facilitate the development of <b>LLMs</b> capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving. Initial experiments with this dataset have showcased its significant potential; models trained on this data exhibited enhanced accuracy in Coq <b>code</b> <b>generation.</b> Notably, a particular experiment revealed that a <b>fine-tuned</b> <b>LLM</b> was capable of generating 141 valid proofs for a basic lemma, highlighting the dataset&rsquo;s utility in facilitating the discovery of diverse and valid proof strategies. This paper discusses the dataset&rsquo;s composition, the methodology behind its creation, and the implications of our findings for the future of machine learning in formal verification. The dataset is accessible for further research and exploration: <a href=https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1>https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1</a></p></p class="citation"></blockquote><h3 id=37--244329-embodied-llm-agents-learn-to-cooperate-in-organized-teams-xudong-guo-et-al-2024>(3/7 | 244/329) Embodied LLM Agents Learn to Cooperate in Organized Teams (Xudong Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia Vélez, Qingyun Wu, Huazheng Wang, Thomas L. Griffiths, Mengdi Wang. (2024)<br><strong>Embodied LLM Agents Learn to Cooperate in Organized Teams</strong><br><button class=copy-to-clipboard title="Embodied LLM Agents Learn to Cooperate in Organized Teams" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs-MA, cs.AI<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12482v1.pdf filename=2403.12482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as integral tools for <b>reasoning,</b> planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. <b>LLMs</b> thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, <b>LLM</b> agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes <b>prompt-based</b> organization structures on <b>LLM</b> agents to mitigate these problems. Through a series of experiments with embodied <b>LLM</b> agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by <b>LLM</b> agents and their spontaneous cooperative behaviors. Further, we harness the potential of <b>LLMs</b> to propose enhanced organizational <b>prompts,</b> via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.</p></p class="citation"></blockquote><h3 id=47--245329-insight-end-to-end-neuro-symbolic-visual-reinforcement-learning-with-language-explanations-lirui-luo-et-al-2024>(4/7 | 245/329) INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations (Lirui Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lirui Luo, Guoxi Zhang, Hongming Xu, Yaodong Yang, Cong Fang, Qing Li. (2024)<br><strong>INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations</strong><br><button class=copy-to-clipboard title="INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Foundation Model, Knowledge Distillation, Reinforcement Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12451v1.pdf filename=2403.12451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neuro-symbolic <b>reinforcement</b> <b>learning</b> (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. For tasks with visual observations, NS-RL entails structured representations for states, but previous algorithms are unable to refine the structured states with reward signals due to a lack of efficiency. Accessibility is also an issue, as extensive domain knowledge is required to interpret current symbolic policies. In this paper, we present a framework that is capable of learning structured states and symbolic policies simultaneously, whose key idea is to overcome the efficiency bottleneck by <b>distilling</b> vision <b>foundation</b> <b>models</b> into a scalable perception module. Moreover, we design a pipeline that uses <b>large</b> <b>language</b> <b>models</b> to generate concise and readable language explanations for policies and decisions. In experiments on nine Atari tasks, our approach demonstrates substantial performance gains over existing NSRL methods. We also showcase explanations for policies and decisions.</p></p class="citation"></blockquote><h3 id=57--246329-contextual-moral-value-alignment-through-context-based-aggregation-pierre-dognin-et-al-2024>(5/7 | 246/329) Contextual Moral Value Alignment Through Context-Based Aggregation (Pierre Dognin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Dognin, Jesus Rios, Ronny Luss, Inkit Padhi, Matthew D Riemer, Miao Liu, Prasanna Sattigeri, Manish Nagireddy, Kush R. Varshney, Djallel Bouneffouf. (2024)<br><strong>Contextual Moral Value Alignment Through Context-Based Aggregation</strong><br><button class=copy-to-clipboard title="Contextual Moral Value Alignment Through Context-Based Aggregation" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12805v1.pdf filename=2403.12805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI. Specifically within the domain of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance. In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation. Here, aggregation is defined as the process of integrating a subset of <b>LLM</b> responses that are best suited to respond to a user input, taking into account features extracted from the user&rsquo;s input. The proposed system shows better results in term of alignment to human value compared to the state of the art.</p></p class="citation"></blockquote><h3 id=67--247329-on-predictive-planning-and-counterfactual-learning-in-active-inference-aswin-paul-et-al-2024>(6/7 | 247/329) On Predictive planning and counterfactual learning in active inference (Aswin Paul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aswin Paul, Takuya Isomura, Adeel Razi. (2024)<br><strong>On Predictive planning and counterfactual learning in active inference</strong><br><button class=copy-to-clipboard title="On Predictive planning and counterfactual learning in active inference" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI, stat-ME<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12417v1.pdf filename=2403.12417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent behaviour is increasingly important. Active inference, regarded as a general theory of behaviour, offers a principled approach to probing the basis of sophistication in planning and decision-making. In this paper, we examine two decision-making schemes in active inference based on &lsquo;planning&rsquo; and &rsquo;learning from experience&rsquo;. Furthermore, we also introduce a mixed model that navigates the data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate balanced decision-making. We evaluate our proposed model in a challenging grid-world scenario that requires adaptability from the agent. Additionally, our model provides the opportunity to analyze the evolution of various parameters, offering valuable insights and contributing to an explainable framework for intelligent decision-making.</p></p class="citation"></blockquote><h3 id=77--248329-offline-imitation-of-badminton-player-behavior-via-experiential-contexts-and-brownian-motion-kuang-da-wang-et-al-2024>(7/7 | 248/329) Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion (Kuang-Da Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kuang-Da Wang, Wei-Yao Wang, Ping-Chun Hsieh, Wen-Chih Peng. (2024)<br><strong>Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion</strong><br><button class=copy-to-clipboard title="Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12406v1.pdf filename=2403.12406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players. While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored. Replicating opponents&rsquo; behavior benefits players by allowing them to undergo strategic development with direction before matches. However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions. In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players&rsquo; decision dependencies by modeling decision-making processes as a contextual <b>Markov</b> <b>decision</b> <b>process.</b> (ii) RallyNet leverages the experience to generate context as the agent&rsquo;s intent in the rally. (iii) To generate more realistic behavior, RallyNet leverages Geometric Brownian Motion (GBM) to model the interactions between players by introducing a valuable inductive bias for learning player behaviors. In this manner, RallyNet links player intents with interaction models with GBM, providing an understanding of interactions for sports analytics. We extensively validate RallyNet with the largest available real-world badminton dataset consisting of men&rsquo;s and women&rsquo;s singles, demonstrating its ability to imitate player behaviors. Results reveal RallyNet&rsquo;s superiority over offline imitation learning methods and state-of-the-art turn-based approaches, outperforming them by at least 16% in mean rule-based agent normalization score. Furthermore, we discuss various practical use cases to highlight RallyNet&rsquo;s applicability.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--249329-interpretable-user-satisfaction-estimation-for-conversational-systems-with-large-language-models-ying-chun-lin-et-al-2024>(1/5 | 249/329) Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models (Ying-Chun Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying-Chun Lin, Jennifer Neville, Jack W. Stokes, Longqi Yang, Tara Safavi, Mengting Wan, Scott Counts, Siddharth Suri, Reid Andersen, Xiaofeng Xu, Deepak Gupta, Sujay Kumar Jauhar, Xia Song, Georg Buscher, Saurabh Tiwary, Brent Hecht, Jaime Teevan. (2024)<br><strong>Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models</strong><br><button class=copy-to-clipboard title="Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 70<br>Keywords: Supervised Learning, ChatGPT, Chatbot, Large Language Model, Large Language Model, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12388v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12388v1.pdf filename=2403.12388v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose <b>(ChatGPT</b> and Bing Copilot) and task-oriented (customer service <b>chatbot)</b> conversational systems. Existing approaches based on featurized ML models or <b>text</b> <b>embeddings</b> fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that <b>LLMs</b> can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an <b>LLM</b> can be tailored for USE via an iterative <b>prompting</b> framework using supervision from labeled examples. The resulting method, <b>Supervised</b> <b>Prompting</b> for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.</p></p class="citation"></blockquote><h3 id=25--250329-inbox-recommendation-with-knowledge-graph-using-interest-box-embedding-zezhong-xu-et-al-2024>(2/5 | 250/329) InBox: Recommendation with Knowledge Graph using Interest Box Embedding (Zezhong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zezhong Xu, Yincen Qu, Wen Zhang, Lei Liang, Huajun Chen. (2024)<br><strong>InBox: Recommendation with Knowledge Graph using Interest Box Embedding</strong><br><button class=copy-to-clipboard title="InBox: Recommendation with Knowledge Graph using Interest Box Embedding" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 43<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Recommendation, Recommender System, Box Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12649v1.pdf filename=2403.12649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graphs</b> <b>(KGs)</b> have become vitally important in modern <b>recommender</b> <b>systems,</b> effectively improving performance and interpretability. Fundamentally, <b>recommender</b> <b>systems</b> aim to identify user interests based on historical interactions and recommend suitable items. However, existing works overlook two key challenges: (1) an interest corresponds to a potentially large set of related items, and (2) the lack of explicit, fine-grained exploitation of <b>KG</b> information and interest connectivity. This leads to an inability to reflect distinctions between entities and interests when modeling them in a single way. Additionally, the granularity of concepts in the <b>knowledge</b> <b>graphs</b> used for <b>recommendations</b> tends to be coarse, failing to match the fine-grained nature of user interests. This homogenization limits the precise exploitation of <b>knowledge</b> <b>graph</b> data and interest connectivity. To address these limitations, we introduce a novel embedding-based model called InBox. Specifically, various <b>knowledge</b> <b>graph</b> entities and relations are embedded as points or boxes, while user interests are modeled as boxes encompassing interaction history. Representing interests as boxes enables containing collections of item points related to that interest. We further propose that an interest comprises diverse basic concepts, and <b>box</b> <b>intersection</b> naturally supports concept combination. Across three training steps, InBox significantly outperforms state-of-the-art methods like HAKG and KGIN on <b>recommendation</b> tasks. Further analysis provides meaningful insights into the variable value of different <b>KG</b> data for <b>recommendations.</b> In summary, InBox advances <b>recommender</b> <b>systems</b> through <b>box-based</b> <b>interest</b> and concept modeling for sophisticated <b>knowledge</b> <b>graph</b> exploitation.</p></p class="citation"></blockquote><h3 id=35--251329-erase-benchmarking-feature-selection-methods-for-deep-recommender-systems-pengyue-jia-et-al-2024>(3/5 | 251/329) ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems (Pengyue Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang, Bo Chen, Wanyu Wang, Huifeng Guo, Ruiming Tang. (2024)<br><strong>ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems</strong><br><button class=copy-to-clipboard title="ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12660v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12660v2.pdf filename=2403.12660v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Recommender</b> <b>Systems</b> (DRS) are increasingly dependent on a large number of feature fields for more precise <b>recommendations.</b> Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature&rsquo;s lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infeasible for identifying the optimal hyperparameters and overlooks evaluating the robustness and stability of these methods. To bridge these gaps, this paper presents ERASE, a comprehensive <b>bEnchmaRk</b> for feAture SElection for DRS. ERASE comprises a thorough evaluation of eleven feature selection methods, covering both traditional and deep learning approaches, across four public datasets, private industrial datasets, and a real-world commercial platform, achieving significant enhancement. Our code is available online for ease of reproduction.</p></p class="citation"></blockquote><h3 id=45--252329-context-based-fast-recommendation-strategy-for-long-user-behavior-sequence-in-meituan-waimai-zhichao-feng-et-al-2024>(4/5 | 252/329) Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai (Zhichao Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichao Feng, Junjiie Xie, Kaiyuan Li, Yu Qin, Pengfei Wang, Qianzhong Li, Bin Yin, Xiang Li, Wei Lin, Shangguang Wang. (2024)<br><strong>Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai</strong><br><button class=copy-to-clipboard title="Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Graph, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12566v1.pdf filename=2403.12566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the <b>recommender</b> <b>system</b> of Meituan Waimai, we are dealing with ever-lengthening user behavior sequences, which pose an increasing challenge to modeling user preference effectively. Existing sequential <b>recommendation</b> models often fail to capture long-term dependencies or are too complex, complicating the fulfillment of Meituan Waimai&rsquo;s unique business needs. To better model user interests, we consider selecting relevant sub-sequences from users&rsquo; extensive historical behaviors based on their preferences. In this specific scenario, we&rsquo;ve noticed that the contexts in which users interact have a significant impact on their preferences. For this purpose, we introduce a novel method called Context-based Fast <b>Recommendation</b> Strategy to tackle the issue of long sequences. We first identify contexts that share similar user preferences with the target context and then locate the corresponding PoIs based on these identified contexts. This approach eliminates the necessity to select a sub-sequence for every candidate PoI, thereby avoiding high time complexity. Specifically, we implement a prototype-based approach to pinpoint contexts that mirror similar user preferences. To amplify accuracy and interpretability, we employ JS divergence of PoI attributes such as categories and prices as a measure of similarity between contexts. A temporal <b>graph</b> integrating both prototype and context nodes helps incorporate temporal information. We then identify appropriate prototypes considering both target contexts and short-term user preferences. Following this, we utilize contexts aligned with these prototypes to generate a sub-sequence, aimed at predicting CTR and CTCVR scores with target attention. Since its inception in 2023, this strategy has been adopted in Meituan Waimai&rsquo;s display <b>recommender</b> <b>system,</b> leading to a 4.6% surge in CTR and a 4.2% boost in GMV.</p></p class="citation"></blockquote><h3 id=55--253329-an-aligning-and-training-framework-for-multimodal-recommendations-yifan-liu-et-al-2024>(5/5 | 253/329) An Aligning and Training Framework for Multimodal Recommendations (Yifan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Liu, Kangning Zhang, Xiangyuan Ren, Yanhua Huang, Jiarui Jin, Yingjie Qin, Ruilong Su, Ruiwen Xu, Weinan Zhang. (2024)<br><strong>An Aligning and Training Framework for Multimodal Recommendations</strong><br><button class=copy-to-clipboard title="An Aligning and Training Framework for Multimodal Recommendations" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12384v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12384v2.pdf filename=2403.12384v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of multimedia applications, <b>multimodal</b> <b>recommendations</b> are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard <b>multimodal</b> information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among <b>multimodal</b> content features and ID features, for which directly using <b>multimodal</b> information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in <b>multimodal</b> <b>recommendations,</b> and propose a solution named AlignRec. In AlignRec, the <b>recommendation</b> objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our <b>multimodal</b> <b>recommendation</b> framework. To effectively train our AlignRec, we propose starting from pre-training the first alignment to obtain unified <b>multimodal</b> features and subsequently training the following two alignments together with these features as input. As it is essential to analyze whether each <b>multimodal</b> feature helps in training, we design three new classes of metrics to evaluate intermediate performance. Our extensive experiments on three real-world datasets consistently verify the superiority of AlignRec compared to nine baselines. We also find that the <b>multimodal</b> features generated by AlignRec are better than currently used ones, which are to be open-sourced.</p></p class="citation"></blockquote><h2 id=cscr-8>cs.CR (8)</h2><h3 id=18--254329-a-study-of-vulnerability-repair-in-javascript-programs-with-large-language-models-tan-khang-le-et-al-2024>(1/8 | 254/329) A Study of Vulnerability Repair in JavaScript Programs with Large Language Models (Tan Khang Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tan Khang Le, Saba Alimadadi, Steven Y. Ko. (2024)<br><strong>A Study of Vulnerability Repair in JavaScript Programs with Large Language Models</strong><br><button class=copy-to-clipboard title="A Study of Vulnerability Repair in JavaScript Programs with Large Language Models" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: Bard, ChatGPT, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13193v1.pdf filename=2403.13193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, JavaScript has become the most widely used programming language, especially in web development. However, writing secure JavaScript <b>code</b> <b>is</b> not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic <b>code</b> <b>generation</b> based on a required specification, including automatic bug fixing. In this study, we explore the accuracy of <b>LLMs,</b> namely <b>ChatGPT</b> and <b>Bard,</b> in finding and fixing security vulnerabilities in JavaScript programs. We also investigate the impact of context in a <b>prompt</b> on directing <b>LLMs</b> to produce a correct patch of vulnerable JavaScript <b>code.</b> <b>Our</b> experiments on real-world software vulnerabilities show that while <b>LLMs</b> are promising in automatic program repair of JavaScript <b>code,</b> <b>achieving</b> a correct bug fix often requires an appropriate amount of context in the <b>prompt.</b></p></p class="citation"></blockquote><h3 id=28--255329-rigorllm-resilient-guardrails-for-large-language-models-against-undesired-content-zhuowen-yuan-et-al-2024>(2/8 | 255/329) RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content (Zhuowen Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, Bo Li. (2024)<br><strong>RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content</strong><br><button class=copy-to-clipboard title="RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 40<br>Keywords: Data Augmentation, Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13031v1.pdf filename=2403.13031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in <b>LLMs,</b> particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under <b>adversarial</b> <b>attacks.</b> This paper introduces Resilient Guardrails for <b>Large</b> <b>Language</b> <b>Models</b> (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for <b>LLMs.</b> By employing a multi-faceted approach that includes energy-based training <b>data</b> <b>augmentation</b> through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with <b>LLMs</b> based on our <b>data</b> <b>augmentation,</b> RigorLLM offers a robust solution to harmful content moderation. Our experimental evaluations demonstrate that RigorLLM not only outperforms existing baselines like OpenAI API and Perspective API in detecting harmful content but also exhibits unparalleled resilience to jailbreaking attacks. The innovative use of constrained optimization and a fusion-based guardrail approach represents a significant step forward in developing more secure and reliable <b>LLMs,</b> setting a new standard for content moderation frameworks in the face of evolving digital threats.</p></p class="citation"></blockquote><h3 id=38--256329-bypassing-llm-watermarks-with-color-aware-substitutions-qilong-wu-et-al-2024>(3/8 | 256/329) Bypassing LLM Watermarks with Color-Aware Substitutions (Qilong Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qilong Wu, Varun Chandrasekaran. (2024)<br><strong>Bypassing LLM Watermarks with Color-Aware Substitutions</strong><br><button class=copy-to-clipboard title="Bypassing LLM Watermarks with Color-Aware Substitutions" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14719v1.pdf filename=2403.14719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Watermarking approaches are proposed to identify if text being circulated is human or <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> generated. The state-of-the-art watermarking strategy of Kirchenbauer et al. (2023a) biases the <b>LLM</b> to generate specific (<code>green'') tokens. However, determining the robustness of this watermarking method is an open problem. Existing attack methods fail to evade detection for longer text segments. We overcome this limitation, and propose {\em Self Color Testing-based Substitution (SCTS)}, the first </code>color-aware&rsquo;&rsquo; attack. SCTS obtains color information by strategically <b>prompting</b> the watermarked <b>LLM</b> and comparing output tokens frequencies. It uses this information to determine token colors, and substitutes green tokens with non-green ones. In our experiments, SCTS successfully evades watermark detection using fewer number of edits than related work. Additionally, we show both theoretically and empirically that SCTS can remove the watermark for arbitrarily long watermarked text.</p></p class="citation"></blockquote><h3 id=48--257329-securing-large-language-models-threats-vulnerabilities-and-responsible-practices-sara-abdali-et-al-2024>(4/8 | 257/329) Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices (Sara Abdali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Abdali, Richard Anarfi, CJ Barberan, Jia He. (2024)<br><strong>Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices</strong><br><button class=copy-to-clipboard title="Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12503v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12503v1.pdf filename=2403.12503v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, <b>LLMs</b> introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to <b>LLMs</b> from five thematic perspectives: security and privacy concerns, vulnerabilities against <b>adversarial</b> <b>attacks,</b> potential harms caused by misuses of <b>LLMs,</b> mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=58--258329-enhancing-security-of-ai-based-code-synthesis-with-github-copilot-via-cheap-and-efficient-prompt-engineering-jakub-res-et-al-2024>(5/8 | 258/329) Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering (Jakub Res et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakub Res, Ivan Homoliak, Martin Perešíni, Aleš Smrčka, Kamil Malinka, Petr Hanacek. (2024)<br><strong>Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering</strong><br><button class=copy-to-clipboard title="Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 15<br>Keywords: Black Box, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12671v1.pdf filename=2403.12671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI assistants for coding are on the rise. However one of the reasons developers and companies avoid harnessing their full potential is the questionable security of the generated code. This paper first reviews the current state-of-the-art and identifies areas for improvement on this issue. Then, we propose a systematic approach based on <b>prompt-altering</b> methods to achieve better code security of (even proprietary <b>black-box)</b> <b>AI-based</b> code generators such as GitHub Copilot, while minimizing the complexity of the application from the user point-of-view, the computational resources, and operational costs. In sum, we propose and evaluate three <b>prompt</b> altering methods: (1) scenario-specific, (2) iterative, and (3) general clause, while we discuss their combination. Contrary to the audit of code security, the latter two of the proposed methods require no expert knowledge from the user. We assess the effectiveness of the proposed methods on the GitHub Copilot using the OpenVPN project in realistic scenarios, and we demonstrate that the proposed methods reduce the number of insecure generated code samples by up to 16% and increase the number of secure code by up to 8%. Since our approach does not require access to the internals of the AI models, it can be in general applied to any AI-based code synthesizer, not only GitHub Copilot.</p></p class="citation"></blockquote><h3 id=68--259329-provable-privacy-with-non-private-pre-processing-yaxi-hu-et-al-2024>(6/8 | 259/329) Provable Privacy with Non-Private Pre-Processing (Yaxi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaxi Hu, Amartya Sanyal, Bernhard Schölkopf. (2024)<br><strong>Provable Privacy with Non-Private Pre-Processing</strong><br><button class=copy-to-clipboard title="Provable Privacy with Non-Private Pre-Processing" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR, stat-ML<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13041v1.pdf filename=2403.13041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, <b>quantization,</b> deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.</p></p class="citation"></blockquote><h3 id=78--260329-the-emergence-of-hardware-fuzzing-a-critical-review-of-its-significance-raghul-saravanan-et-al-2024>(7/8 | 260/329) The Emergence of Hardware Fuzzing: A Critical Review of its Significance (Raghul Saravanan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raghul Saravanan, Sai Manoj Pudukotai Dinakarrao. (2024)<br><strong>The Emergence of Hardware Fuzzing: A Critical Review of its Significance</strong><br><button class=copy-to-clipboard title="The Emergence of Hardware Fuzzing: A Critical Review of its Significance" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12812v1.pdf filename=2403.12812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been a notable surge in attention towards hardware security, driven by the increasing complexity and integration of processors, SoCs, and third-party IPs aimed at delivering advanced solutions. However, this complexity also introduces vulnerabilities and bugs into hardware systems, necessitating early detection during the IC design cycle to uphold system integrity and mitigate re-engineering costs. While the Design Verification (DV) community employs dynamic and formal verification strategies, they encounter challenges such as scalability for intricate designs and significant <b>human</b> <b>intervention,</b> leading to prolonged verification durations. As an alternative approach, hardware fuzzing, inspired by software testing methodologies, has gained prominence for its efficacy in identifying bugs within complex hardware designs. Despite the introduction of various hardware fuzzing techniques, obstacles such as inefficient conversion of hardware modules into software models impede their effectiveness. This Systematization of Knowledge (SoK) initiative delves into the fundamental principles of existing hardware fuzzing, methodologies, and their applicability across diverse hardware designs. Additionally, it evaluates factors such as the utilization of golden reference models (GRMs), coverage metrics, and toolchains to gauge their potential for broader adoption, akin to traditional formal verification methods. Furthermore, this work examines the reliability of existing hardware fuzzing techniques in identifying vulnerabilities and identifies research gaps for future advancements in design verification techniques.</p></p class="citation"></blockquote><h3 id=88--261329-tags-real-time-intrusion-detection-with-tag-propagation-based-provenance-graph-alignment-on-streaming-events-zhenyuan-li-et-al-2024>(8/8 | 261/329) TAGS: Real-time Intrusion Detection with Tag-Propagation-based Provenance Graph Alignment on Streaming Events (Zhenyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyuan Li, Yangyang Wei, Xiangmin Shen, Lingzhi Wang, Yan Chen, Haitao Xu, Shouling Ji, Fan Zhang. (2024)<br><strong>TAGS: Real-time Intrusion Detection with Tag-Propagation-based Provenance Graph Alignment on Streaming Events</strong><br><button class=copy-to-clipboard title="TAGS: Real-time Intrusion Detection with Tag-Propagation-based Provenance Graph Alignment on Streaming Events" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12541v1.pdf filename=2403.12541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution and advancement of cyberattacks pose challenges to existing security products. Recent concentrated research on provenance <b>graph-based</b> detection has proved its effectiveness in attack detection and investigation. However, implementing these approaches in practice encounters challenges such as high overhead, slow responsiveness, and low interpretability and extensibility. Towards practical attack detection and investigation with provenance <b>graphs,</b> we propose TAGS, a tag-propagation-based streaming provenance <b>graph</b> alignment system. Utilizing the tag-based intermediate result caching mechanism alongside carefully crafted propagation rules, we eliminate the need to store and duplicate raw data processing. This approach effectively mitigates in-memory storage requirements and minimizes data processing overhead, facilitating rapid on-stream <b>graph</b> alignment while significantly conserving CPU and memory resources. As a result, TAGS can detect and investigate various cyber-attacks in real-time. Moreover, TAGS allows analysts to customize attack query <b>graphs</b> flexibly to identify extended attacks in data streams. We conduct experimental evaluations on two large-scale public datasets containing 257.42 GB of audit logs with 12 relevant query <b>graphs</b> of varying sizes, covering multiple attack techniques and scenarios. The results demonstrate that TAGS is sufficiently efficient to process 176K events per second while sufficiently accurately identifying all 29 alignments in massive data. Moreover, it can effectively handle the dependency explosion problem with steady, low-level memory consumption (less than 300MB), producing only 3 false positives. Overall, the performance of TAGS significantly outperforms the state-of-the-art methods.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--262329-graph-neural-network-based-multi-agent-reinforcement-learning-for-resilient-distributed-coordination-of-multi-robot-systems-anthony-goeckner-et-al-2024>(1/2 | 262/329) Graph Neural Network-based Multi-agent Reinforcement Learning for Resilient Distributed Coordination of Multi-Robot Systems (Anthony Goeckner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anthony Goeckner, Yueyuan Sui, Nicolas Martinet, Xinliang Li, Qi Zhu. (2024)<br><strong>Graph Neural Network-based Multi-agent Reinforcement Learning for Resilient Distributed Coordination of Multi-Robot Systems</strong><br><button class=copy-to-clipboard title="Graph Neural Network-based Multi-agent Reinforcement Learning for Resilient Distributed Coordination of Multi-Robot Systems" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs-RO, cs.MA<br>Keyword Score: 43<br>Keywords: Graph, Graph Embedding, Graph Neural Network, Graph Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13093v1.pdf filename=2403.13093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing multi-agent coordination techniques are often fragile and vulnerable to anomalies such as agent attrition and communication disturbances, which are quite common in the real-world deployment of systems like field robotics. To better prepare these systems for the real world, we present a <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)-based</b> multi-agent <b>reinforcement</b> <b>learning</b> (MARL) method for resilient distributed coordination of a multi-robot system. Our method, Multi-Agent <b>Graph</b> <b>Embedding-based</b> <b>Coordination</b> (MAGEC), is trained using multi-agent proximal policy optimization (PPO) and enables distributed coordination around global objectives under agent attrition, partial observability, and limited or disturbed communications. We use a multi-robot patrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator and then compare its performance with prior coordination approaches. Results demonstrate that MAGEC outperforms existing methods in several experiments involving agent attrition and communication disturbance, and provides competitive results in scenarios without such anomalies.</p></p class="citation"></blockquote><h3 id=22--263329-uber-stable-formulating-the-rideshare-system-as-a-stable-matching-problem-rhea-acharya-et-al-2024>(2/2 | 263/329) Uber Stable: Formulating the Rideshare System as a Stable Matching Problem (Rhea Acharya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rhea Acharya, Jessica Chen, Helen Xiao. (2024)<br><strong>Uber Stable: Formulating the Rideshare System as a Stable Matching Problem</strong><br><button class=copy-to-clipboard title="Uber Stable: Formulating the Rideshare System as a Stable Matching Problem" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-GT, cs-MA, cs.MA<br>Keyword Score: 30<br>Keywords: Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13083v1.pdf filename=2403.13083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Peer-to-peer ride-sharing platforms like Uber, Lyft, and DiDi have revolutionized the transportation industry and labor market. At its essence, these systems tackle the bipartite matching problem between two populations: riders and drivers. This research paper comprises two main components: an initial literature review of existing ride-sharing platforms and efforts to enhance driver satisfaction, and the development of a novel algorithm implemented through <b>simulation</b> testing to allow us to make our own observations. The core algorithm utilized is the Gale-Shapley deferred acceptance algorithm, applied to a static matching problem over multiple time periods. In this <b>simulation,</b> we construct a preference-aware task assignment model, considering both overall revenue maximization and individual preference satisfaction. Specifically, the algorithm design incorporates factors such as passenger willingness-to-pay, driver preferences, and location attractiveness, with an overarching goal of achieving equitable income distribution for drivers while maintaining overall system efficiency. Through <b>simulation,</b> the paper compares the performance of the proposed algorithm with random matching and closest neighbor algorithms, looking at metrics such as total revenue, revenue per ride, and standard deviation to identify trends and impacts of shifting priorities. Additionally, the DA algorithm is compared to the Boston algorithm, and the paper explores the effect of prioritizing proximity to passengers versus distance from city center. Ultimately, the research underscores the importance of continued exploration in areas such as dynamic pricing models and additional modeling for unconventional driving times to further enhance the findings on the effectiveness and <b>fairness</b> of ride-sharing platforms.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--264329-a-new-intelligent-reflecting-surface-aided-electromagnetic-stealth-strategy-xue-xiong-et-al-2024>(1/1 | 264/329) A New Intelligent Reflecting Surface-Aided Electromagnetic Stealth Strategy (Xue Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xue Xiong, Beixiong Zheng, A. Lee Swindlehurst, Jie Tang, Wen Wu. (2024)<br><strong>A New Intelligent Reflecting Surface-Aided Electromagnetic Stealth Strategy</strong><br><button class=copy-to-clipboard title="A New Intelligent Reflecting Surface-Aided Electromagnetic Stealth Strategy" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 43<br>Keywords: Benchmarking, Karush-Kuhn-Tucker, Karush-Kuhn-Tucker, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12352v1.pdf filename=2403.12352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electromagnetic wave absorbing material (EWAM) plays an essential role in manufacturing stealth aircraft, which can achieve the electromagnetic stealth (ES) by reducing the strength of the signal reflected back to the radar system. However, the stealth performance is limited by the coating thickness, incident wave angles, and working frequencies. To tackle these limitations, we propose a new intelligent reflecting surface (IRS)-aided ES system where an IRS is deployed at the target to synergize with EWAM for effectively mitigating the echo signal and thus reducing the radar detection probability. Considering the monotonic relationship between the detection probability and the received signal-to-noise-ratio (SNR) at the radar, we formulate an optimization problem that minimizes the SNR under the reflection constraint of each IRS element, and a semi-closed-form solution is derived by using <b>Karush-Kuhn-Tucker</b> <b>(KKT)</b> conditions. <b>Simulation</b> results validate the superiority of the proposed IRS-aided ES system compared to various <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--265329-a-physics-embedded-deep-learning-framework-for-cloth-simulation-zhiwei-zhao-2024>(1/1 | 265/329) A Physics-embedded Deep Learning Framework for Cloth Simulation (Zhiwei Zhao, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiwei Zhao. (2024)<br><strong>A Physics-embedded Deep Learning Framework for Cloth Simulation</strong><br><button class=copy-to-clipboard title="A Physics-embedded Deep Learning Framework for Cloth Simulation" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: I-2-0; I-3-7, cs-GR, cs-LG, cs.GR<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12820v1.pdf filename=2403.12820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Delicate cloth <b>simulations</b> have long been desired in computer graphics. Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations. Deep learning has the potential to achieve fast and real-time <b>simulation,</b> but common neural network structures often demand many parameters to capture cloth dynamics. This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth <b>simulation.</b> The <b>convolutional</b> <b>neural</b> <b>network</b> is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics. The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. The model is tested across different cloth animation cases, without training with new data. Agreement with baselines and predictive realism successfully validate its generalization ability. Inference efficiency of the proposed model also defeats traditional physics <b>simulation.</b> This framework is also designed to easily integrate with other visual refinement techniques like wrinkle carving, which leaves significant chances to incorporate prevailing macing learning techniques in 3D cloth amination.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=14--266329-knowledge-and-data-dual-driven-channel-estimation-and-feedback-for-ultra-massive-mimo-systems-under-hybrid-field-beam-squint-effect-kuiyu-wang-et-al-2024>(1/4 | 266/329) Knowledge and Data Dual-Driven Channel Estimation and Feedback for Ultra-Massive MIMO Systems under Hybrid Field Beam Squint Effect (Kuiyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kuiyu Wang, Zhen Gao, Sheng Chen, Boyu Ning, Gaojie Chen, Yu Su, Zhaocheng Wang, H. Vincent Poor. (2024)<br><strong>Knowledge and Data Dual-Driven Channel Estimation and Feedback for Ultra-Massive MIMO Systems under Hybrid Field Beam Squint Effect</strong><br><button class=copy-to-clipboard title="Knowledge and Data Dual-Driven Channel Estimation and Feedback for Ultra-Massive MIMO Systems under Hybrid Field Beam Squint Effect" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 40<br>Keywords: Message-Passing, Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12813v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12813v1.pdf filename=2403.12813v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acquiring accurate channel state information (CSI) at an access point (AP) is challenging for wideband millimeter wave (mmWave) ultra-massive multiple-input and multiple-output (UMMIMO) systems, due to the high-dimensional channel matrices, hybrid near- and far- field channel feature, beam squint effects, and imperfect hardware constraints, such as low-resolution analog-to-digital converters, and in-phase and quadrature imbalance. To overcome these challenges, this paper proposes an efficient downlink channel estimation (CE) and CSI feedback approach based on knowledge and data dual-driven deep learning (DL) networks. Specifically, we first propose a data-driven residual neural network de-quantizer (ResNet-DQ) to pre-process the received pilot signals at user equipment (UEs), where the noise and distortion brought by imperfect hardware can be mitigated. A knowledge-driven generalized multiple measurement vector learned approximate message passing (GMMV-LAMP) network is then developed to jointly estimate the channels by exploiting the approximately same physical angle shared by different subcarriers. In particular, two wideband redundant dictionaries (WRDs) are proposed such that the measurement matrices of the GMMV-LAMP network can accommodate the far-field and near-field beam squint effect, respectively. Finally, we propose an encoder at the UEs and a decoder at the AP by a data-driven CSI residual network (CSI-ResNet) to compress the CSI matrix into a low-dimensional <b>quantized</b> bit vector for feedback, thereby reducing the feedback overhead substantially. <b>Simulation</b> results show that the proposed knowledge and data dual-driven approach outperforms conventional downlink CE and CSI feedback methods, especially in the case of low signal-to-noise ratios.</p></p class="citation"></blockquote><h3 id=24--267329-wmmse-based-rate-maximization-for-ris-assisted-mu-mimo-systems-hyuckjin-choi-et-al-2024>(2/4 | 267/329) WMMSE-Based Rate Maximization for RIS-Assisted MU-MIMO Systems (Hyuckjin Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyuckjin Choi, A. Lee Swindlehurst, Junil Choi. (2024)<br><strong>WMMSE-Based Rate Maximization for RIS-Assisted MU-MIMO Systems</strong><br><button class=copy-to-clipboard title="WMMSE-Based Rate Maximization for RIS-Assisted MU-MIMO Systems" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12498v1.pdf filename=2403.12498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS) technology, given its ability to favorably modify wireless communication environments, will play a pivotal role in the evolution of future communication systems. This paper proposes rate maximization techniques for both single-user and multiuser MIMO systems, based on the well-known weighted minimum mean square error (WMMSE) criterion. Using a suitable weight matrix, the WMMSE algorithm tackles an equivalent weighted mean square error (WMSE) minimization problem to achieve the sum-rate maximization. By considering a more practical RIS system model that employs a tensor-based representation enforced by the electromagnetic behavior exhibited by the RIS panel, we detail both the sum-rate maximizing and WMSE minimizing strategies for RIS phase shift optimization by deriving the closed-form gradient of the WMSE and the sum-rate with respect to the RIS phase shift vector. Our <b>simulations</b> reveal that the proposed rate maximization technique, rooted in the WMMSE algorithm, exhibits superior performance when compared to other <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=34--268329-wideband-modeling-and-beamforming-for-beyond-diagonal-reconfigurable-intelligent-surfaces-hongyu-li-et-al-2024>(3/4 | 268/329) Wideband Modeling and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces (Hongyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyu Li, Matteo Nerini, Shanpu Shen, Bruno Clerckx. (2024)<br><strong>Wideband Modeling and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces</strong><br><button class=copy-to-clipboard title="Wideband Modeling and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12893v1.pdf filename=2403.12893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work studies the wideband modeling and beamforming design of beyond diagonal reconfigurable intelligent surface (BD-RIS), which generalizes and goes beyond conventional RIS with diagonal phase shift matrices to achieve enhanced channel gain. Specifically, we investigate the response of BD-RIS in wideband systems by going back to its hardware circuit realizations. We propose a novel wideband model which has simple expressions while capturing the response variations of BD-RIS for signals with different frequencies. With this wideband model, we propose a BD-RIS design algorithm for an orthogonal frequency division multiplexing system to maximize the average rate over all subcarriers. Finally, we provide <b>simulation</b> results to evaluate the performance of the proposed design and show the importance of wideband modeling for BD-RIS.</p></p class="citation"></blockquote><h3 id=44--269329-sparse-estimation-for-xl-mimo-with-unified-losnlos-representation-xu-shi-et-al-2024>(4/4 | 269/329) Sparse Estimation for XL-MIMO with Unified LoS/NLoS Representation (Xu Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Shi, Xuehan Wang, Jingbo Tan, Jintao Wang. (2024)<br><strong>Sparse Estimation for XL-MIMO with Unified LoS/NLoS Representation</strong><br><button class=copy-to-clipboard title="Sparse Estimation for XL-MIMO with Unified LoS/NLoS Representation" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12506v1.pdf filename=2403.12506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extremely large-scale antenna array (ELAA) is promising as one of the key ingredients for the sixth generation (6G) of wireless communications. The electromagnetic propagation of spherical wavefronts introduces an additional distance-dependent dimension beyond conventional beamspace. In this paper, we first present one concise closed-form channel formulation for extremely large-scale multiple-input multiple-output (XL-MIMO). All line-of-sight (LoS) and non-line-of-sight (NLoS) paths, far-field and near-field scenarios, and XL-MIMO and XL-MISO channels are unified under the framework, where additional Vandermonde windowing matrix is exclusively considered for LoS path. Under this framework, we further propose one low-complexity unified LoS/NLoS orthogonal matching pursuit (XL-UOMP) algorithm for XL-MIMO channel estimation. The <b>simulation</b> results demonstrate the superiority of the proposed algorithm on both estimation accuracy and pilot consumption.</p></p class="citation"></blockquote><h2 id=csmm-2>cs.MM (2)</h2><h3 id=12--270329-ice-interactive-3d-game-character-editing-via-dialogue-haoqian-wu-et-al-2024>(1/2 | 270/329) ICE: Interactive 3D Game Character Editing via Dialogue (Haoqian Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoqian Wu, Yunjie Wu, Zhipeng Hu, Lincheng Li, Weijie Chen, Rui Zhao, Changjie Fan, Xin Yu. (2024)<br><strong>ICE: Interactive 3D Game Character Editing via Dialogue</strong><br><button class=copy-to-clipboard title="ICE: Interactive 3D Game Character Editing via Dialogue" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-HC, cs-MM, cs.MM<br>Keyword Score: 40<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12667v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12667v2.pdf filename=2403.12667v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-driven in-game 3D character auto-customization systems eliminate the complicated process of manipulating intricate character control parameters. However, current methods are limited by their single-round generation, incapable of further editing and fine-grained modification. In this paper, we propose an Interactive Character Editing framework (ICE) to achieve a multi-round dialogue-based refinement process. In a nutshell, our ICE offers a more user-friendly way to enable players to convey creative ideas iteratively while ensuring that created characters align with the expectations of players. Specifically, we propose an Instruction Parsing Module (IPM) that utilizes <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to parse multi-round dialogues into clear editing instruction <b>prompts</b> in each round. To reliably and swiftly modify character control parameters at a fine-grained level, we propose a Semantic-guided Low-dimension Parameter Solver (SLPS) that edits character control parameters according to <b>prompts</b> in a <b>zero-shot</b> manner. Our SLPS first localizes the character control parameters related to the fine-grained modification, and then optimizes the corresponding parameters in a low-dimension space to avoid unrealistic results. Extensive experimental results demonstrate the effectiveness of our proposed ICE for in-game character creation and the superior editing performance of ICE. Project page: <a href=https://iceedit.github.io/>https://iceedit.github.io/</a>.</p></p class="citation"></blockquote><h3 id=22--271329-newscaption-named-entity-aware-captioning-for-out-of-context-media-anurag-singh-et-al-2024>(2/2 | 271/329) NewsCaption: Named-Entity aware Captioning for Out-of-Context Media (Anurag Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anurag Singh, Shivangi Aneja. (2024)<br><strong>NewsCaption: Named-Entity aware Captioning for Out-of-Context Media</strong><br><button class=copy-to-clipboard title="NewsCaption: Named-Entity aware Captioning for Out-of-Context Media" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs-SI, cs.MM<br>Keyword Score: 10<br>Keywords: Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12618v1.pdf filename=2403.12618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing influence of social media, online misinformation has grown to become a societal issue. The motivation for our work comes from the threat caused by cheapfakes, where an unaltered image is described using a news caption in a new but false-context. The main challenge in detecting such out-of-context multimedia is the unavailability of large-scale datasets. Several detection methods employ randomly selected captions to generate out-of-context training inputs. However, these randomly matched captions are not truly representative of out-of-context scenarios due to inconsistencies between the image description and the matched caption. We aim to address these limitations by introducing a novel task of out-of-context caption generation. In this work, we propose a new method that generates a realistic out-of-context caption given visual and textual context. We also demonstrate that the semantics of the generated captions can be controlled using the textual context. We also evaluate our method against several baselines and our method improves over the image captioning baseline by 6.2% BLUE-4, 2.96% CiDEr, 11.5% <b>ROUGE,</b> and 7.3% METEOR</p></p class="citation"></blockquote><h2 id=csne-3>cs.NE (3)</h2><h3 id=13--272329-topological-representations-of-heterogeneous-learning-dynamics-of-recurrent-spiking-neural-networks-biswadeep-chakraborty-et-al-2024>(1/3 | 272/329) Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks (Biswadeep Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biswadeep Chakraborty, Saibal Mukhopadhyay. (2024)<br><strong>Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks</strong><br><button class=copy-to-clipboard title="Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 40<br>Keywords: Autoencoder, Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12462v1.pdf filename=2403.12462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spiking Neural Networks (SNNs) have become an essential paradigm in neuroscience and artificial intelligence, providing brain-inspired computation. Recent advances in literature have studied the network representations of deep neural networks. However, there has been little work that studies representations learned by SNNs, especially using <b>unsupervised</b> local learning methods like spike-timing dependent plasticity (STDP). Recent work by \cite{barannikov2021representation} has introduced a novel method to compare topological mappings of learned representations called Representation Topology Divergence (RTD). Though useful, this method is engineered particularly for feedforward deep neural networks and cannot be used for recurrent networks like Recurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to measure the difference between distributed representations of RSNN models with different learning methods. We propose a novel reformulation of RSNNs using feedforward <b>autoencoder</b> networks with skip connections to help us compute the RTD for recurrent networks. Thus, we investigate the learning capabilities of RSNN trained using STDP and the role of heterogeneity in the synaptic dynamics in learning such representations. We demonstrate that heterogeneous STDP in RSNNs yield distinct representations than their homogeneous and surrogate gradient-based <b>supervised</b> <b>learning</b> counterparts. Our results provide insights into the potential of heterogeneous SNN models, aiding the development of more efficient and biologically plausible hybrid artificial intelligence systems.</p></p class="citation"></blockquote><h3 id=23--273329-evolutionary-optimization-of-model-merging-recipes-takuya-akiba-et-al-2024>(2/3 | 273/329) Evolutionary Optimization of Model Merging Recipes (Takuya Akiba et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha. (2024)<br><strong>Evolutionary Optimization of Model Merging Recipes</strong><br><button class=copy-to-clipboard title="Evolutionary Optimization of Model Merging Recipes" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 33<br>Keywords: Benchmarking, Foundation Model, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13187v1.pdf filename=2403.13187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel application of evolutionary algorithms to automate the creation of powerful <b>foundation</b> <b>models.</b> While model merging has emerged as a promising approach for <b>LLM</b> development due to its cost-effectiveness, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese <b>LLM</b> with Math <b>reasoning</b> capabilities. Surprisingly, our Japanese Math <b>LLM</b> achieved state-of-the-art performance on a variety of established Japanese <b>LLM</b> <b>benchmarks,</b> even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to <b>foundation</b> <b>model</b> development.</p></p class="citation"></blockquote><h3 id=33--274329-learning-guided-iterated-local-search-for-the-minmax-multiple-traveling-salesman-problem-pengfei-he-et-al-2024>(3/3 | 274/329) Learning-guided iterated local search for the minmax multiple traveling salesman problem (Pengfei He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengfei He, Jin-Kao Hao, Jinhui Xia. (2024)<br><strong>Learning-guided iterated local search for the minmax multiple traveling salesman problem</strong><br><button class=copy-to-clipboard title="Learning-guided iterated local search for the minmax multiple traveling salesman problem" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 23<br>Keywords: Bandit Algorithm, Bandit Algorithm, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12389v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12389v1.pdf filename=2403.12389v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The minmax multiple traveling salesman problem involves minimizing the longest tour among a set of tours. The problem is of great practical interest because it can be used to formulate several real-life applications. To solve this computationally challenging problem, we propose a leaning-driven iterated local search approach that combines an aggressive local search procedure with a probabilistic acceptance criterion to find high-quality local optimal solutions and a multi-armed <b>bandit</b> <b>algorithm</b> to select various removal and insertion operators to escape local optimal traps. Extensive experiments on 77 commonly used <b>benchmark</b> instances show that our algorithm achieves excellent results in terms of solution quality and running time. In particular, it achieves 32 new best-known results and matches the best-known results for 35 other instances. Additional experiments shed light on the understanding of the composing elements of the algorithm.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--275329-on-the-effectiveness-of-large-language-models-for-github-workflows-xinyu-zhang-et-al-2024>(1/3 | 275/329) On the effectiveness of Large Language Models for GitHub Workflows (Xinyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Zhang, Siddharth Muralee, Sourag Cherupattamoolayil, Aravind Machiry. (2024)<br><strong>On the effectiveness of Large Language Models for GitHub Workflows</strong><br><button class=copy-to-clipboard title="On the effectiveness of Large Language Models for GitHub Workflows" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12446v1.pdf filename=2403.12446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>GitHub workflows or GitHub CI is a popular continuous integration platform that enables developers to automate various software engineering tasks by specifying them as workflows, i.e., YAML files with a list of jobs. However, engineering valid workflows is tedious. They are also prone to severe security issues, which can result in supply chain vulnerabilities. Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated their effectiveness in various software development tasks. However, GitHub workflows differ from regular programs in both structure and semantics. We perform the first comprehensive study to understand the effectiveness of <b>LLMs</b> on five workflow-related tasks with different levels of <b>prompts.</b> We curated a set of $\sim$400K workflows and generated <b>prompts</b> with varying detail. We also <b>fine-tuned</b> <b>LLMs</b> on GitHub workflow tasks. Our evaluation of three state-of-the-art <b>LLMs</b> and their <b>fine-tuned</b> variants revealed various interesting findings on the current effectiveness and drawbacks of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=23--276329-professional-insights-into-benefits-and-limitations-of-implementing-mlops-principles-gabriel-araujo-et-al-2024>(2/3 | 276/329) Professional Insights into Benefits and Limitations of Implementing MLOps Principles (Gabriel Araujo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel Araujo, Marcos Kalinowski, Markus Endler, Fabio Calefato. (2024)<br><strong>Professional Insights into Benefits and Limitations of Implementing MLOps Principles</strong><br><button class=copy-to-clipboard title="Professional Insights into Benefits and Limitations of Implementing MLOps Principles" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13115v1.pdf filename=2403.13115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: Machine Learning Operations (MLOps) has emerged as a set of practices that combines development, testing, and operations to deploy and maintain machine learning applications. Objective: In this paper, we assess the benefits and limitations of using the MLOps principles in online <b>supervised</b> <b>learning.</b> Method: We conducted two focus group sessions on the benefits and limitations of applying MLOps principles for online machine learning applications with six experienced machine learning developers. Results: The focus group revealed that machine learning developers see many benefits of using MLOps principles but also that these do not apply to all the projects they worked on. According to experts, this investment tends to pay off for larger applications with continuous deployment that require well-prepared automated processes. However, for initial versions of machine learning applications, the effort taken to implement the principles could enlarge the project&rsquo;s scope and increase the time needed to deploy a first version to production. The discussion brought up that most of the benefits are related to avoiding error-prone manual steps, enabling to restore the application to a previous state, and having a robust continuous automated deployment pipeline. Conclusions: It is important to balance the trade-offs of investing time and effort in implementing the MLOps principles considering the scope and needs of the project, favoring such investments for larger applications with continuous model deployment requirements.</p></p class="citation"></blockquote><h3 id=33--277329-navigating-compiler-errors-with-ai-assistance----a-study-of-gpt-hints-in-an-introductory-programming-course-maciej-pankiewicz-et-al-2024>(3/3 | 277/329) Navigating Compiler Errors with AI Assistance &ndash; A Study of GPT Hints in an Introductory Programming Course (Maciej Pankiewicz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maciej Pankiewicz, Ryan S. Baker. (2024)<br><strong>Navigating Compiler Errors with AI Assistance &ndash; A Study of GPT Hints in an Introductory Programming Course</strong><br><button class=copy-to-clipboard title="Navigating Compiler Errors with AI Assistance -- A Study of GPT Hints in an Introductory Programming Course" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-HC, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12737v1.pdf filename=2403.12737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We examined the efficacy of AI-assisted learning in an introductory programming course at the university level by using a <b>GPT-4</b> model to generate personalized hints for compiler errors within a platform for automated assessment of programming assignments. The control group had no access to <b>GPT</b> hints. In the experimental condition <b>GPT</b> hints were provided when a compiler error was detected, for the first half of the problems in each module. For the latter half of the module, hints were disabled. Students highly rated the usefulness of <b>GPT</b> hints. In affect surveys, the experimental group reported significantly higher levels of focus and lower levels of confrustion (confusion and frustration) than the control group. For the six most commonly occurring error types we observed mixed results in terms of performance when access to <b>GPT</b> hints was enabled for the experimental group. However, in the absence of <b>GPT</b> hints, the experimental group&rsquo;s performance surpassed the control group for five out of the six error types.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--278329-modal-analysis-of-spatiotemporal-data-via-multivariate-gaussian-process-regression-jiwoo-song-et-al-2024>(1/1 | 278/329) Modal Analysis of Spatiotemporal Data via Multivariate Gaussian Process Regression (Jiwoo Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiwoo Song, Daning Huang. (2024)<br><strong>Modal Analysis of Spatiotemporal Data via Multivariate Gaussian Process Regression</strong><br><button class=copy-to-clipboard title="Modal Analysis of Spatiotemporal Data via Multivariate Gaussian Process Regression" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-LG, math-DS, math-SP, stat-ME, stat-ML, stat.ME<br>Keyword Score: 33<br>Keywords: Benchmarking, Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13118v1.pdf filename=2403.13118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modal analysis has become an essential tool to understand the coherent structure of complex flows. The classical modal analysis methods, such as dynamic mode decomposition (DMD) and spectral proper orthogonal decomposition (SPOD), rely on a sufficient amount of data that is regularly sampled in time. However, often one needs to deal with sparse temporally irregular data, e.g., due to experimental measurements and <b>simulation</b> algorithm. To overcome the limitations of data scarcity and irregular sampling, we propose a novel modal analysis technique using multi-variate <b>Gaussian</b> <b>process</b> regression (MVGPR). We first establish the connection between MVGPR and the existing modal analysis techniques, DMD and SPOD, from a linear system identification perspective. Next, leveraging this connection, we develop a MVGPR-based modal analysis technique that addresses the aforementioned limitations. The capability of MVGPR is endowed by its judiciously designed kernel structure for correlation function, that is derived from the assumed linear dynamics. Subsequently, the proposed MVGPR method is <b>benchmarked</b> against DMD and SPOD on a range of examples, from academic and synthesized data to unsteady airfoil aerodynamics. The results demonstrate MVGPR as a promising alternative to classical modal analysis methods, especially in the scenario of scarce and temporally irregular data.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--279329-generating-automatic-feedback-on-ui-mockups-with-large-language-models-peitong-duan-et-al-2024>(1/4 | 279/329) Generating Automatic Feedback on UI Mockups with Large Language Models (Peitong Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peitong Duan, Jeremy Warner, Yang Li, Bjoern Hartmann. (2024)<br><strong>Generating Automatic Feedback on UI Mockups with Large Language Models</strong><br><button class=copy-to-clipboard title="Generating Automatic Feedback on UI Mockups with Large Language Models" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13139v1.pdf filename=2403.13139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feedback on user interface (UI) mockups is crucial in design. However, human feedback is not always readily available. We explore the potential of using <b>large</b> <b>language</b> <b>models</b> for automatic feedback. Specifically, we focus on applying <b>GPT-4</b> to automate heuristic evaluation, which currently entails a human expert assessing a UI&rsquo;s compliance with a set of design guidelines. We implemented a Figma plugin that takes in a UI design and a set of written heuristics, and renders automatically-generated feedback as constructive suggestions. We assessed performance on 51 UIs using three sets of guidelines, compared <b>GPT-4-generated</b> design suggestions with those from human experts, and conducted a study with 12 expert designers to understand fit with existing practice. We found that <b>GPT-4-based</b> feedback is useful for catching subtle errors, improving text, and considering UI semantics, but feedback also decreased in utility over iterations. Participants described several uses for this plugin despite its imperfect suggestions.</p></p class="citation"></blockquote><h3 id=24--280329-rapid-aideation-generating-ideas-with-the-self-and-in-collaboration-with-large-language-models-gionnieve-lim-et-al-2024>(2/4 | 280/329) Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models (Gionnieve Lim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gionnieve Lim, Simon T. Perrault. (2024)<br><strong>Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models</strong><br><button class=copy-to-clipboard title="Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12928v1.pdf filename=2403.12928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative artificial intelligence (GenAI) can rapidly produce <b>large</b> <b>and</b> <b>diverse</b> volumes of content. This lends to it a quality of creativity which can be empowering in the early stages of design. In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to brainstorm potential solutions and evaluate them. We found that the <b>LLM</b> produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas. Participants typically <b>prompted</b> in a straightforward manner with concise instructions. We also observed two collaborative dynamics with the <b>LLM</b> fulfilling a consulting role or an assisting role depending on the goals of the users. Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to <b>prompt</b> the <b>LLM.</b></p></p class="citation"></blockquote><h3 id=34--281329-fact-checking-chatbot-a-misinformation-intervention-for-instant-messaging-apps-and-an-analysis-of-trust-in-the-fact-checkers-gionnieve-lim-et-al-2024>(3/4 | 281/329) Fact Checking Chatbot: A Misinformation Intervention for Instant Messaging Apps and an Analysis of Trust in the Fact Checkers (Gionnieve Lim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gionnieve Lim, Simon T. Perrault. (2024)<br><strong>Fact Checking Chatbot: A Misinformation Intervention for Instant Messaging Apps and an Analysis of Trust in the Fact Checkers</strong><br><button class=copy-to-clipboard title="Fact Checking Chatbot: A Misinformation Intervention for Instant Messaging Apps and an Analysis of Trust in the Fact Checkers" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Chatbot, Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12913v1.pdf filename=2403.12913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Singapore, there has been a rise in misinformation on mobile instant messaging services (MIMS). MIMS support both small peer-to-peer networks and large groups. Misinformation in the former may spread due to recipients&rsquo; trust in the sender while in the latter, misinformation can directly reach a wide audience. The encryption of MIMS makes it difficult to address misinformation directly. As such, <b>chatbots</b> have become an alternative solution where users can disclose their chat content directly to <b>fact</b> <b>checking</b> services. To understand how effective <b>fact</b> <b>checking</b> <b>chatbots</b> are as an intervention and how trust in three different <b>fact</b> <b>checkers</b> (i.e., Government, News Outlets, and Artificial Intelligence) may affect this trust, we conducted a within-subjects experiment with 527 Singapore residents. We found mixed results for the <b>fact</b> <b>checkers</b> but support for the <b>chatbot</b> intervention overall. We also found a striking contradiction between participants&rsquo; trust in the <b>fact</b> <b>checkers</b> and their behaviour towards them. Specifically, those who reported a high level of trust in the government performed worse and tended to follow the <b>fact</b> <b>checking</b> tool less when it was endorsed by the government.</p></p class="citation"></blockquote><h3 id=44--282329-effects-of-automated-misinformation-warning-labels-on-the-intents-to-like-comment-and-share-posts-gionnieve-lim-et-al-2024>(4/4 | 282/329) Effects of Automated Misinformation Warning Labels on the Intents to Like, Comment and Share Posts (Gionnieve Lim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gionnieve Lim, Simon T. Perrault. (2024)<br><strong>Effects of Automated Misinformation Warning Labels on the Intents to Like, Comment and Share Posts</strong><br><button class=copy-to-clipboard title="Effects of Automated Misinformation Warning Labels on the Intents to Like, Comment and Share Posts" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12916v1.pdf filename=2403.12916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With <b>fact-checking</b> <b>by</b> professionals being difficult to scale on social media, algorithmic techniques have been considered. However, it is uncertain how the public may react to labels by automated <b>fact-checkers.</b> <b>In</b> this study, we investigate the use of automated warning labels derived from misinformation detection literature and investigate their effects on three forms of post engagement. Focusing on political posts, we also consider how partisanship affects engagement. In a two-phases within-subjects experiment with 200 participants, we found that the generic warnings suppressed intents to comment on and share posts, but not on the intent to like them. Furthermore, when different reasons for the labels were provided, their effects on post engagement were inconsistent, suggesting that the reasons could have undesirably motivated engagement instead. Partisanship effects were observed across the labels with higher engagement for politically congruent posts. We discuss the implications on the design and use of automated warning labels.</p></p class="citation"></blockquote><h2 id=csdc-3>cs.DC (3)</h2><h3 id=13--283329-toward-sustainable-genai-using-generation-directives-for-carbon-friendly-large-language-model-inference-baolin-li-et-al-2024>(1/3 | 283/329) Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference (Baolin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari. (2024)<br><strong>Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference</strong><br><button class=copy-to-clipboard title="Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AI, cs-CL, cs-DC, cs-LG, cs.DC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12900v1.pdf filename=2403.12900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> inference services. Sprout leverages the innovative concept of &ldquo;generation directives&rdquo; to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user <b>prompts</b> and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations using the Llama2 <b>LLM</b> and global electricity grid data. This research marks a critical step toward aligning AI technology with sustainable practices, highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence.</p></p class="citation"></blockquote><h3 id=23--284329-performance-portable-monte-carlo-particle-transport-on-intel-nvidia-and-amd-gpus-john-tramm-et-al-2024>(2/3 | 284/329) Performance Portable Monte Carlo Particle Transport on Intel, NVIDIA, and AMD GPUs (John Tramm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John Tramm, Paul Romano, Patrick Shriwise, Amanda Lund, Johannes Doerfert, Patrick Steinbrecher, Andrew Siegel, Gavin Ridley. (2024)<br><strong>Performance Portable Monte Carlo Particle Transport on Intel, NVIDIA, and AMD GPUs</strong><br><button class=copy-to-clipboard title="Performance Portable Monte Carlo Particle Transport on Intel, NVIDIA, and AMD GPUs" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12345v1.pdf filename=2403.12345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>OpenMC is an open source Monte Carlo neutral particle transport application that has recently been ported to GPU using the OpenMP target offloading model. We examine the performance of OpenMC at scale on the Frontier, Polaris, and Aurora supercomputers, demonstrating that performance portability has been achieved by OpenMC across all three major GPU vendors (AMD, NVIDIA, and Intel). OpenMC&rsquo;s GPU performance is compared to both the traditional CPU-based version of OpenMC as well as several other state-of-the-art CPU-based Monte Carlo particle transport applications. We also provide historical context by analyzing OpenMC&rsquo;s performance on several legacy GPU and CPU architectures. This work includes some of the first published results for a scientific <b>simulation</b> application at scale on a supercomputer featuring Intel&rsquo;s Max series &ldquo;Ponte Vecchio&rdquo; GPUs. It is also one of the first demonstrations of a large scientific production application using the OpenMP target offloading model to achieve high performance on all three major GPU platforms.</p></p class="citation"></blockquote><h3 id=33--285329-parallel-gaussian-process-with-kernel-approximation-in-cuda-davide-carminati-2024>(3/3 | 285/329) Parallel Gaussian process with kernel approximation in CUDA (Davide Carminati, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Carminati. (2024)<br><strong>Parallel Gaussian process with kernel approximation in CUDA</strong><br><button class=copy-to-clipboard title="Parallel Gaussian process with kernel approximation in CUDA" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 13<br>Keywords: Benchmarking, Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12797v1.pdf filename=2403.12797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a parallel implementation in CUDA/C++ of the <b>Gaussian</b> <b>process</b> with a decomposed kernel. This recent formulation, introduced by Joukov and Kuli'c (2022), is characterized by an approximated &ndash; but much smaller &ndash; matrix to be inverted compared to plain <b>Gaussian</b> <b>process.</b> However, it exhibits a limitation when dealing with higher-dimensional samples which degrades execution times. The solution presented in this paper relies on parallelizing the computation of the predictive posterior statistics on a GPU using CUDA and its libraries. The CPU code and GPU code are then <b>benchmarked</b> on different CPU-GPU configurations to show the benefits of the parallel implementation on GPU over the CPU.</p></p class="citation"></blockquote><h2 id=mathna-8>math.NA (8)</h2><h3 id=18--286329-adaptive-multilevel-neural-networks-for-parametric-pdes-with-error-estimation-janina-e-schütte-et-al-2024>(1/8 | 286/329) Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation (Janina E. Schütte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Janina E. Schütte, Martin Eigel. (2024)<br><strong>Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation</strong><br><button class=copy-to-clipboard title="Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M55, 68T07, G-1-8, cs-LG, cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12650v1.pdf filename=2403.12650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To solve high-dimensional parameter-dependent partial differential equations (pPDEs), a neural network architecture is presented. It is constructed to map parameters of the model data to corresponding finite element solutions. To improve training efficiency and to enable control of the approximation error, the network mimics an adaptive finite element method (AFEM). It outputs a coarse grid solution and a series of corrections as produced in an AFEM, allowing a tracking of the error decay over successive layers of the network. The observed errors are measured by a reliable residual based a posteriori error estimator, enabling the reduction to only few parameters for the approximation in the output of the network. This leads to a problem adapted representation of the solution on locally refined grids. Furthermore, each solution of the AFEM is discretized in a hierarchical basis. For the architecture, <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> are chosen. The hierarchical basis then allows to handle sparse images for finely discretized meshes. Additionally, as corrections on finer levels decrease in amplitude, i.e., importance for the overall approximation, the accuracy of the network approximation is allowed to decrease successively. This can either be incorporated in the number of generated high fidelity samples used for training or the size of the network components responsible for the fine grid outputs. The architecture is described and preliminary numerical examples are presented.</p></p class="citation"></blockquote><h3 id=28--287329-a-note-on-incomplete-cholesky-factorizations-in-half-precision-arithmetic-jennifer-scott-et-al-2024>(2/8 | 287/329) A note on incomplete Cholesky factorizations in half precision arithmetic (Jennifer Scott et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jennifer Scott, Miroslav Tůma. (2024)<br><strong>A note on incomplete Cholesky factorizations in half precision arithmetic</strong><br><button class=copy-to-clipboard title="A note on incomplete Cholesky factorizations in half precision arithmetic" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65F08, 65F10, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13123v1.pdf filename=2403.13123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incomplete factorizations have long been popular general-purpose algebraic preconditioners for solving large sparse linear systems of equations. Guaranteeing the factorization is breakdown free while computing a high quality preconditioner is challenging. A resurgence of interest in using low precision arithmetic makes the search for robustness more urgent and tougher. In this paper, we focus on symmetric positive definite problems and explore a number of approaches: a look-ahead strategy to anticipate break down as early as possible, the use of global shifts, and a modification of an idea developed in the field of numerical optimization for the complete Cholesky factorization of dense matrices. Our numerical <b>simulations</b> target highly ill-conditioned sparse linear systems with the goal of computing the factors in half precision arithmetic and then achieving double precision accuracy using mixed precision refinement.</p></p class="citation"></blockquote><h3 id=38--288329-local-reconstruction-analysis-of-inverting-the-radon-transform-in-the-plane-from-noisy-discrete-data-anuj-abhishek-et-al-2024>(3/8 | 288/329) Local reconstruction analysis of inverting the Radon transform in the plane from noisy discrete data (Anuj Abhishek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anuj Abhishek, Alexander Katsevich, James W. Webber. (2024)<br><strong>Local reconstruction analysis of inverting the Radon transform in the plane from noisy discrete data</strong><br><button class=copy-to-clipboard title="Local reconstruction analysis of inverting the Radon transform in the plane from noisy discrete data" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12909v1.pdf filename=2403.12909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the reconstruction error, $N_\e^{\text{rec}}(x)$, when a linear, filtered back-projection (FBP) algorithm is applied to noisy, discrete Radon transform data with sampling step size $\epsilon$ in two-dimensions. Specifically, we analyze $N_\e^{\text{rec}}(x)$ for $x$ in small, $O(\e)$-sized neighborhoods around a generic fixed point, $x_0$, in the plane, where the measurement noise values, $\eta_{k,j}$ (i.e., the errors in the sinogram space), are random variables. The latter are independent, but not necessarily identically distributed. We show, under suitable assumptions on the first three moments of the $\eta_{k,j}$, that the following limit exists: $N^{\text{rec}}(\chx;x_0) = \lim_{\e\to0}N_\e^{\text{rec}}(x_0+\e\chx)$, for $\check x$ in a bounded domain. Here, $N_\e^{\text{rec}}$ and $ N^{\text{rec}}$ are viewed as continuous random variables, and the limit is understood in the sense of distributions. Once the limit is established, we prove that $N^{\text{rec}}$ is a zero mean Gaussian random field and compute explicitly its covariance. In addition, we validate our theory using numerical <b>simulations</b> and pseudo random noise.</p></p class="citation"></blockquote><h3 id=48--289329-a-second-order-iterative-time-integration-scheme-for-linear-poroelasticity-r-altmann-et-al-2024>(4/8 | 289/329) A second-order iterative time integration scheme for linear poroelasticity (R. Altmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>R. Altmann, M. Deiml. (2024)<br><strong>A second-order iterative time integration scheme for linear poroelasticity</strong><br><button class=copy-to-clipboard title="A second-order iterative time integration scheme for linear poroelasticity" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12699v1.pdf filename=2403.12699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel time stepping method for linear poroelasticity by extending a recent iterative decoupling approach to the second-order case. This results in a two-step scheme with an inner iteration and a relaxation step. We prove second-order convergence for a prescribed number of inner iteration steps, only depending on the coupling strength of the elastic and the flow equation. The efficiency of the scheme is illustrated by a number of numerical experiments, including a <b>simulation</b> of three-dimensional brain tissue.</p></p class="citation"></blockquote><h3 id=58--290329-numerical-approximation-of-a-class-of-constrained-hamilton-jacobi-equations-benoît-gaudeul-et-al-2024>(5/8 | 290/329) Numerical approximation of a class of constrained Hamilton-Jacobi equations (Benoît Gaudeul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benoît Gaudeul, Hélène Hivert. (2024)<br><strong>Numerical approximation of a class of constrained Hamilton-Jacobi equations</strong><br><button class=copy-to-clipboard title="Numerical approximation of a class of constrained Hamilton-Jacobi equations" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12557v1.pdf filename=2403.12557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a framework for the discretization of a class of constrained Hamilton-Jacobi equations, a system coupling a Hamilton-Jacobi equation with a Lagrange multiplier determined by the constraint. The equation is non-local, and the constraint has bounded variations. We show that, under a set of general hypothesis, the approximation obtained with a finite-differences monotonic scheme, converges towards the viscosity solution of the constrained Hamilton-Jacobi equation. Constrained Hamilton-Jacobi equations often arise as the long time and small mutation asymptotics of population models in quantitative genetics. As an example, we detail the construction of a scheme for the limit of an integral Lotka-Volterra equation. We also construct and analyze an Asymptotic-Preserving (AP) scheme for the model outside of the asymptotics. We prove that it is stable along the transition towards the asymptotics. The theoretical analysis of the schemes is illustrated and discussed with numerical <b>simulations.</b> The AP scheme is also used to conjecture the asymptotic behavior of the integral Lotka-Volterra equation, when the environment varies in time.</p></p class="citation"></blockquote><h3 id=68--291329-to-blow-up-or-not-to-blow-up-for-a-granular-kinetic-equation-josé-a-carrillo-et-al-2024>(6/8 | 291/329) To blow-up or not to blow-up for a granular kinetic equation (José A. Carrillo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José A. Carrillo, Ruiwen Shu, Li Wang, Wuzhe Xu. (2024)<br><strong>To blow-up or not to blow-up for a granular kinetic equation</strong><br><button class=copy-to-clipboard title="To blow-up or not to blow-up for a granular kinetic equation" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-AP, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12735v1.pdf filename=2403.12735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A simplified kinetic description of rapid granular media leads to a nonlocal Vlasov-type equation with a <b>convolution</b> integral operator that is of the same form as the continuity equations for aggregation-diffusion macroscopic dynamics. While the singular behavior of these nonlinear continuity equations is well studied in the literature, the extension to the corresponding granular kinetic equation is highly nontrivial. The main question is whether the singularity formed in velocity direction will be enhanced or mitigated by the shear in phase space due to free transport. We present a preliminary study through a meticulous numerical investigation and heuristic arguments. We have numerically developed a structure-preserving method with adaptive mesh refinement that can effectively capture potential blow-up behavior in the solution for granular kinetic equations. We have analytically constructed a finite-time blow-up infinite mass solution and discussed how this can provide insights into the finite mass scenario.</p></p class="citation"></blockquote><h3 id=78--292329-rate-optimal-higher-order-adaptive-conforming-fem-for-biharmonic-eigenvalue-problems-on-polygonal-domains-carsten-carstensen-et-al-2024>(7/8 | 292/329) Rate-optimal higher-order adaptive conforming FEM for biharmonic eigenvalue problems on polygonal domains (Carsten Carstensen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carsten Carstensen, Benedikt Gräßle. (2024)<br><strong>Rate-optimal higher-order adaptive conforming FEM for biharmonic eigenvalue problems on polygonal domains</strong><br><button class=copy-to-clipboard title="Rate-optimal higher-order adaptive conforming FEM for biharmonic eigenvalue problems on polygonal domains" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N12, 65N30, 65Y20, cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12577v1.pdf filename=2403.12577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The a posteriori error analysis of the classical Argyris finite element methods dates back to 1996, while the optimal convergence rates of associated adaptive finite element schemes are established only very recently in 2021. It took a long time to realise the necessity of an extension of the classical finite element spaces to make them hierarchical. This paper establishes the novel adaptive schemes for the biharmonic eigenvalue problems and provides a mathematical proof of optimal convergence rates towards a simple eigenvalue and numerical evidence thereof. This makes the suggested algorithm highly competitive and clearly justifies the higher computational and implementational costs compared to low-order nonconforming schemes. The numerical experiments provide overwhelming evidence that higher polynomial degrees pay off with higher convergence rates and underline that adaptive mesh-refining is mandatory. Five computational <b>benchmarks</b> display accurate reference eigenvalues up to 30 digits.</p></p class="citation"></blockquote><h3 id=88--293329-stochastic-variance-reduced-gradient-method-for-linear-ill-posed-inverse-problems-qinian-jin-et-al-2024>(8/8 | 293/329) Stochastic variance reduced gradient method for linear ill-posed inverse problems (Qinian Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinian Jin, Liuhong Chen. (2024)<br><strong>Stochastic variance reduced gradient method for linear ill-posed inverse problems</strong><br><button class=copy-to-clipboard title="Stochastic variance reduced gradient method for linear ill-posed inverse problems" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12460v1.pdf filename=2403.12460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we apply the stochastic variance reduced gradient (SVRG) method, which is a popular variance reduction method in optimization for accelerating the stochastic gradient method, to solve large scale linear ill-posed systems in Hilbert spaces. Under {\it a priori} choices of stopping indices, we derive a convergence rate result when the sought solution satisfies a <b>benchmark</b> source condition and establish a convergence result without using any source condition. To terminate the method in an {\it a posteriori} manner, we consider the discrepancy principle and show that it terminates the method in finite many iteration steps almost surely. Various numerical results are reported to test the performance of the method.</p></p class="citation"></blockquote><h2 id=csdb-4>cs.DB (4)</h2><h3 id=14--294329-efficient-k-step-weighted-reachability-query-processing-algorithms-lian-chen-et-al-2024>(1/4 | 294/329) Efficient k-step Weighted Reachability Query Processing Algorithms (Lian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lian Chen, Junfeng Zhou, Ming Du, Sheng Yu, Xian Tang, Ziyang Chen. (2024)<br><strong>Efficient k-step Weighted Reachability Query Processing Algorithms</strong><br><button class=copy-to-clipboard title="Efficient k-step Weighted Reachability Query Processing Algorithms" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 23<br>Keywords: Graph, Pruning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13181v1.pdf filename=2403.13181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a data <b>graph</b> G, a source vertex u and a target vertex v of a reachability query, the reachability query is used to answer whether there exists a path from u to v in G. Reachability query processing is one of the fundamental operations in <b>graph</b> data management, which is widely used in biological networks, communication networks, and social networks to assist data analysis. The data <b>graphs</b> in practical applications usually contain information such as <b>quantization</b> weights associated with the structural relationships, in addition to the structural relationships between vertices. Thus, in addition to the traditional reachability relationships, users may want to further understand whether such reachability relationships satisfy specific constraints. In this paper, we study the problem of efficiently processing k -step reachability queries with weighted constraints in weighted <b>graphs.</b> The k -step weighted reachability query questions are used to answer the question of whether there exists a path from a source vertex u to a goal vertex v in a given weighted <b>graph.</b> If it exists, the path needs to satisfy 1) all edges in the path satisfy the given weight constraints, and 2) the length of the path does not exceed the given distance threshold k. To address the problem, firstly, WKRI index supporting k -step weighted reachability query processing and index construction methods based on efficient <b>pruning</b> strategies are proposed. Secondly, the idea of constructing index based on part of the vertexs is proposed to reduce the size of the index. We design and implement two optimized indexes GWKRI and LWKRI based on the vertex coverage set. Finally, experiments are conducted on several real datasets. The experimental results verify the efficiency of the method proposed in this paper in answering k -step weighted reachability queries.</p></p class="citation"></blockquote><h3 id=24--295329-quantixar-high-performance-vector-data-management-system-gulshan-yadav-et-al-2024>(2/4 | 295/329) Quantixar: High-performance Vector Data Management System (Gulshan Yadav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gulshan Yadav, RahulKumar Yadav, Mansi Viramgama, Mayank Viramgama, Apeksha Mohite. (2024)<br><strong>Quantixar: High-performance Vector Data Management System</strong><br><button class=copy-to-clipboard title="Quantixar: High-performance Vector Data Management System" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 20<br>Keywords: Quantization, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12583v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12583v1.pdf filename=2403.12583v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional database management systems need help efficiently represent and querying the complex, high-dimensional data prevalent in modern applications. Vector databases offer a solution by storing data as numerical vectors within a multi-dimensional space. This enables similarity-based search and analysis, such as image retrieval, <b>recommendation</b> engine generation, and natural language processing. This paper introduces Quantixar, a vector database project designed for efficiency in high-dimensional settings. Quantixar tackles the challenge of managing high-dimensional data by strategically combining advanced indexing and <b>quantization</b> techniques. It employs HNSW indexing for accelerated ANN search. Additionally, Quantixar incorporates binary and product <b>quantization</b> to compress high-dimensional vectors, reducing storage requirements and computational costs during search. The paper delves into Quantixar&rsquo;s architecture, specific implementation, and experimental methodology.</p></p class="citation"></blockquote><h3 id=34--296329-evaluating-datalog-over-semirings-a-grounding-based-approach-hangdong-zhao-et-al-2024>(3/4 | 296/329) Evaluating Datalog over Semirings: A Grounding-based Approach (Hangdong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hangdong Zhao, Shaleen Deep, Paraschos Koutris, Sudeepa Roy, Val Tannen. (2024)<br><strong>Evaluating Datalog over Semirings: A Grounding-based Approach</strong><br><button class=copy-to-clipboard title="Evaluating Datalog over Semirings: A Grounding-based Approach" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12436v1.pdf filename=2403.12436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Datalog is a powerful yet elegant language that allows expressing recursive computation. Although Datalog evaluation has been extensively studied in the literature, so far, only loose upper bounds are known on how fast a Datalog program can be evaluated. In this work, we ask the following question: given a Datalog program over a naturally-ordered semiring $\sigma$, what is the tightest possible runtime? To this end, our main contribution is a general two-phase framework for analyzing the data complexity of Datalog over $\sigma$: first ground the program into an equivalent system of polynomial equations (i.e. <b>grounding)</b> and then find the least fixpoint of the <b>grounding</b> over $\sigma$. We present algorithms that use structure-aware query evaluation techniques to obtain the smallest possible <b>groundings.</b> Next, efficient algorithms for fixpoint evaluation are introduced over two classes of semirings: (1) finite-rank semirings and (2) absorptive semirings of total order. Combining both phases, we obtain state-of-the-art and new algorithmic results. Finally, we complement our results with a matching fine-grained lower bound.</p></p class="citation"></blockquote><h3 id=44--297329-a-benchmark-for-data-management-challenges-in-microservices-rodrigo-laigner-et-al-2024>(4/4 | 297/329) A Benchmark for Data Management Challenges in Microservices (Rodrigo Laigner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rodrigo Laigner, Zhexiang Zhang, Yijian Liu, Leonardo Freitas Gomes, Yongluan Zhou. (2024)<br><strong>A Benchmark for Data Management Challenges in Microservices</strong><br><button class=copy-to-clipboard title="A Benchmark for Data Management Challenges in Microservices" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs-SE, cs.DB<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12605v1.pdf filename=2403.12605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microservice architectures emerged as a popular architecture for designing scalable distributed applications. Although microservices have been extensively employed in industry settings for over a decade, there is little understanding of the data management challenges that arise in these applications. As a result, it is difficult to advance data system technologies for supporting microservice applications. To fill this gap, we present Online Marketplace, a microservice <b>benchmark</b> that incorporates core data management challenges that existing <b>benchmarks</b> have not sufficiently addressed. These challenges include transaction processing, query processing, event processing, constraint enforcement, and data replication. We have defined criteria for various data management issues to enable proper comparison across data systems and platforms. After specifying the <b>benchmark,</b> we present the challenges we faced in creating workloads that accurately reflect the dynamic state of the microservices. We also discuss implementation issues that we encountered when developing Online Marketplace in state-of-the-art data platforms, which prevented us from meeting the specified data management requirements and criteria. Our evaluation demonstrates that the <b>benchmark</b> is a valuable tool for testing important properties sought by microservice practitioners. As a result, our proposed <b>benchmark</b> will facilitate the design of future data systems to meet the expectations of microservice practitioners.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--298329-revisiting-local-computation-of-pagerank-simple-and-optimal-hanzhi-wang-et-al-2024>(1/2 | 298/329) Revisiting Local Computation of PageRank: Simple and Optimal (Hanzhi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzhi Wang, Zhewei Wei, Ji-Rong Wen, Mingji Yang. (2024)<br><strong>Revisiting Local Computation of PageRank: Simple and Optimal</strong><br><button class=copy-to-clipboard title="Revisiting Local Computation of PageRank: Simple and Optimal" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12648v1.pdf filename=2403.12648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We revisit the classic local <b>graph</b> exploration algorithm ApproxContributions proposed by Andersen, Borgs, Chayes, Hopcroft, Mirrokni, and Teng (WAW &lsquo;07, Internet Math. &lsquo;08) for computing an $\epsilon$-approximation of the PageRank contribution vector for a target node $t$ on a <b>graph</b> with $n$ nodes and $m$ edges. We give a worst-case complexity bound of ApproxContributions as $O(n\pi(t)/\epsilon\cdot\min(\Delta_{in},\Delta_{out},\sqrt{m}))$, where $\pi(t)$ is the PageRank score of $t$, and $\Delta_{in}$ and $\Delta_{out}$ are the maximum in-degree and out-degree of the <b>graph,</b> resp. We also give a lower bound of $\Omega(\min(\Delta_{in}/\delta,\Delta_{out}/\delta,\sqrt{m}/\delta,m))$ for detecting the $\delta$-contributing set of $t$, showing that the simple ApproxContributions algorithm is already optimal. We also investigate the computational complexity of locally estimating a node&rsquo;s PageRank centrality. We improve the best-known upper bound of $\widetilde{O}(n^{2/3}\cdot\min(\Delta_{out}^{1/3},m^{1/6}))$ given by Bressan, Peserico, and Pretto (SICOMP &lsquo;23) to $O(n^{1/2}\cdot\min(\Delta_{in}^{1/2},\Delta_{out}^{1/2},m^{1/4}))$ by simply combining ApproxContributions with the Monte Carlo <b>simulation</b> method. We also improve their lower bound of $\Omega(\min(n^{1/2}\Delta_{out}^{1/2},n^{1/3}m^{1/3}))$ to $\Omega(n^{1/2}\cdot\min(\Delta_{in}^{1/2},\Delta_{out}^{1/2},m^{1/4}))$ if $\min(\Delta_{in},\Delta_{out})=\Omega(n^{1/3})$, and to $\Omega(n^{1/2-\gamma}(\min(\Delta_{in},\Delta_{out}))^{1/2+\gamma})$ if $\min(\Delta_{in},\Delta_{out})=o(n^{1/3})$, where $\gamma>0$ is an arbitrarily small constant. Our matching upper and lower bounds resolve the open problem of whether one can tighten the bounds given by Bressan, Peserico, and Pretto (FOCS &lsquo;18, SICOMP &lsquo;23). Remarkably, the techniques and analyses for proving all our results are surprisingly simple.</p></p class="citation"></blockquote><h3 id=22--299329-exact-and-heuristic-computation-of-the-scanwidth-of-directed-acyclic-graphs-niels-holtgrefe-et-al-2024>(2/2 | 299/329) Exact and Heuristic Computation of the Scanwidth of Directed Acyclic Graphs (Niels Holtgrefe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niels Holtgrefe, Leo van Iersel, Mark Jones. (2024)<br><strong>Exact and Heuristic Computation of the Scanwidth of Directed Acyclic Graphs</strong><br><button class=copy-to-clipboard title="Exact and Heuristic Computation of the Scanwidth of Directed Acyclic Graphs" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12734v1.pdf filename=2403.12734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To measure the tree-likeness of a directed acyclic <b>graph</b> (DAG), a new width parameter that considers the directions of the arcs was recently introduced: scanwidth. We present the first algorithm that efficiently computes the exact scanwidth of general DAGs. For DAGs with one root and scanwidth $k$ it runs in $O(k \cdot n^k \cdot m)$ time. The algorithm also functions as an FPT algorithm with complexity $O(2^{4 \ell - 1} \cdot \ell \cdot n + n^2)$ for phylogenetic networks of level-$\ell$, a type of DAG used to depict evolutionary relationships among species. Our algorithm performs well in practice, being able to compute the scanwidth of synthetic networks up to 30 reticulations and 100 leaves within 500 seconds. Furthermore, we propose a heuristic that obtains an average practical approximation ratio of 1.5 on these networks. While we prove that the scanwidth is bounded from below by the treewidth of the underlying undirected <b>graph,</b> experiments suggest that for networks the parameters are close in practice.</p></p class="citation"></blockquote><h2 id=cssi-3>cs.SI (3)</h2><h3 id=13--300329-community-detection-by-spectral-methods-in-multi-layer-networks-huan-qing-2024>(1/3 | 300/329) Community detection by spectral methods in multi-layer networks (Huan Qing, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huan Qing. (2024)<br><strong>Community detection by spectral methods in multi-layer networks</strong><br><button class=copy-to-clipboard title="Community detection by spectral methods in multi-layer networks" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, stat-ML<br>Keyword Score: 23<br>Keywords: Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12540v1.pdf filename=2403.12540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Community detection in multi-layer networks is a crucial problem in network analysis. In this paper, we analyze the performance of two spectral <b>clustering</b> algorithms for community detection within the multi-layer degree-corrected stochastic block model (MLDCSBM) framework. One algorithm is based on the sum of adjacency matrices, while the other utilizes the debiased sum of squared adjacency matrices. We establish consistency results for community detection using these methods under MLDCSBM as the size of the network and/or the number of layers increases. Our theorems demonstrate the advantages of utilizing multiple layers for community detection. Moreover, our analysis indicates that spectral <b>clustering</b> with the debiased sum of squared adjacency matrices is generally superior to spectral <b>clustering</b> with the sum of adjacency matrices. Numerical <b>simulations</b> confirm that our algorithm, employing the debiased sum of squared adjacency matrices, surpasses existing methods for community detection in multi-layer networks. Finally, the analysis of several real-world multi-layer networks yields meaningful insights.</p></p class="citation"></blockquote><h3 id=23--301329-tiktok-and-the-art-of-personalization-investigating-exploration-and-exploitation-on-social-media-feeds-karan-vombatkere-et-al-2024>(2/3 | 301/329) TikTok and the Art of Personalization: Investigating Exploration and Exploitation on Social Media Feeds (Karan Vombatkere et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karan Vombatkere, Sepehr Mousavi, Savvas Zannettou, Franziska Roesner, Krishna P. Gummadi. (2024)<br><strong>TikTok and the Art of Personalization: Investigating Exploration and Exploitation on Social Media Feeds</strong><br><button class=copy-to-clipboard title="TikTok and the Art of Personalization: Investigating Exploration and Exploitation on Social Media Feeds" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12410v1.pdf filename=2403.12410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommendation</b> algorithms for social media feeds often function as black boxes from the perspective of users. We aim to detect whether social media feed <b>recommendations</b> are personalized to users, and to characterize the factors contributing to personalization in these feeds. We introduce a general framework to examine a set of social media feed <b>recommendations</b> for a user as a timeline. We label items in the timeline as the result of exploration vs. exploitation of the user&rsquo;s interests on the part of the <b>recommendation</b> algorithm and introduce a set of metrics to capture the extent of personalization across user timelines. We apply our framework to a real TikTok dataset and validate our results using a baseline generated from automated TikTok bots, as well as a randomized baseline. We also investigate the extent to which factors such as video viewing duration, liking, and following drive the personalization of content on TikTok. Our results demonstrate that our framework produces intuitive and explainable results, and can be used to audit and understand personalization in social media feeds.</p></p class="citation"></blockquote><h3 id=33--302329-detection-of-malicious-agents-in-social-learning-valentina-shumovskaia-et-al-2024>(3/3 | 302/329) Detection of Malicious Agents in Social Learning (Valentina Shumovskaia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valentina Shumovskaia, Mert Kayaalp, Ali H. Sayed. (2024)<br><strong>Detection of Malicious Agents in Social Learning</strong><br><button class=copy-to-clipboard title="Detection of Malicious Agents in Social Learning" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-MA, cs-SI, cs.SI, eess-SP<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12619v1.pdf filename=2403.12619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social learning is a non-Bayesian framework for distributed hypothesis testing aimed at learning the true state of the environment. Traditionally, the agents are assumed to receive observations conditioned on the same true state, although it is also possible to examine the case of heterogeneous models across the <b>graph.</b> One important special case is when heterogeneity is caused by the presence of malicious agents whose goal is to move the agents towards a wrong hypothesis. In this work, we propose an algorithm that allows to discover the true state of every individual agent based on the sequence of their beliefs. In so doing, the methodology is also able to locate malicious behavior.</p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=11--303329-deep-few-view-high-resolution-photon-counting-extremity-ct-at-halved-dose-for-a-clinical-trial-mengzhou-li-et-al-2024>(1/1 | 303/329) Deep Few-view High-resolution Photon-counting Extremity CT at Halved Dose for a Clinical Trial (Mengzhou Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengzhou Li, Chuang Niu, Ge Wang, Maya R Amma, Krishna M Chapagain, Stefan Gabrielson, Andrew Li, Kevin Jonker, Niels de Ruiter, Jennifer A Clark, Phil Butler, Anthony Butler, Hengyong Yu. (2024)<br><strong>Deep Few-view High-resolution Photon-counting Extremity CT at Halved Dose for a Clinical Trial</strong><br><button class=copy-to-clipboard title="Deep Few-view High-resolution Photon-counting Extremity CT at Halved Dose for a Clinical Trial" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: cs-CV, physics-med-ph, physics.med-ph<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12331v1.pdf filename=2403.12331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The latest X-ray photon-counting computed tomography (PCCT) for extremity allows multi-energy high-resolution (HR) imaging for tissue characterization and material decomposition. However, both radiation dose and imaging speed need improvement for contrast-enhanced and other studies. Despite the success of deep learning methods for 2D few-view reconstruction, applying them to HR volumetric reconstruction of extremity scans for clinical diagnosis has been limited due to GPU memory constraints, training data scarcity, and domain gap issues. In this paper, we propose a deep learning-based approach for PCCT image reconstruction at halved dose and doubled speed in a New Zealand clinical trial. Particularly, we present a patch-based volumetric refinement network to alleviate the GPU memory limitation, train network with synthetic data, and use model-based iterative refinement to bridge the gap between synthetic and real-world data. The <b>simulation</b> and phantom experiments demonstrate consistently improved results under different acquisition conditions on both in- and off-domain structures using a fixed network. The image quality of 8 patients from the clinical trial are evaluated by three radiologists in comparison with the standard image reconstruction with a full-view dataset. It is shown that our proposed approach is essentially identical to or better than the clinical <b>benchmark</b> in terms of diagnostic image quality scores. Our approach has a great potential to improve the safety and efficiency of PCCT without compromising image quality.</p></p class="citation"></blockquote><h2 id=cset-1>cs.ET (1)</h2><h3 id=11--304329-pruning-for-improved-adc-efficiency-in-crossbar-based-analog-in-memory-accelerators-timur-ibrayev-et-al-2024>(1/1 | 304/329) Pruning for Improved ADC Efficiency in Crossbar-based Analog In-memory Accelerators (Timur Ibrayev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timur Ibrayev, Isha Garg, Indranil Chakraborty, Kaushik Roy. (2024)<br><strong>Pruning for Improved ADC Efficiency in Crossbar-based Analog In-memory Accelerators</strong><br><button class=copy-to-clipboard title="Pruning for Improved ADC Efficiency in Crossbar-based Analog In-memory Accelerators" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs-LG, cs.ET<br>Keyword Score: 20<br>Keywords: Convolution, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13082v1.pdf filename=2403.13082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning has proved successful in many applications but suffers from high computational demands and requires custom accelerators for deployment. Crossbar-based analog in-memory architectures are attractive for acceleration of deep neural networks (DNN), due to their high data reuse and high efficiency enabled by combining storage and computation in memory. However, they require analog-to-digital converters (ADCs) to communicate crossbar outputs. ADCs consume a significant portion of energy and area of every crossbar processing unit, thus diminishing the potential efficiency benefits. <b>Pruning</b> is a well-studied technique to improve the efficiency of DNNs but requires modifications to be effective for crossbars. In this paper, we motivate crossbar-attuned <b>pruning</b> to target ADC-specific inefficiencies. This is achieved by identifying three key properties (dubbed D.U.B.) that induce sparsity that can be utilized to reduce ADC energy without sacrificing accuracy. The first property ensures that sparsity translates effectively to hardware efficiency by restricting sparsity levels to Discrete powers of 2. The other 2 properties encourage columns in the same crossbar to achieve both Unstructured and Balanced sparsity in order to amortize the accuracy drop. The desired D.U.B. sparsity is then achieved by regularizing the variance of $L_{0}$ norms of neighboring columns within the same crossbar. Our proposed implementation allows it to be directly used in end-to-end gradient-based training. We apply the proposed algorithm to <b>convolutional</b> layers of VGG11 and ResNet18 models, trained on CIFAR-10 and ImageNet datasets, and achieve up to 7.13x and 1.27x improvement, respectively, in ADC energy with less than 1% drop in accuracy.</p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--305329-a-canary-in-the-ai-coal-mine-american-jews-may-be-disproportionately-harmed-by-intellectual-property-dispossession-in-large-language-model-training-heila-precel-et-al-2024>(1/3 | 305/329) A Canary in the AI Coal Mine: American Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training (Heila Precel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heila Precel, Allison McDonald, Brent Hecht, Nicholas Vincent. (2024)<br><strong>A Canary in the AI Coal Mine: American Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training</strong><br><button class=copy-to-clipboard title="A Canary in the AI Coal Mine: American Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13073v1.pdf filename=2403.13073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Systemic property dispossession from minority groups has often been carried out in the name of technological progress. In this paper, we identify evidence that the current paradigm of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> likely continues this long history. Examining common <b>LLM</b> training datasets, we find that a disproportionate amount of content authored by Jewish Americans is used for training without their consent. The degree of over-representation ranges from around 2x to around 6.5x. Given that <b>LLMs</b> may substitute for the paid labor of those who produced their training data, they have the potential to cause even more substantial and disproportionate economic harm to Jewish Americans in the coming years. This paper focuses on Jewish Americans as a case study, but it is probable that other minority communities (e.g., Asian Americans, Hindu Americans) may be similarly affected and, most importantly, the results should likely be interpreted as a &ldquo;canary in the coal mine&rdquo; that highlights deep structural concerns about the current <b>LLM</b> paradigm whose harms could soon affect nearly everyone. We discuss the implications of these results for the policymakers thinking about how to regulate <b>LLMs</b> as well as for those in the AI field who are working to advance <b>LLMs.</b> Our findings stress the importance of working together towards alternative <b>LLM</b> paradigms that avoid both disparate impacts and widespread societal harms.</p></p class="citation"></blockquote><h3 id=23--306329-the-journey-to-trustworthy-ai--part-1-pursuit-of-pragmatic-frameworks-mohamad-m-nasr-azadani-et-al-2024>(2/3 | 306/329) The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks (Mohamad M Nasr-Azadani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamad M Nasr-Azadani, Jean-Luc Chatelain. (2024)<br><strong>The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks</strong><br><button class=copy-to-clipboard title="The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs-HC, cs.CY<br>Keyword Score: 13<br>Keywords: Benchmarking, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15457v1.pdf filename=2403.15457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper reviews Trustworthy Artificial Intelligence (TAI) and its various definitions. Considering the principles respected in any society, TAI is often characterized by a few attributes, some of which have led to confusion in regulatory or engineering contexts. We argue against using terms such as Responsible or Ethical AI as substitutes for TAI. And to help clarify any confusion, we suggest leaving them behind. Given the subjectivity and complexity inherent in TAI, developing a universal framework is deemed infeasible. Instead, we advocate for approaches centered on addressing key attributes and properties such as <b>fairness,</b> bias, risk, security, explainability, and reliability. We examine the ongoing regulatory landscape, with a focus on initiatives in the EU, China, and the USA. We recognize that differences in AI regulations based on geopolitical and geographical reasons pose an additional challenge for multinational companies. We identify risk as a core factor in AI regulation and TAI. For example, as outlined in the EU-AI Act, organizations must gauge the risk level of their AI products to act accordingly (or risk hefty fines). We compare modalities of TAI implementation and how multiple cross-functional teams are engaged in the overall process. Thus, a brute force approach for enacting TAI renders its efficiency and agility, moot. To address this, we introduce our framework Set-Formalize-Measure-Act (SFMA). Our solution highlights the importance of transforming TAI-aware metrics, drivers of TAI, stakeholders, and business/legal requirements into actual <b>benchmarks</b> or tests. Finally, over-regulation driven by panic of powerful AI models can, in fact, harm TAI too. Based on GitHub user-activity data, in 2023, AI open-source projects rose to top projects by contributor account. Enabling innovation in TAI hinges on the independent contributions of the open-source community.</p></p class="citation"></blockquote><h3 id=33--307329-is-open-source-software-culture-enough-to-make-ai-a-common--robin-quillivic-et-al-2024>(3/3 | 307/329) Is open source software culture enough to make AI a common ? (Robin Quillivic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robin Quillivic, Salma Mesmoudi. (2024)<br><strong>Is open source software culture enough to make AI a common ?</strong><br><button class=copy-to-clipboard title="Is open source software culture enough to make AI a common ?" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12774v1.pdf filename=2403.12774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models (LM or <b>LLM)</b> are increasingly deployed in the field of artificial intelligence (AI) and its applications, but the question arises as to whether they can be a common resource managed and maintained by a community of users. Indeed, the dominance of private companies with exclusive access to massive data and language processing resources can create inequalities and biases in LM, as well as obstacles to innovation for those who do not have the same resources necessary for their implementation. In this contribution, we examine the concept of the commons and its relevance for thinking about LM. We highlight the potential benefits of treating the data and resources needed to create LMs as commons, including increased accessibility, equity, and transparency in the development and use of AI technologies. Finally, we present a case study centered on the Hugging Face platform, an open-source platform for deep learning designed to encourage collaboration and sharing among AI designers.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--308329-developing-algorithms-for-the-internet-of-flying-things-through-environments-with-varying-degrees-of-realism----extended-version-thiago-de-souza-lamenza-et-al-2024>(1/3 | 308/329) Developing Algorithms for the Internet of Flying Things Through Environments With Varying Degrees of Realism &ndash; Extended Version (Thiago de Souza Lamenza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thiago de Souza Lamenza, Josef Kamysek, Bruno Jose Olivieri de Souza, Markus Endler. (2024)<br><strong>Developing Algorithms for the Internet of Flying Things Through Environments With Varying Degrees of Realism &ndash; Extended Version</strong><br><button class=copy-to-clipboard title="Developing Algorithms for the Internet of Flying Things Through Environments With Varying Degrees of Realism -- Extended Version" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12753v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12753v2.pdf filename=2403.12753v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work discusses the benefits of having multiple simulated environments with different degrees of realism for the development of algorithms in scenarios populated by autonomous nodes capable of communication and mobility. This approach aids the development experience and generates robust algorithms. It also proposes GrADyS-SIM NextGen as a solution that enables development on a single programming language and toolset over multiple environments with varying levels of realism. Finally, we illustrate the usefulness of this approach with a toy problem that makes use of the <b>simulation</b> framework, taking advantage of the proposed environments to iteratively develop a robust solution.</p></p class="citation"></blockquote><h3 id=23--309329-hierarchical-digital-twin-for-efficient-6g-network-orchestration-via-adaptive-attribute-selection-and-scalable-network-modeling-pengyi-jia-et-al-2024>(2/3 | 309/329) Hierarchical Digital Twin for Efficient 6G Network Orchestration via Adaptive Attribute Selection and Scalable Network Modeling (Pengyi Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengyi Jia, Xianbin Wang, Xuemin, Shen. (2024)<br><strong>Hierarchical Digital Twin for Efficient 6G Network Orchestration via Adaptive Attribute Selection and Scalable Network Modeling</strong><br><button class=copy-to-clipboard title="Hierarchical Digital Twin for Efficient 6G Network Orchestration via Adaptive Attribute Selection and Scalable Network Modeling" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12398v1.pdf filename=2403.12398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving a holistic and long-term understanding through accurate network modeling is essential for orchestrating future networks with increasing service diversity and infrastructure complexities. However, due to unselective data collection and uniform processing, traditional modeling approaches undermine the efficacy and timeliness of network orchestration. Additionally, temporal disparities arising from various modeling delays further impair the centralized decision-making with distributed models. In this paper, we propose a new hierarchical digital twin paradigm adapting to real-time network situations for problem-centered model construction. Specifically, we introduce an adaptive attribute selection mechanism that evaluates the distinct modeling values of diverse network attributes, considering their relevance to current network scenarios and inherent modeling complexity. By prioritizing critical attributes at higher layers, an efficient evaluation of network situations is achieved to identify target areas. Subsequently, scalable network modeling facilitates the inclusion of all identified elements at the lower layers, where more fine-grained digital twins are developed to generate targeted solutions for user association and power allocation. Furthermore, virtual-physical domain synchronization is implemented to maintain accurate temporal alignment between the digital twins and their physical counterparts, spanning from the construction to the utilization of the proposed paradigm. Extensive <b>simulations</b> validate the proposed approach, demonstrating its effectiveness in efficiently identifying pressing issues and delivering network orchestration solutions in complex 6G HetNets.</p></p class="citation"></blockquote><h3 id=33--310329-a-novel-energy-efficient-cross-layer-design-for-scheduling-and-routing-in-6tisch-networks-ahlam-hannachi-et-al-2024>(3/3 | 310/329) A Novel Energy-Efficient Cross-Layer Design for Scheduling and Routing in 6TiSCH Networks (Ahlam Hannachi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahlam Hannachi, Wael Jaafar, Salim Bitam, Nabil Ouazene. (2024)<br><strong>A Novel Energy-Efficient Cross-Layer Design for Scheduling and Routing in 6TiSCH Networks</strong><br><button class=copy-to-clipboard title="A Novel Energy-Efficient Cross-Layer Design for Scheduling and Routing in 6TiSCH Networks" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12949v1.pdf filename=2403.12949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The 6TiSCH protocol stack was proposed to ensure high-performance communications in the Industrial Internet of Things (IIoT). However, the lack of sufficient time slots for nodes outside the 6TiSCH&rsquo;s Destination Oriented Directed Acyclic <b>Graph</b> (DODAG) to transmit their Destination Advertisement Object (DAO) messages and cell reservation requests significantly hinders their integration into the DODAG. This oversight not only prolongs the device&rsquo;s join time but also increases energy consumption during the network formation phase. Moreover, challenges emerge due to the substantial number of control packets employed by both the 6TiSCH Scheduling Function (SF) and routing protocol (RPL), thus draining more energy resources, increasing medium contention, and decreasing spatial reuse. Furthermore, an SF that overlooks previously allocated slots when assigning new ones to the same node may increase jitter, and more complications ensue when it neglects the state of the TSCH queue, thus leading to packet dropping due to queue saturation. Additional complexity arises when the RPL disregards the new parent&rsquo;s schedule saturation during parent switching, which results in inefficient energy and time usage. To address these issues, we introduce in this paper novel mechanisms, strategically situated at the intersection of SF and RPL that are designed to balance the control packet distribution and adaptively manage parent switching. Our proposal, implemented within the 6TiSCH simulator, demonstrates significant improvements across vital performance metrics, such as node&rsquo;s joining time, jitter, latency, energy consumption, and amount of traffic, in comparison to the conventional 6TiSCH <b>benchmark.</b></p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--311329-bayesian-estimation-and-uncertainty-quantification-of-a-temperature-dependent-thermal-conductivity-rodrigo-l-s-silva-et-al-2024>(1/1 | 311/329) Bayesian estimation and uncertainty quantification of a temperature-dependent thermal conductivity (Rodrigo L. S. Silva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rodrigo L. S. Silva, Clemens Verhoosel, Erik Quaeghebeur. (2024)<br><strong>Bayesian estimation and uncertainty quantification of a temperature-dependent thermal conductivity</strong><br><button class=copy-to-clipboard title="Bayesian estimation and uncertainty quantification of a temperature-dependent thermal conductivity" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12696v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12696v2.pdf filename=2403.12696v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of estimating a temperature-dependent thermal conductivity model (curve) from temperature measurements. We apply a Bayesian estimation approach that takes into account measurement errors and limited prior information of system properties. The approach intertwines system <b>simulation</b> and Markov chain Monte Carlo (MCMC) sampling. We investigate the impact of assuming different model classes - cubic polynomials and piecewise linear functions - their parametrization, and different types of prior information - ranging from uninformative to informative. Piecewise linear functions require more parameters (conductivity values) to be estimated than the four parameters (coefficients or conductivity values) needed for cubic polynomials. The former model class is more flexible, but the latter requires less MCMC samples. While parametrizing polynomials with coefficients may feel more natural, it turns out that parametrizing them using conductivity values is far more natural for the specification of prior information. Robust estimation is possible for all model classes and parametrizations, as long as the prior information is accurate or not too informative. Gaussian Markov random field priors are especially well-suited for piecewise linear functions.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--312329-zeolite-adsorption-property-prediction-using-deep-learning-marko-petković-et-al-2024>(1/1 | 312/329) Zeolite Adsorption Property Prediction using Deep Learning (Marko Petković et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marko Petković, José Manuel Vicent-Luna, Vlado Menkovski, Sofía Calero. (2024)<br><strong>Zeolite Adsorption Property Prediction using Deep Learning</strong><br><button class=copy-to-clipboard title="Zeolite Adsorption Property Prediction using Deep Learning" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12659v1.pdf filename=2403.12659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to efficiently predict adsorption properties of zeolites can be of large benefit in accelerating the design process of novel materials. The existing configuration space for these materials is wide, while existing molecular <b>simulation</b> methods are computationally expensive. In this work, we propose a model which is 4 to 5 orders of magnitude faster at adsorption properties compared to molecular <b>simulations.</b> To validate the model, we generated datasets containing various aluminium configurations for the MOR, MFI, RHO and ITW zeolites along with their heat of adsorptions and Henry coefficients for CO$_2$, obtained from Monte Carlo <b>simulations.</b> The predictions obtained from the Machine Learning model are in agreement with the values obtained from the Monte Carlo <b>simulations,</b> confirming that the model can be used for property prediction. Furthermore, we show that the model can be used for identifying adsorption sites. Finally, we evaluate the capability of our model for generating novel zeolite configurations by using it in combination with a genetic algorithm.</p></p class="citation"></blockquote><h2 id=eesssy-3>eess.SY (3)</h2><h3 id=13--313329-state-estimation-using-single-body-frame-bearing-measurements-sifeddine-benahmed-et-al-2024>(1/3 | 313/329) State Estimation Using Single Body-Frame Bearing Measurements (Sifeddine Benahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sifeddine Benahmed, Soulaimane Berkane. (2024)<br><strong>State Estimation Using Single Body-Frame Bearing Measurements</strong><br><button class=copy-to-clipboard title="State Estimation Using Single Body-Frame Bearing Measurements" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12633v1.pdf filename=2403.12633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of simultaneous estimation of the position, linear velocity and orientation of a rigid body using single bearing measurements. We introduce a Riccati observer-based estimator that fuses measurements from a 3-axis accelerometer, a 3-axis gyroscope, a single body-frame vector observation (e.g., magnetometer), and a single bearing-to-landmark measurement to obtain the full vehicle&rsquo;s state (position, velocity, orientation). The proposed observer guarantees global exponential convergence under some persistency of excitation (PE) condition on the vehicle&rsquo;s motion. <b>Simulation</b> results are presented to show the effectiveness of the proposed approach.</p></p class="citation"></blockquote><h3 id=23--314329-robust-fuel-optimal-landing-guidance-for-hazardous-terrain-using-multiple-sliding-surfaces-sheikh-zeeshan-basar-et-al-2024>(2/3 | 314/329) Robust Fuel-Optimal Landing Guidance for Hazardous Terrain using Multiple Sliding Surfaces (Sheikh Zeeshan Basar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheikh Zeeshan Basar, Satadal Ghosh. (2024)<br><strong>Robust Fuel-Optimal Landing Guidance for Hazardous Terrain using Multiple Sliding Surfaces</strong><br><button class=copy-to-clipboard title="Robust Fuel-Optimal Landing Guidance for Hazardous Terrain using Multiple Sliding Surfaces" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12584v1.pdf filename=2403.12584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In any spacecraft landing mission, precision soft landing in a fuel-efficient way while also avoiding nearby hazardous terrain is of utmost importance. Very few existing literature have attempted addressing both the problems of precision soft landing and terrain avoidance simultaneously. To this end, an optimal terrain avoidance landing guidance (OTALG) was recently developed, which showed promising performance in avoiding the terrain while consuming near-minimum fuel. However, its performance significantly degrades in the face of external disturbances, indicating lack of robustness. In order to mitigate this problem, in this paper, a novel near fuel-optimal guidance law is developed to avoid terrain and land precisely and softly at the desired landing site, under atmospheric perturbations and thrust deviations and constraints. Expanding the OTALG formulation by using sliding mode control with multiple sliding surfaces (MSS), the presented guidance law, named `MSS-OTALG&rsquo;, improves in terms of precision soft landing accuracy. Further, the sliding parameter is designed to allow the lander to avoid terrain by leaving the trajectory enforced by the sliding mode, and eventually returning to it when the terrain avoidance phase is completed. And finally, the robustness of the MSS-OTALG is established by proving practical fixed-time stability. Extensive numerical <b>simulations</b> are also presented to showcase its performance in terms of terrain avoidance, low fuel consumption and accuracy of precision soft landing. Comparative studies against existing relevant literature validates a balanced trade-off of all these performance measures achieved by the developed MSS-OTALG.</p></p class="citation"></blockquote><h3 id=33--315329-ensuring-solution-uniqueness-in-fixed-point-based-harmonic-power-flow-analysis-with-converter-interfaced-resources-ex-post-conditions-antonio-di-pasquale-et-al-2024>(3/3 | 315/329) Ensuring Solution Uniqueness in Fixed-Point-Based Harmonic Power Flow Analysis with Converter-Interfaced Resources: Ex-post Conditions (Antonio Di Pasquale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonio Di Pasquale, Johanna Kristin Maria Becker, Andreas Martin Kettner, Mario Paolone. (2024)<br><strong>Ensuring Solution Uniqueness in Fixed-Point-Based Harmonic Power Flow Analysis with Converter-Interfaced Resources: Ex-post Conditions</strong><br><button class=copy-to-clipboard title="Ensuring Solution Uniqueness in Fixed-Point-Based Harmonic Power Flow Analysis with Converter-Interfaced Resources: Ex-post Conditions" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12595v1.pdf filename=2403.12595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the authors of this paper proposed a method for the Harmonic Power-Flow (HPF) calculus in polyphase grids with widespread deployment of Converter-Interfaced Distributed Energy Resources (CIDERs). The HPF problem was formulated by integrating the hybrid nodal equations of the grid with a detailed representation of the CIDERs hardware, sensing, and controls as Linear Time-Periodic (LTP) systems, and solving the resulting mismatch equations using the Newton-Raphson (NR) method. This work introduces a novel problem formulation based on the fixed-point algorithm that, combined with the contraction property of the HPF problem, provides insights into the uniqueness of its solution. Notably, the effectiveness of the fixed-point formulation and the uniqueness of the solution are evaluated through numerical analyses conducted on a modified version of the CIGRE low-voltage <b>benchmark</b> microgrid.</p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--316329-reproducing-the-acoustic-velocity-vectors-in-a-circular-listening-area-jiarui-wang-et-al-2024>(1/1 | 316/329) Reproducing the Acoustic Velocity Vectors in a Circular Listening Area (Jiarui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiarui Wang, Thushara Abhayapala, Jihui Aimee Zhang, Prasanga Samarasinghe. (2024)<br><strong>Reproducing the Acoustic Velocity Vectors in a Circular Listening Area</strong><br><button class=copy-to-clipboard title="Reproducing the Acoustic Velocity Vectors in a Circular Listening Area" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12630v1.pdf filename=2403.12630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acoustic velocity vectors are important for human&rsquo;s localization of sound at low frequencies. This paper proposes a sound field reproduction algorithm, which matches the acoustic velocity vectors in a circular listening area. In previous work, acoustic velocity vectors are matched either at sweet spots or on the boundary of the listening area. Sweet spots restrict listener&rsquo;s movement, whereas measuring the acoustic velocity vectors on the boundary requires complicated measurement setup. This paper proposes the cylindrical harmonic coefficients of the acoustic velocity vectors in a circular area (CHV coefficients), which are calculated from the cylindrical harmonic coefficients of the global pressure (global CHP coefficients) by using the sound field translation formula. The global CHP coefficients can be measured by a circular microphone array, which can be bought off-the-shelf. By matching the CHV coefficients, the acoustic velocity vectors are reproduced throughout the listening area. Hence, listener&rsquo;s movements are allowed. <b>Simulations</b> show that at low frequency, where the acoustic velocity vectors are the dominant factor for localization, the proposed reproduction method based on the CHV coefficients results in higher accuracy in reproduced acoustic velocity vectors when compared with traditional method based on the global CHP coefficients.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--317329-semisupervised-score-based-matching-algorithm-to-evaluate-the-effect-of-public-health-interventions-hongzhe-zhang-et-al-2024>(1/3 | 317/329) Semisupervised score based matching algorithm to evaluate the effect of public health interventions (Hongzhe Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongzhe Zhang, Jiasheng Shi, Jing Huang. (2024)<br><strong>Semisupervised score based matching algorithm to evaluate the effect of public health interventions</strong><br><button class=copy-to-clipboard title="Semisupervised score based matching algorithm to evaluate the effect of public health interventions" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12367v1.pdf filename=2403.12367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multivariate matching algorithms &ldquo;pair&rdquo; similar study units in an observational study to remove potential bias and confounding effects caused by the absence of randomizations. In one-to-one multivariate matching algorithms, a large number of &ldquo;pairs&rdquo; to be matched could mean both the information from a large sample and a large number of tasks, and therefore, to best match the pairs, such a matching algorithm with efficiency and comparatively limited auxiliary matching knowledge provided through a &ldquo;training&rdquo; set of paired units by domain experts, is practically intriguing. We proposed a novel one-to-one matching algorithm based on a quadratic score function $S_{\beta}(x_i,x_j)= \beta^T (x_i-x_j)(x_i-x_j)^T \beta$. The weights $\beta$, which can be interpreted as a variable importance measure, are designed to minimize the score difference between paired training units while maximizing the score difference between unpaired training units. Further, in the typical but intricate case where the training set is much smaller than the unpaired set, we propose a \underline{s}emisupervised \underline{c}ompanion \underline{o}ne-\underline{t}o-\underline{o}ne \underline{m}atching \underline{a}lgorithm (SCOTOMA) that makes the best use of the unpaired units. The proposed weight estimator is proved to be consistent when the truth matching criterion is indeed the quadratic score function. When the model assumptions are violated, we demonstrate that the proposed algorithm still outperforms some popular competing matching algorithms through a series of <b>simulations.</b> We applied the proposed algorithm to a real-world study to investigate the effect of in-person schooling on community Covid-19 transmission rate for policy making purpose.</p></p class="citation"></blockquote><h3 id=23--318329-fast-value-tracking-for-deep-reinforcement-learning-frank-shih-et-al-2024>(2/3 | 318/329) Fast Value Tracking for Deep Reinforcement Learning (Frank Shih et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frank Shih, Faming Liang. (2024)<br><strong>Fast Value Tracking for Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Fast Value Tracking for Deep Reinforcement Learning" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13178v1.pdf filename=2403.13178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) tackles sequential decision-making problems by creating agents that interacts with their environment. However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic dynamics of agent-environment interactions and the critical role of uncertainty quantification. Our research leverages the Kalman filtering paradigm to introduce a novel and scalable sampling algorithm called Langevinized Kalman Temporal-Difference (LKTD) for deep <b>reinforcement</b> <b>learning.</b> This algorithm, grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently draws samples from the posterior distribution of deep neural network parameters. Under mild conditions, we prove that the posterior samples generated by the LKTD algorithm converge to a stationary distribution. This convergence not only enables us to quantify uncertainties associated with the value function and model parameters but also allows us to monitor these uncertainties during policy updates throughout the training phase. The LKTD algorithm paves the way for more robust and adaptable <b>reinforcement</b> <b>learning</b> approaches.</p></p class="citation"></blockquote><h3 id=33--319329-tighter-confidence-bounds-for-sequential-kernel-regression-hamish-flynn-et-al-2024>(3/3 | 319/329) Tighter Confidence Bounds for Sequential Kernel Regression (Hamish Flynn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamish Flynn, David Reeb. (2024)<br><strong>Tighter Confidence Bounds for Sequential Kernel Regression</strong><br><button class=copy-to-clipboard title="Tighter Confidence Bounds for Sequential Kernel Regression" index=319>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-319 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12732v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12732v1.pdf filename=2403.12732v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions. In this capacity, they can inform the exploration-exploitation trade-off and form a core component in many sequential learning and decision-making algorithms. Tighter confidence bounds give rise to algorithms with better empirical performance and better performance guarantees. In this work, we use martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs to establish new confidence bounds for sequential kernel regression. We prove that our new confidence bounds are always tighter than existing ones in this setting. We apply our confidence bounds to the kernel <b>bandit</b> problem, where future actions depend on the previous history. When our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and comparable computational cost. Our new confidence bounds can be used as a generic tool to design improved algorithms for other kernelised learning and decision-making problems.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--320329-resolving-sets-in-temporal-graphs-jan-bok-et-al-2024>(1/1 | 320/329) Resolving Sets in Temporal Graphs (Jan Bok et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Bok, Antoine Dailly, Tuomo Lehtilä. (2024)<br><strong>Resolving Sets in Temporal Graphs</strong><br><button class=copy-to-clipboard title="Resolving Sets in Temporal Graphs" index=320>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-320 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C69, 68Rxx, cs-DM, cs-DS, math-CO, math.CO<br>Keyword Score: 13<br>Keywords: Graph, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13183v1.pdf filename=2403.13183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A $\textit{resolving set}$ $R$ in a <b>graph</b> $G$ is a set of vertices such that every vertex of $G$ is uniquely identified by its distances to the vertices of $R$. Introduced in the 1970s, this concept has been since then extensively studied from both combinatorial and algorithmic point of view. We propose a generalization of the concept of resolving sets to temporal <b>graphs,</b> i.e., <b>graphs</b> with edge sets that change over <b>discrete</b> <b>time-steps.</b> In this setting, the $\textit{temporal distance}$ from $u$ to $v$ is the earliest possible time-step at which a journey with strictly increasing time-steps on edges leaving $u$ reaches $v$, i.e., the first time-step at which $v$ could receive a message broadcast from $u$. A $\textit{temporal resolving set}$ of a temporal <b>graph</b> $\mathcal{G}$ is a subset $R$ of its vertices such that every vertex of $\mathcal{G}$ is uniquely identified by its temporal distances from vertices of $R$. We study the problem of finding a minimum-size temporal resolving set, and show that it is NP-complete even on very restricted <b>graph</b> classes and with strong constraints on the time-steps: temporal complete <b>graphs</b> where every edge appears in either time-step 1 or 2, temporal trees where every edge appears in at most two consecutive time-steps, and even temporal subdivided stars where every edge appears in at most two (not necessarily consecutive) time-steps. On the other hand, we give polynomial-time algorithms for temporal paths and temporal stars where every edge appears in exactly one time-step, and give a combinatorial analysis and algorithms for several temporal <b>graph</b> classes where the edges appear in periodic time-steps.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--321329-listenable-maps-for-audio-classifiers-francesco-paissan-et-al-2024>(1/2 | 321/329) Listenable Maps for Audio Classifiers (Francesco Paissan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Paissan, Mirco Ravanelli, Cem Subakan. (2024)<br><strong>Listenable Maps for Audio Classifiers</strong><br><button class=copy-to-clipboard title="Listenable Maps for Audio Classifiers" index=321>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-321 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS, eess-SP<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13086v1.pdf filename=2403.13086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the impressive performance of deep learning models across diverse tasks, their complexity poses challenges for interpretation. This challenge is particularly evident for audio signals, where conveying interpretations becomes inherently difficult. To address this issue, we introduce Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method that generates faithful and listenable interpretations. L-MAC utilizes a decoder on top of a pretrained classifier to generate binary masks that highlight relevant portions of the input audio. We train the decoder with a special loss that maximizes the confidence of the classifier decision on the masked-in portion of the audio while minimizing the probability of model output for the masked-out portion. Quantitative evaluations on both in-domain and <b>out-of-domain</b> data demonstrate that L-MAC consistently produces more faithful interpretations than several gradient and masking-based methodologies. Furthermore, a user study confirms that, on average, users prefer the interpretations generated by the proposed technique.</p></p class="citation"></blockquote><h3 id=22--322329-real-time-speech-extraction-using-spatially-regularized-independent-low-rank-matrix-analysis-and-rank-constrained-spatial-covariance-matrix-estimation-yuto-ishikawa-et-al-2024>(2/2 | 322/329) Real-time Speech Extraction Using Spatially Regularized Independent Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix Estimation (Yuto Ishikawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuto Ishikawa, Kohei Konaka, Tomohiko Nakamura, Norihiro Takamune, Hiroshi Saruwatari. (2024)<br><strong>Real-time Speech Extraction Using Spatially Regularized Independent Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix Estimation</strong><br><button class=copy-to-clipboard title="Real-time Speech Extraction Using Spatially Regularized Independent Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix Estimation" index=322>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-322 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12477v1.pdf filename=2403.12477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-time <b>speech</b> <b>extraction</b> is an important challenge with various applications such as <b>speech</b> <b>recognition</b> in a human-like avatar/robot. In this paper, we propose the real-time extension of a <b>speech</b> <b>extraction</b> method based on independent low-rank matrix analysis (ILRMA) and rank-constrained spatial covariance matrix estimation (RCSCME). The RCSCME-based method is a multichannel blind <b>speech</b> <b>extraction</b> method that demonstrates superior <b>speech</b> <b>extraction</b> performance in diffuse noise environments. To improve the performance, we introduce spatial regularization into the ILRMA part of the RCSCME-based <b>speech</b> <b>extraction</b> and design two regularizers. <b>Speech</b> <b>extraction</b> experiments demonstrated that the proposed methods can function in real time and the designed regularizers improve the <b>speech</b> <b>extraction</b> performance.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--323329-a-new-framework-for-constrained-optimization-via-feedback-control-of-lagrange-multipliers-v-cerone-et-al-2024>(1/2 | 323/329) A new framework for constrained optimization via feedback control of Lagrange multipliers (V. Cerone et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>V. Cerone, S. M. Fosson, S. Pirrera, D. Regruto. (2024)<br><strong>A new framework for constrained optimization via feedback control of Lagrange multipliers</strong><br><button class=copy-to-clipboard title="A new framework for constrained optimization via feedback control of Lagrange multipliers" index=323>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-323 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12738v1.pdf filename=2403.12738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>continuous-time</b> <b>analysis</b> of existing iterative algorithms for optimization has a long history. This work proposes a novel <b>continuous-time</b> <b>control-theoretic</b> framework for equality-constrained optimization. The key idea is to design a feedback control system where the Lagrange multipliers are the control input, and the output represents the constraints. The system converges to a stationary point of the constrained optimization problem through suitable regulation. Regarding the Lagrange multipliers, we consider two control laws: proportional-integral control and feedback linearization. These choices give rise to a family of different methods. We rigorously develop the related algorithms, theoretically analyze their convergence and present several numerical experiments to support their effectiveness concerning the state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=22--324329-stochastic-halpern-iteration-in-normed-spaces-and-applications-to-reinforcement-learning-mario-bravo-et-al-2024>(2/2 | 324/329) Stochastic Halpern iteration in normed spaces and applications to reinforcement learning (Mario Bravo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mario Bravo, Juan Pablo Contreras. (2024)<br><strong>Stochastic Halpern iteration in normed spaces and applications to reinforcement learning</strong><br><button class=copy-to-clipboard title="Stochastic Halpern iteration in normed spaces and applications to reinforcement learning" index=324>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-324 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12338v1.pdf filename=2403.12338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We analyze the oracle complexity of the stochastic Halpern iteration with variance reduction, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space. We show that if the underlying stochastic oracle is with uniformly bounded variance, our method exhibits an overall oracle complexity of $\tilde{O}(\varepsilon^{-5})$, improving recent rates established for the stochastic Krasnoselskii-Mann iteration. Also, we establish a lower bound of $\Omega(\varepsilon^{-3})$, which applies to a wide range of algorithms, including all averaged iterations even with minibatching. Using a suitable modification of our approach, we derive a $O(\varepsilon^{-2}(1-\gamma)^{-3})$ complexity bound in the case in which the operator is a $\gamma$-contraction. As an application, we propose new synchronous algorithms for average reward and discounted reward Markov decision processes. In particular, for the average reward, our method improves on the best-known sample complexity.</p></p class="citation"></blockquote><h2 id=mathat-1>math.AT (1)</h2><h3 id=11--325329-some-geometric-and-topological-data-driven-methods-in-robot-motion-path-planning-boris-goldfarb-2024>(1/1 | 325/329) Some geometric and topological data-driven methods in robot motion path planning (Boris Goldfarb, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boris Goldfarb. (2024)<br><strong>Some geometric and topological data-driven methods in robot motion path planning</strong><br><button class=copy-to-clipboard title="Some geometric and topological data-driven methods in robot motion path planning" index=325>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-325 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AT<br>Categories: 57Q70, 68T40, 68U05, 68W15, cs-RO, math-AT, math.AT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12725v1.pdf filename=2403.12725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motion path planning is an intrinsically geometric problem which is central for design of robot systems. Since the early years of AI, robotics together with computer vision have been the areas of computer science that drove its development. Many questions that arise, such as existence, optimality, and diversity of motion paths in the configuration space that describes feasible robot configurations, are of topological nature. The recent advances in topological data analysis and related metric <b>geometry,</b> topology and combinatorics have provided new tools to address these engineering tasks. We will survey some questions, issues, recent work and promising directions in data-driven geometric and topological methods with some emphasis on the use of discrete Morse theory.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--326329-plane-hamiltonian-cycles-in-convex-drawings-helena-bergold-et-al-2024>(1/1 | 326/329) Plane Hamiltonian Cycles in Convex Drawings (Helena Bergold et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Helena Bergold, Stefan Felsner, Meghana M. Reddy, Joachim Orthaber, Manfred Scheucher. (2024)<br><strong>Plane Hamiltonian Cycles in Convex Drawings</strong><br><button class=copy-to-clipboard title="Plane Hamiltonian Cycles in Convex Drawings" index=326>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-326 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-DM, cs.CG, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12898v1.pdf filename=2403.12898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A conjecture by Rafla from 1988 asserts that every simple drawing of the complete <b>graph</b> $K_n$ admits a plane Hamiltonian cycle. It turned out that already the existence of much simpler non-crossing substructures in such drawings is hard to prove. Recent progress was made by Aichholzer et al. and by Suk and Zeng who proved the existence of a plane path of length $\Omega(\log n / \log \log n)$ and of a plane matching of size $\Omega(n^{1/2})$ in every simple drawing of $K_{n}$. Instead of studying simpler substructures, we prove Rafla&rsquo;s conjecture for the subclass of convex drawings, the most general class in the convexity hierarchy introduced by Arroyo et al. Moreover, we show that every convex drawing of $K_n$ contains a plane Hamiltonian path between each pair of vertices (Hamiltonian connectivity) and a plane $k$-cycle for each $3 \leq k \leq n$ (pancyclicity), and present further results on maximal plane subdrawings.</p></p class="citation"></blockquote><h2 id=csdm-2>cs.DM (2)</h2><h3 id=12--327329-the-ultrametric-backbone-is-the-union-of-all-minimum-spanning-forests-jordan-c-rozum-et-al-2024>(1/2 | 327/329) The ultrametric backbone is the union of all minimum spanning forests (Jordan C Rozum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jordan C Rozum, Luis M Rocha. (2024)<br><strong>The ultrametric backbone is the union of all minimum spanning forests</strong><br><button class=copy-to-clipboard title="The ultrametric backbone is the union of all minimum spanning forests" index=327>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-327 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12705v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12705v2.pdf filename=2403.12705v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Minimum spanning trees and forests are powerful sparsification techniques that remove cycles from weighted <b>graphs</b> to minimize total edge weight while preserving node connectivity. They have applications in computer science, network science, and <b>graph</b> theory. Despite their utility and ubiquity, they have several limitations, including that they are only defined for undirected networks, they significantly alter dynamics on networks, and they do not generally preserve important network features such as shortest distances, shortest path distribution, and community structure. In contrast, distance backbones, which are subgraphs formed by all edges that obey a generalized triangle inequality, are well defined in both directed and undirected <b>graphs</b> and preserve those and other important network features. The backbone of a <b>graph</b> is defined with respect to a specified path-length operator that aggregates weights along a path to define its length, thereby associating a cost to indirect connections. The backbone is the union of all shortest paths between each pair of nodes according to the specified operator. One such operator, the max function, computes the length of a path as the largest weight of the edges that compose it (a weakest link criterion). It is the only operator that yields an algebraic structure for computing shortest paths that is consistent with De Morgan&rsquo;s laws. Applying this operator yields the ultrametric backbone of a <b>graph</b> in that (semi-triangular) edges whose weights are larger than the length of an indirect path connecting the same nodes (i.e., those that break the generalized triangle inequality based on max as a path-length operator) are removed. We show that the ultrametric backbone is the union of all minimum spanning forests in undirected <b>graphs</b> and provides a new generalization of minimum spanning trees to directed <b>graphs.</b></p></p class="citation"></blockquote><h3 id=22--328329-an-upper-bound-on-the-weisfeiler-leman-dimension-thomas-schneider-et-al-2024>(2/2 | 328/329) An Upper Bound on the Weisfeiler-Leman Dimension (Thomas Schneider et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Schneider, Pascal Schweitzer. (2024)<br><strong>An Upper Bound on the Weisfeiler-Leman Dimension</strong><br><button class=copy-to-clipboard title="An Upper Bound on the Weisfeiler-Leman Dimension" index=328>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-328 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs-LO, cs.DM, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12581v1.pdf filename=2403.12581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Weisfeiler-Leman (WL) dimension is a standard measure in descriptive complexity theory for the structural complexity of a <b>graph.</b> We prove that the WL-dimension of a <b>graph</b> on $n$ vertices is at most $3/20 \cdot n + o(n)= 0.15 \cdot n + o(n)$. The proof develops various techniques to analyze the structure of coherent configurations. This includes sufficient conditions under which a fiber can be restored up to isomorphism if it is removed, a recursive proof exploiting a degree reduction and treewidth bounds, as well as an analysis of interspaces involving small fibers. As a base case, we also analyze the dimension of coherent configurations with small fiber size and thereby <b>graphs</b> with small color class size.</p></p class="citation"></blockquote><h2 id=cond-matstat-mech-1>cond-mat.stat-mech (1)</h2><h3 id=11--329329-the-sis-process-on-erdös-rényi-graphs-determining-the-infected-fraction-o-s-awolude-et-al-2024>(1/1 | 329/329) The SIS process on Erdös-Rényi graphs: determining the infected fraction (O. S. Awolude et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>O. S. Awolude, E. Cator, H. Don. (2024)<br><strong>The SIS process on Erdös-Rényi graphs: determining the infected fraction</strong><br><button class=copy-to-clipboard title="The SIS process on Erdös-Rényi graphs: determining the infected fraction" index=329>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-329 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.stat-mech<br>Categories: 60J27, cond-mat-stat-mech, cond-mat.stat-mech, cs-SI, physics-soc-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12560v1.pdf filename=2403.12560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The SIS process on a <b>graph</b> poses many challenges. An important problem is to identify characteristics of the metastable behaviour. Existing mean-field methods (such as Heterogeneous Mean Field and the N-intertwined Mean Field Approximation) overestimate the metastable infected fraction, because they ignore correlations. Especially in sparse <b>graphs,</b> this leads to serious inaccuracies. We propose quenched and annealed methods incorporating correlations and giving significantly more accurate approximations. We use the Erd"os-R'enyi <b>graph</b> as a test case, but the methods can be generalized easily. Our methods are computationally very friendly and can be applied to fairly large <b>graphs,</b> in contrast with some other second-order mean field approximations.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.20</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.22</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cslg-59>cs.LG (59)</a><ul><li><a href=#159--1329-vl-icl-bench-the-devil-in-the-details-of-benchmarking-multimodal-in-context-learning-yongshuo-zong-et-al-2024>(1/59 | 1/329) VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning (Yongshuo Zong et al., 2024)</a></li><li><a href=#259--2329-a-comparison-of-deep-learning-architectures-for-spacecraft-anomaly-detection-daniel-lakey-et-al-2024>(2/59 | 2/329) A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection (Daniel Lakey et al., 2024)</a></li><li><a href=#359--3329-adapt-to-robustify-prompt-tuning-vision-transformers-masih-eskandar-et-al-2024>(3/59 | 3/329) ADAPT to Robustify Prompt Tuning Vision Transformers (Masih Eskandar et al., 2024)</a></li><li><a href=#459--4329-wildfire-danger-prediction-optimization-with-transfer-learning-spiros-maggioros-et-al-2024>(4/59 | 4/329) Wildfire danger prediction optimization with transfer learning (Spiros Maggioros et al., 2024)</a></li><li><a href=#559--5329-pretraining-codomain-attention-neural-operators-for-solving-multiphysics-pdes-md-ashiqur-rahman-et-al-2024>(5/59 | 5/329) Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs (Md Ashiqur Rahman et al., 2024)</a></li><li><a href=#659--6329-melting-point-mobile-evaluation-of-language-transformers-stefanos-laskaridis-et-al-2024>(6/59 | 6/329) MELTing point: Mobile Evaluation of Language Transformers (Stefanos Laskaridis et al., 2024)</a></li><li><a href=#759--7329-affinequant-affine-transformation-quantization-for-large-language-models-yuexiao-ma-et-al-2024>(7/59 | 7/329) AffineQuant: Affine Transformation Quantization for Large Language Models (Yuexiao Ma et al., 2024)</a></li><li><a href=#859--8329-seven-pruning-transformer-model-by-reserving-sentinels-jinying-xiao-et-al-2024>(8/59 | 8/329) SEVEN: Pruning Transformer Model by Reserving Sentinels (Jinying Xiao et al., 2024)</a></li><li><a href=#959--9329-sun-teams-contribution-to-abaw-2024-competition-audio-visual-valence-arousal-estimation-and-expression-recognition-denis-dresvyanskiy-et-al-2024>(9/59 | 9/329) SUN Team&rsquo;s Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition (Denis Dresvyanskiy et al., 2024)</a></li><li><a href=#1059--10329-contextualized-messages-boost-graph-representations-brian-godwin-lim-2024>(10/59 | 10/329) Contextualized Messages Boost Graph Representations (Brian Godwin Lim, 2024)</a></li><li><a href=#1159--11329-ntk-guided-few-shot-class-incremental-learning-jingren-liu-et-al-2024>(11/59 | 11/329) NTK-Guided Few-Shot Class Incremental Learning (Jingren Liu et al., 2024)</a></li><li><a href=#1259--12329-flowerformer-empowering-neural-architecture-encoding-using-a-flow-aware-graph-transformer-dongyeong-hwang-et-al-2024>(12/59 | 12/329) FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer (Dongyeong Hwang et al., 2024)</a></li><li><a href=#1359--13329-do-generated-data-always-help-contrastive-learning-yifei-wang-et-al-2024>(13/59 | 13/329) Do Generated Data Always Help Contrastive Learning? (Yifei Wang et al., 2024)</a></li><li><a href=#1459--14329-tt-blip-enhancing-fake-news-detection-using-blip-and-tri-transformer-eunjee-choi-et-al-2024>(14/59 | 14/329) TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer (Eunjee Choi et al., 2024)</a></li><li><a href=#1559--15329-fairsin-achieving-fairness-in-graph-neural-networks-through-sensitive-information-neutralization-cheng-yang-et-al-2024>(15/59 | 15/329) FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization (Cheng Yang et al., 2024)</a></li><li><a href=#1659--16329-finding-the-missing-data-a-bert-inspired-approach-against-package-loss-in-wireless-sensing-zijian-zhao-et-al-2024>(16/59 | 16/329) Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing (Zijian Zhao et al., 2024)</a></li><li><a href=#1759--17329-learning-transferable-time-series-classifier-with-cross-domain-pre-training-from-language-model-mingyue-cheng-et-al-2024>(17/59 | 17/329) Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model (Mingyue Cheng et al., 2024)</a></li><li><a href=#1859--18329-sim2real-in-reconstructive-spectroscopy-deep-learning-with-augmented-device-informed-data-simulation-jiyi-chen-et-al-2024>(18/59 | 18/329) Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation (Jiyi Chen et al., 2024)</a></li><li><a href=#1959--19329-advancing-time-series-classification-with-multimodal-language-modeling-mingyue-cheng-et-al-2024>(19/59 | 19/329) Advancing Time Series Classification with Multimodal Language Modeling (Mingyue Cheng et al., 2024)</a></li><li><a href=#2059--20329-prompt-fused-framework-for-inductive-logical-query-answering-zezhong-xu-et-al-2024>(20/59 | 20/329) Prompt-fused framework for Inductive Logical Query Answering (Zezhong Xu et al., 2024)</a></li><li><a href=#2159--21329-automated-contrastive-learning-strategy-search-for-time-series-baoyu-jing-et-al-2024>(21/59 | 21/329) Automated Contrastive Learning Strategy Search for Time Series (Baoyu Jing et al., 2024)</a></li><li><a href=#2259--22329-non-negative-contrastive-learning-yifei-wang-et-al-2024>(22/59 | 22/329) Non-negative Contrastive Learning (Yifei Wang et al., 2024)</a></li><li><a href=#2359--23329-adaptsfl-adaptive-split-federated-learning-in-resource-constrained-edge-networks-zheng-lin-et-al-2024>(23/59 | 23/329) AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks (Zheng Lin et al., 2024)</a></li><li><a href=#2459--24329-towards-better-statistical-understanding-of-watermarking-llms-zhongze-cai-et-al-2024>(24/59 | 24/329) Towards Better Statistical Understanding of Watermarking LLMs (Zhongze Cai et al., 2024)</a></li><li><a href=#2559--25329-sample-complexity-of-offline-distributionally-robust-linear-markov-decision-processes-he-wang-et-al-2024>(25/59 | 25/329) Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes (He Wang et al., 2024)</a></li><li><a href=#2659--26329-jetfire-efficient-and-accurate-transformer-pretraining-with-int8-data-flow-and-per-block-quantization-haocheng-xi-et-al-2024>(26/59 | 26/329) Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization (Haocheng Xi et al., 2024)</a></li><li><a href=#2759--27329-understanding-training-free-diffusion-guidance-mechanisms-and-limitations-yifei-shen-et-al-2024>(27/59 | 27/329) Understanding Training-free Diffusion Guidance: Mechanisms and Limitations (Yifei Shen et al., 2024)</a></li><li><a href=#2859--28329-stg-mamba-spatial-temporal-graph-learning-via-selective-state-space-model-lincan-li-et-al-2024>(28/59 | 28/329) STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model (Lincan Li et al., 2024)</a></li><li><a href=#2959--29329-simple-ingredients-for-offline-reinforcement-learning-edoardo-cetin-et-al-2024>(29/59 | 29/329) Simple Ingredients for Offline Reinforcement Learning (Edoardo Cetin et al., 2024)</a></li><li><a href=#3059--30329-fairstg-countering-performance-heterogeneity-via-collaborative-sample-level-optimization-gengyu-lin-et-al-2024>(30/59 | 30/329) FairSTG: Countering performance heterogeneity via collaborative sample-level optimization (Gengyu Lin et al., 2024)</a></li><li><a href=#3159--31329-multi-fidelity-surrogate-with-heterogeneous-input-spaces-for-modeling-melt-pools-in-laser-directed-energy-deposition-nandana-menon-et-al-2024>(31/59 | 31/329) Multi-fidelity surrogate with heterogeneous input spaces for modeling melt pools in laser-directed energy deposition (Nandana Menon et al., 2024)</a></li><li><a href=#3259--32329-most-likely-sequence-generation-for-n-grams-transformers-hmms-and-markov-chains-by-using-rollout-algorithms-yuchao-li-et-al-2024>(32/59 | 32/329) Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms (Yuchao Li et al., 2024)</a></li><li><a href=#3359--33329-probabilistic-circuits-with-constraints-via-convex-optimization-soroush-ghandi-et-al-2024>(33/59 | 33/329) Probabilistic Circuits with Constraints via Convex Optimization (Soroush Ghandi et al., 2024)</a></li><li><a href=#3459--34329-neural-parameter-regression-for-explicit-representations-of-pde-solution-operators-konrad-mundinger-et-al-2024>(34/59 | 34/329) Neural Parameter Regression for Explicit Representations of PDE Solution Operators (Konrad Mundinger et al., 2024)</a></li><li><a href=#3559--35329-bilora-a-bi-level-optimization-framework-for-overfitting-resilient-low-rank-adaptation-of-large-pre-trained-models-rushi-qiang-et-al-2024>(35/59 | 35/329) BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models (Rushi Qiang et al., 2024)</a></li><li><a href=#3659--36329-lnpt-label-free-network-pruning-and-training-jinying-xiao-et-al-2024>(36/59 | 36/329) LNPT: Label-free Network Pruning and Training (Jinying Xiao et al., 2024)</a></li><li><a href=#3759--37329-hybrid-unsupervised-learning-strategy-for-monitoring-industrial-batch-processes-christian-w-frey-2024>(37/59 | 37/329) Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch Processes (Christian W. Frey, 2024)</a></li><li><a href=#3859--38329-electioneering-the-network-dynamic-multi-step-adversarial-attacks-for-community-canvassing-saurabh-sharma-et-al-2024>(38/59 | 38/329) Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for Community Canvassing (Saurabh Sharma et al., 2024)</a></li><li><a href=#3959--39329-u-net-kalman-filter-unetkf-an-example-of-machine-learning-assisted-ensemble-data-assimilation-feiyu-lu-2024>(39/59 | 39/329) U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted Ensemble Data Assimilation (Feiyu Lu, 2024)</a></li><li><a href=#4059--40329-policy-bifurcation-in-safe-reinforcement-learning-wenjun-zou-et-al-2024>(40/59 | 40/329) Policy Bifurcation in Safe Reinforcement Learning (Wenjun Zou et al., 2024)</a></li><li><a href=#4159--41329-has-approximate-machine-unlearning-been-evaluated-properly-from-auditing-to-side-effects-cheng-long-wang-et-al-2024>(41/59 | 41/329) Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects (Cheng-Long Wang et al., 2024)</a></li><li><a href=#4259--42329-robust-nas-under-adversarial-training-benchmark-theory-and-beyond-yongtao-wu-et-al-2024>(42/59 | 42/329) Robust NAS under adversarial training: benchmark, theory, and beyond (Yongtao Wu et al., 2024)</a></li><li><a href=#4359--43329-predictive-scalable-and-interpretable-knowledge-tracing-on-structured-domains-hanqi-zhou-et-al-2024>(43/59 | 43/329) Predictive, scalable and interpretable knowledge tracing on structured domains (Hanqi Zhou et al., 2024)</a></li><li><a href=#4459--44329-adafish-fast-low-rank-parameter-efficient-fine-tuning-by-using-second-order-information-jiang-hu-et-al-2024>(44/59 | 44/329) AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information (Jiang Hu et al., 2024)</a></li><li><a href=#4559--45329-deep-learning-with-noisy-labels-in-medical-prediction-problems-a-scoping-review-yishu-wei-et-al-2024>(45/59 | 45/329) Deep learning with noisy labels in medical prediction problems: a scoping review (Yishu Wei et al., 2024)</a></li><li><a href=#4659--46329-analyzing-the-impact-of-partial-sharing-on-the-resilience-of-online-federated-learning-against-model-poisoning-attacks-ehsan-lari-et-al-2024>(46/59 | 46/329) Analyzing the Impact of Partial Sharing on the Resilience of Online Federated Learning Against Model Poisoning Attacks (Ehsan Lari et al., 2024)</a></li><li><a href=#4759--47329-knowing-your-nonlinearities-shapley-interactions-reveal-the-underlying-structure-of-data-divyansh-singhvi-et-al-2024>(47/59 | 47/329) Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data (Divyansh Singhvi et al., 2024)</a></li><li><a href=#4859--48329-jaxued-a-simple-and-useable-ued-library-in-jax-samuel-coward-et-al-2024>(48/59 | 48/329) JaxUED: A simple and useable UED library in Jax (Samuel Coward et al., 2024)</a></li><li><a href=#4959--49329-optimal-and-adaptive-non-stationary-dueling-bandits-under-a-generalized-borda-criterion-joe-suk-et-al-2024>(49/59 | 49/329) Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion (Joe Suk et al., 2024)</a></li><li><a href=#5059--50329-on-safety-in-safe-bayesian-optimization-christian-fiedler-et-al-2024>(50/59 | 50/329) On Safety in Safe Bayesian Optimization (Christian Fiedler et al., 2024)</a></li><li><a href=#5159--51329-equivariant-ensembles-and-regularization-for-reinforcement-learning-in-map-based-path-planning-mirco-theile-et-al-2024>(51/59 | 51/329) Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning (Mirco Theile et al., 2024)</a></li><li><a href=#5259--52329-improving-interpretability-of-scores-in-anomaly-detection-based-on-gaussian-bernoulli-restricted-boltzmann-machine-kaiji-sekimoto-et-al-2024>(52/59 | 52/329) Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine (Kaiji Sekimoto et al., 2024)</a></li><li><a href=#5359--53329-fedsr-a-semi-decentralized-federated-learning-algorithm-for-non-iidness-in-iot-system-jianjun-huang-et-al-2024>(53/59 | 53/329) FedSR: A Semi-Decentralized Federated Learning Algorithm for Non-IIDness in IoT System (Jianjun Huang et al., 2024)</a></li><li><a href=#5459--54329-understanding-why-label-smoothing-degrades-selective-classification-and-how-to-fix-it-guoxuan-xia-et-al-2024>(54/59 | 54/329) Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It (Guoxuan Xia et al., 2024)</a></li><li><a href=#5559--55329-transfer-in-sequential-multi-armed-bandits-via-reward-samples-rahul-n-r-et-al-2024>(55/59 | 55/329) Transfer in Sequential Multi-armed Bandits via Reward Samples (Rahul N R et al., 2024)</a></li><li><a href=#5659--56329-temporally-consistent-koopman-autoencoders-for-forecasting-dynamical-systems-indranil-nayak-et-al-2024>(56/59 | 56/329) Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems (Indranil Nayak et al., 2024)</a></li><li><a href=#5759--57329-fedfisher-leveraging-fisher-information-for-one-shot-federated-learning-divyansh-jhunjhunwala-et-al-2024>(57/59 | 57/329) FedFisher: Leveraging Fisher Information for One-Shot Federated Learning (Divyansh Jhunjhunwala et al., 2024)</a></li><li><a href=#5859--58329-bilevel-hypergraph-networks-for-multi-modal-alzheimers-diagnosis-angelica-i-aviles-rivero-et-al-2024>(58/59 | 58/329) Bilevel Hypergraph Networks for Multi-Modal Alzheimer&rsquo;s Diagnosis (Angelica I. Aviles-Rivero et al., 2024)</a></li><li><a href=#5959--59329-dynamic-survival-analysis-for-early-event-prediction-hugo-yèche-et-al-2024>(59/59 | 59/329) Dynamic Survival Analysis for Early Event Prediction (Hugo Yèche et al., 2024)</a></li></ul></li><li><a href=#cscl-45>cs.CL (45)</a><ul><li><a href=#145--60329-automated-data-curation-for-robust-language-model-fine-tuning-jiuhai-chen-et-al-2024>(1/45 | 60/329) Automated Data Curation for Robust Language Model Fine-Tuning (Jiuhai Chen et al., 2024)</a></li><li><a href=#245--61329-chart-based-reasoning-transferring-capabilities-from-llms-to-vlms-victor-carbune-et-al-2024>(2/45 | 61/329) Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs (Victor Carbune et al., 2024)</a></li><li><a href=#345--62329-automatic-summarization-of-doctor-patient-encounter-dialogues-using-large-language-model-through-prompt-tuning-mengxian-lyu-et-al-2024>(3/45 | 62/329) Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning (Mengxian Lyu et al., 2024)</a></li><li><a href=#445--63329-rankprompt-step-by-step-comparisons-make-language-models-better-reasoners-chi-hu-et-al-2024>(4/45 | 63/329) RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners (Chi Hu et al., 2024)</a></li><li><a href=#545--64329-crosstune-black-box-few-shot-classification-with-label-enhancement-danqing-luo-et-al-2024>(5/45 | 64/329) CrossTune: Black-Box Few-Shot Classification with Label Enhancement (Danqing Luo et al., 2024)</a></li><li><a href=#645--65329-pragmatic-competence-evaluation-of-large-language-models-for-korean-dojun-park-et-al-2024>(6/45 | 65/329) Pragmatic Competence Evaluation of Large Language Models for Korean (Dojun Park et al., 2024)</a></li><li><a href=#745--66329-encode-once-and-decode-in-parallel-efficient-transformer-decoding-bo-ru-lu-et-al-2024>(7/45 | 66/329) Encode Once and Decode in Parallel: Efficient Transformer Decoding (Bo-Ru Lu et al., 2024)</a></li><li><a href=#845--67329-towards-unsupervised-question-answering-system-with-multi-level-summarization-for-legal-text-m-manvith-prabhu-et-al-2024>(8/45 | 67/329) Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text (M Manvith Prabhu et al., 2024)</a></li><li><a href=#945--68329-llms-based-few-shot-disease-predictions-using-ehr-a-novel-approach-combining-predictive-agent-reasoning-and-critical-agent-instruction-hejie-cui-et-al-2024>(9/45 | 68/329) LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction (Hejie Cui et al., 2024)</a></li><li><a href=#1045--69329-instructing-large-language-models-to-identify-and-ignore-irrelevant-conditions-zhenyu-wu-et-al-2024>(10/45 | 69/329) Instructing Large Language Models to Identify and Ignore Irrelevant Conditions (Zhenyu Wu et al., 2024)</a></li><li><a href=#1145--70329-alphafin-benchmarking-financial-analysis-with-retrieval-augmented-stock-chain-framework-xiang-li-et-al-2024>(11/45 | 70/329) AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework (Xiang Li et al., 2024)</a></li><li><a href=#1245--71329-improving-generalizability-of-extracting-social-determinants-of-health-using-large-language-models-through-prompt-tuning-cheng-peng-et-al-2024>(12/45 | 71/329) Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning (Cheng Peng et al., 2024)</a></li><li><a href=#1345--72329-llmlingua-2-data-distillation-for-efficient-and-faithful-task-agnostic-prompt-compression-zhuoshi-pan-et-al-2024>(13/45 | 72/329) LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression (Zhuoshi Pan et al., 2024)</a></li><li><a href=#1445--73329-lhmke-a-large-scale-holistic-multi-subject-knowledge-evaluation-benchmark-for-chinese-large-language-models-chuang-liu-et-al-2024>(14/45 | 73/329) LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models (Chuang Liu et al., 2024)</a></li><li><a href=#1545--74329-automatic-information-extraction-from-employment-tribunal-judgements-using-large-language-models-joana-ribeiro-de-faria-et-al-2024>(15/45 | 74/329) Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models (Joana Ribeiro de Faria et al., 2024)</a></li><li><a href=#1645--75329-fine-tuning-pre-trained-language-models-to-detect-in-game-trash-talks-daniel-fesalbon-et-al-2024>(16/45 | 75/329) Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks (Daniel Fesalbon et al., 2024)</a></li><li><a href=#1745--76329-cross-lingual-transfer-for-natural-language-inference-via-multilingual-prompt-translator-xiaoyu-qiu-et-al-2024>(17/45 | 76/329) Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator (Xiaoyu Qiu et al., 2024)</a></li><li><a href=#1845--77329-agent-flan-designing-data-and-methods-of-effective-agent-tuning-for-large-language-models-zehui-chen-et-al-2024>(18/45 | 77/329) Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models (Zehui Chen et al., 2024)</a></li><li><a href=#1945--78329-generalizable-and-stable-finetuning-of-pretrained-language-models-on-low-resource-texts-sai-ashish-somayajula-et-al-2024>(19/45 | 78/329) Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts (Sai Ashish Somayajula et al., 2024)</a></li><li><a href=#2045--79329-investigating-text-shortening-strategy-in-bert-truncation-vs-summarization-mirza-alim-mutasodirin-et-al-2024>(20/45 | 79/329) Investigating Text Shortening Strategy in BERT: Truncation vs Summarization (Mirza Alim Mutasodirin et al., 2024)</a></li><li><a href=#2145--80329-dr3-ask-large-language-models-not-to-give-off-topic-answers-in-open-domain-multi-hop-question-answering-yuan-gao-et-al-2024>(21/45 | 80/329) Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering (Yuan Gao et al., 2024)</a></li><li><a href=#2245--81329-towards-interpretable-hate-speech-detection-using-large-language-model-extracted-rationales-ayushi-nirmal-et-al-2024>(22/45 | 81/329) Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales (Ayushi Nirmal et al., 2024)</a></li><li><a href=#2345--82329-multi-dimensional-machine-translation-evaluation-model-evaluation-and-resource-for-korean-dojun-park-et-al-2024>(23/45 | 82/329) Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean (Dojun Park et al., 2024)</a></li><li><a href=#2445--83329-characteristic-ai-agents-via-large-language-models-xi-wang-et-al-2024>(24/45 | 83/329) Characteristic AI Agents via Large Language Models (Xi Wang et al., 2024)</a></li><li><a href=#2545--84329-self-generated-replay-memories-for-continual-neural-machine-translation-michele-resta-et-al-2024>(25/45 | 84/329) Self-generated Replay Memories for Continual Neural Machine Translation (Michele Resta et al., 2024)</a></li><li><a href=#2645--85329-sebastian-basti-wastl-recognizing-named-entities-in-bavarian-dialectal-data-siyao-peng-et-al-2024>(26/45 | 85/329) Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data (Siyao Peng et al., 2024)</a></li><li><a href=#2745--86329-factorized-learning-assisted-with-large-language-model-for-gloss-free-sign-language-translation-zhigang-chen-et-al-2024>(27/45 | 86/329) Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation (Zhigang Chen et al., 2024)</a></li><li><a href=#2845--87329-an-empirical-study-of-speech-language-models-for-prompt-conditioned-speech-synthesis-yifan-peng-et-al-2024>(28/45 | 87/329) An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis (Yifan Peng et al., 2024)</a></li><li><a href=#2945--88329-pipelined-biomedical-event-extraction-rivaling-joint-learning-pengchao-wu-et-al-2024>(29/45 | 88/329) Pipelined Biomedical Event Extraction Rivaling Joint Learning (Pengchao Wu et al., 2024)</a></li><li><a href=#3045--89329-assessing-effect-sizes-variability-and-power-in-the-on-line-study-of-language-production-bürki-audrey-et-al-2024>(30/45 | 89/329) Assessing effect sizes, variability, and power in the on-line study of language production (Bürki Audrey et al., 2024)</a></li><li><a href=#3145--90329-graphere-jointly-multiple-event-event-relation-extraction-via-graph-enhanced-event-embeddings-haochen-li-et-al-2024>(31/45 | 90/329) GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings (Haochen Li et al., 2024)</a></li><li><a href=#3245--91329-third-party-language-model-performance-prediction-from-instruction-rahul-nadkarni-et-al-2024>(32/45 | 91/329) Third-Party Language Model Performance Prediction from Instruction (Rahul Nadkarni et al., 2024)</a></li><li><a href=#3345--92329-dated-data-tracing-knowledge-cutoffs-in-large-language-models-jeffrey-cheng-et-al-2024>(33/45 | 92/329) Dated Data: Tracing Knowledge Cutoffs in Large Language Models (Jeffrey Cheng et al., 2024)</a></li><li><a href=#3445--93329-supporting-energy-policy-research-with-large-language-models-grant-buster-et-al-2024>(34/45 | 93/329) Supporting Energy Policy Research with Large Language Models (Grant Buster et al., 2024)</a></li><li><a href=#3545--94329-epistemology-of-language-models-do-language-models-have-holistic-knowledge-minsu-kim-et-al-2024>(35/45 | 94/329) Epistemology of Language Models: Do Language Models Have Holistic Knowledge? (Minsu Kim et al., 2024)</a></li><li><a href=#3645--95329-simple-hack-for-transformers-against-heavy-long-text-classification-on-a-time--and-memory-limited-gpu-service-mirza-alim-mutasodirin-et-al-2024>(36/45 | 95/329) Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service (Mirza Alim Mutasodirin et al., 2024)</a></li><li><a href=#3745--96329-a-large-collection-of-model-generated-contradictory-responses-for-consistency-aware-dialogue-systems-shiki-sato-et-al-2024>(37/45 | 96/329) A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems (Shiki Sato et al., 2024)</a></li><li><a href=#3845--97329-arapoembert-a-pretrained-language-model-for-arabic-poetry-analysis-faisal-qarah-2024>(38/45 | 97/329) AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis (Faisal Qarah, 2024)</a></li><li><a href=#3945--98329-prompt-based-graph-model-for-joint-liberal-event-extraction-and-event-schema-induction-haochen-li-et-al-2024>(39/45 | 98/329) Prompt-based Graph Model for Joint Liberal Event Extraction and Event Schema Induction (Haochen Li et al., 2024)</a></li><li><a href=#4045--99329-comparing-explanation-faithfulness-between-multilingual-and-monolingual-fine-tuned-language-models-zhixue-zhao-et-al-2024>(40/45 | 99/329) Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models (Zhixue Zhao et al., 2024)</a></li><li><a href=#4145--100329-classla-web-comparable-web-corpora-of-south-slavic-languages-enriched-with-linguistic-and-genre-annotation-nikola-ljubešić-et-al-2024>(41/45 | 100/329) CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation (Nikola Ljubešić et al., 2024)</a></li><li><a href=#4245--101329-empowering-air-travelers-a-chatbot-for-canadian-air-passenger-rights-maksym-taranukhin-et-al-2024>(42/45 | 101/329) Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights (Maksym Taranukhin et al., 2024)</a></li><li><a href=#4345--102329-mslm-s2st-a-multitask-speech-language-model-for-textless-speech-to-speech-translation-with-speaker-style-preservation-yifan-peng-et-al-2024>(43/45 | 102/329) MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation (Yifan Peng et al., 2024)</a></li><li><a href=#4445--103329-wav2gloss-generating-interlinear-glossed-text-from-speech-taiqi-he-et-al-2024>(44/45 | 103/329) Wav2Gloss: Generating Interlinear Glossed Text from Speech (Taiqi He et al., 2024)</a></li><li><a href=#4545--104329-when-do-more-contexts-help-with-sarcasm-recognition-ojas-nimase-et-al-2024>(45/45 | 104/329) When Do &lsquo;More Contexts&rsquo; Help with Sarcasm Recognition? (Ojas Nimase et al., 2024)</a></li></ul></li><li><a href=#cscv-104>cs.CV (104)</a><ul><li><a href=#1104--105329-emotion-recognition-using-transformers-with-masked-learning-seongjae-min-et-al-2024>(1/104 | 105/329) Emotion Recognition Using Transformers with Masked Learning (Seongjae Min et al., 2024)</a></li><li><a href=#2104--106329-towards-multimodal-in-context-learning-for-vision--language-models-sivan-doveh-et-al-2024>(2/104 | 106/329) Towards Multimodal In-Context Learning for Vision & Language Models (Sivan Doveh et al., 2024)</a></li><li><a href=#3104--107329-medbind-unifying-language-and-multimodal-medical-data-embeddings-yuan-gao-et-al-2024>(3/104 | 107/329) MEDBind: Unifying Language and Multimodal Medical Data Embeddings (Yuan Gao et al., 2024)</a></li><li><a href=#4104--108329-visiongpt-llm-assisted-real-time-anomaly-detection-for-safe-visual-navigation-hao-wang-et-al-2024>(4/104 | 108/329) VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation (Hao Wang et al., 2024)</a></li><li><a href=#5104--109329-dettoolchain-a-new-prompting-paradigm-to-unleash-detection-ability-of-mllm-yixuan-wu-et-al-2024>(5/104 | 109/329) DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM (Yixuan Wu et al., 2024)</a></li><li><a href=#6104--110329-unibind-llm-augmented-unified-and-balanced-representation-space-to-bind-them-all-yuanhuiyi-lyu-et-al-2024>(6/104 | 110/329) UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All (Yuanhuiyi Lyu et al., 2024)</a></li><li><a href=#7104--111329-relationvlm-making-large-vision-language-models-understand-visual-relations-zhipeng-huang-et-al-2024>(7/104 | 111/329) RelationVLM: Making Large Vision-Language Models Understand Visual Relations (Zhipeng Huang et al., 2024)</a></li><li><a href=#8104--112329-vitgaze-gaze-following-with-interaction-features-in-vision-transformers-yuehao-song-et-al-2024>(8/104 | 112/329) ViTGaze: Gaze Following with Interaction Features in Vision Transformers (Yuehao Song et al., 2024)</a></li><li><a href=#9104--113329-as-firm-as-their-foundations-can-open-sourced-foundation-models-be-used-to-create-adversarial-examples-for-downstream-tasks-anjun-hu-et-al-2024>(9/104 | 113/329) As Firm As Their Foundations: Can open-sourced foundation models be used to create adversarial examples for downstream tasks? (Anjun Hu et al., 2024)</a></li><li><a href=#10104--114329-compound-expression-recognition-via-multi-model-ensemble-jun-yu-et-al-2024>(10/104 | 114/329) Compound Expression Recognition via Multi Model Ensemble (Jun Yu et al., 2024)</a></li><li><a href=#11104--115329-transformmix-learning-transformation-and-mixing-strategies-from-data-tsz-him-cheung-et-al-2024>(11/104 | 115/329) TransformMix: Learning Transformation and Mixing Strategies from Data (Tsz-Him Cheung et al., 2024)</a></li><li><a href=#12104--116329-mplug-docowl-15-unified-structure-learning-for-ocr-free-document-understanding-anwen-hu-et-al-2024>(12/104 | 116/329) mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding (Anwen Hu et al., 2024)</a></li><li><a href=#13104--117329-compositional-3d-scene-synthesis-with-scene-graph-guided-layout-shape-generation-yao-wei-et-al-2024>(13/104 | 117/329) Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation (Yao Wei et al., 2024)</a></li><li><a href=#14104--118329-better-call-sal-towards-learning-to-segment-anything-in-lidar-aljoša-ošep-et-al-2024>(14/104 | 118/329) Better Call SAL: Towards Learning to Segment Anything in Lidar (Aljoša Ošep et al., 2024)</a></li><li><a href=#15104--119329-improved-eatformer-a-vision-transformer-for-medical-image-classification-yulong-shisu-et-al-2024>(15/104 | 119/329) Improved EATFormer: A Vision Transformer for Medical Image Classification (Yulong Shisu et al., 2024)</a></li><li><a href=#16104--120329-just-shift-it-test-time-prototype-shifting-for-zero-shot-generalization-with-vision-language-models-elaine-sui-et-al-2024>(16/104 | 120/329) Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models (Elaine Sui et al., 2024)</a></li><li><a href=#17104--121329-you-only-sample-once-taming-one-step-text-to-image-synthesis-by-self-cooperative-diffusion-gans-yihong-luo-et-al-2024>(17/104 | 121/329) You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs (Yihong Luo et al., 2024)</a></li><li><a href=#18104--122329-hydra-a-hyper-agent-for-dynamic-compositional-visual-reasoning-fucai-ke-et-al-2024>(18/104 | 122/329) HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning (Fucai Ke et al., 2024)</a></li><li><a href=#19104--123329-learning-cross-view-visual-geo-localization-without-ground-truth-haoyuan-li-et-al-2024>(19/104 | 123/329) Learning Cross-view Visual Geo-localization without Ground Truth (Haoyuan Li et al., 2024)</a></li><li><a href=#20104--124329-boosting-transferability-in-vision-language-attacks-via-diversification-along-the-intersection-region-of-adversarial-trajectory-sensen-gao-et-al-2024>(20/104 | 124/329) Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory (Sensen Gao et al., 2024)</a></li><li><a href=#21104--125329-diffusion-driven-self-supervised-learning-for-shape-reconstruction-and-pose-estimation-jingtao-sun-et-al-2024>(21/104 | 125/329) Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation (Jingtao Sun et al., 2024)</a></li><li><a href=#22104--126329-videobadminton-a-video-dataset-for-badminton-action-recognition-qi-li-et-al-2024>(22/104 | 126/329) VideoBadminton: A Video Dataset for Badminton Action Recognition (Qi Li et al., 2024)</a></li><li><a href=#23104--127329-deblurdinat-a-lightweight-and-effective-transformer-for-image-deblurring-hanzhou-liu-et-al-2024>(23/104 | 127/329) DeblurDiNAT: A Lightweight and Effective Transformer for Image Deblurring (Hanzhou Liu et al., 2024)</a></li><li><a href=#24104--128329-negative-yields-positive-unified-dual-path-adapter-for-vision-language-models-ce-zhang-et-al-2024>(24/104 | 128/329) Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models (Ce Zhang et al., 2024)</a></li><li><a href=#25104--129329-tuning-free-image-customization-with-image-and-text-guidance-pengzhi-li-et-al-2024>(25/104 | 129/329) Tuning-Free Image Customization with Image and Text Guidance (Pengzhi Li et al., 2024)</a></li><li><a href=#26104--130329-chain-of-spot-interactive-reasoning-improves-large-vision-language-models-zuyan-liu-et-al-2024>(26/104 | 130/329) Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models (Zuyan Liu et al., 2024)</a></li><li><a href=#27104--131329-texdreamer-towards-zero-shot-high-fidelity-3d-human-texture-generation-yufei-liu-et-al-2024>(27/104 | 131/329) TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation (Yufei Liu et al., 2024)</a></li><li><a href=#28104--132329-dynamic-spatial-temporal-aggregation-for-skeleton-aware-sign-language-recognition-lianyu-hu-et-al-2024>(28/104 | 132/329) Dynamic Spatial-Temporal Aggregation for Skeleton-Aware Sign Language Recognition (Lianyu Hu et al., 2024)</a></li><li><a href=#29104--133329-multimodal-fusion-method-with-spatiotemporal-sequences-and-relationship-learning-for-valence-arousal-estimation-jun-yu-et-al-2024>(29/104 | 133/329) Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation (Jun Yu et al., 2024)</a></li><li><a href=#30104--134329-adapting-visual-language-models-for-generalizable-anomaly-detection-in-medical-images-chaoqin-huang-et-al-2024>(30/104 | 134/329) Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images (Chaoqin Huang et al., 2024)</a></li><li><a href=#31104--135329-confidence-self-calibration-for-multi-label-class-incremental-learning-kaile-du-et-al-2024>(31/104 | 135/329) Confidence Self-Calibration for Multi-Label Class-Incremental Learning (Kaile Du et al., 2024)</a></li><li><a href=#32104--136329-semantics-distortion-and-style-matter-towards-source-free-uda-for-panoramic-segmentation-xu-zheng-et-al-2024>(32/104 | 136/329) Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation (Xu Zheng et al., 2024)</a></li><li><a href=#33104--137329-task-customized-mixture-of-adapters-for-general-image-fusion-pengfei-zhu-et-al-2024>(33/104 | 137/329) Task-Customized Mixture of Adapters for General Image Fusion (Pengfei Zhu et al., 2024)</a></li><li><a href=#34104--138329-scenescript-reconstructing-scenes-with-an-autoregressive-structured-language-model-armen-avetisyan-et-al-2024>(34/104 | 138/329) SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model (Armen Avetisyan et al., 2024)</a></li><li><a href=#35104--139329-fouriscale-a-frequency-perspective-on-training-free-high-resolution-image-synthesis-linjiang-huang-et-al-2024>(35/104 | 139/329) FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis (Linjiang Huang et al., 2024)</a></li><li><a href=#36104--140329-fresco-spatial-temporal-correspondence-for-zero-shot-video-translation-shuai-yang-et-al-2024>(36/104 | 140/329) FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation (Shuai Yang et al., 2024)</a></li><li><a href=#37104--141329-taptr-tracking-any-point-with-transformers-as-detection-hongyang-li-et-al-2024>(37/104 | 141/329) TAPTR: Tracking Any Point with Transformers as Detection (Hongyang Li et al., 2024)</a></li><li><a href=#38104--142329-emotic-masked-autoencoder-with-attention-fusion-for-facial-expression-recognition-bach-nguyen-xuan-et-al-2024>(38/104 | 142/329) Emotic Masked Autoencoder with Attention Fusion for Facial Expression Recognition (Bach Nguyen-Xuan et al., 2024)</a></li><li><a href=#39104--143329-animatediff-lightning-cross-model-diffusion-distillation-shanchuan-lin-et-al-2024>(39/104 | 143/329) AnimateDiff-Lightning: Cross-Model Diffusion Distillation (Shanchuan Lin et al., 2024)</a></li><li><a href=#40104--144329-laspa-latent-spatial-alignment-for-fast-training-free-single-image-editing-yazeed-alharbi-et-al-2024>(40/104 | 144/329) LASPA: Latent Spatial Alignment for Fast Training-free Single Image Editing (Yazeed Alharbi et al., 2024)</a></li><li><a href=#41104--145329-pct-perspective-cue-training-framework-for-multi-camera-bev-segmentation-haruya-ishikawa-et-al-2024>(41/104 | 145/329) PCT: Perspective Cue Training Framework for Multi-Camera BEV Segmentation (Haruya Ishikawa et al., 2024)</a></li><li><a href=#42104--146329-clip-vis-adapting-clip-for-open-vocabulary-video-instance-segmentation-wenqi-zhu-et-al-2024>(42/104 | 146/329) CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation (Wenqi Zhu et al., 2024)</a></li><li><a href=#43104--147329-ov9d-open-vocabulary-category-level-9d-object-pose-and-size-estimation-junhao-cai-et-al-2024>(43/104 | 147/329) OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation (Junhao Cai et al., 2024)</a></li><li><a href=#44104--148329-when-do-we-not-need-larger-vision-models-baifeng-shi-et-al-2024>(44/104 | 148/329) When Do We Not Need Larger Vision Models? (Baifeng Shi et al., 2024)</a></li><li><a href=#45104--149329-real-iad-a-real-world-multi-view-dataset-for-benchmarking-versatile-industrial-anomaly-detection-chengjie-wang-et-al-2024>(45/104 | 149/329) Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection (Chengjie Wang et al., 2024)</a></li><li><a href=#46104--150329-self-learning-canonical-space-for-multi-view-3d-human-pose-estimation-xiaoben-li-et-al-2024>(46/104 | 150/329) Self-learning Canonical Space for Multi-view 3D Human Pose Estimation (Xiaoben Li et al., 2024)</a></li><li><a href=#47104--151329-precise-physics-driven-text-to-3d-generation-qingshan-xu-et-al-2024>(47/104 | 151/329) Precise-Physics Driven Text-to-3D Generation (Qingshan Xu et al., 2024)</a></li><li><a href=#48104--152329-dmad-dual-memory-bank-for-real-world-anomaly-detection-jianlong-hu-et-al-2024>(48/104 | 152/329) DMAD: Dual Memory Bank for Real-World Anomaly Detection (Jianlong Hu et al., 2024)</a></li><li><a href=#49104--153329-unveiling-the-anomalies-in-an-ever-changing-world-a-benchmark-for-pixel-level-anomaly-detection-in-continual-learning-nikola-bugarin-et-al-2024>(49/104 | 153/329) Unveiling the Anomalies in an Ever-Changing World: A Benchmark for Pixel-Level Anomaly Detection in Continual Learning (Nikola Bugarin et al., 2024)</a></li><li><a href=#50104--154329-learning-neural-volumetric-pose-features-for-camera-localization-jingyu-lin-et-al-2024>(50/104 | 154/329) Learning Neural Volumetric Pose Features for Camera Localization (Jingyu Lin et al., 2024)</a></li><li><a href=#51104--155329-watervg-waterway-visual-grounding-based-on-text-guided-vision-and-mmwave-radar-runwei-guan-et-al-2024>(51/104 | 155/329) WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar (Runwei Guan et al., 2024)</a></li><li><a href=#52104--156329-prompt-guided-adaptive-model-transformation-for-whole-slide-image-classification-yi-lin-et-al-2024>(52/104 | 156/329) Prompt-Guided Adaptive Model Transformation for Whole Slide Image Classification (Yi Lin et al., 2024)</a></li><li><a href=#53104--157329-entity6k-a-large-open-domain-evaluation-dataset-for-real-world-entity-recognition-jielin-qiu-et-al-2024>(53/104 | 157/329) Entity6K: A Large Open-Domain Evaluation Dataset for Real-World Entity Recognition (Jielin Qiu et al., 2024)</a></li><li><a href=#54104--158329-luwa-dataset-learning-lithic-use-wear-analysis-on-microscopic-images-jing-zhang-et-al-2024>(54/104 | 158/329) LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images (Jing Zhang et al., 2024)</a></li><li><a href=#55104--159329-ultra-high-resolution-image-synthesis-with-pyramid-diffusion-model-jiajie-yang-2024>(55/104 | 159/329) Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model (Jiajie Yang, 2024)</a></li><li><a href=#56104--160329-confusing-pair-correction-based-on-category-prototype-for-domain-adaptation-under-noisy-environments-churan-zhi-et-al-2024>(56/104 | 160/329) Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments (Churan Zhi et al., 2024)</a></li><li><a href=#57104--161329-dreamda-generative-data-augmentation-with-diffusion-models-yunxiang-fu-et-al-2024>(57/104 | 161/329) DreamDA: Generative Data Augmentation with Diffusion Models (Yunxiang Fu et al., 2024)</a></li><li><a href=#58104--162329-discover-and-mitigate-multiple-biased-subgroups-in-image-classifiers-zeliang-zhang-et-al-2024>(58/104 | 162/329) Discover and Mitigate Multiple Biased Subgroups in Image Classifiers (Zeliang Zhang et al., 2024)</a></li><li><a href=#59104--163329-towards-controllable-face-generation-with-semantic-latent-diffusion-models-alex-ergasti-et-al-2024>(59/104 | 163/329) Towards Controllable Face Generation with Semantic Latent Diffusion Models (Alex Ergasti et al., 2024)</a></li><li><a href=#60104--164329-eas-snn-end-to-end-adaptive-sampling-and-representation-for-event-based-detection-with-recurrent-spiking-neural-networks-ziming-wang-et-al-2024>(60/104 | 164/329) EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks (Ziming Wang et al., 2024)</a></li><li><a href=#61104--165329-a-hybrid-transformer-sequencer-approach-for-age-and-gender-classification-from-in-wild-facial-images-aakash-singh-et-al-2024>(61/104 | 165/329) A Hybrid Transformer-Sequencer approach for Age and Gender classification from in-wild facial images (Aakash Singh et al., 2024)</a></li><li><a href=#62104--166329-privacy-preserving-face-recognition-using-trainable-feature-subtraction-yuxi-mi-et-al-2024>(62/104 | 166/329) Privacy-Preserving Face Recognition Using Trainable Feature Subtraction (Yuxi Mi et al., 2024)</a></li><li><a href=#63104--167329-human-mesh-recovery-from-arbitrary-multi-view-images-xiaoben-li-et-al-2024>(63/104 | 167/329) Human Mesh Recovery from Arbitrary Multi-view Images (Xiaoben Li et al., 2024)</a></li><li><a href=#64104--168329-comboverse-compositional-3d-assets-creation-using-spatially-aware-diffusion-guidance-yongwei-chen-et-al-2024>(64/104 | 168/329) ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance (Yongwei Chen et al., 2024)</a></li><li><a href=#65104--169329-xpose-explainable-human-pose-estimation-luyu-qiu-et-al-2024>(65/104 | 169/329) XPose: eXplainable Human Pose Estimation (Luyu Qiu et al., 2024)</a></li><li><a href=#66104--170329-contextual-ad-narration-with-interleaved-multimodal-sequence-hanlin-wang-et-al-2024>(66/104 | 170/329) Contextual AD Narration with Interleaved Multimodal Sequence (Hanlin Wang et al., 2024)</a></li><li><a href=#67104--171329-visualcritic-making-lmms-perceive-visual-quality-like-humans-zhipeng-huang-et-al-2024>(67/104 | 171/329) VisualCritic: Making LMMs Perceive Visual Quality Like Humans (Zhipeng Huang et al., 2024)</a></li><li><a href=#68104--172329-m2da-multi-modal-fusion-transformer-incorporating-driver-attention-for-autonomous-driving-dongyang-xu-et-al-2024>(68/104 | 172/329) M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving (Dongyang Xu et al., 2024)</a></li><li><a href=#69104--173329-reflectivity-is-all-you-need-advancing-lidar-semantic-segmentation-kasi-viswanath-et-al-2024>(69/104 | 173/329) Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation (Kasi Viswanath et al., 2024)</a></li><li><a href=#70104--174329-gvgen-text-to-3d-generation-with-volumetric-representation-xianglong-he-et-al-2024>(70/104 | 174/329) GVGEN: Text-to-3D Generation with Volumetric Representation (Xianglong He et al., 2024)</a></li><li><a href=#71104--175329-ponq-a-neural-qem-based-mesh-representation-nissim-maruani-et-al-2024>(71/104 | 175/329) PoNQ: a Neural QEM-based Mesh Representation (Nissim Maruani et al., 2024)</a></li><li><a href=#72104--176329-global-guided-focal-neural-radiance-field-for-large-scale-scene-rendering-mingqi-shao-et-al-2024>(72/104 | 176/329) Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering (Mingqi Shao et al., 2024)</a></li><li><a href=#73104--177329-facexformer-a-unified-transformer-for-facial-analysis-kartik-narayan-et-al-2024>(73/104 | 177/329) FaceXFormer: A Unified Transformer for Facial Analysis (Kartik Narayan et al., 2024)</a></li><li><a href=#74104--178329-waveface-authentic-face-restoration-with-efficient-frequency-recovery-yunqi-miao-et-al-2024>(74/104 | 178/329) WaveFace: Authentic Face Restoration with Efficient Frequency Recovery (Yunqi Miao et al., 2024)</a></li><li><a href=#75104--179329-postometro-pose-token-enhanced-mesh-transformer-for-robust-3d-human-mesh-recovery-wendi-yang-et-al-2024>(75/104 | 179/329) PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery (Wendi Yang et al., 2024)</a></li><li><a href=#76104--180329-sc-diff-3d-shape-completion-with-latent-diffusion-models-juan-d-galvis-et-al-2024>(76/104 | 180/329) SC-Diff: 3D Shape Completion with Latent Diffusion Models (Juan D. Galvis et al., 2024)</a></li><li><a href=#77104--181329-few-shot-object-localization-yunhan-ren-et-al-2024>(77/104 | 181/329) Few-shot Object Localization (Yunhan Ren et al., 2024)</a></li><li><a href=#78104--182329-eye-gaze-guided-multi-modal-alignment-framework-for-radiology-chong-ma-et-al-2024>(78/104 | 182/329) Eye-gaze Guided Multi-modal Alignment Framework for Radiology (Chong Ma et al., 2024)</a></li><li><a href=#79104--183329-depth-guided-nerf-training-via-earth-movers-distance-anita-rau-et-al-2024>(79/104 | 183/329) Depth-guided NeRF Training via Earth Mover&rsquo;s Distance (Anita Rau et al., 2024)</a></li><li><a href=#80104--184329-hermite-coordinate-interpolation-kernels-application-to-image-zooming-konstantinos-k-delibasis-et-al-2024>(80/104 | 184/329) Hermite coordinate interpolation kernels: application to image zooming (Konstantinos K. Delibasis et al., 2024)</a></li><li><a href=#81104--185329-3d-semantic-mapnet-building-maps-for-multi-object-re-identification-in-3d-vincent-cartillier-et-al-2024>(81/104 | 185/329) 3D Semantic MapNet: Building Maps for Multi-Object Re-Identification in 3D (Vincent Cartillier et al., 2024)</a></li><li><a href=#82104--186329-train-ego-path-detection-on-railway-tracks-using-end-to-end-deep-learning-thomas-laurent-2024>(82/104 | 186/329) Train Ego-Path Detection on Railway Tracks Using End-to-End Deep Learning (Thomas Laurent, 2024)</a></li><li><a href=#83104--187329-hulp-human-in-the-loop-for-prognosis-muhammad-ridzuan-et-al-2024>(83/104 | 187/329) HuLP: Human-in-the-Loop for Prognosis (Muhammad Ridzuan et al., 2024)</a></li><li><a href=#84104--188329-magic-fixup-streamlining-photo-editing-by-watching-dynamic-videos-hadi-alzayer-et-al-2024>(84/104 | 188/329) Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos (Hadi Alzayer et al., 2024)</a></li><li><a href=#85104--189329-textile-a-differentiable-metric-for-texture-tileability-carlos-rodriguez-pardo-et-al-2024>(85/104 | 189/329) TexTile: A Differentiable Metric for Texture Tileability (Carlos Rodriguez-Pardo et al., 2024)</a></li><li><a href=#86104--190329-segment-anything-for-comprehensive-analysis-of-grapevine-cluster-architecture-and-berry-properties-efrain-torres-lomas-et-al-2024>(86/104 | 190/329) Segment Anything for comprehensive analysis of grapevine cluster architecture and berry properties (Efrain Torres-Lomas et al., 2024)</a></li><li><a href=#87104--191329-zero-reference-low-light-enhancement-via-physical-quadruple-priors-wenjing-wang-et-al-2024>(87/104 | 191/329) Zero-Reference Low-Light Enhancement via Physical Quadruple Priors (Wenjing Wang et al., 2024)</a></li><li><a href=#88104--192329-ddsb-an-unsupervised-and-training-free-method-for-phase-detection-in-echocardiography-zhenyu-bu-et-al-2024>(88/104 | 192/329) DDSB: An Unsupervised and Training-free Method for Phase Detection in Echocardiography (Zhenyu Bu et al., 2024)</a></li><li><a href=#89104--193329-inter--and-intra-uncertainty-based-feature-aggregation-model-for-semi-supervised-histopathology-image-segmentation-qiangguo-jin-et-al-2024>(89/104 | 193/329) Inter- and intra-uncertainty based feature aggregation model for semi-supervised histopathology image segmentation (Qiangguo Jin et al., 2024)</a></li><li><a href=#90104--194329-building-brain-tumor-segmentation-networks-with-user-assisted-filter-estimation-and-selection-matheus-a-cerqueira-et-al-2024>(90/104 | 194/329) Building Brain Tumor Segmentation Networks with User-Assisted Filter Estimation and Selection (Matheus A. Cerqueira et al., 2024)</a></li><li><a href=#91104--195329-addressing-source-scale-bias-via-image-warping-for-domain-adaptation-shen-zheng-et-al-2024>(91/104 | 195/329) Addressing Source Scale Bias via Image Warping for Domain Adaptation (Shen Zheng et al., 2024)</a></li><li><a href=#92104--196329-audio-visual-compound-expression-recognition-method-based-on-late-modality-fusion-and-rule-based-decision-elena-ryumina-et-al-2024>(92/104 | 196/329) Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision (Elena Ryumina et al., 2024)</a></li><li><a href=#93104--197329-hcpm-hierarchical-candidates-pruning-for-efficient-detector-free-matching-ying-chen-et-al-2024>(93/104 | 197/329) HCPM: Hierarchical Candidates Pruning for Efficient Detector-Free Matching (Ying Chen et al., 2024)</a></li><li><a href=#94104--198329-exact-language-guided-conceptual-reasoning-and-uncertainty-estimation-for-event-based-action-recognition-and-more-jiazhou-zhou-et-al-2024>(94/104 | 198/329) ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More (Jiazhou Zhou et al., 2024)</a></li><li><a href=#95104--199329-generalized-consistency-trajectory-models-for-image-manipulation-beomsu-kim-et-al-2024>(95/104 | 199/329) Generalized Consistency Trajectory Models for Image Manipulation (Beomsu Kim et al., 2024)</a></li><li><a href=#96104--200329-vq-nerv-a-vector-quantized-neural-representation-for-videos-yunjie-xu-et-al-2024>(96/104 | 200/329) VQ-NeRV: A Vector Quantized Neural Representation for Videos (Yunjie Xu et al., 2024)</a></li><li><a href=#97104--201329-embarrassingly-simple-scribble-supervision-for-3d-medical-segmentation-karol-gotkowski-et-al-2024>(97/104 | 201/329) Embarrassingly Simple Scribble Supervision for 3D Medical Segmentation (Karol Gotkowski et al., 2024)</a></li><li><a href=#98104--202329-hugs-holistic-urban-3d-scene-understanding-via-gaussian-splatting-hongyu-zhou-et-al-2024>(98/104 | 202/329) HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting (Hongyu Zhou et al., 2024)</a></li><li><a href=#99104--203329-vox-fusion-voxel-based-neural-implicit-dense-tracking-and-mapping-with-multi-maps-hongjia-zhai-et-al-2024>(99/104 | 203/329) Vox-Fusion++: Voxel-based Neural Implicit Dense Tracking and Mapping with Multi-maps (Hongjia Zhai et al., 2024)</a></li><li><a href=#100104--204329-geometric-constraints-in-deep-learning-frameworks-a-survey-vibhas-k-vats-et-al-2024>(100/104 | 204/329) Geometric Constraints in Deep Learning Frameworks: A Survey (Vibhas K Vats et al., 2024)</a></li><li><a href=#101104--205329-whac-world-grounded-humans-and-cameras-wanqi-yin-et-al-2024>(101/104 | 205/329) WHAC: World-grounded Humans and Cameras (Wanqi Yin et al., 2024)</a></li><li><a href=#102104--206329-futuredepth-learning-to-predict-the-future-improves-video-depth-estimation-rajeev-yasarla-et-al-2024>(102/104 | 206/329) FutureDepth: Learning to Predict the Future Improves Video Depth Estimation (Rajeev Yasarla et al., 2024)</a></li><li><a href=#103104--207329-selective-domain-invariant-feature-for-generalizable-deepfake-detection-yingxin-lai-et-al-2024>(103/104 | 207/329) Selective Domain-Invariant Feature for Generalizable Deepfake Detection (Yingxin Lai et al., 2024)</a></li><li><a href=#104104--208329-class-and-region-adaptive-constraints-for-network-calibration-balamurali-murugesan-et-al-2024>(104/104 | 208/329) Class and Region-Adaptive Constraints for Network Calibration (Balamurali Murugesan et al., 2024)</a></li></ul></li><li><a href=#physicsacc-ph-1>physics.acc-ph (1)</a><ul><li><a href=#11--209329-a-conditional-latent-autoregressive-recurrent-model-for-generation-and-forecasting-of-beam-dynamics-in-particle-accelerators-mahindra-rautela-et-al-2024>(1/1 | 209/329) A conditional latent autoregressive recurrent model for generation and forecasting of beam dynamics in particle accelerators (Mahindra Rautela et al., 2024)</a></li></ul></li><li><a href=#csro-24>cs.RO (24)</a><ul><li><a href=#124--210329-btgenbot-behavior-tree-generation-for-robotic-tasks-with-lightweight-llms-riccardo-andrea-izzo-et-al-2024>(1/24 | 210/329) BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs (Riccardo Andrea Izzo et al., 2024)</a></li><li><a href=#224--211329-towards-robots-that-know-when-they-need-help-affordance-based-uncertainty-for-large-language-model-planners-james-f-mullen-jr-et-al-2024>(2/24 | 211/329) Towards Robots That Know When They Need Help: Affordance-Based Uncertainty for Large Language Model Planners (James F. Mullen Jr. et al., 2024)</a></li><li><a href=#324--212329-to-help-or-not-to-help-llm-based-attentive-support-for-human-robot-group-interactions-daniel-tanneberg-et-al-2024>(3/24 | 212/329) To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions (Daniel Tanneberg et al., 2024)</a></li><li><a href=#424--213329-cadre-controllable-and-diverse-generation-of-safety-critical-driving-scenarios-using-real-world-trajectories-peide-huang-et-al-2024>(4/24 | 213/329) CaDRE: Controllable and Diverse Generation of Safety-Critical Driving Scenarios using Real-World Trajectories (Peide Huang et al., 2024)</a></li><li><a href=#524--214329-d-cubed-latent-diffusion-trajectory-optimisation-for-dexterous-deformable-manipulation-jun-yamada-et-al-2024>(5/24 | 214/329) D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation (Jun Yamada et al., 2024)</a></li><li><a href=#624--215329-yell-at-your-robot-improving-on-the-fly-from-language-corrections-lucy-xiaoyang-shi-et-al-2024>(6/24 | 215/329) Yell At Your Robot: Improving On-the-Fly from Language Corrections (Lucy Xiaoyang Shi et al., 2024)</a></li><li><a href=#724--216329-footstepnet-an-efficient-actor-critic-method-for-fast-on-line-bipedal-footstep-planning-and-forecasting-clément-gaspard-et-al-2024>(7/24 | 216/329) FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting (Clément Gaspard et al., 2024)</a></li><li><a href=#824--217329-the-interplay-between-symmetries-and-impact-effects-on-hybrid-mechanical-systems-william-clark-et-al-2024>(8/24 | 217/329) The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical Systems (William Clark et al., 2024)</a></li><li><a href=#924--218329-meta-learning-for-fast-adaptation-in-intent-inferral-on-a-robotic-hand-orthosis-for-stroke-pedro-leandro-la-rotta-et-al-2024>(9/24 | 218/329) Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand Orthosis for Stroke (Pedro Leandro La Rotta et al., 2024)</a></li><li><a href=#1024--219329-vid2robot-end-to-end-video-conditioned-policy-learning-with-cross-attention-transformers-vidhi-jain-et-al-2024>(10/24 | 219/329) Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers (Vidhi Jain et al., 2024)</a></li><li><a href=#1124--220329-semantic-layering-in-room-segmentation-via-llms-taehyeon-kim-et-al-2024>(11/24 | 220/329) Semantic Layering in Room Segmentation via LLMs (Taehyeon Kim et al., 2024)</a></li><li><a href=#1224--221329-pe-planner-a-performance-enhanced-quadrotor-motion-planner-for-autonomous-flight-in-complex-and-dynamic-environments-jiaxin-qiu-et-al-2024>(12/24 | 221/329) PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for Autonomous Flight in Complex and Dynamic Environments (Jiaxin Qiu et al., 2024)</a></li><li><a href=#1324--222329-shared-autonomy-via-variable-impedance-control-and-virtual-potential-fields-for-encoding-human-demonstration-shail-jadav-et-al-2024>(13/24 | 222/329) Shared Autonomy via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstration (Shail Jadav et al., 2024)</a></li><li><a href=#1424--223329-dynamic-manipulation-of-deformable-objects-using-imitation-learning-with-adaptation-to-hardware-constraints-eric-hannus-et-al-2024>(14/24 | 223/329) Dynamic Manipulation of Deformable Objects using Imitation Learning with Adaptation to Hardware Constraints (Eric Hannus et al., 2024)</a></li><li><a href=#1524--224329-ton-vio-online-time-offset-modeling-networks-for-robust-temporal-alignment-in-high-dynamic-motion-vio-chaoran-xiong-et-al-2024>(15/24 | 224/329) TON-VIO: Online Time Offset Modeling Networks for Robust Temporal Alignment in High Dynamic Motion VIO (Chaoran Xiong et al., 2024)</a></li><li><a href=#1624--225329-theoretical-modeling-and-bio-inspired-trajectory-optimization-of-a-multiple-locomotion-origami-robot-keqi-zhu-et-al-2024>(16/24 | 225/329) Theoretical Modeling and Bio-inspired Trajectory Optimization of A Multiple-locomotion Origami Robot (Keqi Zhu et al., 2024)</a></li><li><a href=#1724--226329-multi-object-ransac-efficient-plane-clustering-method-in-a-clutter-seunghyeon-lim-et-al-2024>(17/24 | 226/329) Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter (Seunghyeon Lim et al., 2024)</a></li><li><a href=#1824--227329-interactive-robot-environment-self-calibration-via-compliant-exploratory-actions-podshara-chanrungmaneekul-et-al-2024>(18/24 | 227/329) Interactive Robot-Environment Self-Calibration via Compliant Exploratory Actions (Podshara Chanrungmaneekul et al., 2024)</a></li><li><a href=#1924--228329-digital-twin-driven-reinforcement-learning-for-obstacle-avoidance-in-robot-manipulators-a-self-improving-online-training-framework-yuzhu-sun-et-al-2024>(19/24 | 228/329) Digital Twin-Driven Reinforcement Learning for Obstacle Avoidance in Robot Manipulators: A Self-Improving Online Training Framework (Yuzhu Sun et al., 2024)</a></li><li><a href=#2024--229329-adaptive-visual-imitation-learning-for-robotic-assisted-feeding-across-varied-bowl-configurations-and-food-types-rui-liu-et-al-2024>(20/24 | 229/329) Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types (Rui Liu et al., 2024)</a></li><li><a href=#2124--230329-opti-acoustic-semantic-slam-with-unknown-objects-in-underwater-environments-kurran-singh-et-al-2024>(21/24 | 230/329) Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments (Kurran Singh et al., 2024)</a></li><li><a href=#2224--231329-bin-packing-optimization-via-deep-reinforcement-learning-baoying-wang-et-al-2024>(22/24 | 231/329) Bin Packing Optimization via Deep Reinforcement Learning (Baoying Wang et al., 2024)</a></li><li><a href=#2324--232329-cooperative-modular-manipulation-with-numerous-cable-driven-robots-for-assistive-construction-and-gap-crossing-kevin-murphy-et-al-2024>(23/24 | 232/329) Cooperative Modular Manipulation with Numerous Cable-Driven Robots for Assistive Construction and Gap Crossing (Kevin Murphy et al., 2024)</a></li><li><a href=#2424--233329-on-designing-consistent-covariance-recovery-from-a-deep-learning-visual-odometry-engine-jagatpreet-singh-nir-et-al-2024>(24/24 | 233/329) On Designing Consistent Covariance Recovery from a Deep Learning Visual Odometry Engine (Jagatpreet Singh Nir et al., 2024)</a></li></ul></li><li><a href=#eessiv-8>eess.IV (8)</a><ul><li><a href=#18--234329-federated-semi-supervised-learning-for-medical-image-segmentation-with-intra-client-and-inter-client-consistency-yubin-zheng-et-al-2024>(1/8 | 234/329) Federated Semi-supervised Learning for Medical Image Segmentation with intra-client and inter-client Consistency (Yubin Zheng et al., 2024)</a></li><li><a href=#28--235329-low-trace-adaptation-of-zero-shot-self-supervised-blind-image-denoising-jintong-hu-et-al-2024>(2/8 | 235/329) Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising (Jintong Hu et al., 2024)</a></li><li><a href=#38--236329-trustworthiness-of-pretrained-transformers-for-lung-cancer-segmentation-aneesh-rangnekar-et-al-2024>(3/8 | 236/329) Trustworthiness of Pretrained Transformers for Lung Cancer Segmentation (Aneesh Rangnekar et al., 2024)</a></li><li><a href=#48--237329-sift-dbt-self-supervised-initialization-and-fine-tuning-for-imbalanced-digital-breast-tomosynthesis-image-classification-yuexi-du-et-al-2024>(4/8 | 237/329) SIFT-DBT: Self-supervised Initialization and Fine-Tuning for Imbalanced Digital Breast Tomosynthesis Image Classification (Yuexi Du et al., 2024)</a></li><li><a href=#58--238329-generative-enhancement-for-3d-medical-images-lingting-zhu-et-al-2024>(5/8 | 238/329) Generative Enhancement for 3D Medical Images (Lingting Zhu et al., 2024)</a></li><li><a href=#68--239329-fuelvision-a-multimodal-data-fusion-and-multimodel-ensemble-algorithm-for-wildfire-fuels-mapping-riyaaz-uddien-shaik-et-al-2024>(6/8 | 239/329) FUELVISION: A Multimodal Data Fusion and Multimodel Ensemble Algorithm for Wildfire Fuels Mapping (Riyaaz Uddien Shaik et al., 2024)</a></li><li><a href=#78--240329-physics-guided-neural-networks-for-intraventricular-vector-flow-mapping-hang-jung-ling-et-al-2024>(7/8 | 240/329) Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping (Hang Jung Ling et al., 2024)</a></li><li><a href=#88--241329-super-high-fidelity-image-compression-via-hierarchical-roi-and-adaptive-quantization-jixiang-luo-et-al-2024>(8/8 | 241/329) Super-High-Fidelity Image Compression via Hierarchical-ROI and Adaptive Quantization (Jixiang Luo et al., 2024)</a></li></ul></li><li><a href=#csai-7>cs.AI (7)</a><ul><li><a href=#17--242329-wolf-large-language-model-framework-for-cxr-understanding-seil-kang-et-al-2024>(1/7 | 242/329) WoLF: Large Language Model Framework for CXR Understanding (Seil Kang et al., 2024)</a></li><li><a href=#27--243329-enhancing-formal-theorem-proving-a-comprehensive-dataset-for-training-ai-models-on-coq-code-andreas-florath-2024>(2/7 | 243/329) Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code (Andreas Florath, 2024)</a></li><li><a href=#37--244329-embodied-llm-agents-learn-to-cooperate-in-organized-teams-xudong-guo-et-al-2024>(3/7 | 244/329) Embodied LLM Agents Learn to Cooperate in Organized Teams (Xudong Guo et al., 2024)</a></li><li><a href=#47--245329-insight-end-to-end-neuro-symbolic-visual-reinforcement-learning-with-language-explanations-lirui-luo-et-al-2024>(4/7 | 245/329) INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations (Lirui Luo et al., 2024)</a></li><li><a href=#57--246329-contextual-moral-value-alignment-through-context-based-aggregation-pierre-dognin-et-al-2024>(5/7 | 246/329) Contextual Moral Value Alignment Through Context-Based Aggregation (Pierre Dognin et al., 2024)</a></li><li><a href=#67--247329-on-predictive-planning-and-counterfactual-learning-in-active-inference-aswin-paul-et-al-2024>(6/7 | 247/329) On Predictive planning and counterfactual learning in active inference (Aswin Paul et al., 2024)</a></li><li><a href=#77--248329-offline-imitation-of-badminton-player-behavior-via-experiential-contexts-and-brownian-motion-kuang-da-wang-et-al-2024>(7/7 | 248/329) Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion (Kuang-Da Wang et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--249329-interpretable-user-satisfaction-estimation-for-conversational-systems-with-large-language-models-ying-chun-lin-et-al-2024>(1/5 | 249/329) Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models (Ying-Chun Lin et al., 2024)</a></li><li><a href=#25--250329-inbox-recommendation-with-knowledge-graph-using-interest-box-embedding-zezhong-xu-et-al-2024>(2/5 | 250/329) InBox: Recommendation with Knowledge Graph using Interest Box Embedding (Zezhong Xu et al., 2024)</a></li><li><a href=#35--251329-erase-benchmarking-feature-selection-methods-for-deep-recommender-systems-pengyue-jia-et-al-2024>(3/5 | 251/329) ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems (Pengyue Jia et al., 2024)</a></li><li><a href=#45--252329-context-based-fast-recommendation-strategy-for-long-user-behavior-sequence-in-meituan-waimai-zhichao-feng-et-al-2024>(4/5 | 252/329) Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai (Zhichao Feng et al., 2024)</a></li><li><a href=#55--253329-an-aligning-and-training-framework-for-multimodal-recommendations-yifan-liu-et-al-2024>(5/5 | 253/329) An Aligning and Training Framework for Multimodal Recommendations (Yifan Liu et al., 2024)</a></li></ul></li><li><a href=#cscr-8>cs.CR (8)</a><ul><li><a href=#18--254329-a-study-of-vulnerability-repair-in-javascript-programs-with-large-language-models-tan-khang-le-et-al-2024>(1/8 | 254/329) A Study of Vulnerability Repair in JavaScript Programs with Large Language Models (Tan Khang Le et al., 2024)</a></li><li><a href=#28--255329-rigorllm-resilient-guardrails-for-large-language-models-against-undesired-content-zhuowen-yuan-et-al-2024>(2/8 | 255/329) RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content (Zhuowen Yuan et al., 2024)</a></li><li><a href=#38--256329-bypassing-llm-watermarks-with-color-aware-substitutions-qilong-wu-et-al-2024>(3/8 | 256/329) Bypassing LLM Watermarks with Color-Aware Substitutions (Qilong Wu et al., 2024)</a></li><li><a href=#48--257329-securing-large-language-models-threats-vulnerabilities-and-responsible-practices-sara-abdali-et-al-2024>(4/8 | 257/329) Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices (Sara Abdali et al., 2024)</a></li><li><a href=#58--258329-enhancing-security-of-ai-based-code-synthesis-with-github-copilot-via-cheap-and-efficient-prompt-engineering-jakub-res-et-al-2024>(5/8 | 258/329) Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering (Jakub Res et al., 2024)</a></li><li><a href=#68--259329-provable-privacy-with-non-private-pre-processing-yaxi-hu-et-al-2024>(6/8 | 259/329) Provable Privacy with Non-Private Pre-Processing (Yaxi Hu et al., 2024)</a></li><li><a href=#78--260329-the-emergence-of-hardware-fuzzing-a-critical-review-of-its-significance-raghul-saravanan-et-al-2024>(7/8 | 260/329) The Emergence of Hardware Fuzzing: A Critical Review of its Significance (Raghul Saravanan et al., 2024)</a></li><li><a href=#88--261329-tags-real-time-intrusion-detection-with-tag-propagation-based-provenance-graph-alignment-on-streaming-events-zhenyuan-li-et-al-2024>(8/8 | 261/329) TAGS: Real-time Intrusion Detection with Tag-Propagation-based Provenance Graph Alignment on Streaming Events (Zhenyuan Li et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--262329-graph-neural-network-based-multi-agent-reinforcement-learning-for-resilient-distributed-coordination-of-multi-robot-systems-anthony-goeckner-et-al-2024>(1/2 | 262/329) Graph Neural Network-based Multi-agent Reinforcement Learning for Resilient Distributed Coordination of Multi-Robot Systems (Anthony Goeckner et al., 2024)</a></li><li><a href=#22--263329-uber-stable-formulating-the-rideshare-system-as-a-stable-matching-problem-rhea-acharya-et-al-2024>(2/2 | 263/329) Uber Stable: Formulating the Rideshare System as a Stable Matching Problem (Rhea Acharya et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--264329-a-new-intelligent-reflecting-surface-aided-electromagnetic-stealth-strategy-xue-xiong-et-al-2024>(1/1 | 264/329) A New Intelligent Reflecting Surface-Aided Electromagnetic Stealth Strategy (Xue Xiong et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--265329-a-physics-embedded-deep-learning-framework-for-cloth-simulation-zhiwei-zhao-2024>(1/1 | 265/329) A Physics-embedded Deep Learning Framework for Cloth Simulation (Zhiwei Zhao, 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#14--266329-knowledge-and-data-dual-driven-channel-estimation-and-feedback-for-ultra-massive-mimo-systems-under-hybrid-field-beam-squint-effect-kuiyu-wang-et-al-2024>(1/4 | 266/329) Knowledge and Data Dual-Driven Channel Estimation and Feedback for Ultra-Massive MIMO Systems under Hybrid Field Beam Squint Effect (Kuiyu Wang et al., 2024)</a></li><li><a href=#24--267329-wmmse-based-rate-maximization-for-ris-assisted-mu-mimo-systems-hyuckjin-choi-et-al-2024>(2/4 | 267/329) WMMSE-Based Rate Maximization for RIS-Assisted MU-MIMO Systems (Hyuckjin Choi et al., 2024)</a></li><li><a href=#34--268329-wideband-modeling-and-beamforming-for-beyond-diagonal-reconfigurable-intelligent-surfaces-hongyu-li-et-al-2024>(3/4 | 268/329) Wideband Modeling and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces (Hongyu Li et al., 2024)</a></li><li><a href=#44--269329-sparse-estimation-for-xl-mimo-with-unified-losnlos-representation-xu-shi-et-al-2024>(4/4 | 269/329) Sparse Estimation for XL-MIMO with Unified LoS/NLoS Representation (Xu Shi et al., 2024)</a></li></ul></li><li><a href=#csmm-2>cs.MM (2)</a><ul><li><a href=#12--270329-ice-interactive-3d-game-character-editing-via-dialogue-haoqian-wu-et-al-2024>(1/2 | 270/329) ICE: Interactive 3D Game Character Editing via Dialogue (Haoqian Wu et al., 2024)</a></li><li><a href=#22--271329-newscaption-named-entity-aware-captioning-for-out-of-context-media-anurag-singh-et-al-2024>(2/2 | 271/329) NewsCaption: Named-Entity aware Captioning for Out-of-Context Media (Anurag Singh et al., 2024)</a></li></ul></li><li><a href=#csne-3>cs.NE (3)</a><ul><li><a href=#13--272329-topological-representations-of-heterogeneous-learning-dynamics-of-recurrent-spiking-neural-networks-biswadeep-chakraborty-et-al-2024>(1/3 | 272/329) Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks (Biswadeep Chakraborty et al., 2024)</a></li><li><a href=#23--273329-evolutionary-optimization-of-model-merging-recipes-takuya-akiba-et-al-2024>(2/3 | 273/329) Evolutionary Optimization of Model Merging Recipes (Takuya Akiba et al., 2024)</a></li><li><a href=#33--274329-learning-guided-iterated-local-search-for-the-minmax-multiple-traveling-salesman-problem-pengfei-he-et-al-2024>(3/3 | 274/329) Learning-guided iterated local search for the minmax multiple traveling salesman problem (Pengfei He et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--275329-on-the-effectiveness-of-large-language-models-for-github-workflows-xinyu-zhang-et-al-2024>(1/3 | 275/329) On the effectiveness of Large Language Models for GitHub Workflows (Xinyu Zhang et al., 2024)</a></li><li><a href=#23--276329-professional-insights-into-benefits-and-limitations-of-implementing-mlops-principles-gabriel-araujo-et-al-2024>(2/3 | 276/329) Professional Insights into Benefits and Limitations of Implementing MLOps Principles (Gabriel Araujo et al., 2024)</a></li><li><a href=#33--277329-navigating-compiler-errors-with-ai-assistance----a-study-of-gpt-hints-in-an-introductory-programming-course-maciej-pankiewicz-et-al-2024>(3/3 | 277/329) Navigating Compiler Errors with AI Assistance &ndash; A Study of GPT Hints in an Introductory Programming Course (Maciej Pankiewicz et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--278329-modal-analysis-of-spatiotemporal-data-via-multivariate-gaussian-process-regression-jiwoo-song-et-al-2024>(1/1 | 278/329) Modal Analysis of Spatiotemporal Data via Multivariate Gaussian Process Regression (Jiwoo Song et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--279329-generating-automatic-feedback-on-ui-mockups-with-large-language-models-peitong-duan-et-al-2024>(1/4 | 279/329) Generating Automatic Feedback on UI Mockups with Large Language Models (Peitong Duan et al., 2024)</a></li><li><a href=#24--280329-rapid-aideation-generating-ideas-with-the-self-and-in-collaboration-with-large-language-models-gionnieve-lim-et-al-2024>(2/4 | 280/329) Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models (Gionnieve Lim et al., 2024)</a></li><li><a href=#34--281329-fact-checking-chatbot-a-misinformation-intervention-for-instant-messaging-apps-and-an-analysis-of-trust-in-the-fact-checkers-gionnieve-lim-et-al-2024>(3/4 | 281/329) Fact Checking Chatbot: A Misinformation Intervention for Instant Messaging Apps and an Analysis of Trust in the Fact Checkers (Gionnieve Lim et al., 2024)</a></li><li><a href=#44--282329-effects-of-automated-misinformation-warning-labels-on-the-intents-to-like-comment-and-share-posts-gionnieve-lim-et-al-2024>(4/4 | 282/329) Effects of Automated Misinformation Warning Labels on the Intents to Like, Comment and Share Posts (Gionnieve Lim et al., 2024)</a></li></ul></li><li><a href=#csdc-3>cs.DC (3)</a><ul><li><a href=#13--283329-toward-sustainable-genai-using-generation-directives-for-carbon-friendly-large-language-model-inference-baolin-li-et-al-2024>(1/3 | 283/329) Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference (Baolin Li et al., 2024)</a></li><li><a href=#23--284329-performance-portable-monte-carlo-particle-transport-on-intel-nvidia-and-amd-gpus-john-tramm-et-al-2024>(2/3 | 284/329) Performance Portable Monte Carlo Particle Transport on Intel, NVIDIA, and AMD GPUs (John Tramm et al., 2024)</a></li><li><a href=#33--285329-parallel-gaussian-process-with-kernel-approximation-in-cuda-davide-carminati-2024>(3/3 | 285/329) Parallel Gaussian process with kernel approximation in CUDA (Davide Carminati, 2024)</a></li></ul></li><li><a href=#mathna-8>math.NA (8)</a><ul><li><a href=#18--286329-adaptive-multilevel-neural-networks-for-parametric-pdes-with-error-estimation-janina-e-schütte-et-al-2024>(1/8 | 286/329) Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation (Janina E. Schütte et al., 2024)</a></li><li><a href=#28--287329-a-note-on-incomplete-cholesky-factorizations-in-half-precision-arithmetic-jennifer-scott-et-al-2024>(2/8 | 287/329) A note on incomplete Cholesky factorizations in half precision arithmetic (Jennifer Scott et al., 2024)</a></li><li><a href=#38--288329-local-reconstruction-analysis-of-inverting-the-radon-transform-in-the-plane-from-noisy-discrete-data-anuj-abhishek-et-al-2024>(3/8 | 288/329) Local reconstruction analysis of inverting the Radon transform in the plane from noisy discrete data (Anuj Abhishek et al., 2024)</a></li><li><a href=#48--289329-a-second-order-iterative-time-integration-scheme-for-linear-poroelasticity-r-altmann-et-al-2024>(4/8 | 289/329) A second-order iterative time integration scheme for linear poroelasticity (R. Altmann et al., 2024)</a></li><li><a href=#58--290329-numerical-approximation-of-a-class-of-constrained-hamilton-jacobi-equations-benoît-gaudeul-et-al-2024>(5/8 | 290/329) Numerical approximation of a class of constrained Hamilton-Jacobi equations (Benoît Gaudeul et al., 2024)</a></li><li><a href=#68--291329-to-blow-up-or-not-to-blow-up-for-a-granular-kinetic-equation-josé-a-carrillo-et-al-2024>(6/8 | 291/329) To blow-up or not to blow-up for a granular kinetic equation (José A. Carrillo et al., 2024)</a></li><li><a href=#78--292329-rate-optimal-higher-order-adaptive-conforming-fem-for-biharmonic-eigenvalue-problems-on-polygonal-domains-carsten-carstensen-et-al-2024>(7/8 | 292/329) Rate-optimal higher-order adaptive conforming FEM for biharmonic eigenvalue problems on polygonal domains (Carsten Carstensen et al., 2024)</a></li><li><a href=#88--293329-stochastic-variance-reduced-gradient-method-for-linear-ill-posed-inverse-problems-qinian-jin-et-al-2024>(8/8 | 293/329) Stochastic variance reduced gradient method for linear ill-posed inverse problems (Qinian Jin et al., 2024)</a></li></ul></li><li><a href=#csdb-4>cs.DB (4)</a><ul><li><a href=#14--294329-efficient-k-step-weighted-reachability-query-processing-algorithms-lian-chen-et-al-2024>(1/4 | 294/329) Efficient k-step Weighted Reachability Query Processing Algorithms (Lian Chen et al., 2024)</a></li><li><a href=#24--295329-quantixar-high-performance-vector-data-management-system-gulshan-yadav-et-al-2024>(2/4 | 295/329) Quantixar: High-performance Vector Data Management System (Gulshan Yadav et al., 2024)</a></li><li><a href=#34--296329-evaluating-datalog-over-semirings-a-grounding-based-approach-hangdong-zhao-et-al-2024>(3/4 | 296/329) Evaluating Datalog over Semirings: A Grounding-based Approach (Hangdong Zhao et al., 2024)</a></li><li><a href=#44--297329-a-benchmark-for-data-management-challenges-in-microservices-rodrigo-laigner-et-al-2024>(4/4 | 297/329) A Benchmark for Data Management Challenges in Microservices (Rodrigo Laigner et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--298329-revisiting-local-computation-of-pagerank-simple-and-optimal-hanzhi-wang-et-al-2024>(1/2 | 298/329) Revisiting Local Computation of PageRank: Simple and Optimal (Hanzhi Wang et al., 2024)</a></li><li><a href=#22--299329-exact-and-heuristic-computation-of-the-scanwidth-of-directed-acyclic-graphs-niels-holtgrefe-et-al-2024>(2/2 | 299/329) Exact and Heuristic Computation of the Scanwidth of Directed Acyclic Graphs (Niels Holtgrefe et al., 2024)</a></li></ul></li><li><a href=#cssi-3>cs.SI (3)</a><ul><li><a href=#13--300329-community-detection-by-spectral-methods-in-multi-layer-networks-huan-qing-2024>(1/3 | 300/329) Community detection by spectral methods in multi-layer networks (Huan Qing, 2024)</a></li><li><a href=#23--301329-tiktok-and-the-art-of-personalization-investigating-exploration-and-exploitation-on-social-media-feeds-karan-vombatkere-et-al-2024>(2/3 | 301/329) TikTok and the Art of Personalization: Investigating Exploration and Exploitation on Social Media Feeds (Karan Vombatkere et al., 2024)</a></li><li><a href=#33--302329-detection-of-malicious-agents-in-social-learning-valentina-shumovskaia-et-al-2024>(3/3 | 302/329) Detection of Malicious Agents in Social Learning (Valentina Shumovskaia et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#11--303329-deep-few-view-high-resolution-photon-counting-extremity-ct-at-halved-dose-for-a-clinical-trial-mengzhou-li-et-al-2024>(1/1 | 303/329) Deep Few-view High-resolution Photon-counting Extremity CT at Halved Dose for a Clinical Trial (Mengzhou Li et al., 2024)</a></li></ul></li><li><a href=#cset-1>cs.ET (1)</a><ul><li><a href=#11--304329-pruning-for-improved-adc-efficiency-in-crossbar-based-analog-in-memory-accelerators-timur-ibrayev-et-al-2024>(1/1 | 304/329) Pruning for Improved ADC Efficiency in Crossbar-based Analog In-memory Accelerators (Timur Ibrayev et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--305329-a-canary-in-the-ai-coal-mine-american-jews-may-be-disproportionately-harmed-by-intellectual-property-dispossession-in-large-language-model-training-heila-precel-et-al-2024>(1/3 | 305/329) A Canary in the AI Coal Mine: American Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training (Heila Precel et al., 2024)</a></li><li><a href=#23--306329-the-journey-to-trustworthy-ai--part-1-pursuit-of-pragmatic-frameworks-mohamad-m-nasr-azadani-et-al-2024>(2/3 | 306/329) The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks (Mohamad M Nasr-Azadani et al., 2024)</a></li><li><a href=#33--307329-is-open-source-software-culture-enough-to-make-ai-a-common--robin-quillivic-et-al-2024>(3/3 | 307/329) Is open source software culture enough to make AI a common ? (Robin Quillivic et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--308329-developing-algorithms-for-the-internet-of-flying-things-through-environments-with-varying-degrees-of-realism----extended-version-thiago-de-souza-lamenza-et-al-2024>(1/3 | 308/329) Developing Algorithms for the Internet of Flying Things Through Environments With Varying Degrees of Realism &ndash; Extended Version (Thiago de Souza Lamenza et al., 2024)</a></li><li><a href=#23--309329-hierarchical-digital-twin-for-efficient-6g-network-orchestration-via-adaptive-attribute-selection-and-scalable-network-modeling-pengyi-jia-et-al-2024>(2/3 | 309/329) Hierarchical Digital Twin for Efficient 6G Network Orchestration via Adaptive Attribute Selection and Scalable Network Modeling (Pengyi Jia et al., 2024)</a></li><li><a href=#33--310329-a-novel-energy-efficient-cross-layer-design-for-scheduling-and-routing-in-6tisch-networks-ahlam-hannachi-et-al-2024>(3/3 | 310/329) A Novel Energy-Efficient Cross-Layer Design for Scheduling and Routing in 6TiSCH Networks (Ahlam Hannachi et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--311329-bayesian-estimation-and-uncertainty-quantification-of-a-temperature-dependent-thermal-conductivity-rodrigo-l-s-silva-et-al-2024>(1/1 | 311/329) Bayesian estimation and uncertainty quantification of a temperature-dependent thermal conductivity (Rodrigo L. S. Silva et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--312329-zeolite-adsorption-property-prediction-using-deep-learning-marko-petković-et-al-2024>(1/1 | 312/329) Zeolite Adsorption Property Prediction using Deep Learning (Marko Petković et al., 2024)</a></li></ul></li><li><a href=#eesssy-3>eess.SY (3)</a><ul><li><a href=#13--313329-state-estimation-using-single-body-frame-bearing-measurements-sifeddine-benahmed-et-al-2024>(1/3 | 313/329) State Estimation Using Single Body-Frame Bearing Measurements (Sifeddine Benahmed et al., 2024)</a></li><li><a href=#23--314329-robust-fuel-optimal-landing-guidance-for-hazardous-terrain-using-multiple-sliding-surfaces-sheikh-zeeshan-basar-et-al-2024>(2/3 | 314/329) Robust Fuel-Optimal Landing Guidance for Hazardous Terrain using Multiple Sliding Surfaces (Sheikh Zeeshan Basar et al., 2024)</a></li><li><a href=#33--315329-ensuring-solution-uniqueness-in-fixed-point-based-harmonic-power-flow-analysis-with-converter-interfaced-resources-ex-post-conditions-antonio-di-pasquale-et-al-2024>(3/3 | 315/329) Ensuring Solution Uniqueness in Fixed-Point-Based Harmonic Power Flow Analysis with Converter-Interfaced Resources: Ex-post Conditions (Antonio Di Pasquale et al., 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--316329-reproducing-the-acoustic-velocity-vectors-in-a-circular-listening-area-jiarui-wang-et-al-2024>(1/1 | 316/329) Reproducing the Acoustic Velocity Vectors in a Circular Listening Area (Jiarui Wang et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--317329-semisupervised-score-based-matching-algorithm-to-evaluate-the-effect-of-public-health-interventions-hongzhe-zhang-et-al-2024>(1/3 | 317/329) Semisupervised score based matching algorithm to evaluate the effect of public health interventions (Hongzhe Zhang et al., 2024)</a></li><li><a href=#23--318329-fast-value-tracking-for-deep-reinforcement-learning-frank-shih-et-al-2024>(2/3 | 318/329) Fast Value Tracking for Deep Reinforcement Learning (Frank Shih et al., 2024)</a></li><li><a href=#33--319329-tighter-confidence-bounds-for-sequential-kernel-regression-hamish-flynn-et-al-2024>(3/3 | 319/329) Tighter Confidence Bounds for Sequential Kernel Regression (Hamish Flynn et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--320329-resolving-sets-in-temporal-graphs-jan-bok-et-al-2024>(1/1 | 320/329) Resolving Sets in Temporal Graphs (Jan Bok et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--321329-listenable-maps-for-audio-classifiers-francesco-paissan-et-al-2024>(1/2 | 321/329) Listenable Maps for Audio Classifiers (Francesco Paissan et al., 2024)</a></li><li><a href=#22--322329-real-time-speech-extraction-using-spatially-regularized-independent-low-rank-matrix-analysis-and-rank-constrained-spatial-covariance-matrix-estimation-yuto-ishikawa-et-al-2024>(2/2 | 322/329) Real-time Speech Extraction Using Spatially Regularized Independent Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix Estimation (Yuto Ishikawa et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--323329-a-new-framework-for-constrained-optimization-via-feedback-control-of-lagrange-multipliers-v-cerone-et-al-2024>(1/2 | 323/329) A new framework for constrained optimization via feedback control of Lagrange multipliers (V. Cerone et al., 2024)</a></li><li><a href=#22--324329-stochastic-halpern-iteration-in-normed-spaces-and-applications-to-reinforcement-learning-mario-bravo-et-al-2024>(2/2 | 324/329) Stochastic Halpern iteration in normed spaces and applications to reinforcement learning (Mario Bravo et al., 2024)</a></li></ul></li><li><a href=#mathat-1>math.AT (1)</a><ul><li><a href=#11--325329-some-geometric-and-topological-data-driven-methods-in-robot-motion-path-planning-boris-goldfarb-2024>(1/1 | 325/329) Some geometric and topological data-driven methods in robot motion path planning (Boris Goldfarb, 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--326329-plane-hamiltonian-cycles-in-convex-drawings-helena-bergold-et-al-2024>(1/1 | 326/329) Plane Hamiltonian Cycles in Convex Drawings (Helena Bergold et al., 2024)</a></li></ul></li><li><a href=#csdm-2>cs.DM (2)</a><ul><li><a href=#12--327329-the-ultrametric-backbone-is-the-union-of-all-minimum-spanning-forests-jordan-c-rozum-et-al-2024>(1/2 | 327/329) The ultrametric backbone is the union of all minimum spanning forests (Jordan C Rozum et al., 2024)</a></li><li><a href=#22--328329-an-upper-bound-on-the-weisfeiler-leman-dimension-thomas-schneider-et-al-2024>(2/2 | 328/329) An Upper Bound on the Weisfeiler-Leman Dimension (Thomas Schneider et al., 2024)</a></li></ul></li><li><a href=#cond-matstat-mech-1>cond-mat.stat-mech (1)</a><ul><li><a href=#11--329329-the-sis-process-on-erdös-rényi-graphs-determining-the-infected-fraction-o-s-awolude-et-al-2024>(1/1 | 329/329) The SIS process on Erdös-Rényi graphs: determining the infected fraction (O. S. Awolude et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>