<!doctype html><html><head><title>arXiv @ 2024.03.29</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.29"><meta property="og:description" content="Primary Categories astro-ph.SR (1) cs.AI (14) cs.AR (2) cs.CE (2) cs.CL (42) cs.CR (11) cs.CV (82) cs.CY (1) cs.DC (5) cs.DS (5) cs.GR (1) cs.GT (1) cs.HC (3) cs.IR (18) cs.IT (4) cs.LG (48) cs.LO (2) cs.NE (1) cs.NI (1) cs.PL (1) cs.RO (20) cs.SD (2) cs.SE (6) cs.SI (2) cs.SY (1) eess.AS (3) eess.IV (9) eess.SY (10) hep-ph (1) math.CO (1) math.NA (7) math.OC (1) math.ST (1) q-bio.QM (2) quant-ph (1) stat."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240329000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-29T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-29T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.29"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240329000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Mar 29, 2024</p></div><div class=title><h1>arXiv @ 2024.03.29</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#astro-phsr-1>astro-ph.SR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csai-14>cs.AI (14)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cscl-42>cs.CL (42)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cscr-11>cs.CR (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cscv-82>cs.CV (82)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csdc-5>cs.DC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csds-5>cs.DS (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csir-18>cs.IR (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cslg-48>cs.LG (48)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cslo-2>cs.LO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csro-20>cs.RO (20)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#csse-6>cs.SE (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#cssy-1>cs.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#eessiv-9>eess.IV (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#eesssy-10>eess.SY (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#hep-ph-1>hep-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#mathna-7>math.NA (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#mathst-1>math.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#q-bioqm-2>q-bio.QM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/#statml-6>stat.ML (6)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CR</th><th>cs.CV</th><th>cs.IR</th><th>cs.LG</th><th>cs.RO</th><th>eess.SY</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>1</td><td>1</td><td>3</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td>1</td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td></td><td>2</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Automatic Evaluation</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>3</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>BERT</td><td>2</td><td>4</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>BLOOM</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>2</td><td>15</td><td>3</td><td>28</td><td>6</td><td>6</td><td>1</td><td>1</td></tr><tr><td>Black Box</td><td>1</td><td>1</td><td>1</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>ChatGPT</td><td></td><td>2</td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Claude</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td></td><td></td><td></td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td></td><td>1</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td></td><td>2</td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>1</td><td>7</td><td></td><td>6</td><td></td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td></td><td>12</td><td></td><td>10</td><td></td><td></td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>2</td><td></td><td>1</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Dense Retrieval</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Dialogue System</td><td></td><td>2</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>1</td><td>17</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Distribution Shift</td><td></td><td>2</td><td></td><td>4</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Document Ranking</td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>2</td><td></td><td>3</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td></td><td></td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td></td><td></td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Fake News Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Few-shot</td><td>1</td><td>3</td><td></td><td>8</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>3</td><td>12</td><td></td><td>11</td><td>1</td><td>6</td><td>1</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td></td><td>3</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>GLUE</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT</td><td></td><td>6</td><td></td><td>2</td><td></td><td></td><td>2</td><td></td></tr><tr><td>GPT-3</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GPT-4</td><td></td><td>5</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td></td><td>3</td><td></td><td>3</td><td></td><td>1</td></tr><tr><td>Geometry</td><td></td><td></td><td></td><td>3</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph</td><td></td><td>1</td><td>1</td><td></td><td>5</td><td>5</td><td></td><td>2</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td></td><td></td><td>2</td><td>4</td><td></td><td></td></tr><tr><td>Grounding</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Heuristic Approach</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Hierarchical Clustering</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>High-Resource</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td></td><td>2</td><td></td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Information Compression</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>2</td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Intent Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>4</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td></td><td></td><td></td><td>8</td><td></td><td></td></tr><tr><td>Label Smoothing</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Language Generation</td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>13</td><td>36</td><td>2</td><td>9</td><td>5</td><td>3</td><td>6</td><td></td></tr><tr><td>Lemmatization</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td></td><td>3</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Metaphor Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Model Extraction</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>1</td><td>4</td><td></td><td>15</td><td></td><td>3</td><td>3</td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Next Sentence Prediction</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td></td><td>9</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Opinion Summarization</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td></td><td>4</td><td></td><td>1</td><td></td><td></td></tr><tr><td>PaLM</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Parameter Sharing</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td>5</td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>2</td><td>6</td><td>1</td><td>7</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Prompt Learning</td><td></td><td>1</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Question Answering</td><td>2</td><td>5</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>3</td><td>6</td><td></td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td></td><td></td><td>13</td><td></td><td></td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td></td><td></td><td>7</td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>2</td><td></td><td>4</td><td></td><td>6</td><td>5</td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td></td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Rerank</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Retrieval Augmentation</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td>3</td><td>3</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td></td><td>3</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Rouge-L</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Scaling Law</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>1</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td></td><td>7</td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Semi-Supervised Training</td><td></td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentence Embedding</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td>1</td><td>1</td><td>2</td><td></td><td>4</td><td>6</td><td>2</td></tr><tr><td>Simulator</td><td>1</td><td>1</td><td>1</td><td>2</td><td></td><td>4</td><td>6</td><td>2</td></tr><tr><td>Stemming</td><td>1</td><td></td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Style Transfer</td><td></td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td></td><td>1</td><td>1</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td>4</td><td></td><td>5</td><td>1</td><td>4</td><td>2</td><td></td></tr><tr><td>T5</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>1</td><td>1</td><td>9</td><td></td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Transformer</td><td></td><td>4</td><td>2</td><td>10</td><td>1</td><td>6</td><td></td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td></td><td>3</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>2</td><td>10</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td></td><td>8</td><td></td><td></td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Word Embedding</td><td></td><td>3</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td></td><td>1</td><td></td><td>7</td><td>2</td><td>1</td><td>2</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-42>cs.CL (42)</h2><h3 id=142--1318-evaluating-large-language-models-for-health-related-text-classification-tasks-with-public-social-media-data-yuting-guo-et-al-2024>(1/42 | 1/318) Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data (Yuting Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuting Guo, Anthony Ovadje, Mohammed Ali Al-Garadi, Abeed Sarker. (2024)<br><strong>Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data</strong><br><button class=copy-to-clipboard title="Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 143<br>Keywords: Benchmarking, Data Augmentation, Few-shot, Supervised Learning, Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, RoBERTa, Text Classification, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19031v1.pdf filename=2403.19031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable success in NLP tasks. However, there is a paucity of studies that attempt to evaluate their performances on social media-based health-related natural language processing tasks, which have traditionally been difficult to achieve high scores in. We <b>benchmarked</b> one <b>supervised</b> classic machine learning model based on Support Vector Machines (SVMs), three <b>supervised</b> <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> based on <b>RoBERTa,</b> BERTweet, and SocBERT, and two <b>LLM</b> based classifiers <b>(GPT3.5</b> and <b>GPT4),</b> across 6 <b>text</b> <b>classification</b> tasks. We developed three approaches for leveraging <b>LLMs</b> for <b>text</b> <b>classification:</b> employing <b>LLMs</b> as <b>zero-shot</b> classifiers, us-ing <b>LLMs</b> as annotators to annotate training <b>data</b> <b>for</b> <b>supervised</b> classifiers, and utilizing <b>LLMs</b> with <b>few-shot</b> examples for augmentation of manually annotated <b>data.</b> <b>Our</b> comprehensive experiments demonstrate that employ-ing <b>data</b> <b>augmentation</b> using <b>LLMs</b> <b>(GPT-4)</b> with relatively small human-annotated <b>data</b> <b>to</b> train lightweight <b>supervised</b> classification models achieves superior results compared to training with human-annotated <b>data</b> <b>alone.</b> <b>Supervised</b> learners also outperform <b>GPT-4</b> and <b>GPT-3.5</b> in <b>zero-shot</b> settings. By leveraging this <b>data</b> <b>augmentation</b> strategy, we can harness the power of <b>LLMs</b> to develop smaller, more effective domain-specific NLP models. <b>LLM-annotated</b> <b>data</b> <b>without</b> human guidance for training light-weight <b>supervised</b> classification models is an ineffective strategy. However, <b>LLM,</b> as a <b>zero-shot</b> classifier, shows promise in excluding false negatives and potentially reducing the human effort required for <b>data</b> <b>annotation.</b> Future investigations are imperative to explore optimal training <b>data</b> <b>sizes</b> and the optimal amounts of augmented data.</p></p class="citation"></blockquote><h3 id=242--2318-reshaping-free-text-radiology-notes-into-structured-reports-with-generative-transformers-laura-bergomi-et-al-2024>(2/42 | 2/318) Reshaping Free-Text Radiology Notes Into Structured Reports With Generative Transformers (Laura Bergomi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laura Bergomi, Tommaso M. Buonocore, Paolo Antonazzo, Lorenzo Alberghi, Riccardo Bellazzi, Lorenzo Preda, Chandra Bortolotto, Enea Parimbelli. (2024)<br><strong>Reshaping Free-Text Radiology Notes Into Structured Reports With Generative Transformers</strong><br><button class=copy-to-clipboard title="Reshaping Free-Text Radiology Notes Into Structured Reports With Generative Transformers" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; J-3, cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Fine-tuning, GPT, GPT-3, GPT-3.5, T5, Transformer, Information Retrieval, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18938v1.pdf filename=2403.18938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>BACKGROUND: Radiology reports are typically written in a free-text format, making clinical <b>information</b> <b>difficult</b> to extract and use. Recently the adoption of structured reporting (SR) has been recommended by various medical societies thanks to the advantages it offers, e.g. standardization, completeness and <b>information</b> <b>retrieval.</b> We propose a pipeline to extract <b>information</b> <b>from</b> free-text radiology reports, that fits with the items of the reference SR registry proposed by a national society of interventional and medical radiology, focusing on CT staging of patients with lymphoma. METHODS: Our work aims to leverage the potential of Natural Language Processing (NLP) and <b>Transformer-based</b> models to deal with automatic SR registry filling. With the availability of 174 radiology reports, we investigate a rule-free generative <b>Question</b> <b>Answering</b> approach based on a domain-specific version of <b>T5</b> (IT5). Two strategies (batch-truncation and ex-post combination) are implemented to comply with the model&rsquo;s context length limitations. Performance is evaluated in terms of strict accuracy, F1, and format accuracy, and compared with the widely used <b>GPT-3.5</b> <b>Large</b> <b>Language</b> <b>Model.</b> A 5-point Likert scale questionnaire is used to collect human-expert feedback on the similarity between medical annotations and generated answers. RESULTS: The combination of <b>fine-tuning</b> and batch splitting allows IT5 to achieve notable results; it performs on par with <b>GPT-3.5</b> albeit its size being a thousand times smaller in terms of parameters. Human-based assessment scores show a high correlation (Spearman&rsquo;s correlation coefficients>0.88, p-values&lt;0.001) with AI performance metrics (F1) and confirm the superior ability of <b>LLMs</b> (i.e., <b>GPT-3.5,</b> 175B of parameters) in generating plausible human-like statements.</p></p class="citation"></blockquote><h3 id=342--3318-long-form-factuality-in-large-language-models-jerry-wei-et-al-2024>(3/42 | 3/318) Long-form factuality in large language models (Jerry Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le. (2024)<br><strong>Long-form factuality in large language models</strong><br><button class=copy-to-clipboard title="Long-form factuality in large language models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Claude, GPT, GPT-4, Gemini, PaLM, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18802v1.pdf filename=2403.18802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> often generate content that contains factual errors when responding to fact-seeking <b>prompts</b> on open-ended topics. To <b>benchmark</b> a model&rsquo;s long-form factuality in open domains, we first use <b>GPT-4</b> to generate LongFact, a <b>prompt</b> set comprising thousands of questions spanning 38 topics. We then propose that <b>LLM</b> agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an <b>LLM</b> to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step <b>reasoning</b> process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user&rsquo;s preferred response length (recall). Empirically, we demonstrate that <b>LLM</b> agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also <b>benchmark</b> thirteen language models on LongFact across four model families <b>(Gemini,</b> <b>GPT,</b> <b>Claude,</b> and <b>PaLM-2),</b> finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at <a href=https://github.com/google-deepmind/long-form-factuality>https://github.com/google-deepmind/long-form-factuality</a>.</p></p class="citation"></blockquote><h3 id=442--4318-is-modularity-transferable-a-case-study-through-the-lens-of-knowledge-distillation-mateusz-klimaszewski-et-al-2024>(4/42 | 4/318) Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation (Mateusz Klimaszewski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mateusz Klimaszewski, Piotr Andruszkiewicz, Alexandra Birch. (2024)<br><strong>Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Knowledge Distillation, Knowledge Distillation, Model Compression, Named Entity Recognition, Natural Language Inference, Domain Adaptation, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18804v1.pdf filename=2403.18804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of Modular Deep Learning showcases its potential in various <b>Natural</b> <b>Language</b> <b>Processing</b> applications. Parameter-efficient <b>fine-tuning</b> (PEFT) modularity has been shown to work for various use cases, from <b>domain</b> <b>adaptation</b> to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single <b>Pre-trained</b> <b>Language</b> <b>Model</b> <b>(PLM).</b> This <b>model-specific</b> <b>setup</b> is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between <b>models</b> <b>and</b> whether we can transfer the modules from more robust and larger <b>PLMs</b> to smaller ones. In this work, we aim to fill this gap via a lens of <b>Knowledge</b> <b>Distillation,</b> commonly used for <b>model</b> <b>compression,</b> and present an extremely straightforward approach to transferring <b>pre-trained,</b> <b>task-specific</b> <b>PEFT</b> modules between same-family <b>PLMs.</b> Moreover, we propose a method that allows the transfer of modules between incompatible <b>PLMs</b> without any change in the inference complexity. The experiments on <b>Named</b> <b>Entity</b> <b>Recognition,</b> <b>Natural</b> <b>Language</b> <b>Inference,</b> and Paraphrase Identification tasks over multiple languages and PEFT methods showcase the initial potential of transferable modularity.</p></p class="citation"></blockquote><h3 id=542--5318-blade-enhancing-black-box-large-language-models-with-small-domain-specific-models-haitao-li-et-al-2024>(5/42 | 5/318) BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models (Haitao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, Qi Tian. (2024)<br><strong>BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models</strong><br><button class=copy-to-clipboard title="BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 88<br>Keywords: Benchmarking, Black Box, Fine-tuning, ChatGPT, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18365v1.pdf filename=2403.18365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT</b> and <b>GPT-4</b> are versatile and capable of addressing a diverse range of tasks. However, general <b>LLMs,</b> which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc. To address this issue, previous approaches either conduct continuous pre-training with domain-specific data or employ <b>retrieval</b> <b>augmentation</b> to support general <b>LLMs.</b> Unfortunately, these strategies are either cost-intensive or unreliable in practical applications. To this end, we present a novel framework named BLADE, which enhances <b>Black-box</b> <b>LArge</b> <b>language</b> <b>models</b> with small Domain-spEcific models. BLADE consists of a <b>black-box</b> <b>LLM</b> and a small domain-specific LM. The small LM preserves domain-specific knowledge and offers specialized insights, while the general <b>LLM</b> contributes robust language comprehension and <b>reasoning</b> capabilities. Specifically, our method involves three steps: 1) pre-training the small LM with domain-specific data, 2) <b>fine-tuning</b> this model using knowledge instruction data, and 3) joint Bayesian optimization of the general <b>LLM</b> and the small LM. Extensive experiments conducted on public legal and medical <b>benchmarks</b> reveal that BLADE significantly outperforms existing approaches. This shows the potential of BLADE as an effective and cost-efficient solution in adapting general <b>LLMs</b> for vertical domains.</p></p class="citation"></blockquote><h3 id=642--6318-quantifying-and-mitigating-unimodal-biases-in-multimodal-large-language-models-a-causal-perspective-meiqi-chen-et-al-2024>(6/42 | 6/318) Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective (Meiqi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu. (2024)<br><strong>Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective</strong><br><button class=copy-to-clipboard title="Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 79<br>Keywords: Graph, Fine-tuning, Multi-modal, Multi-modal, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18346v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18346v2.pdf filename=2403.18346v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have facilitated the development of <b>Multimodal</b> <b>LLMs</b> (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex <b>multimodal</b> tasks. To investigate this issue, we propose a causal framework to interpret the biases in <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> problems. Within our framework, we devise a causal <b>graph</b> to elucidate the predictions of MLLMs on <b>VQA</b> problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal <b>graph,</b> we introduce a novel MORE dataset, consisting of 12,000 <b>VQA</b> instances. This dataset is designed to challenge MLLMs&rsquo; abilities, necessitating multi-hop <b>reasoning</b> and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs&rsquo; <b>reasoning</b> capabilities, including a Decompose-Verify-Answer (DeVA) framework for limited-access MLLMs and the refinement of open-source MLLMs through <b>fine-tuning.</b> Extensive quantitative and qualitative experiments offer valuable insights for future research. Our project page is at <a href=https://opencausalab.github.io/MORE>https://opencausalab.github.io/MORE</a>.</p></p class="citation"></blockquote><h3 id=742--7318-cause-counterfactual-assessment-of-user-satisfaction-estimation-in-task-oriented-dialogue-systems-amin-abolghasemi-et-al-2024>(7/42 | 7/318) CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems (Amin Abolghasemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Abolghasemi, Zhaochun Ren, Arian Askari, Mohammad Aliannejadi, Maarten de Rijke, Suzan Verberne. (2024)<br><strong>CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems</strong><br><button class=copy-to-clipboard title="CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Counter-factual, Data Augmentation, Few-shot, Fine-tuning, Dialogue System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19056v1.pdf filename=2403.19056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An important unexplored aspect in previous work on user satisfaction estimation for Task-Oriented <b>Dialogue</b> <b>(TOD)</b> systems is their evaluation in terms of robustness for the identification of user dissatisfaction: current <b>benchmarks</b> for user satisfaction estimation in TOD systems are highly skewed towards <b>dialogues</b> <b>for</b> which the user is satisfied. The effect of having a more balanced set of satisfaction labels on performance is unknown. However, balancing the <b>data</b> <b>with</b> more dissatisfactory <b>dialogue</b> <b>samples</b> requires further <b>data</b> <b>collection</b> and human annotation, which is costly and time-consuming. In this work, we leverage <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and unlock their ability to generate satisfaction-aware <b>counterfactual</b> <b>dialogues</b> <b>to</b> augment the set of original <b>dialogues</b> <b>of</b> a test collection. We gather human annotations to ensure the reliability of the generated samples. We evaluate two open-source <b>LLMs</b> as user satisfaction estimators on our augmented collection against state-of-the-art <b>fine-tuned</b> models. Our experiments show that when used as <b>few-shot</b> user satisfaction estimators, open-source <b>LLMs</b> show higher robustness to the increase in the number of dissatisfaction labels in the test collection than the <b>fine-tuned</b> state-of-the-art models. Our results shed light on the need for <b>data</b> <b>augmentation</b> approaches for user satisfaction estimation in TOD systems. We release our aligned <b>counterfactual</b> <b>dialogues,</b> <b>which</b> are curated by human annotation, to facilitate further research on this topic.</p></p class="citation"></blockquote><h3 id=842--8318-dual-instruction-tuning-with-large-language-models-for-mathematical-reasoning-yongwei-zhou-et-al-2024>(8/42 | 8/318) Dual Instruction Tuning with Large Language Models for Mathematical Reasoning (Yongwei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongwei Zhou, Tiejun Zhao. (2024)<br><strong>Dual Instruction Tuning with Large Language Models for Mathematical Reasoning</strong><br><button class=copy-to-clipboard title="Dual Instruction Tuning with Large Language Models for Mathematical Reasoning" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Mathematical Reasoning, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18295v1.pdf filename=2403.18295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements highlight the success of <b>instruction</b> <b>tuning</b> with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> utilizing Chain-of-Thought (CoT) data for <b>mathematical</b> <b>reasoning</b> tasks. Despite the <b>fine-tuned</b> <b>LLMs,</b> challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions. To alleviate this problem, we propose a dual <b>instruction</b> <b>tuning</b> strategy to meticulously model <b>mathematical</b> <b>reasoning</b> from both forward and reverse directions. This involves introducing the Intermediate <b>Reasoning</b> State Prediction task (forward <b>reasoning)</b> and the <b>Instruction</b> <b>Reconstruction</b> task (reverse <b>reasoning)</b> to enhance the <b>LLMs&rsquo;</b> understanding and execution of <b>instructions.</b> <b>Training</b> instances for these tasks are constructed based on existing <b>mathematical</b> <b>instruction</b> <b>tuning</b> datasets. Subsequently, <b>LLMs</b> undergo multi-task <b>fine-tuning</b> using both existing <b>mathematical</b> <b>instructions</b> <b>and</b> the newly created data. Comprehensive experiments validate the effectiveness and domain generalization of the dual <b>instruction</b> <b>tuning</b> strategy across various <b>mathematical</b> <b>reasoning</b> tasks.</p></p class="citation"></blockquote><h3 id=942--9318-md-pk-metaphor-detection-via-prompt-learning-and-knowledge-distillation-kaidi-jia-et-al-2024>(9/42 | 9/318) MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation (Kaidi Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaidi Jia, Rongsheng Li. (2024)<br><strong>MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation</strong><br><button class=copy-to-clipboard title="MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Knowledge Distillation, Label Smoothing, Metaphor Detection, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18253v1.pdf filename=2403.18253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Metaphors</b> <b>are</b> ubiquitous in daily life, yet detecting them poses a significant challenge. Previous approaches often struggled with improper application of language rules and overlooked the issue of data sparsity. To address these challenges, we introduce <b>knowledge</b> <b>distillation</b> and <b>prompt</b> <b>learning</b> into <b>metaphor</b> <b>detection.</b> Specifically, we devise a <b>prompt</b> <b>learning</b> template tailored for the <b>metaphor</b> <b>detection</b> task. By masking target words and providing relevant <b>prompt</b> <b>information,</b> we guide the model to accurately infer the contextual meaning of these words. This approach not only mitigates the interference from the literal meaning of target words but also ensures the proper utilization of MIP language rules for <b>metaphor</b> <b>detection.</b> Moreover, we employ a teacher model equipped with prior <b>knowledge</b> <b>to</b> generate meaningful soft <b>labels,</b> <b>guiding</b> the optimization process of the student model. The inclusion of soft <b>labels,</b> <b>akin</b> to <b>label</b> <b>smoothing,</b> helps alleviate the model&rsquo;s tendency towards over-confidence and effectively addresses the challenge of data sparsity. Experimental results demonstrate that our proposed model achieves state-of-the-art performance across multiple datasets.</p></p class="citation"></blockquote><h3 id=1042--10318-exploring-the-deceptive-power-of-llm-generated-fake-news-a-study-of-real-world-detection-challenges-yanshen-sun-et-al-2024>(10/42 | 10/318) Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges (Yanshen Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu. (2024)<br><strong>Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges</strong><br><button class=copy-to-clipboard title="Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SI, cs.CL<br>Keyword Score: 60<br>Keywords: Human Intervention, Variational Autoencoder, Fake News Detection, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18249v1.pdf filename=2403.18249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have enabled the creation of <b>fake</b> <b>news,</b> particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of <b>LLM-generated</b> <b>fake</b> <b>news</b> with and without <b>human</b> <b>assistance,</b> yet the potential of <b>prompting</b> techniques has not been fully explored. Thus, this work aims to determine whether <b>prompting</b> strategies can effectively narrow this gap. Current <b>LLM-based</b> <b>fake</b> <b>news</b> attacks require <b>human</b> <b>intervention</b> for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong <b>fake</b> <b>news</b> attack method called conditional <b>Variational-autoencoder-Like</b> <b>Prompt</b> (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt <b>fake</b> <b>news</b> (VLPFN) containing real and <b>fake</b> <b>texts.</b> Our experiments, including various detection methods and novel <b>human</b> <b>study</b> metrics, were conducted to assess their performance on our dataset, yielding numerous findings.</p></p class="citation"></blockquote><h3 id=1142--11318-can-language-beat-numerical-regression-language-based-multimodal-trajectory-prediction-inhwan-bae-et-al-2024>(11/42 | 11/318) Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction (Inhwan Bae et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Inhwan Bae, Junoh Lee, Hae-Gon Jeon. (2024)<br><strong>Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction</strong><br><button class=copy-to-clipboard title="Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-LG, cs-RO, cs.CL<br>Keyword Score: 56<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Question Answering, Reasoning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18447v1.pdf filename=2403.18447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language <b>foundation</b> <b>models,</b> in this paper, we propose LMTraj (Language-based <b>Multimodal</b> Trajectory predictor), which recasts the trajectory prediction task into a sort of <b>question-answering</b> <b>problem.</b> Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text <b>prompts.</b> Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text <b>prompt,</b> and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the <b>question-answering</b> <b>template</b> for use in a language model. Next, to guide the language model in understanding and <b>reasoning</b> high-level knowledge, such as scene context and social relationships between pedestrians, we introduce an auxiliary multi-task <b>question</b> <b>and</b> answering. We then train a numerical tokenizer with the <b>prompt</b> data. We encourage the tokenizer to separate the integer and decimal parts well, and leverage it to capture correlations between the consecutive numbers in the language model. Lastly, we train the language model using the numerical tokenizer and all of the <b>question-answer</b> <b>prompts.</b> Here, we propose a beam-search-based most-likely prediction and a temperature-based <b>multimodal</b> prediction to implement both deterministic and stochastic inferences. Applying our LMTraj, we show that the language-based model can be a powerful pedestrian trajectory predictor, and outperforms existing numerical-based predictor methods. Code is publicly available at <a href=https://github.com/inhwanbae/LMTrajectory>https://github.com/inhwanbae/LMTrajectory</a> .</p></p class="citation"></blockquote><h3 id=1242--12318-projective-methods-for-mitigating-gender-bias-in-pre-trained-language-models-hillary-dawkins-et-al-2024>(12/42 | 12/318) Projective Methods for Mitigating Gender Bias in Pre-trained Language Models (Hillary Dawkins et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hillary Dawkins, Isar Nejadgholi, Daniel Gillis, Judi McCuaig. (2024)<br><strong>Projective Methods for Mitigating Gender Bias in Pre-trained Language Models</strong><br><button class=copy-to-clipboard title="Projective Methods for Mitigating Gender Bias in Pre-trained Language Models" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, BERT, Next Sentence Prediction, Pre-trained Language Model, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18803v1.pdf filename=2403.18803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mitigation of gender bias in NLP has a long history tied to debiasing static <b>word</b> <b>embeddings.</b> More recently, attention has shifted to debiasing <b>pre-trained</b> <b>language</b> <b>models.</b> We study to what extent the simplest projective debiasing methods, developed for <b>word</b> <b>embeddings,</b> can help when applied to <b>BERT&rsquo;s</b> internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by <b>BERT&rsquo;s</b> <b>next</b> <b>sentence</b> <b>prediction</b> task, and in mitigating observed bias in a downstream setting when <b>fine-tuned.</b> To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or <b>next</b> <b>sentence</b> <b>prediction,</b> should not be the only <b>benchmark</b> in developing a debiased language model.</p></p class="citation"></blockquote><h3 id=1342--13318-a-survey-on-large-language-models-from-concept-to-implementation-chen-wang-et-al-2024>(13/42 | 13/318) A Survey on Large Language Models from Concept to Implementation (Chen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Wang, Jin Zhao, Jiaqi Gong. (2024)<br><strong>A Survey on Large Language Models from Concept to Implementation</strong><br><button class=copy-to-clipboard title="A Survey on Large Language Models from Concept to Implementation" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IT, cs-LG, cs.CL, math-IT<br>Keyword Score: 50<br>Keywords: GPT, Transformer, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18969v1.pdf filename=2403.18969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> particularly those built on <b>Transformer</b> architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in <b>chatbot</b> technology. This paper investigates the multifaceted applications of these models, with an emphasis on the <b>GPT</b> series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, <b>Transformer</b> models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in <b>Transformer</b> models, highlighting their versatility and the potential they hold for transforming diverse application sectors, thereby offering readers a comprehensive understanding of the current and future landscape of <b>Transformer-based</b> <b>LLMs</b> in practical applications.</p></p class="citation"></blockquote><h3 id=1442--14318-acted-automatic-acquisition-of-typical-event-duration-for-semi-supervised-temporal-commonsense-qa-felix-virgo-et-al-2024>(14/42 | 14/318) AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA (Felix Virgo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Virgo, Fei Cheng, Lis Kanashiro Pereira, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi. (2024)<br><strong>AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA</strong><br><button class=copy-to-clipboard title="AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Supervised Learning, Weakly-supervised Learning, BERT, RoBERTa, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18504v1.pdf filename=2403.18504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a voting-driven semi-supervised approach to automatically acquire the typical duration of an event and use it as pseudo-labeled data. The human evaluation demonstrates that our pseudo labels exhibit surprisingly high accuracy and balanced coverage. In the temporal commonsense <b>QA</b> task, experimental results show that using only pseudo examples of 400 events, we achieve performance comparable to the existing <b>BERT-based</b> weakly <b>supervised</b> approaches that require a significant amount of training examples. When compared to the <b>RoBERTa</b> baselines, our best approach establishes state-of-the-art performance with a 7% improvement in Exact Match.</p></p class="citation"></blockquote><h3 id=1542--15318-semrode-macro-adversarial-training-to-learn-representations-that-are-robust-to-word-level-attacks-brian-formento-et-al-2024>(15/42 | 15/318) SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks (Brian Formento et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng. (2024)<br><strong>SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks</strong><br><button class=copy-to-clipboard title="SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Adversarial Learning, BERT, RoBERTa, Word Embedding, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18423v1.pdf filename=2403.18423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to <b>adversarial</b> <b>attacks</b> remains a concern. While current research has explored <b>adversarial</b> <b>training</b> techniques, their improvements to defend against <b>word-level</b> <b>attacks</b> have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro <b>Adversarial</b> <b>Training</b> strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, <b>adversarial</b> <b>samples</b> generated via <b>word</b> <b>substitutions</b> do indeed belong to an <b>adversarial</b> <b>domain</b> exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an <b>adversarial</b> <b>domain,</b> but instead to a domain with minimal shift, it would improve attack robustness. We align the domains by incorporating a new distance-based objective. With this, our model is able to learn more generalized representations by aligning the model&rsquo;s high-level output features and therefore better handling unseen <b>adversarial</b> <b>samples.</b> This method can be generalized across <b>word</b> <b>embeddings,</b> even when they share minimal overlap at both vocabulary and <b>word-substitution</b> <b>levels.</b> To evaluate the effectiveness of our approach, we conduct experiments on <b>BERT</b> and <b>RoBERTa</b> models on three datasets. The results demonstrate promising state-of-the-art robustness.</p></p class="citation"></blockquote><h3 id=1642--16318-biomedlm-a-27b-parameter-language-model-trained-on-biomedical-text-elliot-bolton-et-al-2024>(16/42 | 16/318) BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text (Elliot Bolton et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D. Manning. (2024)<br><strong>BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text</strong><br><button class=copy-to-clipboard title="BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, GPT, GPT-4, Massive Multitask Language Understanding (MMLU), Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18421v1.pdf filename=2403.18421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Models such as <b>GPT-4</b> and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this <b>question,</b> <b>we</b> build and release BioMedLM, a 2.7 billion parameter <b>GPT-style</b> autoregressive model trained exclusively on PubMed abstracts and full articles. When <b>fine-tuned,</b> BioMedLM can produce strong multiple-choice biomedical <b>question-answering</b> <b>results</b> competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the <b>MMLU</b> Medical Genetics exam. BioMedLM can also be <b>fine-tuned</b> to produce useful answers to patient <b>questions</b> <b>on</b> medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-preserving, economical and environmentally friendly foundations for particular NLP applications, such as in biomedicine. The model is available on the Hugging Face Hub: <a href=https://huggingface.co/stanford-crfm/BioMedLM>https://huggingface.co/stanford-crfm/BioMedLM</a>.</p></p class="citation"></blockquote><h3 id=1742--17318-zaebuc-spoken-a-multilingual-multidialectal-arabic-english-speech-corpus-injy-hamed-et-al-2024>(17/42 | 17/318) ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus (Injy Hamed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Injy Hamed, Fadhl Eryani, David Palfreyman, Nizar Habash. (2024)<br><strong>ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus</strong><br><button class=copy-to-clipboard title="ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Lemmatization, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18182v1.pdf filename=2403.18182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English <b>speech</b> <b>corpus.</b> The corpus comprises twelve hours of Zoom meetings involving multiple speakers role-playing a work situation where Students brainstorm ideas for a certain topic and then discuss it with an Interlocutor. The meetings cover different topics and are divided into phases with different language setups. The corpus presents a challenging set for <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR),</b> including two languages (Arabic and English) with Arabic spoken in multiple variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English used with various accents. Adding to the complexity of the corpus, there is also code-switching between these languages and dialects. As part of our work, we take inspiration from established sets of transcription guidelines to present a set of guidelines handling issues of conversational <b>speech,</b> <b>code-switching</b> and orthography of both languages. We further enrich the corpus with two layers of annotations; (1) dialectness level annotation for the portion of the corpus where mixing occurs between different variants of Arabic, and (2) <b>automatic</b> <b>morphological</b> <b>annotations,</b> including <b>tokenization,</b> <b>lemmatization,</b> and part-of-speech tagging.</p></p class="citation"></blockquote><h3 id=1842--18318-iteralign-iterative-constitutional-alignment-of-large-language-models-xiusi-chen-et-al-2024>(18/42 | 18/318) IterAlign: Iterative Constitutional Alignment of Large Language Models (Xiusi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiusi Chen, Hongzhi Wen, Sreyashi Nag, Chen Luo, Qingyu Yin, Ruirui Li, Zheng Li, Wei Wang. (2024)<br><strong>IterAlign: Iterative Constitutional Alignment of Large Language Models</strong><br><button class=copy-to-clipboard title="IterAlign: Iterative Constitutional Alignment of Large Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Reinforcement Learning, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18341v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18341v1.pdf filename=2403.18341v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> aligning <b>LLMs</b> with human values and societal norms to ensure their reliability and safety has become crucial. <b>Reinforcement</b> <b>learning</b> with human feedback <b>(RLHF)</b> and Constitutional AI (CAI) have been proposed for <b>LLM</b> alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based <b>LLM</b> alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an <b>LLM</b> and automatically discovers new constitutions using a stronger <b>LLM.</b> These constitutions are then used to guide self-correction of the base <b>LLM.</b> Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current <b>LLM.</b> Empirical results on several safety <b>benchmark</b> datasets and multiple base <b>LLMs</b> show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the <b>LLM</b> alignment by up to $13.5%$ in harmlessness.</p></p class="citation"></blockquote><h3 id=1942--19318-sorry-come-again-prompting----enhancing-comprehension-and-diminishing-hallucination-with-pause-injected-optimal-paraphrasing-vipula-rawte-et-al-2024>(19/42 | 19/318) &lsquo;Sorry, Come Again?&rsquo; Prompting &ndash; Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing (Vipula Rawte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vipula Rawte, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Prachi Priya, Aman Chadha, Amit P. Sheth, Amitava Das. (2024)<br><strong>&lsquo;Sorry, Come Again?&rsquo; Prompting &ndash; Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing</strong><br><button class=copy-to-clipboard title="'Sorry, Come Again?' Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18976v1.pdf filename=2403.18976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hallucination has emerged as the most vulnerable aspect of contemporary <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> In this paper, we introduce the Sorry, Come Again (SCA) <b>prompting,</b> aimed to avoid <b>LLM</b> hallucinations by enhancing comprehension through: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay <b>LLM</b> generation. First, we provide an in-depth analysis of linguistic nuances: formality, readability, and concreteness of <b>prompts</b> for 21 <b>LLMs,</b> and elucidate how these nuances contribute to hallucinated generation. <b>Prompts</b> with lower readability, formality, or concreteness pose comprehension challenges for <b>LLMs,</b> similar to those faced by humans. In such scenarios, an <b>LLM</b> tends to speculate and generate content based on its imagination (associative memory) to fill these information gaps. Although these speculations may occasionally align with factual information, their accuracy is not assured, often resulting in hallucination. Recent studies reveal that an <b>LLM</b> often neglects the middle sections of extended <b>prompts,</b> a phenomenon termed as lost in the middle. While a specific paraphrase may suit one <b>LLM,</b> the same paraphrased version may elicit a different response from another <b>LLM.</b> Therefore, we propose an optimal paraphrasing technique to identify the most comprehensible paraphrase of a given <b>prompt,</b> evaluated using Integrated Gradient (and its variations) to guarantee that the <b>LLM</b> accurately processes all words. While reading lengthy sentences, humans often pause at various points to better comprehend the meaning read thus far. We have <b>fine-tuned</b> an <b>LLM</b> with injected [PAUSE] tokens, allowing the <b>LLM</b> to pause while reading lengthier <b>prompts.</b> This has brought several key contributions: (i) determining the optimal position to inject [PAUSE], (ii) determining the number of [PAUSE] tokens to be inserted, and (iii) introducing reverse proxy tuning to <b>fine-tune</b> the <b>LLM</b> for [PAUSE] insertion.</p></p class="citation"></blockquote><h3 id=2042--20318-fact-checking-beyond-training-set-payam-karisani-et-al-2024>(20/42 | 20/318) Fact Checking Beyond Training Set (Payam Karisani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Payam Karisani, Heng Ji. (2024)<br><strong>Fact Checking Beyond Training Set</strong><br><button class=copy-to-clipboard title="Fact Checking Beyond Training Set" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, GPT-4, Fact Verification, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18671v1.pdf filename=2403.18671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating the veracity of everyday claims is time consuming and in some cases requires <b>domain</b> <b>expertise.</b> We empirically demonstrate that the commonly used <b>fact</b> <b>checking</b> pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one <b>domain</b> <b>and</b> used in another <b>domain.</b> <b>Afterwards,</b> we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against <b>distribution</b> <b>shift.</b> Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher robustness against <b>distribution</b> <b>shift.</b> To our knowledge, there is no publicly available multi-topic <b>fact</b> <b>checking</b> dataset. Thus, we propose a simple automatic method to re-purpose two well-known <b>fact</b> <b>checking</b> datasets. We then construct eight <b>fact</b> <b>checking</b> scenarios from these datasets, and compare our model to a set of strong baseline models, including recent <b>domain</b> <b>adaptation</b> models that use <b>GPT4</b> for generating synthetic data.</p></p class="citation"></blockquote><h3 id=2142--21318-nl-iti-optimizing-probing-and-intervention-for-improvement-of-iti-method-jakub-hoscilowicz-et-al-2024>(21/42 | 21/318) NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method (Jakub Hoscilowicz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki. (2024)<br><strong>NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method</strong><br><button class=copy-to-clipboard title="NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Massive Multitask Language Understanding (MMLU), Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18680v1.pdf filename=2403.18680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, <b>LLM</b> activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice <b>benchmarks,</b> including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of <b>MMLU,</b> around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in the behavior of <b>LLM</b> at the same time (as measured by Kullback-Leibler divergence).</p></p class="citation"></blockquote><h3 id=2242--22318-evaluation-of-semantic-search-and-its-role-in-retrieved-augmented-generation-rag-for-arabic-language-ali-mahboub-et-al-2024>(22/42 | 22/318) Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language (Ali Mahboub et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Mahboub, Muhy Eddin Za&rsquo;ter, Bashar Alfrou, Yazan Estaitia, Adnan Jaljuli, Asma Hakouz. (2024)<br><strong>Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language</strong><br><button class=copy-to-clipboard title="Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18350v1.pdf filename=2403.18350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The latest advancements in machine learning and deep learning have brought forth the concept of semantic similarity, which has proven immensely beneficial in multiple applications and has largely replaced keyword search. However, evaluating semantic similarity and conducting searches for a specific query across various documents continue to be a complicated task. This complexity is due to the multifaceted nature of the task, the lack of standard <b>benchmarks,</b> whereas these challenges are further amplified for Arabic language. This paper endeavors to establish a straightforward yet potent <b>benchmark</b> for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of these metrics and the dataset, we conduct our assessment of semantic search within the framework of <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG).</b></p></p class="citation"></blockquote><h3 id=2342--23318-a-novel-corpus-of-annotated-medical-imaging-reports-and-information-extraction-results-using-bert-based-language-models-namu-park-et-al-2024>(23/42 | 23/318) A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models (Namu Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Namu Park, Kevin Lybarger, Giridhar Kaushik Ramachandran, Spencer Lewis, Aashka Damani, Ozlem Uzuner, Martin Gunn, Meliha Yetisgen. (2024)<br><strong>A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models</strong><br><button class=copy-to-clipboard title="A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BERT, Transformer, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18975v1.pdf filename=2403.18975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical imaging is critical to the diagnosis, surveillance, and treatment of many health conditions, including oncological, neurological, cardiovascular, and musculoskeletal disorders, among others. Radiologists interpret these complex, unstructured images and articulate their assessments through narrative reports that remain largely unstructured. This unstructured narrative must be converted into a structured semantic representation to facilitate secondary applications such as retrospective analyses or clinical decision support. Here, we introduce the Corpus of Annotated Medical Imaging Reports (CAMIR), which includes 609 annotated radiology reports from three imaging modality types: Computed Tomography, Magnetic Resonance Imaging, and Positron Emission Tomography-Computed Tomography. Reports were annotated using an event-based schema that captures clinical indications, lesions, and medical problems. Each event consists of a trigger and multiple arguments, and a majority of the argument types, including anatomy, normalize the spans to pre-defined concepts to facilitate secondary use. CAMIR uniquely combines a granular event structure and concept normalization. To extract CAMIR events, we explored two <b>BERT</b> (Bi-directional Encoder Representation from <b>Transformers)-based</b> architectures, including an existing architecture (mSpERT) that jointly extracts all event <b>information</b> <b>and</b> a multi-step approach (PL-Marker++) that we augmented for the CAMIR schema.</p></p class="citation"></blockquote><h3 id=2442--24318-sdsat-accelerating-llm-inference-through-speculative-decoding-with-semantic-adaptive-tokens-chengbo-liu-et-al-2024>(24/42 | 24/318) SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens (Chengbo Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengbo Liu, Yong Zhu. (2024)<br><strong>SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens</strong><br><button class=copy-to-clipboard title="SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18647v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18647v2.pdf filename=2403.18647v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose an acceleration scheme for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the <b>LLM</b> model&rsquo;s ability to generate draft tokens more accurately without compromising the model&rsquo;s accuracy. The core strategies involve: 1) <b>Fine-tune</b> the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the &ldquo;two-step-draft-then-verify&rdquo; generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to <a href=https://github.com/hasuoshenyun/SDSAT>https://github.com/hasuoshenyun/SDSAT</a>.</p></p class="citation"></blockquote><h3 id=2542--25318-attention-aware-semantic-relevance-predicting-chinese-sentence-reading-kun-sun-2024>(25/42 | 25/318) Attention-aware semantic relevance predicting Chinese sentence reading (Kun Sun, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Sun. (2024)<br><strong>Attention-aware semantic relevance predicting Chinese sentence reading</strong><br><button class=copy-to-clipboard title="Attention-aware semantic relevance predicting Chinese sentence reading" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18542v1.pdf filename=2403.18542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, several influential computational models and metrics have been proposed to predict how humans comprehend and process sentence. One particularly promising approach is contextual semantic similarity. Inspired by the attention algorithm in <b>Transformer</b> and human memory mechanisms, this study proposes an <code>attention-aware'' approach for computing contextual semantic relevance. This new approach takes into account the different contributions of contextual parts and the expectation effect, allowing it to incorporate contextual information fully. The attention-aware approach also facilitates the &lt;b>simulation&lt;/b> of existing reading models and evaluate them. The resulting </code>attention-aware&rsquo;&rsquo; metrics of semantic relevance can more accurately predict fixation durations in Chinese reading tasks recorded in an eye-tracking corpus than those calculated by existing approaches. The study&rsquo;s findings further provide strong support for the presence of semantic preview benefits in Chinese naturalistic reading. Furthermore, the attention-aware metrics of semantic relevance, being memory-based, possess high interpretability from both linguistic and cognitive standpoints, making them a valuable computational tool for modeling eye-movements in reading and further gaining insight into the process of language comprehension. Our approach underscores the potential of these metrics to advance our comprehension of how humans understand and process language, ultimately leading to a better understanding of language comprehension and processing.</p></p class="citation"></blockquote><h3 id=2642--26318-triviahg-a-dataset-for-automatic-hint-generation-from-factoid-questions-jamshid-mozafari-et-al-2024>(26/42 | 26/318) TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions (Jamshid Mozafari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jamshid Mozafari, Anubhav Jangra, Adam Jatowt. (2024)<br><strong>TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions</strong><br><button class=copy-to-clipboard title="TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Evaluation, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18426v1.pdf filename=2403.18426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, individuals tend to engage in dialogues with <b>Large</b> <b>Language</b> <b>Models,</b> seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human&rsquo;s cognitive abilities, as well as the assurance of maintaining good <b>reasoning</b> skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the <b>automatic</b> <b>hint</b> generation for factoid questions, employing it to construct TriviaHG, a novel <b>large-scale</b> <b>dataset</b> <b>featuring</b> 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an <b>automatic</b> <b>evaluation</b> method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 humans with answering questions using the provided hints. The effectiveness of hints varied, with success rates of 96%, 78%, and 36% for questions with easy, medium, and hard answers, respectively. Moreover, the proposed <b>automatic</b> <b>evaluation</b> methods showed a robust correlation with annotators&rsquo; results. Conclusively, the findings highlight three key insights: the facilitative role of hints in resolving unknown questions, the dependence of hint quality on answer difficulty, and the feasibility of employing <b>automatic</b> <b>evaluation</b> methods for hint assessment.</p></p class="citation"></blockquote><h3 id=2742--27318-improving-attributed-text-generation-of-large-language-models-via-preference-learning-dongfang-li-et-al-2024>(27/42 | 27/318) Improving Attributed Text Generation of Large Language Models via Preference Learning (Dongfang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, Min Zhang. (2024)<br><strong>Improving Attributed Text Generation of Large Language Models via Preference Learning</strong><br><button class=copy-to-clipboard title="Improving Attributed Text Generation of Large Language Models via Preference Learning" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Evaluation, Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18381v1.pdf filename=2403.18381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content. Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations). However, current attribution methods usually focus on the retrieval stage and <b>automatic</b> <b>evaluation</b> that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility. In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an <b>Automatic</b> <b>Preference</b> Optimization (APO) framework. First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets. Second, considering the high cost of labelling preference data, we further propose an <b>automatic</b> <b>method</b> to synthesize attribution preference data resulting in 95,263 pairs. Moreover, inspired by the human citation process, we further propose a progressive preference optimization method by leveraging fine-grained information. Extensive experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate that APO achieves state-of-the-art citation F1 with higher answer quality.</p></p class="citation"></blockquote><h3 id=2842--28318-rejection-improves-reliability-training-llms-to-refuse-unknown-questions-using-rl-from-knowledge-feedback-hongshen-xu-et-al-2024>(28/42 | 28/318) Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback (Hongshen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai Yu. (2024)<br><strong>Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback</strong><br><button class=copy-to-clipboard title="Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18349v1.pdf filename=2403.18349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model&rsquo;s ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of <b>LLMs,</b> we present a novel alignment framework called <b>Reinforcement</b> <b>Learning</b> from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model&rsquo;s knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions. Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing <b>LLM</b> reliability.</p></p class="citation"></blockquote><h3 id=2942--29318-reflectsumm-a-benchmark-for-course-reflection-summarization-yang-zhong-et-al-2024>(29/42 | 29/318) ReflectSumm: A Benchmark for Course Reflection Summarization (Yang Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Zhong, Mohamed Elaraby, Diane Litman, Ahmed Ashraf Butt, Muhsin Menekse. (2024)<br><strong>ReflectSumm: A Benchmark for Course Reflection Summarization</strong><br><button class=copy-to-clipboard title="ReflectSumm: A Benchmark for Course Reflection Summarization" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Opinion Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19012v1.pdf filename=2403.19012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces ReflectSumm, a novel <b>summarization</b> dataset specifically designed for summarizing students&rsquo; reflective writing. The goal of ReflectSumm is to facilitate developing and evaluating novel <b>summarization</b> techniques tailored to real-world scenarios with little training data, %practical tasks with potential implications in the <b>opinion</b> <b>summarization</b> domain in general and the educational domain in particular. The dataset encompasses a diverse range of <b>summarization</b> tasks and includes comprehensive metadata, enabling the exploration of various research questions and supporting different applications. To showcase its utility, we conducted extensive evaluations using multiple state-of-the-art baselines. The results provide <b>benchmarks</b> for facilitating further research in this area.</p></p class="citation"></blockquote><h3 id=3042--30318-measuring-political-bias-in-large-language-models-what-is-said-and-how-it-is-said-yejin-bang-et-al-2024>(30/42 | 30/318) Measuring Political Bias in Large Language Models: What Is Said and How It Is Said (Yejin Bang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yejin Bang, Delong Chen, Nayeon Lee, Pascale Fung. (2024)<br><strong>Measuring Political Bias in Large Language Models: What Is Said and How It Is Said</strong><br><button class=copy-to-clipboard title="Measuring Political Bias in Large Language Models: What Is Said and How It Is Said" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18932v1.pdf filename=2403.18932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose to measure political bias in <b>LLMs</b> by analyzing both the content and style of their generated content regarding political issues. Existing <b>benchmarks</b> and measures focus on gender and racial biases. However, political bias exists in <b>LLMs</b> and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by <b>LLMs.</b> Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced <b>LLMs</b> and showed that our proposed framework is easily scalable to other topics and is explainable.</p></p class="citation"></blockquote><h3 id=3142--31318-capability-aware-prompt-reformulation-learning-for-text-to-image-generation-jingtao-zhan-et-al-2024>(31/42 | 31/318) Capability-aware Prompt Reformulation Learning for Text-to-Image Generation (Jingtao Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingtao Zhan, Qingyao Ai, Yiqun Liu, Jia Chen, Shaoping Ma. (2024)<br><strong>Capability-aware Prompt Reformulation Learning for Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Capability-aware Prompt Reformulation Learning for Text-to-Image Generation" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-IR, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19716v1.pdf filename=2403.19716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> generation systems have emerged as revolutionary tools in the realm of artistic creation, offering unprecedented ease in transforming textual <b>prompts</b> into visual art. However, the efficacy of these systems is intricately linked to the quality of user-provided <b>prompts,</b> which often poses a challenge to users unfamiliar with <b>prompt</b> crafting. This paper addresses this challenge by leveraging user reformulation data from interaction logs to develop an automatic <b>prompt</b> reformulation model. Our in-depth analysis of these logs reveals that user <b>prompt</b> reformulation is heavily dependent on the individual user&rsquo;s capability, resulting in significant variance in the quality of reformulation pairs. To effectively use this data for training, we introduce the Capability-aware <b>Prompt</b> Reformulation (CAPR) framework. CAPR innovatively integrates user capability into the reformulation process through two key components: the Conditional Reformulation Model (CRM) and Configurable Capability Features (CCF). CRM reformulates <b>prompts</b> according to a specified user capability, as represented by CCF. The CCF, in turn, offers the flexibility to tune and guide the CRM&rsquo;s behavior. This enables CAPR to effectively learn diverse reformulation strategies across various user capacities and to simulate high-capability user reformulation during inference. Extensive experiments on standard <b>text-to-image</b> generation <b>benchmarks</b> showcase CAPR&rsquo;s superior performance over existing baselines and its remarkable robustness on unseen systems. Furthermore, comprehensive analyses validate the effectiveness of different components. CAPR can facilitate user-friendly interaction with <b>text-to-image</b> systems and make advanced artistic creation more achievable for a broader range of users.</p></p class="citation"></blockquote><h3 id=3242--32318-checkeval-robust-evaluation-framework-using-large-language-model-via-checklist-yukyung-lee-et-al-2024>(32/42 | 32/318) CheckEval: Robust Evaluation Framework using Large Language Model via Checklist (Yukyung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang. (2024)<br><strong>CheckEval: Robust Evaluation Framework using Large Language Model via Checklist</strong><br><button class=copy-to-clipboard title="CheckEval: Robust Evaluation Framework using Large Language Model via Checklist" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18771v1.pdf filename=2403.18771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce CheckEval, a novel evaluation framework using <b>Large</b> <b>Language</b> <b>Models,</b> addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval <b>benchmark,</b> CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of <b>LLMs</b> in evaluation, responding to the evolving needs of the field and establishing a clear method for future <b>LLM-based</b> evaluation.</p></p class="citation"></blockquote><h3 id=3342--33318-the-invalsi-benchmark-measuring-language-models-mathematical-and-language-understanding-in-italian-andrea-esuli-et-al-2024>(33/42 | 33/318) The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian (Andrea Esuli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Esuli, Giovanni Puccetti. (2024)<br><strong>The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian</strong><br><button class=copy-to-clipboard title="The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, High-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18697v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18697v1.pdf filename=2403.18697v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While Italian is by all metrics a high resource language, currently, there are isn&rsquo;t a Language Model pre-trained exclusively in this language. This results in a lower number of available <b>benchmarks</b> to evaluate the performance of language models in Italian. This work presents two new <b>benchmarks</b> to evaluate the models performance on mathematical understanding and language understanding in Italian. These <b>benchmarks</b> are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy. To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own <b>fine-tuned</b> models. We show that this is a challenging <b>benchmark</b> where current language models are bound by 60% accuracy. We believe that the release of this dataset paves the way for improving future models mathematical and language understanding in Italian.</p></p class="citation"></blockquote><h3 id=3442--34318-conformal-intent-classification-and-clarification-for-fast-and-accurate-intent-recognition-floris-den-hengst-et-al-2024>(34/42 | 34/318) Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition (Floris den Hengst et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Floris den Hengst, Ralf Wolter, Patrick Altmeyer, Arda Kaygan. (2024)<br><strong>Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition</strong><br><button class=copy-to-clipboard title="Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Dialogue System, Intent Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18973v1.pdf filename=2403.18973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Conformal <b>Intent</b> <b>Classification</b> and Clarification (CICC), a framework for fast and accurate <b>intent</b> <b>classification</b> for task-oriented <b>dialogue</b> <b>systems.</b> The framework turns heuristic uncertainty scores of any <b>intent</b> <b>classifier</b> into a clarification question that is guaranteed to contain the true <b>intent</b> <b>at</b> a pre-defined confidence level. By disambiguating between a small number of likely <b>intents,</b> <b>the</b> user query can be resolved quickly and accurately. Additionally, we propose to augment the framework for out-of-scope detection. In a comparative evaluation using seven <b>intent</b> <b>recognition</b> datasets we find that CICC generates small clarification questions and is capable of out-of-scope detection. CICC can help practitioners and researchers substantially in improving the user experience of <b>dialogue</b> <b>agents</b> with specific clarification questions.</p></p class="citation"></blockquote><h3 id=3542--35318-semeval-task-1-semantic-textual-relatedness-for-african-and-asian-languages-nedjma-ousidhoum-et-al-2024>(35/42 | 35/318) SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages (Nedjma Ousidhoum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir Araujo, Meriem Beloucif, Christine De Kock, Oumaima Hourrane, Manish Shrivastava, Thamar Solorio, Nirmal Surange, Krishnapriya Vishnubhotla, Seid Muhie Yimam, Saif M. Mohammad. (2024)<br><strong>SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages</strong><br><button class=copy-to-clipboard title="SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18933v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18933v2.pdf filename=2403.18933v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the first shared task on Semantic Textual Relatedness (STR). While earlier shared tasks primarily focused on semantic similarity, we instead investigate the broader phenomenon of semantic relatedness across 14 languages: Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia &ndash; regions characterised by the relatively limited availability of NLP resources. Each instance in the datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. Participating systems were asked to rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) in the 14 languages in three main tracks: (a) <b>supervised,</b> (b) <b>unsupervised,</b> and (c) crosslingual. The task attracted 163 participants. We received 70 submissions in total (across all tasks) from 51 different teams, and 38 system description papers. We report on the best-performing systems as well as the most common and the most effective approaches for the three different tracks.</p></p class="citation"></blockquote><h3 id=3642--36318-improved-neural-protoform-reconstruction-via-reflex-prediction-liang-lu-et-al-2024>(36/42 | 36/318) Improved Neural Protoform Reconstruction via Reflex Prediction (Liang Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Lu, Jingzhi Wang, David R. Mortensen. (2024)<br><strong>Improved Neural Protoform Reconstruction via Reflex Prediction</strong><br><button class=copy-to-clipboard title="Improved Neural Protoform Reconstruction via Reflex Prediction" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Rerank, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18769v1.pdf filename=2403.18769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Protolanguage reconstruction is central to historical linguistics. The comparative method, one of the most influential theoretical and methodological frameworks in the history of the language sciences, allows linguists to infer protoforms (reconstructed ancestral words) from their reflexes (related modern words) based on the assumption of regular sound change. Not surprisingly, numerous computational linguists have attempted to operationalize comparative reconstruction through various computational models, the most successful of which have been <b>supervised</b> encoder-decoder models, which treat the problem of predicting protoforms given sets of reflexes as a sequence-to-sequence problem. We argue that this framework ignores one of the most important aspects of the comparative method: not only should protoforms be inferable from cognate sets (sets of related reflexes) but the reflexes should also be inferable from the protoforms. Leveraging another line of research &ndash; reflex prediction &ndash; we propose a system in which candidate protoforms from a reconstruction model are <b>reranked</b> by a reflex prediction model. We show that this more complete implementation of the comparative method allows us to surpass state-of-the-art protoform reconstruction methods on three of four Chinese and Romance datasets.</p></p class="citation"></blockquote><h3 id=3742--37318-debiasing-sentence-embedders-through-contrastive-word-pairs-philip-kenneweg-et-al-2024>(37/42 | 37/318) Debiasing Sentence Embedders through Contrastive Word Pairs (Philip Kenneweg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Kenneweg, Sarah Schröder, Alexander Schulz, Barbara Hammer. (2024)<br><strong>Debiasing Sentence Embedders through Contrastive Word Pairs</strong><br><button class=copy-to-clipboard title="Debiasing Sentence Embedders through Contrastive Word Pairs" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Sentence Embedding, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18555v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18555v1.pdf filename=2403.18555v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the last years, various <b>sentence</b> <b>embedders</b> have been an integral part in the success of current machine learning approaches to Natural Language Processing (NLP). Unfortunately, multiple sources have shown that the bias, inherent in the datasets upon which these embedding methods are trained, is learned by them. A variety of different approaches to remove biases in embeddings exists in the literature. Most of these approaches are applicable to <b>word</b> <b>embeddings</b> and in fewer cases to <b>sentence</b> <b>embeddings.</b> It is problematic that most debiasing approaches are directly transferred from <b>word</b> <b>embeddings,</b> therefore these approaches fail to take into account the nonlinear nature of <b>sentence</b> <b>embedders</b> and the embeddings they produce. It has been shown in literature that bias information is still present if <b>sentence</b> <b>embeddings</b> are debiased using such methods. In this contribution, we explore an approach to remove linear and nonlinear bias information for NLP solutions, without impacting downstream performance. We compare our approach to common debiasing methods on classical bias metrics and on bias metrics which take nonlinear information into account.</p></p class="citation"></blockquote><h3 id=3842--38318-can-llms-converse-formally-automatically-assessing-llms-in-translating-and-interpreting-formal-specifications-rushang-karia-et-al-2024>(38/42 | 38/318) Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications (Rushang Karia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava. (2024)<br><strong>Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications</strong><br><button class=copy-to-clipboard title="Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18327v1.pdf filename=2403.18327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of <b>LLMs</b> in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of <b>LLMs,</b> and often require human-annotated datasets. We propose an approach that can use two copies of an <b>LLM</b> in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA <b>LLMs</b> cannot adequately solve this task, limiting their current utility in the design of complex systems.</p></p class="citation"></blockquote><h3 id=3942--39318-few-shot-recalibration-of-language-models-xiang-lisa-li-et-al-2024>(39/42 | 39/318) Few-Shot Recalibration of Language Models (Xiang Lisa Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu. (2024)<br><strong>Few-Shot Recalibration of Language Models</strong><br><button class=copy-to-clipboard title="Few-Shot Recalibration of Language Models" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Few-shot, Massive Multitask Language Understanding (MMLU)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18286v1.pdf filename=2403.18286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model&rsquo;s confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for <b>few-shot</b> slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM&rsquo;s predictions can be trusted, and below which it should abstain. Experiments show that our <b>few-shot</b> recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on <b>MMLU</b> by 16%, as compared to temperature scaling.</p></p class="citation"></blockquote><h3 id=4042--40318-blendx-complex-multi-intent-detection-with-blended-patterns-yejin-yoon-et-al-2024>(40/42 | 40/318) BlendX: Complex Multi-Intent Detection with Blended Patterns (Yejin Yoon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park, Taeuk Kim. (2024)<br><strong>BlendX: Complex Multi-Intent Detection with Blended Patterns</strong><br><button class=copy-to-clipboard title="BlendX: Complex Multi-Intent Detection with Blended Patterns" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18277v1.pdf filename=2403.18277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool &ndash; OpenAI&rsquo;s <b>ChatGPT</b> &ndash; which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field. The dataset is available at <a href=https://github.com/HYU-NLP/BlendX>https://github.com/HYU-NLP/BlendX</a>.</p></p class="citation"></blockquote><h3 id=4142--41318-chinese-offensive-language-detectioncurrent-status-and-future-directions-yunze-xiao-et-al-2024>(41/42 | 41/318) Chinese Offensive Language Detection:Current Status and Future Directions (Yunze Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunze Xiao, Houda Bouamor, Wajdi Zaghouani. (2024)<br><strong>Chinese Offensive Language Detection:Current Status and Future Directions</strong><br><button class=copy-to-clipboard title="Chinese Offensive Language Detection:Current Status and Future Directions" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18314v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18314v3.pdf filename=2403.18314v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language&rsquo;s complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current <b>benchmarks</b> and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary objective of this survey is to explore the existing techniques and identify potential avenues for further research that can address the cultural and linguistic complexities of Chinese.</p></p class="citation"></blockquote><h3 id=4242--42318-since-the-scientific-literature-is-multilingual-our-models-should-be-too-abteen-ebrahimi-et-al-2024>(42/42 | 42/318) Since the Scientific Literature Is Multilingual, Our Models Should Be Too (Abteen Ebrahimi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abteen Ebrahimi, Kenneth Church. (2024)<br><strong>Since the Scientific Literature Is Multilingual, Our Models Should Be Too</strong><br><button class=copy-to-clipboard title="Since the Scientific Literature Is Multilingual, Our Models Should Be Too" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18251v1.pdf filename=2403.18251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>English has long been assumed the $\textit{lingua franca}$ of scientific research, and this notion is reflected in the natural language processing (NLP) research involving scientific document representation. In this position piece, we quantitatively show that the literature is largely multilingual and argue that current models and <b>benchmarks</b> should reflect this linguistic diversity. We provide evidence that text-based models fail to create meaningful representations for non-English papers and highlight the negative user-facing impacts of using English-only models non-discriminately across a multilingual domain. We end with suggestions for the NLP community on how to improve performance on non-English documents.</p></p class="citation"></blockquote><h2 id=csir-18>cs.IR (18)</h2><h3 id=118--43318-to-recommend-or-not-recommendability-identification-in-conversations-with-pre-trained-language-models-zhefan-wang-et-al-2024>(1/18 | 43/318) To Recommend or Not: Recommendability Identification in Conversations with Pre-trained Language Models (Zhefan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhefan Wang, Weizhi Ma, Min Zhang. (2024)<br><strong>To Recommend or Not: Recommendability Identification in Conversations with Pre-trained Language Models</strong><br><button class=copy-to-clipboard title="To Recommend or Not: Recommendability Identification in Conversations with Pre-trained Language Models" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 100<br>Keywords: Fine-tuning, Recommendation, Recommender System, Zero-shot, ChatGPT, Chatbot, Dialogue System, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18628v1.pdf filename=2403.18628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most current <b>recommender</b> <b>systems</b> primarily focus on what to recommend, assuming users always require personalized <b>recommendations.</b> However, with the widely spread of <b>ChatGPT</b> and other <b>chatbots,</b> a more crucial problem in the context of conversational systems is how to minimize user disruption when we provide <b>recommendation</b> services for users. While previous research has extensively explored different user intents in <b>dialogue</b> <b>systems,</b> fewer efforts are made to investigate whether <b>recommendations</b> should be provided. In this paper, we formally define the recommendability identification problem, which aims to determine whether <b>recommendations</b> are necessary in a specific scenario. First, we propose and define the recommendability identification task, which investigates the need for <b>recommendations</b> in the current conversational context. A new dataset is constructed. Subsequently, we discuss and evaluate the feasibility of leveraging <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> for recommendability identification. Finally, through comparative experiments, we demonstrate that directly employing <b>PLMs</b> with <b>zero-shot</b> results falls short of meeting the task requirements. Besides, <b>fine-tuning</b> or utilizing soft <b>prompt</b> techniques yields comparable results to traditional classification methods. Our work is the first to study recommendability before <b>recommendation</b> and provides preliminary ways to make it a fundamental component of the future <b>recommendation</b> system.</p></p class="citation"></blockquote><h3 id=218--44318-towards-llm-recsys-alignment-with-textual-id-learning-juntao-tan-et-al-2024>(2/18 | 44/318) Towards LLM-RecSys Alignment with Textual ID Learning (Juntao Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, Yongfeng Zhang. (2024)<br><strong>Towards LLM-RecSys Alignment with Textual ID Learning</strong><br><button class=copy-to-clipboard title="Towards LLM-RecSys Alignment with Textual ID Learning" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 80<br>Keywords: Foundation Model, Recommendation, Supervised Learning, Zero-shot, Language Generation, Natural Language Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19021v1.pdf filename=2403.19021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative <b>recommendation</b> based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have transformed the traditional ranking-based <b>recommendation</b> style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative <b>recommendations</b> struggles to effectively encode <b>recommendation</b> items within the text-to-text framework using concise yet meaningful ID representations. To better align <b>LLMs</b> with <b>recommendation</b> needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human <b>language</b> <b>tokens.</b> This is achieved by training a textual ID generator alongside the <b>LLM-based</b> recommender, enabling seamless integration of personalized <b>recommendations</b> into <b>natural</b> <b>language</b> <b>generation.</b> Notably, as user history is expressed in <b>natural</b> <b>language</b> <b>and</b> decoupled from the original dataset, our approach suggests the potential for a <b>foundational</b> <b>generative</b> <b>recommendation</b> model. Experiments show that our framework consistently surpasses existing models in sequential <b>recommendation</b> under standard experimental setting. Then, we explore the possibility of training a <b>foundation</b> <b>recommendation</b> model with the proposed method on data collected from 19 different datasets and tested its <b>recommendation</b> performance on 6 unseen datasets across different platforms under a completely <b>zero-shot</b> setting. The results show that the <b>zero-shot</b> performance of the pre-trained <b>foundation</b> <b>model</b> is comparable to or even better than some traditional <b>recommendation</b> models based on <b>supervised</b> training, showing the potential of the IDGen paradigm serving as the <b>foundation</b> <b>model</b> for generative <b>recommendation.</b> Code and data are open-sourced at <a href=https://github.com/agiresearch/IDGenRec>https://github.com/agiresearch/IDGenRec</a>.</p></p class="citation"></blockquote><h3 id=318--45318-sequential-recommendation-with-latent-relations-based-on-large-language-model-shenghao-yang-et-al-2024>(3/18 | 45/318) Sequential Recommendation with Latent Relations based on Large Language Model (Shenghao Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shenghao Yang, Weizhi Ma, Peijie Sun, Qingyao Ai, Yiqun Liu, Mingchen Cai, Min Zhang. (2024)<br><strong>Sequential Recommendation with Latent Relations based on Large Language Model</strong><br><button class=copy-to-clipboard title="Sequential Recommendation with Latent Relations based on Large Language Model" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 78<br>Keywords: Graph, Autoencoder, Knowledge Graph, Recommendation, Recommender System, Self-supervised Learning, Variational Autoencoder, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18348v1.pdf filename=2403.18348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential <b>recommender</b> <b>systems</b> predict items that may interest users by modeling their preferences based on historical interactions. Traditional sequential <b>recommendation</b> methods rely on capturing implicit collaborative filtering signals among items. Recent relation-aware sequential <b>recommendation</b> models have achieved promising performance by explicitly incorporating item relations into the modeling of user historical sequences, where most relations are extracted from <b>knowledge</b> <b>graphs.</b> However, existing methods rely on manually predefined relations and suffer the sparsity issue, limiting the generalization ability in diverse scenarios with varied item relations. In this paper, we propose a novel relation-aware sequential <b>recommendation</b> framework with Latent Relation Discovery (LRD). Different from previous relation-aware models that rely on predefined rules, we propose to leverage the <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to provide new types of relations and connections between items. The motivation is that <b>LLM</b> contains abundant world <b>knowledge,</b> <b>which</b> can be adopted to mine latent relations of items for <b>recommendation.</b> Specifically, inspired by that humans can describe relations between items using natural language, LRD harnesses the <b>LLM</b> that has demonstrated human-like <b>knowledge</b> <b>to</b> obtain language <b>knowledge</b> <b>representations</b> of items. These representations are fed into a latent relation discovery module based on the discrete state <b>variational</b> <b>autoencoder</b> (DVAE). Then the <b>self-supervised</b> relation discovery tasks and <b>recommendation</b> tasks are jointly optimized. Experimental results on multiple public datasets demonstrate our proposed latent relations discovery method can be incorporated with existing relation-aware sequential <b>recommendation</b> models and significantly improve the performance. Further analysis experiments indicate the effectiveness and reliability of the discovered latent relations.</p></p class="citation"></blockquote><h3 id=418--46318-lightweight-embeddings-for-graph-collaborative-filtering-xurong-liang-et-al-2024>(4/18 | 46/318) Lightweight Embeddings for Graph Collaborative Filtering (Xurong Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xurong Liang, Tong Chen, Lizhen Cui, Yang Wang, Meng Wang, Hongzhi Yin. (2024)<br><strong>Lightweight Embeddings for Graph Collaborative Filtering</strong><br><button class=copy-to-clipboard title="Lightweight Embeddings for Graph Collaborative Filtering" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 46<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Parameter Sharing, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18479v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18479v2.pdf filename=2403.18479v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> are currently one of the most performant collaborative filtering methods. Meanwhile, owing to the use of an embedding table to represent each user/item as a distinct vector, <b>GNN-based</b> recommenders have inherited the long-standing defect of <b>parameter</b> <b>inefficiency.</b> As a common practice for scalable embeddings, <b>parameter</b> <b>sharing</b> enables the use of fewer embedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most existing methods are a heuristically designed, predefined mapping from each user&rsquo;s/item&rsquo;s ID to the corresponding meta-embedding indexes, thus simplifying the optimization problem into learning only the meta-embeddings. However, in the context of <b>GNN-based</b> collaborative filtering, such a fixed mapping omits the semantic correlations between entities that are evident in the user-item interaction <b>graph,</b> <b>leading</b> <b>to</b> suboptimal <b>recommendation</b> performance. To this end, we propose Lightweight Embeddings for <b>Graph</b> <b>Collaborative</b> <b>Filtering</b> (LEGCF), a <b>parameter-efficient</b> <b>embedding</b> framework dedicated to <b>GNN-based</b> recommenders. LEGCF innovatively introduces an assignment matrix as an extra learnable component on top of meta-embeddings. To jointly optimize these two heavily entangled components, aside from learning the meta-embeddings by minimizing the <b>recommendation</b> loss, LEGCF further performs efficient assignment update by enforcing a novel semantic similarity constraint and finding its closed-form solution based on matrix pseudo-inverse. The meta-embeddings and assignment matrix are alternately updated, where the latter is sparsified on the fly to ensure negligible storage overhead. Extensive experiments on three <b>benchmark</b> datasets have verified LEGCF&rsquo;s smallest trade-off between size and performance, with consistent accuracy gain over state-of-the-art baselines. The codebase of LEGCF is available in <a href=https://github.com/xurong-liang/LEGCF>https://github.com/xurong-liang/LEGCF</a>.</p></p class="citation"></blockquote><h3 id=518--47318-scaling-laws-for-dense-retrieval-yan-fang-et-al-2024>(5/18 | 47/318) Scaling Laws For Dense Retrieval (Yan Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu. (2024)<br><strong>Scaling Laws For Dense Retrieval</strong><br><button class=copy-to-clipboard title="Scaling Laws For Dense Retrieval" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Data Augmentation, Dense Retrieval, Language Generation, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18684v1.pdf filename=2403.18684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Scaling</b> <b>up</b> neural models has yielded significant advancements in a wide array of tasks, particularly in <b>language</b> <b>generation.</b> Previous studies have found that the performance of neural models frequently adheres to predictable <b>scaling</b> <b>laws,</b> correlated with factors such as training set size and model size. This insight is invaluable, especially as large-scale experiments grow increasingly resource-intensive. Yet, such <b>scaling</b> <b>law</b> has not been fully explored in <b>dense</b> <b>retrieval</b> due to the discrete nature of retrieval metrics and complex relationships between training <b>data</b> <b>and</b> model sizes in retrieval tasks. In this study, we investigate whether the performance of <b>dense</b> <b>retrieval</b> models follows the <b>scaling</b> <b>law</b> as other neural models. We propose to use contrastive log-likelihood as the evaluation metric and conduct extensive experiments with <b>dense</b> <b>retrieval</b> models implemented with different numbers of parameters and trained with different amounts of annotated <b>data.</b> <b>Results</b> indicate that, under our settings, the performance of <b>dense</b> <b>retrieval</b> models follows a precise power-law <b>scaling</b> <b>related</b> to the model size and the number of annotations. Additionally, we examine <b>scaling</b> <b>with</b> prevalent <b>data</b> <b>augmentation</b> methods to assess the impact of annotation quality, and apply the <b>scaling</b> <b>law</b> to find the best resource allocation strategy under a budget constraint. We believe that these insights will significantly contribute to understanding the <b>scaling</b> <b>effect</b> of <b>dense</b> <b>retrieval</b> models and offer meaningful guidance for future research endeavors.</p></p class="citation"></blockquote><h3 id=618--48318-common-sense-enhanced-knowledge-based-recommendation-with-large-language-model-shenghao-yang-et-al-2024>(6/18 | 48/318) Common Sense Enhanced Knowledge-based Recommendation with Large Language Model (Shenghao Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shenghao Yang, Weizhi Ma, Peijie Sun, Min Zhang, Qingyao Ai, Yiqun Liu, Mingchen Cai. (2024)<br><strong>Common Sense Enhanced Knowledge-based Recommendation with Large Language Model</strong><br><button class=copy-to-clipboard title="Common Sense Enhanced Knowledge-based Recommendation with Large Language Model" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 38<br>Keywords: Graph, Knowledge Graph, Mutual Information, Recommendation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18325v1.pdf filename=2403.18325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge-based</b> <b>recommendation</b> models effectively alleviate the data sparsity issue leveraging the side information in the <b>knowledge</b> <b>graph,</b> and have achieved considerable performance. Nevertheless, the <b>knowledge</b> <b>graphs</b> used in previous work, namely metadata-based <b>knowledge</b> <b>graphs,</b> are usually constructed based on the attributes of items and co-occurring relations (e.g., also buy), in which the former provides limited information and the latter relies on sufficient interaction data and still suffers from cold start issue. Common sense, as a form of <b>knowledge</b> <b>with</b> generality and universality, can be used as a supplement to the metadata-based <b>knowledge</b> <b>graph</b> and provides a new perspective for modeling users&rsquo; preferences. Recently, benefiting from the emergent world <b>knowledge</b> <b>of</b> the <b>large</b> <b>language</b> <b>model,</b> efficient acquisition of common sense has become possible. In this paper, we propose a novel <b>knowledge-based</b> <b>recommendation</b> framework incorporating common sense, CSRec, which can be flexibly coupled to existing <b>knowledge-based</b> <b>methods.</b> Considering the challenge of the <b>knowledge</b> <b>gap</b> between the common sense-based <b>knowledge</b> <b>graph</b> and metadata-based <b>knowledge</b> <b>graph,</b> we propose a <b>knowledge</b> <b>fusion</b> approach based on <b>mutual</b> <b>information</b> maximization theory. Experimental results on public datasets demonstrate that our approach significantly improves the performance of existing <b>knowledge-based</b> <b>recommendation</b> models.</p></p class="citation"></blockquote><h3 id=718--49318-a-novel-behavior-based-recommendation-system-for-e-commerce-reza-barzegar-nozari-et-al-2024>(7/18 | 49/318) A Novel Behavior-Based Recommendation System for E-commerce (Reza Barzegar Nozari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reza Barzegar Nozari, Mahdi Divsalar, Sepehr Akbarzadeh Abkenar, Mohammadreza Fadavi Amiri, Ali Divsalar. (2024)<br><strong>A Novel Behavior-Based Recommendation System for E-commerce</strong><br><button class=copy-to-clipboard title="A Novel Behavior-Based Recommendation System for E-commerce" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-HC, cs-IR, cs.IR<br>Keyword Score: 36<br>Keywords: Benchmarking, Clustering, Recommendation, Recommender System, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18536v1.pdf filename=2403.18536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The majority of existing <b>recommender</b> <b>systems</b> rely on user ratings, which are limited by the lack of user collaboration and the sparsity problem. To address these issues, this study proposes a behavior-based <b>recommender</b> <b>system</b> that leverages customers&rsquo; natural behaviors, such as browsing and clicking, on e-commerce platforms. The proposed <b>recommendation</b> system involves <b>clustering</b> active customers, determining neighborhoods, collecting similar users, calculating product reputation based on similar users, and recommending high-reputation products. To overcome the complexity of customer behaviors and traditional <b>clustering</b> methods, an <b>unsupervised</b> <b>clustering</b> approach based on product categories is developed to enhance the <b>recommendation</b> methodology. This study makes notable contributions in several aspects. Firstly, a groundbreaking behavior-based <b>recommendation</b> methodology is developed, incorporating customer behavior to generate accurate and tailored <b>recommendations</b> leading to improved customer satisfaction and engagement. Secondly, an original <b>unsupervised</b> <b>clustering</b> method, focusing on product categories, enables more precise <b>clustering</b> and facilitates accurate <b>recommendations.</b> Finally, an approach to determine neighborhoods for active customers within clusters is established, ensuring grouping of customers with similar behavioral patterns to enhance <b>recommendation</b> accuracy and relevance. The proposed <b>recommendation</b> methodology and <b>clustering</b> method contribute to improved <b>recommendation</b> performance, offering valuable insights for researchers and practitioners in the field of e-commerce <b>recommendation</b> systems. Additionally, the proposed method outperforms <b>benchmark</b> methods in experiments conducted using a behavior dataset from the well-known e-commerce site Alibaba.</p></p class="citation"></blockquote><h3 id=818--50318-rankmamba-benchmarking-mambas-document-ranking-performance-in-the-era-of-transformers-zhichao-xu-2024>(8/18 | 50/318) RankMamba, Benchmarking Mamba&rsquo;s Document Ranking Performance in the Era of Transformers (Zhichao Xu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichao Xu. (2024)<br><strong>RankMamba, Benchmarking Mamba&rsquo;s Document Ranking Performance in the Era of Transformers</strong><br><button class=copy-to-clipboard title="RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Transformer, Document Ranking, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18276v1.pdf filename=2403.18276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer</b> structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and <b>information</b> <b>retrieval</b> (IR). <b>Transformer</b> architecture&rsquo;s core mechanism &ndash; attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism&rsquo;s scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure &ndash; Mamba, which is based on state space models, has achieved <b>transformer-equivalent</b> performance in multiple sequence modeling tasks. In this work, we examine \mamba&rsquo;s efficacy through the lens of a classical IR task &ndash; <b>document</b> <b>ranking.</b> A reranker model takes a query and a <b>document</b> <b>as</b> input, and predicts a scalar relevance score. This task demands the language model&rsquo;s ability to comprehend lengthy contextual inputs and to capture the interaction between query and <b>document</b> <b>tokens.</b> We find that (1) Mamba models achieve competitive performance compared to <b>transformer-based</b> models with the same training recipe; (2) but also have a lower training throughput in comparison to efficient <b>transformer</b> implementations such as flash attention. We hope this study can serve as a starting point to explore Mamba models in other classical IR tasks. Our code implementation and trained checkpoints are made public to facilitate reproducibility.\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.</p></p class="citation"></blockquote><h3 id=918--51318-modeling-sustainable-city-trips-integrating-co2-emissions-popularity-and-seasonality-into-tourism-recommender-systems-ashmi-banerjee-et-al-2024>(9/18 | 51/318) Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity, and Seasonality into Tourism Recommender Systems (Ashmi Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashmi Banerjee, Tunar Mahmudov, Emil Adler, Fitri Nur Aisyah, Wolfgang Wörndl. (2024)<br><strong>Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity, and Seasonality into Tourism Recommender Systems</strong><br><button class=copy-to-clipboard title="Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity, and Seasonality into Tourism Recommender Systems" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Fairness, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18604v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18604v1.pdf filename=2403.18604v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In an era of information overload and complex decision-making processes, <b>Recommender</b> <b>Systems</b> (RS) have emerged as indispensable tools across diverse domains, particularly travel and tourism. These systems simplify trip planning by offering personalized <b>recommendations</b> that consider individual preferences and address broader challenges like seasonality, travel regulations, and capacity constraints. The intricacies of the tourism domain, characterized by multiple stakeholders, including consumers, item providers, platforms, and society, underscore the complexity of achieving balance among diverse interests. Although previous research has focused on <b>fairness</b> in Tourism <b>Recommender</b> <b>Systems</b> (TRS) from a multistakeholder perspective, limited work has focused on generating sustainable <b>recommendations.</b> Our paper introduces a novel approach for assigning a sustainability indicator (SF index) for city trips accessible from the users&rsquo; starting point, integrating Co2e analysis, destination popularity, and seasonal demand. Our methodology involves comprehensive data gathering on transportation modes and emissions, complemented by analyses of destination popularity and seasonal demand. A user study validates our index, showcasing its practicality and efficacy in providing well-rounded and sustainable city trip <b>recommendations.</b> Our findings contribute significantly to the evolution of responsible tourism strategies, harmonizing the interests of tourists, local communities, and the environment while paving the way for future research in responsible and equitable tourism practices.</p></p class="citation"></blockquote><h3 id=1018--52318-improving-content-recommendation-knowledge-graph-based-semantic-contrastive-learning-for-diversity-and-cold-start-users-yejin-kim-et-al-2024>(10/18 | 52/318) Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users (Yejin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yejin Kim, Scott Rome, Kevin Foley, Mayur Nankani, Rimon Melamed, Javier Morales, Abhay Yadav, Maria Peifer, Sardar Hamidian, H. Howie Huang. (2024)<br><strong>Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users</strong><br><button class=copy-to-clipboard title="Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 28<br>Keywords: Graph, Contrastive Learning, Knowledge Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18667v1.pdf filename=2403.18667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the challenges related to data sparsity, cold-start problems, and diversity in <b>recommendation</b> systems is both crucial and demanding. Many current solutions leverage <b>knowledge</b> <b>graphs</b> to tackle these issues by combining both item-based and user-item collaborative signals. A common trend in these approaches focuses on improving ranking performance at the cost of escalating model complexity, reducing diversity, and complicating the task. It is essential to provide <b>recommendations</b> that are both personalized and diverse, rather than solely relying on achieving high rank-based performance, such as Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task learning approach, training on user-item and item-item interactions. We apply item-based <b>contrastive</b> <b>learning</b> on descriptive text, sampling positive and negative pairs based on item metadata. Our approach allows the model to better understand the relationships between entities within the <b>knowledge</b> <b>graph</b> by utilizing semantic information from text. It leads to more accurate, relevant, and diverse user <b>recommendations</b> and a benefit that extends even to cold-start users who have few interactions with items. We perform extensive experiments on two widely used datasets to validate the effectiveness of our approach. Our findings demonstrate that jointly training user-item interactions and item-based signals using synopsis text is highly effective. Furthermore, our results provide evidence that item-based <b>contrastive</b> <b>learning</b> enhances the quality of entity embeddings, as indicated by metrics such as uniformity and alignment.</p></p class="citation"></blockquote><h3 id=1118--53318-enhanced-generative-recommendation-via-content-and-collaboration-integration-yidan-wang-et-al-2024>(11/18 | 53/318) Enhanced Generative Recommendation via Content and Collaboration Integration (Yidan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yidan Wang, Zhaochun Ren, Weiwei Sun, Jiyuan Yang, Zhixiang Liang, Xin Chen, Ruobing Xie, Su Yan, Xu Zhang, Pengjie Ren, Zhumin Chen, Xin Xin. (2024)<br><strong>Enhanced Generative Recommendation via Content and Collaboration Integration</strong><br><button class=copy-to-clipboard title="Enhanced Generative Recommendation via Content and Collaboration Integration" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Benchmarking, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18480v1.pdf filename=2403.18480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative <b>recommendation</b> has emerged as a promising paradigm aimed at augmenting <b>recommender</b> <b>systems</b> with recent advancements in generative artificial intelligence. This task has been formulated as a sequence-to-sequence generation process, wherein the input sequence encompasses data pertaining to the user&rsquo;s previously interacted items, and the output sequence denotes the generative identifier for the suggested item. However, existing generative <b>recommendation</b> approaches still encounter challenges in (i) effectively integrating user-item collaborative signals and item content information within a unified generative framework, and (ii) executing an efficient alignment between content information and collaborative signals. In this paper, we introduce content-based collaborative generation for <b>recommender</b> <b>systems,</b> denoted as ColaRec. To capture collaborative signals, the generative item identifiers are derived from a pretrained collaborative filtering model, while the user is represented through the aggregation of interacted items&rsquo; content. Subsequently, the aggregated textual description of items is fed into a language model to encapsulate content information. This integration enables ColaRec to amalgamate collaborative signals and content information within an end-to-end framework. Regarding the alignment, we propose an item indexing task to facilitate the mapping between the content-based semantic space and the interaction-based collaborative space. Additionally, a contrastive loss is introduced to ensure that items with similar collaborative GIDs possess comparable content representations, thereby enhancing alignment. To validate the efficacy of ColaRec, we conduct experiments on three <b>benchmark</b> datasets. Empirical results substantiate the superior performance of ColaRec.</p></p class="citation"></blockquote><h3 id=1218--54318-delta-pre-train-a-discriminative-encoder-for-legal-case-retrieval-via-structural-word-alignment-haitao-li-et-al-2024>(12/18 | 54/318) DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via Structural Word Alignment (Haitao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haitao Li, Qingyao Ai, Xinyan Han, Jia Chen, Qian Dong, Yiqun Liu, Chong Chen, Qi Tian. (2024)<br><strong>DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via Structural Word Alignment</strong><br><button class=copy-to-clipboard title="DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via Structural Word Alignment" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Benchmarking, Unsupervised Learning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18435v1.pdf filename=2403.18435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research demonstrates the effectiveness of using <b>pre-trained</b> <b>language</b> <b>models</b> for legal case retrieval. Most of the existing works focus on improving the representation ability for the contextualized embedding of the [CLS] token and calculate relevance using textual semantic similarity. However, in the legal domain, textual semantic similarity does not always imply that the cases are relevant enough. Instead, relevance in legal cases primarily depends on the similarity of key facts that impact the final judgment. Without proper treatments, the discriminative ability of learned representations could be limited since legal cases are lengthy and contain numerous non-key facts. To this end, we introduce DELTA, a discriminative model designed for legal case retrieval. The basic idea involves pinpointing key facts in legal cases and pulling the contextualized embedding of the [CLS] token closer to the key facts while pushing away from the non-key facts, which can warm up the case embedding space in an <b>unsupervised</b> manner. To be specific, this study brings the word alignment mechanism to the contextual masked auto-encoder. First, we leverage shallow decoders to create information bottlenecks, aiming to enhance the representation ability. Second, we employ the deep decoder to enable translation between different structures, with the goal of pinpointing key facts to enhance discriminative ability. Comprehensive experiments conducted on publicly available legal <b>benchmarks</b> show that our approach can outperform existing state-of-the-art methods in legal case retrieval. It provides a new perspective on the in-depth understanding and processing of legal case documents.</p></p class="citation"></blockquote><h3 id=1318--55318-a-recommender-system-for-nft-collectibles-with-item-feature-minjoo-choi-et-al-2024>(13/18 | 55/318) A Recommender System for NFT Collectibles with Item Feature (Minjoo Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minjoo Choi, Seonmi Kim, Yejin Kim, Youngbin Lee, Joohwan Hong, Yongjae Lee. (2024)<br><strong>A Recommender System for NFT Collectibles with Item Feature</strong><br><button class=copy-to-clipboard title="A Recommender System for NFT Collectibles with Item Feature" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Graph, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18305v1.pdf filename=2403.18305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> have been actively studied and applied in various domains to deal with information overload. Although there are numerous studies on <b>recommender</b> <b>systems</b> for movies, music, and e-commerce, comparatively less attention has been paid to the <b>recommender</b> <b>system</b> for NFTs despite the continuous growth of the NFT market. This paper presents a <b>recommender</b> <b>system</b> for NFTs that utilizes a variety of data sources, from NFT transaction records to external item features, to generate precise <b>recommendations</b> that cater to individual preferences. We develop a data-efficient <b>graph-based</b> <b>recommender</b> <b>system</b> to efficiently capture the complex relationship between each item and users and generate node(item) embeddings which incorporate both node feature information and <b>graph</b> structure. Furthermore, we exploit inputs beyond user-item interactions, such as image feature, text feature, and price feature. Numerical experiments verify the performance of the <b>graph-based</b> <b>recommender</b> <b>system</b> improves significantly after utilizing all types of item features as side information, thereby outperforming all other baselines.</p></p class="citation"></blockquote><h3 id=1418--56318-high-recall-small-data-the-challenges-of-within-system-evaluation-in-a-live-legal-search-system-gineke-wiggers-et-al-2024>(14/18 | 56/318) High Recall, Small Data: The Challenges of Within-System Evaluation in a Live Legal Search System (Gineke Wiggers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gineke Wiggers, Suzan Verberne, Arjen de Vries, Roel van der Burg. (2024)<br><strong>High Recall, Small Data: The Challenges of Within-System Evaluation in a Live Legal Search System</strong><br><button class=copy-to-clipboard title="High Recall, Small Data: The Challenges of Within-System Evaluation in a Live Legal Search System" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Document Ranking, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18962v1.pdf filename=2403.18962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper illustrates some challenges of common ranking evaluation methods for legal <b>information</b> <b>retrieval</b> (IR). We show these challenges with log data from a live legal search system and two user studies. We provide an overview of aspects of legal IR, and the implications of these aspects for the expected challenges of common evaluation methods: test collections based on explicit and implicit feedback, user surveys, and A/B testing. Next, we illustrate the challenges of common evaluation methods using data from a live, commercial, legal search engine. We specifically focus on methods for monitoring the effectiveness of (continuous) changes to <b>document</b> <b>ranking</b> by a single IR system over time. We show how the combination of characteristics in legal IR systems and limited user data can lead to challenges that cause the common evaluation methods discussed to be sub-optimal. In our future work we will therefore focus on less common evaluation methods, such as cost-based evaluation models.</p></p class="citation"></blockquote><h3 id=1518--57318-a-situation-aware-enhancer-for-personalized-recommendation-jiayu-li-et-al-2024>(15/18 | 57/318) A Situation-aware Enhancer for Personalized Recommendation (Jiayu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayu Li, Peijie Sun, Chumeng Jiang, Weizhi Ma, Qingyao Ai, Min Zhang. (2024)<br><strong>A Situation-aware Enhancer for Personalized Recommendation</strong><br><button class=copy-to-clipboard title="A Situation-aware Enhancer for Personalized Recommendation" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18317v1.pdf filename=2403.18317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When users interact with <b>Recommender</b> <b>Systems</b> (RecSys), current situations, such as time, location, and environment, significantly influence their preferences. Situations serve as the background for interactions, where relationships between users and items evolve with situation changes. However, existing RecSys treat situations, users, and items on the same level. They can only model the relations between situations and users/items respectively, rather than the dynamic impact of situations on user-item associations (i.e., user preferences). In this paper, we provide a new perspective that takes situations as the preconditions for users&rsquo; interactions. This perspective allows us to separate situations from user/item representations, and capture situations&rsquo; influences over the user-item relationship, offering a more comprehensive understanding of situations. Based on it, we propose a novel Situation-Aware <b>Recommender</b> <b>Enhancer</b> (SARE), a pluggable module to integrate situations into various existing RecSys. Since users&rsquo; perception of situations and situations&rsquo; impact on preferences are both personalized, SARE includes a Personalized Situation Fusion (PSF) and a User-Conditioned Preference Encoder (UCPE) to model the perception and impact of situations, respectively. We conduct experiments of applying SARE on seven backbones in various settings on two real-world datasets. Experimental results indicate that SARE improves the <b>recommendation</b> performances significantly compared with backbones and SOTA situation-aware baselines.</p></p class="citation"></blockquote><h3 id=1618--58318-decoy-effect-in-search-interaction-understanding-user-behavior-and-measuring-system-vulnerability-nuo-chen-et-al-2024>(16/18 | 58/318) Decoy Effect In Search Interaction: Understanding User Behavior and Measuring System Vulnerability (Nuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nuo Chen, Jiqun Liu, Hanpei Fang, Yuankai Luo, Tetsuya Sakai, Xiao-Ming Wu. (2024)<br><strong>Decoy Effect In Search Interaction: Understanding User Behavior and Measuring System Vulnerability</strong><br><button class=copy-to-clipboard title="Decoy Effect In Search Interaction: Understanding User Behavior and Measuring System Vulnerability" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18462v1.pdf filename=2403.18462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study examines the decoy effect&rsquo;s underexplored influence on user search interactions and methods for measuring <b>information</b> <b>retrieval</b> (IR) systems&rsquo; vulnerability to this effect. It explores how decoy results alter users&rsquo; interactions on search engine result pages, focusing on metrics like click-through likelihood, browsing time, and perceived document usefulness. By analyzing user interaction logs from multiple datasets, the study demonstrates that decoy results significantly affect users&rsquo; behavior and perceptions. Furthermore, it investigates how different levels of task difficulty and user knowledge modify the decoy effect&rsquo;s impact, finding that easier tasks and lower knowledge levels lead to higher engagement with target documents. In terms of IR system evaluation, the study introduces the DEJA-VU metric to assess systems&rsquo; susceptibility to the decoy effect, testing it on specific retrieval tasks. The results show differences in systems&rsquo; effectiveness and vulnerability, contributing to our understanding of cognitive biases in search behavior and suggesting pathways for creating more balanced and bias-aware IR evaluations.</p></p class="citation"></blockquote><h3 id=1718--59318-improving-out-of-vocabulary-handling-in-recommendation-systems-william-shiao-et-al-2024>(17/18 | 59/318) Improving Out-of-Vocabulary Handling in Recommendation Systems (William Shiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Shiao, Mingxuan Ju, Zhichun Guo, Xin Chen, Evangelos Papalexakis, Tong Zhao, Neil Shah, Yozen Liu. (2024)<br><strong>Improving Out-of-Vocabulary Handling in Recommendation Systems</strong><br><button class=copy-to-clipboard title="Improving Out-of-Vocabulary Handling in Recommendation Systems" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18280v1.pdf filename=2403.18280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommendation</b> systems (RS) are an increasingly relevant area for both academic and industry researchers, given their widespread impact on the daily online experiences of billions of users. One common issue in real RS is the cold-start problem, where users and items may not contain enough information to produce high-quality <b>recommendations.</b> This work focuses on a complementary problem: recommending new users and items unseen (out-of-vocabulary, or OOV) at training time. This setting is known as the inductive setting and is especially problematic for factorization-based models, which rely on encoding only those users/items seen at training time with fixed parameter vectors. Many existing solutions applied in practice are often naive, such as assigning OOV users/items to random buckets. In this work, we tackle this problem and propose approaches that better leverage available user/item features to improve OOV handling at the embedding table level. We discuss general-purpose plug-and-play approaches that are easily applicable to most RS models and improve inductive performance without negatively impacting transductive model performance. We extensively evaluate 9 OOV embedding methods on 5 models across 4 datasets (spanning different domains). One of these datasets is a proprietary production dataset from a prominent RS employed by a large social platform serving hundreds of millions of daily active users. In our experiments, we find that several proposed methods that exploit feature similarity using LSH consistently outperform alternatives on most model-dataset combinations, with the best method showing a mean improvement of 3.74% over the industry standard baseline in inductive performance. We release our code and hope our work helps practitioners make more informed decisions when handling OOV for their RS and further inspires academic research into improving OOV support in RS.</p></p class="citation"></blockquote><h3 id=1818--60318-one-backpropagation-in-two-tower-recommendation-models-erjia-chen-et-al-2024>(18/18 | 60/318) One Backpropagation in Two Tower Recommendation Models (Erjia Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erjia Chen, Bang Wang. (2024)<br><strong>One Backpropagation in Two Tower Recommendation Models</strong><br><button class=copy-to-clipboard title="One Backpropagation in Two Tower Recommendation Models" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18227v1.pdf filename=2403.18227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have witnessed extensive researches on developing two tower <b>recommendation</b> models for relieving information overload. Four building modules can be identified in such models, namely, user-item encoding, negative sampling, loss computing and back-propagation updating. To the best of our knowledge, existing algorithms have researched only on the first three modules, yet neglecting the backpropagation module. They all adopt a kind of two backpropagation strategy, which are based on an implicit assumption of equally treating users and items in the training phase. In this paper, we challenge such an equal training assumption and propose a novel one backpropagation updating strategy, which keeps the normal gradient backpropagation for the item encoding tower, but cuts off the backpropagation for the user encoding tower. Instead, we propose a moving-aggregation updating strategy to update a user encoding in each training epoch. Except the proposed backpropagation updating module, we implement the other three modules with the most straightforward choices. Experiments on four public datasets validate the effectiveness and efficiency of our model in terms of improved <b>recommendation</b> performance and reduced computation overload over the state-of-the-art competitors.</p></p class="citation"></blockquote><h2 id=csro-20>cs.RO (20)</h2><h3 id=120--61318-physicsassistant-an-llm-powered-interactive-learning-robot-for-physics-lab-investigations-ehsan-latif-et-al-2024>(1/20 | 61/318) PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics Lab Investigations (Ehsan Latif et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Latif, Ramviyas Parasuraman, Xiaoming Zhai. (2024)<br><strong>PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics Lab Investigations</strong><br><button class=copy-to-clipboard title="PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics Lab Investigations" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 96<br>Keywords: Object Detection, Multi-modal, Multi-modal, BLOOM, GPT, GPT-4, Automatic Speech Recognition, Chatbot, Natural Language Understanding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18721v1.pdf filename=2403.18721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot systems in education can leverage <b>Large</b> <b>language</b> <b>models&rsquo;</b> <b>(LLMs)</b> <b>natural</b> <b>language</b> <b>understanding</b> capabilities to provide assistance and facilitate learning. This paper proposes a <b>multimodal</b> interactive robot (PhysicsAssistant) built on YOLOv8 <b>object</b> <b>detection,</b> cameras, <b>speech</b> <b>recognition,</b> and <b>chatbot</b> using <b>LLM</b> to provide assistance to students&rsquo; physics labs. We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert. The Expert rates the assistants&rsquo; responses to student queries on a 0-4 scale based on <b>Bloom&rsquo;s</b> taxonomy to provide educational support. We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with <b>GPT-4</b> and found that the human expert rating of both systems for factual understanding is the same. However, the rating of <b>GPT-4</b> for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p &lt; 0.05). However, the response time of <b>GPT-4</b> is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p &lt; 0.05). Hence, despite the relatively lower response quality of PhysicsAssistant than <b>GPT-4,</b> it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers&rsquo; labor to assist with repetitive tasks. To the best of our knowledge, this is the first attempt to build such an interactive <b>multimodal</b> robotic assistant for K-12 science (physics) education.</p></p class="citation"></blockquote><h3 id=220--62318-3p-llm-probabilistic-path-planning-using-large-language-model-for-autonomous-robot-navigation-ehsan-latif-2024>(2/20 | 62/318) 3P-LLM: Probabilistic Path Planning using Large Language Model for Autonomous Robot Navigation (Ehsan Latif, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Latif. (2024)<br><strong>3P-LLM: Probabilistic Path Planning using Large Language Model for Autonomous Robot Navigation</strong><br><button class=copy-to-clipboard title="3P-LLM: Probabilistic Path Planning using Large Language Model for Autonomous Robot Navigation" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 80<br>Keywords: Few-shot, Few-shot Learning, GPT, GPT-3, GPT-3.5, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18778v1.pdf filename=2403.18778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Much worldly semantic knowledge can be encoded in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Such information could be of great use to robots that want to carry out high-level, temporally extended commands stated in natural language. However, the lack of real-world experience that language models have is a key limitation that makes it challenging to use them for decision-making inside a particular embodiment. This research assesses the feasibility of using <b>LLM</b> <b>(GPT-3.5-turbo</b> <b>chatbot</b> by OpenAI) for robotic path planning. The shortcomings of conventional approaches to managing complex environments and developing trustworthy plans for shifting environmental conditions serve as the driving force behind the research. Due to the sophisticated natural language processing abilities of <b>LLM,</b> the capacity to provide effective and adaptive path-planning algorithms in real-time, great accuracy, and <b>few-shot</b> <b>learning</b> capabilities, <b>GPT-3.5-turbo</b> is well suited for path planning in robotics. In numerous simulated scenarios, the research compares the performance of <b>GPT-3.5-turbo</b> with that of state-of-the-art path planners like Rapidly Exploring Random Tree (RRT) and A*. We observed that <b>GPT-3.5-turbo</b> is able to provide real-time path planning feedback to the robot and outperforms its counterparts. This paper establishes the foundation for <b>LLM-powered</b> path planning for robotic systems.</p></p class="citation"></blockquote><h3 id=320--63318-multi-agv-path-planning-method-via-reinforcement-learning-and-particle-filters-shao-shuo-2024>(3/20 | 63/318) Multi-AGV Path Planning Method via Reinforcement Learning and Particle Filters (Shao Shuo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shao Shuo. (2024)<br><strong>Multi-AGV Path Planning Method via Reinforcement Learning and Particle Filters</strong><br><button class=copy-to-clipboard title="Multi-AGV Path Planning Method via Reinforcement Learning and Particle Filters" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Simulation, Simulator, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18236v1.pdf filename=2403.18236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>Reinforcement</b> <b>Learning</b> (RL) algorithm, renowned for its robust learning capability and search stability, has garnered significant attention and found extensive application in Automated Guided Vehicle (AGV) path planning. However, RL planning algorithms encounter challenges <b>stemming</b> from the substantial variance of neural networks caused by environmental instability and significant fluctuations in system structure. These challenges manifest in slow convergence speed and low learning efficiency. To tackle this issue, this paper presents the Particle Filter-Double Deep Q-Network (PF-DDQN) approach, which incorporates the Particle Filter (PF) into multi-AGV <b>reinforcement</b> <b>learning</b> path planning. The PF-DDQN method leverages the imprecise weight values of the network as state values to formulate the state space equation. Through the iterative fusion process of neural networks and particle filters, the DDQN model is optimized to acquire the optimal true weight values, thus enhancing the algorithm&rsquo;s efficiency. The proposed method&rsquo;s effectiveness and superiority are validated through numerical <b>simulations.</b> Overall, the <b>simulation</b> results demonstrate that the proposed algorithm surpasses the traditional DDQN algorithm in terms of path planning superiority and training time indicators by 92.62% and 76.88%, respectively. In conclusion, the PF-DDQN method addresses the challenges encountered by RL planning algorithms in AGV path planning. By integrating the Particle Filter and optimizing the DDQN model, the proposed method achieves enhanced efficiency and outperforms the traditional DDQN algorithm in terms of path planning superiority and training time indicators.</p></p class="citation"></blockquote><h3 id=420--64318-vision-based-force-estimation-for-minimally-invasive-telesurgery-through-contact-detection-and-local-stiffness-models-shuyuan-yang-et-al-2024>(4/20 | 64/318) Vision-Based Force Estimation for Minimally Invasive Telesurgery Through Contact Detection and Local Stiffness Models (Shuyuan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuyuan Yang, My H. Le, Kyle R. Golobish, Juan C. Beaver, Zonghe Chua. (2024)<br><strong>Vision-Based Force Estimation for Minimally Invasive Telesurgery Through Contact Detection and Local Stiffness Models</strong><br><button class=copy-to-clipboard title="Vision-Based Force Estimation for Minimally Invasive Telesurgery Through Contact Detection and Local Stiffness Models" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18172v1.pdf filename=2403.18172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In minimally invasive telesurgery, obtaining accurate force information is difficult due to the complexities of in-vivo end effector force sensing. This constrains development and implementation of haptic feedback and force-based automated performance metrics, respectively. Vision-based force sensing approaches using deep learning are a promising alternative to intrinsic end effector force sensing. However, they have limited ability to generalize to novel scenarios, and require learning on high-quality force sensor training data that can be difficult to obtain. To address these challenges, this paper presents a novel vision-based contact-conditional approach for force estimation in telesurgical environments. Our method leverages <b>supervised</b> <b>learning</b> with human labels and end effector position data to train deep neural networks. Predictions from these trained models are optionally combined with robot joint torque information to estimate forces indirectly from visual data. We <b>benchmark</b> our method against ground truth force sensor data and demonstrate generality by <b>fine-tuning</b> to novel surgical scenarios in a data-efficient manner. Our methods demonstrated greater than 90% accuracy on contact detection and less than 10% force prediction error. These results suggest potential usefulness of contact-conditional force estimation for sensory substitution haptic feedback and tissue handling skill evaluation in clinical settings.</p></p class="citation"></blockquote><h3 id=520--65318-gaussian-process-based-traversability-analysis-for-terrain-mapless-navigation-abe-leininger-et-al-2024>(5/20 | 65/318) Gaussian Process-based Traversability Analysis for Terrain Mapless Navigation (Abe Leininger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abe Leininger, Mahmoud Ali, Hassan Jardali, Lantao Liu. (2024)<br><strong>Gaussian Process-based Traversability Analysis for Terrain Mapless Navigation</strong><br><button class=copy-to-clipboard title="Gaussian Process-based Traversability Analysis for Terrain Mapless Navigation" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19010v1.pdf filename=2403.19010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient navigation through uneven terrain remains a challenging endeavor for autonomous robots. We propose a new geometric-based uneven terrain mapless navigation framework combining a Sparse <b>Gaussian</b> <b>Process</b> (SGP) local map with a Rapidly-Exploring Random Tree* (RRT*) planner. Our approach begins with the generation of a high-resolution SGP local map, providing an interpolated representation of the robot&rsquo;s immediate environment. This map captures crucial environmental variations, including height, uncertainties, and slope characteristics. Subsequently, we construct a traversability map based on the SGP representation to guide our planning process. The RRT* planner efficiently generates real-time navigation paths, avoiding untraversable terrain in pursuit of the goal. This combination of SGP-based terrain interpretation and RRT* planning enables ground robots to safely navigate environments with varying elevations and steep obstacles. We evaluate the performance of our proposed approach through robust <b>simulation</b> testing, highlighting its effectiveness in achieving safe and efficient navigation compared to existing methods.</p></p class="citation"></blockquote><h3 id=620--66318-mldt-multi-level-decomposition-for-complex-long-horizon-robotic-task-planning-with-open-source-large-language-model-yike-wu-et-al-2024>(6/20 | 66/318) MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model (Yike Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yike Wu, Jiatao Zhang, Nan Hu, LanLing Tang, Guilin Qi, Jun Shao, Jie Ren, Wei Song. (2024)<br><strong>MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model</strong><br><button class=copy-to-clipboard title="MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18760v1.pdf filename=2403.18760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of data-driven AI technology, the application of open-source <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in robotic task planning represents a significant milestone. Recent robotic task planning methods based on open-source <b>LLMs</b> typically leverage vast task planning datasets to enhance models&rsquo; planning abilities. While these methods show promise, they struggle with complex long-horizon tasks, which require comprehending more context and generating longer action sequences. This paper addresses this limitation by proposing MLDT, theMulti-Level Decomposition Task planning method. This method innovatively decomposes tasks at the goal-level, task-level, and action-level to mitigate the challenge of complex long-horizon tasks. In order to enhance open-source <b>LLMs&rsquo;</b> planning abilities, we introduce a goal-sensitive corpus generation method to create high-quality training data and conduct <b>instruction</b> <b>tuning</b> on the generated corpus. Since the complexity of the existing datasets is not high enough, we construct a more challenging dataset, LongTasks, to specifically evaluate planning ability on complex long-horizon tasks. We evaluate our method using various <b>LLMs</b> on four datasets in VirtualHome. Our results demonstrate a significant performance enhancement in robotic task planning, showcasing MLDT&rsquo;s effectiveness in overcoming the limitations of existing methods based on open-source <b>LLMs</b> as well as its practicality in complex, real-world scenarios.</p></p class="citation"></blockquote><h3 id=720--67318-inverse-kinematics-learning-of-a-continuum-manipulator-using-limited-real-time-data-alok-ranjan-sahoo-et-al-2024>(7/20 | 67/318) Inverse kinematics learning of a continuum manipulator using limited real time data (Alok Ranjan Sahoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alok Ranjan Sahoo, Pavan Chakraborty. (2024)<br><strong>Inverse kinematics learning of a continuum manipulator using limited real time data</strong><br><button class=copy-to-clipboard title="Inverse kinematics learning of a continuum manipulator using limited real time data" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Meta Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18456v1.pdf filename=2403.18456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data driven control of a continuum manipulator requires a lot of data for training but generating sufficient amount of real time data is not cost efficient. Random actuation of the manipulator can also be unsafe sometimes. <b>Meta</b> <b>learning</b> has been used successfully to adapt to a new environment. Hence, this paper tries to solve the above mentioned problem using <b>meta</b> <b>learning.</b> We consider two cases for that. First, this paper proposes a method to use <b>simulation</b> data for training the model using MAML(Model-Agnostic <b>Meta-Learning).</b> <b>Then,</b> it adapts to the real world using gradient steps. Secondly,if the <b>simulation</b> model is not available or difficult to formulate, then we propose a CGAN(Conditional Generative adversial network)-MAML based method for it. The model is trained using a small amount of real time data and augmented data for different loading conditions. Then, adaptation is done in the real environment. It has been found out from the experiments that the relative positioning error for both the cases are below 3%. The proposed models are experimentally verified on a real continuum manipulator.</p></p class="citation"></blockquote><h3 id=820--68318-lord-large-models-based-opposite-reward-design-for-autonomous-driving-xin-ye-et-al-2024>(8/20 | 68/318) LORD: Large Models based Opposite Reward Design for Autonomous Driving (Xin Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Ye, Feng Tao, Abhirup Mallik, Burhaneddin Yaman, Liu Ren. (2024)<br><strong>LORD: Large Models based Opposite Reward Design for Autonomous Driving</strong><br><button class=copy-to-clipboard title="LORD: Large Models based Opposite Reward Design for Autonomous Driving" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18965v1.pdf filename=2403.18965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) based autonomous driving has emerged as a promising alternative to data-driven imitation learning approaches. However, crafting effective reward functions for RL poses challenges due to the complexity of defining and quantifying good driving behaviors across diverse scenarios. Recently, large pretrained models have gained significant attention as <b>zero-shot</b> reward models for tasks specified with desired linguistic goals. However, the desired linguistic goals for autonomous driving such as &ldquo;drive safely&rdquo; are ambiguous and incomprehensible by pretrained models. On the other hand, undesired linguistic goals like &ldquo;collision&rdquo; are more concrete and tractable. In this work, we introduce LORD, a novel large models based opposite reward design through undesired linguistic goals to enable the efficient use of large pretrained models as <b>zero-shot</b> reward models. Through extensive experiments, our proposed framework shows its efficiency in leveraging the power of large pretrained models for achieving safe and enhanced autonomous driving. Moreover, the proposed approach shows improved generalization capabilities as it outperforms counterpart methods across diverse and challenging driving scenarios.</p></p class="citation"></blockquote><h3 id=920--69318-cobos-constraint-based-online-scheduler-for-human-robot-collaboration-marina-ionova-et-al-2024>(9/20 | 69/318) CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration (Marina Ionova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marina Ionova, Jan Kristof Behrens. (2024)<br><strong>CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration</strong><br><button class=copy-to-clipboard title="CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18459v1.pdf filename=2403.18459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Assembly processes involving humans and robots are challenging scenarios because the individual activities and access to shared workspace have to be coordinated. Fixed robot programs leave no room to diverge from a fixed protocol. Working on such a process can be stressful for the user and lead to ineffective behavior or failure. We propose a novel approach of online constraint-based scheduling in a reactive execution control framework facilitating behavior trees called CoBOS. This allows the robot to adapt to uncertain events such as delayed activity completions and activity selection (by the human). The user will experience less stress as the robotic coworkers adapt their behavior to best complement the human-selected activities to complete the common task. In addition to the improved working conditions, our algorithm leads to increased efficiency, even in highly uncertain scenarios. We evaluate our algorithm using a probabilistic <b>simulation</b> study with 56000 experiments. We outperform all baselines by a margin of 4-10%. Initial real robot experiments using a Franka Emika Panda robot and human tracking based on HTC Vive VR gloves look promising.</p></p class="citation"></blockquote><h3 id=1020--70318-hyrrt-connect-a-bidirectional-rapidly-exploring-random-trees-motion-planning-algorithm-for-hybrid-systems-nan-wang-et-al-2024>(10/20 | 70/318) HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion Planning Algorithm for Hybrid Systems (Nan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nan Wang, Ricardo G. Sanfelice. (2024)<br><strong>HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion Planning Algorithm for Hybrid Systems</strong><br><button class=copy-to-clipboard title="HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion Planning Algorithm for Hybrid Systems" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18413v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18413v2.pdf filename=2403.18413v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a bidirectional rapidly-exploring random trees (RRT) algorithm to solve the motion planning problem for hybrid systems. The proposed algorithm, called HyRRT-Connect, propagates in both forward and backward directions in hybrid time until an overlap between the forward and backward propagation results is detected. Then, HyRRT-Connect constructs a motion plan through the reversal and concatenation of functions defined on hybrid time domains, ensuring the motion plan thoroughly satisfies the given hybrid dynamics. To address the potential discontinuity along the flow caused by tolerating some distance between the forward and backward partial motion plans, we reconstruct the backward partial motion plan by a forward-in-hybrid-time <b>simulation</b> from the final state of the forward partial motion plan. By applying the reversed input of the backward partial motion plan, the reconstruction process effectively eliminates the discontinuity and ensures that as the tolerance distance decreases to zero, the distance between the endpoint of the reconstructed motion plan and the final state set approaches zero. The proposed algorithm is applied to an actuated bouncing ball example and a walking robot example so as to highlight its generality and computational improvement.</p></p class="citation"></blockquote><h3 id=1120--71318-uncertainty-aware-deployment-of-pre-trained-language-conditioned-imitation-learning-policies-bo-wu-et-al-2024>(11/20 | 71/318) Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies (Bo Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Wu, Bruce D. Lee, Kostas Daniilidis, Bernadette Bucher, Nikolai Matni. (2024)<br><strong>Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies</strong><br><button class=copy-to-clipboard title="Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18222v1.pdf filename=2403.18222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in <b>simulation</b> using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: <a href=https://github.com/BobWu1998/uncertainty_quant_all.git>https://github.com/BobWu1998/uncertainty_quant_all.git</a></p></p class="citation"></blockquote><h3 id=1220--72318-online-embedding-multi-scale-clip-features-into-3d-maps-shun-taguchi-et-al-2024>(12/20 | 72/318) Online Embedding Multi-Scale CLIP Features into 3D Maps (Shun Taguchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shun Taguchi, Hideki Deguchi. (2024)<br><strong>Online Embedding Multi-Scale CLIP Features into 3D Maps</strong><br><button class=copy-to-clipboard title="Online Embedding Multi-Scale CLIP Features into 3D Maps" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Multi-modal, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18178v1.pdf filename=2403.18178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a novel approach to online embedding of multi-scale CLIP (Contrastive Language-Image Pre-Training) features into 3D maps. By harnessing CLIP, this methodology surpasses the constraints of conventional vocabulary-limited methods and enables the incorporation of semantic information into the resultant maps. While recent approaches have explored the embedding of <b>multi-modal</b> features in maps, they often impose significant computational costs, lacking practicality for exploring unfamiliar environments in real time. Our approach tackles these challenges by efficiently computing and embedding multi-scale CLIP features, thereby facilitating the exploration of unfamiliar environments through real-time map generation. Moreover, the embedding CLIP features into the resultant maps makes offline retrieval via linguistic queries feasible. In essence, our approach simultaneously achieves real-time object search and mapping of unfamiliar environments. Additionally, we propose a <b>zero-shot</b> object-goal navigation system based on our mapping approach, and we validate its efficacy through object-goal navigation, offline object retrieval, and multi-object-goal navigation in both simulated environments and real robot experiments. The findings demonstrate that our method not only exhibits swifter performance than state-of-the-art mapping methods but also surpasses them in terms of the success rate of object-goal navigation tasks.</p></p class="citation"></blockquote><h3 id=1320--73318-towards-human-centered-construction-robotics-an-rl-driven-companion-robot-for-contextually-assisting-carpentry-workers-yuning-wu-et-al-2024>(13/20 | 73/318) Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers (Yuning Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuning Wu, Jiaying Wei, Jean Oh, Daniel Cardoso Llach. (2024)<br><strong>Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers</strong><br><button class=copy-to-clipboard title="Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-HC, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19060v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19060v2.pdf filename=2403.19060v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a &ldquo;work companion rover&rdquo; designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor&rsquo;s skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual <b>Reinforcement</b> <b>Learning</b> (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive and collaborative human-robot workforce.</p></p class="citation"></blockquote><h3 id=1420--74318-cat-constraints-as-terminations-for-legged-locomotion-reinforcement-learning-elliot-chane-sane-et-al-2024>(14/20 | 74/318) CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning (Elliot Chane-Sane et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elliot Chane-Sane, Pierre-Alexandre Leziart, Thomas Flayols, Olivier Stasse, Philippe Souères, Nicolas Mansard. (2024)<br><strong>CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning</strong><br><button class=copy-to-clipboard title="CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18765v1.pdf filename=2403.18765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Reinforcement</b> <b>Learning</b> (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers to broader adoption. Through empirical evaluation on the real quadruped robot Solo crossing challenging obstacles, we demonstrate that CaT provides a compelling solution for incorporating constraints into RL frameworks. Videos and code are available at <a href=https://constraints-as-terminations.github.io>https://constraints-as-terminations.github.io</a>.</p></p class="citation"></blockquote><h3 id=1520--75318-bridging-the-gap-regularized-reinforcement-learning-for-improved-classical-motion-planning-with-safety-modules-elias-goldsztejn-et-al-2024>(15/20 | 75/318) Bridging the Gap: Regularized Reinforcement Learning for Improved Classical Motion Planning with Safety Modules (Elias Goldsztejn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elias Goldsztejn, Ronen I. Brafman. (2024)<br><strong>Bridging the Gap: Regularized Reinforcement Learning for Improved Classical Motion Planning with Safety Modules</strong><br><button class=copy-to-clipboard title="Bridging the Gap: Regularized Reinforcement Learning for Improved Classical Motion Planning with Safety Modules" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18524v1.pdf filename=2403.18524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classical navigation planners can provide safe navigation, albeit often suboptimally and with hindered human norm compliance. ML-based, contemporary autonomous navigation algorithms can imitate more natural and humancompliant navigation, but usually require large and realistic datasets and do not always provide safety guarantees. We present an approach that leverages a classical algorithm to guide <b>reinforcement</b> <b>learning.</b> This greatly improves the results and convergence rate of the underlying RL algorithm and requires no human-expert demonstrations to jump-start the process. Additionally, we incorporate a practical fallback system that can switch back to a classical planner to ensure safety. The outcome is a sample efficient ML approach for mobile navigation that builds on classical algorithms, improves them to ensure human compliance, and guarantees safety.</p></p class="citation"></blockquote><h3 id=1620--76318-imaging-radar-and-lidar-image-translation-for-3-dof-extrinsic-calibration-sangwoo-jung-et-al-2024>(16/20 | 76/318) Imaging radar and LiDAR image translation for 3-DOF extrinsic calibration (Sangwoo Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangwoo Jung, Hyesu Jang, Minwoo Jung, Ayoung Kim, Myung-Hwan Jeon. (2024)<br><strong>Imaging radar and LiDAR image translation for 3-DOF extrinsic calibration</strong><br><button class=copy-to-clipboard title="Imaging radar and LiDAR image translation for 3-DOF extrinsic calibration" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18358v1.pdf filename=2403.18358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of sensor data is crucial in the field of robotics to take full advantage of the various sensors employed. One critical aspect of this integration is determining the extrinsic calibration parameters, such as the relative transformation, between each sensor. The use of data fusion between complementary sensors, such as radar and LiDAR, can provide significant benefits, particularly in harsh environments where accurate depth data is required. However, noise included in radar sensor data can make the estimation of extrinsic calibration challenging. To address this issue, we present a novel framework for the extrinsic calibration of radar and LiDAR sensors, utilizing CycleGAN as amethod of <b>image-to-image</b> <b>translation.</b> Our proposed method employs translating radar bird-eye-view <b>images</b> <b>into</b> LiDAR-style <b>images</b> <b>to</b> estimate the 3-DOF extrinsic parameters. The use of <b>image</b> <b>registration</b> techniques, as well as deskewing based on sensor odometry and B-spline interpolation, is employed to address the rolling shutter effect commonly present in spinning sensors. Our method demonstrates a notable improvement in extrinsic calibration compared to filter-based methods using the MulRan dataset.</p></p class="citation"></blockquote><h3 id=1720--77318-robokeygen-robot-pose-and-joint-angles-estimation-via-diffusion-based-3d-keypoint-generation-yang-tian-et-al-2024>(17/20 | 77/318) RoboKeyGen: Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation (Yang Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Tian, Jiyao Zhang, Guowei Huang, Bin Wang, Ping Wang, Jiangmiao Pang, Hao Dong. (2024)<br><strong>RoboKeyGen: Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation</strong><br><button class=copy-to-clipboard title="RoboKeyGen: Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18259v1.pdf filename=2403.18259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating robot pose and joint angles is significant in advanced robotics, enabling applications like robot collaboration and online hand-eye calibration.However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality.Previous methods either regress 3D keypoints directly or utilise a render&amp;compare strategy. These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem.This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 2D keypoints detection and lifting 2D keypoints to 3D. This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques.A vital component of our method is the lifting of 2D keypoints to 3D keypoints. Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions.Leveraging the robust modeling potential of <b>diffusion</b> <b>models,</b> we reframe this issue as a conditional 3D keypoints generation task. To bolster cross-camera adaptability, we introduce theNormalised Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics.Experimental results demonstrate that the proposed method outperforms the state-of-the-art render&amp;compare method and achieves higher inference speed.Furthermore, the tests accentuate our method&rsquo;s robust cross-camera generalisation capabilities.We intend to release both the dataset and code in <a href=https://nimolty.github.io/Robokeygen/>https://nimolty.github.io/Robokeygen/</a></p></p class="citation"></blockquote><h3 id=1820--78318-preference-based-planning-in-stochastic-environments-from-partially-ordered-temporal-goals-to-most-preferred-policies-hazhar-rahmani-et-al-2024>(18/20 | 78/318) Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies (Hazhar Rahmani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu. (2024)<br><strong>Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies</strong><br><button class=copy-to-clipboard title="Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-FL, cs-LO, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18212v1.pdf filename=2403.18212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes. In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes <b>(MDPs),</b> given a partially ordered preference over a set of temporally extended goals. Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP. Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP. To synthesize a most preferred policy, our technical approach includes two key steps. In the first step, we develop a procedure to transform a partially ordered preference over temporal goals into a computational model, called preference automaton, which is a semi-automaton with a partial order over acceptance conditions. In the second step, we prove that finding a most preferred policy is equivalent to computing a Pareto-optimal policy in a multi-objective MDP that is constructed from the original MDP, the preference automaton, and the chosen stochastic ordering relation. Throughout the paper, we employ running examples to illustrate the proposed preference specification and solution approaches. We demonstrate the efficacy of our algorithm using these examples, providing detailed analysis, and then discuss several potential future directions.</p></p class="citation"></blockquote><h3 id=1920--79318-sampling-based-motion-planning-with-online-racing-line-generation-for-autonomous-driving-on-three-dimensional-race-tracks-levent-ögretmen-et-al-2024>(19/20 | 79/318) Sampling-Based Motion Planning with Online Racing Line Generation for Autonomous Driving on Three-Dimensional Race Tracks (Levent Ögretmen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Levent Ögretmen, Matthias Rowold, Boris Lohmann. (2024)<br><strong>Sampling-Based Motion Planning with Online Racing Line Generation for Autonomous Driving on Three-Dimensional Race Tracks</strong><br><button class=copy-to-clipboard title="Sampling-Based Motion Planning with Online Racing Line Generation for Autonomous Driving on Three-Dimensional Race Tracks" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18643v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18643v1.pdf filename=2403.18643v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing approaches to trajectory planning for autonomous racing employ sampling-based methods, generating numerous jerk-optimal trajectories and selecting the most favorable feasible trajectory based on a cost function penalizing deviations from an offline-calculated racing line. While successful on oval tracks, these methods face limitations on complex circuits due to the simplistic <b>geometry</b> of jerk-optimal edges failing to capture the complexity of the racing line. Additionally, they only consider two-dimensional tracks, potentially neglecting or surpassing the actual dynamic potential. In this paper, we present a sampling-based local trajectory planning approach for autonomous racing that can maintain the lap time of the racing line even on complex race tracks and consider the race track&rsquo;s three-dimensional effects. In simulative experiments, we demonstrate that our approach achieves lower lap times and improved utilization of dynamic limits compared to existing approaches. We also investigate the impact of online racing line generation, in which the time-optimal solution is planned from the current vehicle state for a limited spatial horizon, in contrast to a closed racing line calculated offline. We show that combining the sampling-based planner with the online racing line generation can significantly reduce lap times in multi-vehicle scenarios.</p></p class="citation"></blockquote><h3 id=2020--80318-manipulating-neural-path-planners-via-slight-perturbations-zikang-xiong-et-al-2024>(20/20 | 80/318) Manipulating Neural Path Planners via Slight Perturbations (Zikang Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zikang Xiong, Suresh Jagannathan. (2024)<br><strong>Manipulating Neural Path Planners via Slight Perturbations</strong><br><button class=copy-to-clipboard title="Manipulating Neural Path Planners via Slight Perturbations" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18256v1.pdf filename=2403.18256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-driven neural path planners are attracting increasing interest in the robotics community. However, their neural network components typically come as <b>black</b> <b>boxes,</b> obscuring their underlying decision-making processes. Their <b>black-box</b> <b>nature</b> exposes them to the risk of being compromised via the insertion of hidden malicious behaviors. For example, an attacker may hide behaviors that, when triggered, hijack a delivery robot by guiding it to a specific (albeit wrong) destination, trapping it in a predefined region, or inducing unnecessary energy expenditure by causing the robot to repeatedly circle a region. In this paper, we propose a novel approach to specify and inject a range of hidden malicious behaviors, known as backdoors, into neural path planners. Our approach provides a concise but flexible way to define these behaviors, and we show that hidden behaviors can be triggered by slight perturbations (e.g., inserting a tiny unnoticeable object), that can nonetheless significantly compromise their integrity. We also discuss potential techniques to identify these backdoors aimed at alleviating such risks. We demonstrate our approach on both sampling-based and search-based neural path planners.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--81318-llms-in-hci-data-work-bridging-the-gap-between-information-retrieval-and-responsible-research-practices-neda-taghizadeh-serajeh-et-al-2024>(1/3 | 81/318) LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices (Neda Taghizadeh Serajeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neda Taghizadeh Serajeh, Iman Mohammadi, Vittorio Fuccella, Mattia De Rosa. (2024)<br><strong>LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices</strong><br><button class=copy-to-clipboard title="LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-IR, cs.HC<br>Keyword Score: 90<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Information Retrieval, Information Retrieval, Text Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18173v1.pdf filename=2403.18173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient and accurate <b>information</b> <b>extraction</b> from scientific papers is significant in the rapidly developing human-computer interaction research in the literature review process. Our paper introduces and analyses a new <b>information</b> <b>retrieval</b> system using state-of-the-art <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in combination with structured <b>text</b> <b>analysis</b> techniques to extract experimental data from HCI literature, emphasizing key elements. Then We analyze the challenges and risks of using <b>LLMs</b> in the world of research. We performed a comprehensive analysis on our conducted dataset, which contained the specified <b>information</b> <b>of</b> 300 CHI 2020-2022 papers, to evaluate the performance of the two <b>large</b> <b>language</b> <b>models,</b> <b>GPT-3.5</b> <b>(text-davinci-003)</b> <b>and</b> <b>Llama-2-70b,</b> paired with structured <b>text</b> <b>analysis</b> techniques. The <b>GPT-3.5</b> model gains an accuracy of 58% and a mean absolute error of 7.00. In contrast, the Llama2 model indicates an accuracy of 56% with a mean absolute error of 7.63. The ability to answer questions was also included in the system in order to work with streamlined data. By evaluating the risks and opportunities presented by <b>LLMs,</b> our work contributes to the ongoing dialogue on establishing methodological validity and ethical guidelines for <b>LLM</b> use in HCI data work.</p></p class="citation"></blockquote><h3 id=23--82318-iface-hand-over-face-gesture-recognition-leveraging-impedance-sensing-mengxi-liu-et-al-2024>(2/3 | 82/318) iFace: Hand-Over-Face Gesture Recognition Leveraging Impedance Sensing (Mengxi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengxi Liu, Hymalai Bello, Bo Zhou, Paul Lukowicz, Jakob Karolus. (2024)<br><strong>iFace: Hand-Over-Face Gesture Recognition Leveraging Impedance Sensing</strong><br><button class=copy-to-clipboard title="iFace: Hand-Over-Face Gesture Recognition Leveraging Impedance Sensing" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18433v1.pdf filename=2403.18433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hand-over-face gestures can provide important implicit interactions during conversations, such as frustration or excitement. However, in situations where interlocutors are not visible, such as phone calls or textual communication, the potential meaning contained in the hand-over-face gestures is lost. In this work, we present iFace, an unobtrusive, wearable impedance-sensing solution for recognizing different hand-over-face gestures. In contrast to most existing works, iFace does not require the placement of sensors on the user&rsquo;s face or hands. Instead, we proposed a novel sensing configuration, the shoulders, which remains invisible to both the user and outside observers. The system can monitor the shoulder-to-shoulder impedance variation caused by gestures through electrodes attached to each shoulder. We evaluated iFace in a user study with eight participants, collecting six kinds of hand-over-face gestures with different meanings. Using a <b>convolutional</b> <b>neural</b> <b>network</b> and a user-dependent classification, iFace reaches 82.58 % macro F1 score. We discuss potential application scenarios of iFace as an implicit interaction interface.</p></p class="citation"></blockquote><h3 id=33--83318-will-you-participate-exploring-the-potential-of-robotics-competitions-on-human-centric-topics-yuchong-zhang-et-al-2024>(3/3 | 83/318) Will You Participate? Exploring the Potential of Robotics Competitions on Human-centric Topics (Yuchong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchong Zhang, Miguel Vasco, Mårten Björkman, Danica Kragic. (2024)<br><strong>Will You Participate? Exploring the Potential of Robotics Competitions on Human-centric Topics</strong><br><button class=copy-to-clipboard title="Will You Participate? Exploring the Potential of Robotics Competitions on Human-centric Topics" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-RO, cs.HC<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18616v1.pdf filename=2403.18616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents findings from an exploratory needfinding study investigating the research current status and potential participation of the competitions on the robotics community towards four human-centric topics: safety, privacy, explainability, and <b>federated</b> <b>learning.</b> We conducted a survey with 34 participants across three distinguished European robotics consortia, nearly 60% of whom possessed over five years of research experience in robotics. Our qualitative and quantitative analysis revealed that current mainstream robotic researchers prioritize safety and explainability, expressing a greater willingness to invest in further research in these areas. Conversely, our results indicate that privacy and <b>federated</b> <b>learning</b> garner less attention and are perceived to have lower potential. Additionally, the study suggests a lack of enthusiasm within the robotics community for participating in competitions related to these topics. Based on these findings, we recommend targeting other communities, such as the machine learning community, for future competitions related to these four human-centric topics.</p></p class="citation"></blockquote><h2 id=cscv-82>cs.CV (82)</h2><h3 id=182--84318-mini-gemini-mining-the-potential-of-multi-modality-vision-language-models-yanwei-li-et-al-2024>(1/82 | 84/318) Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models (Yanwei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia. (2024)<br><strong>Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</strong><br><button class=copy-to-clipboard title="Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 83<br>Keywords: Benchmarking, Zero-shot, GPT, GPT-4, Gemini, Reasoning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18814v1.pdf filename=2403.18814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and <b>reasoning,</b> a performance gap persists compared to advanced models like <b>GPT-4</b> and <b>Gemini.</b> We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and <b>reasoning-based</b> generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, <b>reasoning,</b> and generation simultaneously. Mini-Gemini supports a series of dense and MoE <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> from 2B to 34B. It is demonstrated to achieve leading performance in several <b>zero-shot</b> <b>benchmarks</b> and even surpasses the developed private models. Code and models are available at <a href=https://github.com/dvlab-research/MiniGemini>https://github.com/dvlab-research/MiniGemini</a>.</p></p class="citation"></blockquote><h3 id=282--85318-an-image-grid-can-be-worth-a-video-zero-shot-video-question-answering-using-a-vlm-wonkyun-kim-et-al-2024>(2/82 | 85/318) An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM (Wonkyun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee. (2024)<br><strong>An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM</strong><br><button class=copy-to-clipboard title="An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 73<br>Keywords: Benchmarking, Foundation Model, Zero-shot, Question Answering, Reasoning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18406v1.pdf filename=2403.18406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stimulated by the sophisticated <b>reasoning</b> capabilities of recent <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with <b>LLMs.</b> Recently, an alternative strategy has surfaced, employing readily available <b>foundation</b> <b>models,</b> such as VideoLMs and <b>LLMs,</b> across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging multiple frames in a grid layout. The resulting single image is termed as an image grid. This format, while maintaining the appearance of a solitary image, effectively retains temporal information within the grid structure. Therefore, the image grid approach enables direct application of a single high-performance VLM without necessitating any video-data training. Our extensive experimental analysis across ten <b>zero-shot</b> video <b>question</b> <b>answering</b> <b>benchmarks,</b> including five open-ended and five multiple-choice <b>benchmarks,</b> reveals that the proposed Image Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out of ten <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=382--86318-illicit-object-detection-in-x-ray-images-using-vision-transformers-jorgen-cani-et-al-2024>(3/82 | 86/318) Illicit object detection in X-ray images using Vision Transformers (Jorgen Cani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorgen Cani, Ioannis Mademlis, Adamantia Anna Rebolledo Chrysochoou, Georgios Th. Papadopoulos. (2024)<br><strong>Illicit object detection in X-ray images using Vision Transformers</strong><br><button class=copy-to-clipboard title="Illicit object detection in X-ray images using Vision Transformers" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Object Detection, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19043v1.pdf filename=2403.19043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Illicit <b>object</b> <b>detection</b> is a critical task performed at various high-security locations, including airports, train stations, subways, and ports. The continuous and tedious work of examining thousands of X-ray images per hour can be mentally taxing. Thus, Deep Neural Networks (DNNs) can be used to automate the X-ray image analysis process, improve efficiency and alleviate the security officers&rsquo; inspection burden. The neural architectures typically utilized in relevant literature are <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> with <b>Vision</b> <b>Transformers</b> (ViTs) rarely employed. In order to address this gap, this paper conducts a comprehensive evaluation of relevant ViT architectures on illicit item detection in X-ray images. This study utilizes both <b>Transformer</b> and hybrid backbones, such as SWIN and NextViT, and detectors, such as DINO and RT-DETR. The results demonstrate the remarkable accuracy of the DINO <b>Transformer</b> detector in the low-data regime, the impressive real-time performance of YOLOv8, and the effectiveness of the hybrid NextViT backbone.</p></p class="citation"></blockquote><h3 id=482--87318-dense-vision-transformer-compression-with-few-samples-hanxiao-zhang-et-al-2024>(4/82 | 87/318) Dense Vision Transformer Compression with Few Samples (Hanxiao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxiao Zhang, Yifan Zhou, Guo-Hua Wang, Jianxin Wu. (2024)<br><strong>Dense Vision Transformer Compression with Few Samples</strong><br><button class=copy-to-clipboard title="Dense Vision Transformer Compression with Few Samples" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Convolutional Neural Network, Few-shot, Model Compression, Pruning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18708v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18708v1.pdf filename=2403.18708v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>model</b> <b>compression</b> aims to compress a large <b>model</b> <b>into</b> a more compact one with only a tiny training set (even without labels). Block-level <b>pruning</b> has recently emerged as a leading technique in achieving high accuracy and low latency in <b>few-shot</b> <b>CNN</b> compression. But, <b>few-shot</b> compression for <b>Vision</b> <b>Transformers</b> (ViT) remains largely unexplored, which presents a new challenge. In particular, the issue of sparse compression exists in traditional <b>CNN</b> <b>few-shot</b> methods, which can only produce very few compressed <b>models</b> <b>of</b> different <b>model</b> <b>sizes.</b> This paper proposes a novel framework for <b>few-shot</b> ViT compression named DC-ViT. Instead of dropping the entire block, DC-ViT selectively eliminates the attention module while retaining and reusing portions of the MLP module. DC-ViT enables dense compression, which outputs numerous compressed <b>models</b> <b>that</b> densely populate the range of <b>model</b> <b>complexity.</b> DC-ViT outperforms state-of-the-art <b>few-shot</b> compression methods by a significant margin of 10 percentage points, along with lower latency in the compression of ViT and its variants.</p></p class="citation"></blockquote><h3 id=582--88318-learning-cnn-on-vit-a-hybrid-model-to-explicitly-class-specific-boundaries-for-domain-adaptation-ba-hung-ngo-et-al-2024>(5/82 | 88/318) Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation (Ba Hung Ngo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ba Hung Ngo, Nhat-Tuong Do-Tran, Tuan-Ngoc Nguyen, Hae-Gon Jeon, Tae Jong Choi. (2024)<br><strong>Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation</strong><br><button class=copy-to-clipboard title="Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Domain Adaptation, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18360v1.pdf filename=2403.18360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most <b>domain</b> <b>adaptation</b> (DA) methods are based on either a <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> or a <b>vision</b> <b>transformers</b> (ViTs). They align the distribution differences between <b>domains</b> <b>as</b> encoders without considering their unique characteristics. For instance, ViT excels in accuracy due to its superior ability to capture global representations, while <b>CNN</b> has an advantage in capturing local representations. This fact has led us to design a hybrid method to fully take advantage of both ViT and <b>CNN,</b> called Explicitly Class-specific Boundaries (ECB). ECB learns <b>CNN</b> on ViT to combine their distinct strengths. In particular, we leverage ViT&rsquo;s properties to explicitly find class-specific decision boundaries by maximizing the discrepancy between the outputs of the two classifiers to detect target samples far from the source support. In contrast, the <b>CNN</b> encoder clusters target features based on the previously defined class-specific boundaries by minimizing the discrepancy between the probabilities of the two classifiers. Finally, ViT and <b>CNN</b> mutually exchange knowledge to improve the quality of pseudo labels and reduce the knowledge discrepancies of these models. Compared to conventional DA methods, our ECB achieves superior performance, which verifies its effectiveness in this hybrid model. The project website can be found <a href=https://dotrannhattuong.github.io/ECB/website/>https://dotrannhattuong.github.io/ECB/website/</a>.</p></p class="citation"></blockquote><h3 id=682--89318-textcraftor-your-text-encoder-can-be-image-quality-controller-yanyu-li-et-al-2024>(6/82 | 89/318) TextCraftor: Your Text Encoder Can be Image Quality Controller (Yanyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanyu Li, Xian Liu, Anil Kag, Ju Hu, Yerlan Idelbayev, Dhritiman Sagar, Yanzhi Wang, Sergey Tulyakov, Jian Ren. (2024)<br><strong>TextCraftor: Your Text Encoder Can be Image Quality Controller</strong><br><button class=copy-to-clipboard title="TextCraftor: Your Text Encoder Can be Image Quality Controller" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 63<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning, Fine-tuning, Text2image, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18978v1.pdf filename=2403.18978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion-based</b> <b>text-to-image</b> generative models, e.g., Stable <b>Diffusion,</b> <b>have</b> revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted <b>prompts</b> are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to <b>fine-tune</b> the pre-trained <b>diffusion</b> <b>models,</b> i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of <b>text-to-image</b> <b>diffusion</b> <b>model</b> training has remained largely unexplored: Is it possible and feasible to <b>fine-tune</b> the text encoder to improve the performance of <b>text-to-image</b> <b>diffusion</b> <b>models?</b> Our findings reveal that, instead of replacing the CLIP text encoder used in Stable <b>Diffusion</b> <b>with</b> other <b>large</b> <b>language</b> <b>models,</b> we can enhance it through our proposed <b>fine-tuning</b> approach, TextCraftor, leading to substantial improvements in quantitative <b>benchmarks</b> and human assessments. Interestingly, our technique also empowers controllable image generation through the interpolation of different text encoders <b>fine-tuned</b> with various rewards. We also demonstrate that TextCraftor is orthogonal to UNet <b>finetuning,</b> and can be combined to further improve generative quality.</p></p class="citation"></blockquote><h3 id=782--90318-beyond-embeddings-the-promise-of-visual-table-in-multi-modal-models-yiwu-zhong-et-al-2024>(7/82 | 90/318) Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models (Yiwu Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang. (2024)<br><strong>Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models</strong><br><button class=copy-to-clipboard title="Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-MM, cs.CV<br>Keyword Score: 61<br>Keywords: Benchmarking, Multi-modal, Representation Learning, Supervised Learning, Supervised Learning, Image2text, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18252v1.pdf filename=2403.18252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual <b>representation</b> <b>learning</b> has been a cornerstone in computer vision, evolving from <b>supervised</b> <b>learning</b> with human-annotated labels to aligning <b>image-text</b> pairs from the Internet. Despite recent advancements in <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs), the visual <b>representations</b> <b>they</b> rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual <b>reasoning.</b> In this work, we propose Visual Table, a novel visual <b>representation</b> <b>tailored</b> for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual <b>representations,</b> <b>our</b> model can consistently outperform the state-of-the-art (SOTA) MLLMs across diverse <b>benchmarks.</b> When visual tables serve as standalone visual <b>representations,</b> <b>our</b> model can closely match or even beat the SOTA MLLMs that are built on CLIP visual embeddings. Our code is available at <a href=https://github.com/LaVi-Lab/Visual-Table>https://github.com/LaVi-Lab/Visual-Table</a>.</p></p class="citation"></blockquote><h3 id=882--91318-vitar-vision-transformer-with-any-resolution-qihang-fan-et-al-2024>(8/82 | 91/318) ViTAR: Vision Transformer with Any Resolution (Qihang Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang. (2024)<br><strong>ViTAR: Vision Transformer with Any Resolution</strong><br><button class=copy-to-clipboard title="ViTAR: Vision Transformer with Any Resolution" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Autoencoder, Self-supervised Learning, Self-supervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18361v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18361v2.pdf filename=2403.18361v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper tackles a significant challenge faced by <b>Vision</b> <b>Transformers</b> (ViTs): their constrained scalability across different image resolutions. Typically, ViTs experience a performance decline when processing resolutions different from those seen during training. Our work introduces two key innovations to address this issue. Firstly, we propose a novel module for dynamic resolution adjustment, designed with a single <b>Transformer</b> block, specifically to achieve highly efficient incremental token integration. Secondly, we introduce fuzzy positional encoding in the <b>Vision</b> <b>Transformer</b> to provide consistent positional awareness across multiple resolutions, thereby preventing overfitting to any single training resolution. Our resulting model, ViTAR <b>(Vision</b> <b>Transformer</b> with Any Resolution), demonstrates impressive adaptability, achieving 83.3% top-1 accuracy at a 1120x1120 resolution and 80.4% accuracy at a 4032x4032 resolution, all while reducing computational costs. ViTAR also shows strong performance in downstream tasks such as instance and semantic segmentation and can easily combined with <b>self-supervised</b> <b>learning</b> techniques like Masked <b>AutoEncoder.</b> Our work provides a cost-effective solution for enhancing the resolution scalability of ViTs, paving the way for more versatile and efficient high-resolution image processing.</p></p class="citation"></blockquote><h3 id=982--92318-lita-language-instructed-temporal-localization-assistant-de-an-huang-et-al-2024>(9/82 | 92/318) LITA: Language Instructed Temporal-Localization Assistant (De-An Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, Jan Kautz. (2024)<br><strong>LITA: Language Instructed Temporal-Localization Assistant</strong><br><button class=copy-to-clipboard title="LITA: Language Instructed Temporal-Localization Assistant" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, Instruction Following, Reasoning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19046v1.pdf filename=2403.19046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been tremendous progress in <b>multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Recent works have extended these models to video input with promising <b>instruction</b> <b>following</b> capabilities. However, an important missing piece is temporal localization. These models cannot accurately answer the &ldquo;When?&rdquo; questions. We identify three key aspects that limit their temporal localization capabilities: (i) time representation, (ii) architecture, and (iii) data. We address these shortcomings by proposing Language Instructed Temporal-Localization Assistant (LITA) with the following features: (1) We introduce time tokens that encode timestamps relative to the video length to better represent time in videos. (2) We introduce SlowFast tokens in the architecture to capture temporal information at fine temporal resolution. (3) We emphasize temporal localization data for LITA. In addition to leveraging existing video datasets with timestamps, we propose a new task, <b>Reasoning</b> Temporal Localization (RTL), along with the dataset, ActivityNet-RTL, for learning and evaluating this task. <b>Reasoning</b> temporal localization requires both the <b>reasoning</b> and temporal localization of Video <b>LLMs.</b> LITA demonstrates strong performance on this challenging task, nearly doubling the temporal mean intersection-over-union (mIoU) of baselines. In addition, we show that our emphasis on temporal localization also substantially improves video-based <b>text</b> <b>generation</b> compared to existing Video <b>LLMs,</b> including a 36% relative improvement of Temporal Understanding. Code is available at: <a href=https://github.com/NVlabs/LITA>https://github.com/NVlabs/LITA</a></p></p class="citation"></blockquote><h3 id=1082--93318-versat2i-improving-text-to-image-models-with-versatile-reward-jianshu-guo-et-al-2024>(10/82 | 93/318) VersaT2I: Improving Text-to-Image Models with Versatile Reward (Jianshu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianshu Guo, Wenhao Chai, Jie Deng, Hsiang-Wei Huang, Tian Ye, Yichen Xu, Jiawei Zhang, Jenq-Neng Hwang, Gaoang Wang. (2024)<br><strong>VersaT2I: Improving Text-to-Image Models with Versatile Reward</strong><br><button class=copy-to-clipboard title="VersaT2I: Improving Text-to-Image Models with Versatile Reward" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 55<br>Keywords: Graph Attention Networks, Fine-tuning, Geometry, Reinforcement Learning, Text2image, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18493v1.pdf filename=2403.18493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>text-to-image</b> (T2I) models have benefited from large-scale and high-quality data, demonstrating impressive performance. However, these T2I models still struggle to produce images that are aesthetically pleasing, geometrically accurate, faithful to text, and of good low-level quality. We present VersaT2I, a versatile training framework that can boost the performance with multiple rewards of any T2I model. We decompose the quality of the image into several aspects such as aesthetics, <b>text-image</b> alignment, <b>geometry,</b> low-level quality, etc. Then, for every quality aspect, we select high-quality images in this aspect generated by the model as the training set to <b>finetune</b> the T2I model using the Low-Rank Adaptation (LoRA). Furthermore, we introduce a <b>gating</b> function to combine multiple quality aspects, which can avoid conflicts between different quality aspects. Our method is easy to extend and does not require any manual annotation, <b>reinforcement</b> <b>learning,</b> or model architecture changes. Extensive experiments demonstrate that VersaT2I outperforms the baseline methods across various quality criteria.</p></p class="citation"></blockquote><h3 id=1182--94318-orco-towards-better-generalization-via-orthogonality-and-contrast-for-few-shot-class-incremental-learning-noor-ahmed-et-al-2024>(11/82 | 94/318) OrCo: Towards Better Generalization via Orthogonality and Contrast for Few-Shot Class-Incremental Learning (Noor Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noor Ahmed, Anna Kukleva, Bernt Schiele. (2024)<br><strong>OrCo: Towards Better Generalization via Orthogonality and Contrast for Few-Shot Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="OrCo: Towards Better Generalization via Orthogonality and Contrast for Few-Shot Class-Incremental Learning" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Contrastive Learning, Few-shot, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18550v1.pdf filename=2403.18550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-Shot</b> Class-Incremental Learning (FSCIL) introduces a paradigm in which the problem space expands with limited data. FSCIL methods inherently face the challenge of catastrophic forgetting as data arrives incrementally, making models susceptible to overwriting previously acquired knowledge. Moreover, given the scarcity of labeled samples available at any given time, models may be prone to overfitting and find it challenging to strike a balance between extensive pretraining and the limited incremental data. To address these challenges, we propose the OrCo framework built on two core principles: features&rsquo; orthogonality in the representation space, and <b>contrastive</b> <b>learning.</b> In particular, we improve the generalization of the embedding space by employing a combination of <b>supervised</b> and <b>self-supervised</b> <b>contrastive</b> <b>losses</b> during the pretraining phase. Additionally, we introduce OrCo loss to address challenges arising from data limitations during incremental sessions. Through feature space perturbations and orthogonality between classes, the OrCo loss maximizes margins and reserves space for the following incremental data. This, in turn, ensures the accommodation of incoming classes in the feature space without compromising previously acquired knowledge. Our experimental results showcase state-of-the-art performance across three <b>benchmark</b> datasets, including mini-ImageNet, CIFAR100, and CUB datasets. Code is available at <a href=https://github.com/noorahmedds/OrCo>https://github.com/noorahmedds/OrCo</a></p></p class="citation"></blockquote><h3 id=1282--95318-direct-mineral-content-prediction-from-drill-core-images-via-transfer-learning-romana-boiger-et-al-2024>(12/82 | 95/318) Direct mineral content prediction from drill core images via transfer learning (Romana Boiger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Romana Boiger, Sergey V. Churakov, Ignacio Ballester Llagaria, Georg Kosakowski, Raphael Wüst, Nikolaos I. Prasianakis. (2024)<br><strong>Direct mineral content prediction from drill core images via transfer learning</strong><br><button class=copy-to-clipboard title="Direct mineral content prediction from drill core images via transfer learning" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 43<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18495v1.pdf filename=2403.18495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep subsurface exploration is important for mining, oil and gas industries, as well as in the assessment of geological units for the disposal of chemical or nuclear waste, or the viability of geothermal energy systems. Typically, detailed examinations of subsurface formations or units are performed on cuttings or core materials extracted during drilling campaigns, as well as on geophysical borehole data, which provide detailed information about the petrophysical properties of the rocks. Depending on the volume of rock samples and the analytical program, the laboratory analysis and diagnostics can be very time-consuming. This study investigates the potential of utilizing machine learning, specifically <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNN),</b> to assess the lithology and mineral content solely from analysis of drill core images, aiming to support and expedite the subsurface geological exploration. The paper outlines a comprehensive methodology, encompassing data preprocessing, machine learning methods, and <b>transfer</b> <b>learning</b> techniques. The outcome reveals a remarkable 96.7% accuracy in the classification of drill core segments into distinct formation classes. Furthermore, a <b>CNN</b> model was trained for the evaluation of mineral content using a learning data set from multidimensional log analysis data (silicate, total clay, carbonate). When <b>benchmarked</b> against laboratory XRD measurements on samples from the cores, both the advanced multidimensional log analysis model and the neural network approach developed here provide equally good performance. This work demonstrates that deep learning and particularly <b>transfer</b> <b>learning</b> can support extracting petrophysical properties, including mineral content and formation classification, from drill core images, thus offering a road map for enhancing model performance and data set quality in image-based analysis of drill cores.</p></p class="citation"></blockquote><h3 id=1382--96318-scaling-vision-and-language-navigation-with-offline-rl-valay-bundele-et-al-2024>(13/82 | 96/318) Scaling Vision-and-Language Navigation With Offline RL (Valay Bundele et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valay Bundele, Mahesh Bhupati, Biplab Banerjee, Aditya Grover. (2024)<br><strong>Scaling Vision-and-Language Navigation With Offline RL</strong><br><button class=copy-to-clipboard title="Scaling Vision-and-Language Navigation With Offline RL" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Data Augmentation, Offline Reinforcement Learning, Reinforcement Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18454v1.pdf filename=2403.18454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of <b>vision-and-language</b> navigation (VLN) has typically relied on expert trajectories, which may not always be available in real-world situations due to the significant effort required to collect them. On the other hand, existing approaches to training VLN agents that go beyond available expert <b>data</b> <b>involve</b> <b>data</b> <b>augmentations</b> or online exploration which can be tedious and risky. In contrast, it is easy to access large repositories of suboptimal <b>offline</b> <b>trajectories.</b> <b>Inspired</b> by research in <b>offline</b> <b>reinforcement</b> <b>learning</b> (ORL), we introduce a new problem setup of VLN-ORL which studies VLN using suboptimal demonstration <b>data.</b> <b>We</b> introduce a simple and effective reward-conditioned approach that can account for dataset suboptimality for training VLN agents, as well as <b>benchmarks</b> to evaluate progress and promote research in this area. We empirically study various noise models for characterizing dataset suboptimality among other unique challenges in VLN-ORL and instantiate it for the VLN$\circlearrowright$BERT and MTVM architectures in the R2R and RxR environments. Our experiments demonstrate that the proposed reward-conditioned approach leads to significant performance improvements, even in complex and intricate environments.</p></p class="citation"></blockquote><h3 id=1482--97318-plot-tal----prompt-learning-with-optimal-transport-for-few-shot-temporal-action-localization-edward-fish-et-al-2024>(14/82 | 97/318) PLOT-TAL &ndash; Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization (Edward Fish et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edward Fish, Jon Weinbren, Andrew Gilbert. (2024)<br><strong>PLOT-TAL &ndash; Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization</strong><br><button class=copy-to-clipboard title="PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Few-shot, Few-shot Learning, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18915v1.pdf filename=2403.18915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel approach to temporal action localization (TAL) in <b>few-shot</b> <b>learning.</b> Our work addresses the inherent limitations of conventional single-prompt learning methods that often lead to overfitting due to the inability to generalize across varying contexts in real-world videos. Recognizing the diversity of camera views, backgrounds, and objects in videos, we propose a multi-prompt learning framework enhanced with optimal transport. This design allows the model to learn a set of diverse <b>prompts</b> <b>for</b> each action, capturing general characteristics more effectively and distributing the representation to mitigate the risk of overfitting. Furthermore, by employing optimal transport theory, we efficiently align these <b>prompts</b> <b>with</b> action features, optimizing for a comprehensive representation that adapts to the multifaceted nature of video data. Our experiments demonstrate significant improvements in action localization accuracy and robustness in <b>few-shot</b> <b>settings</b> on the standard challenging datasets of THUMOS-14 and EpicKitchens100, highlighting the efficacy of our multi-prompt optimal transport approach in overcoming the challenges of conventional <b>few-shot</b> <b>TAL</b> methods.</p></p class="citation"></blockquote><h3 id=1582--98318-objectdrop-bootstrapping-counterfactuals-for-photorealistic-object-removal-and-insertion-daniel-winter-et-al-2024>(15/82 | 98/318) ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion (Daniel Winter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, Yedid Hoshen. (2024)<br><strong>ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion</strong><br><button class=copy-to-clipboard title="ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Counter-factual, Fine-tuning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18818v1.pdf filename=2403.18818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections. By analyzing the limitations of <b>self-supervised</b> approaches, we propose a practical solution centered on a \q{counterfactual} dataset. Our method involves capturing a scene before and after removing a single object, while minimizing other changes. By <b>fine-tuning</b> a <b>diffusion</b> <b>model</b> on this dataset, we are able to not only remove objects but also their effects on the scene. However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset. To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small <b>counterfactual</b> dataset, we synthetically expand this dataset considerably. Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene.</p></p class="citation"></blockquote><h3 id=1682--99318-deep-learning-for-robust-and-explainable-models-in-computer-vision-mohammadreza-amirian-2024>(16/82 | 99/318) Deep Learning for Robust and Explainable Models in Computer Vision (Mohammadreza Amirian, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammadreza Amirian. (2024)<br><strong>Deep Learning for Robust and Explainable Models in Computer Vision</strong><br><button class=copy-to-clipboard title="Deep Learning for Robust and Explainable Models in Computer Vision" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18674v1.pdf filename=2403.18674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent breakthroughs in machine and deep learning (ML and DL) research have provided excellent tools for leveraging enormous amounts of data and optimizing huge models with millions of parameters to obtain accurate networks for image processing. These developments open up tremendous opportunities for using artificial intelligence (AI) in the automation and human assisted AI industry. However, as more and more models are deployed and used in practice, many challenges have emerged. This thesis presents various approaches that address robustness and explainability challenges for using ML and DL in practice. Robustness and reliability are the critical components of any model before certification and deployment in practice. Deep <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> exhibit vulnerability to transformations of their inputs, such as rotation and scaling, or intentional manipulations as described in the <b>adversarial</b> <b>attack</b> literature. In addition, building trust in AI-based models requires a better understanding of current models and developing methods that are more explainable and interpretable a priori. This thesis presents developments in computer vision models&rsquo; robustness and explainability. Furthermore, this thesis offers an example of using vision models&rsquo; feature response visualization (models&rsquo; interpretations) to improve robustness despite interpretability and robustness being seemingly unrelated in the related research. Besides methodological developments for robust and explainable vision models, a key message of this thesis is introducing model interpretation techniques as a tool for understanding vision models and improving their design and robustness. In addition to the theoretical developments, this thesis demonstrates several applications of ML and DL in different contexts, such as medical imaging and affective computing.</p></p class="citation"></blockquote><h3 id=1782--100318-artifact-reduction-in-3d-and-4d-cone-beam-computed-tomography-images-with-deep-learning----a-review-mohammadreza-amirian-et-al-2024>(17/82 | 100/318) Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images with Deep Learning &ndash; A Review (Mohammadreza Amirian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammadreza Amirian, Daniel Barco, Ivo Herzig, Frank-Peter Schilling. (2024)<br><strong>Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images with Deep Learning &ndash; A Review</strong><br><button class=copy-to-clipboard title="Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images with Deep Learning -- A Review" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18565v1.pdf filename=2403.18565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning based approaches have been used to improve image quality in cone-beam computed tomography (CBCT), a medical imaging technique often used in applications such as image-guided radiation therapy, implant dentistry or orthopaedics. In particular, while deep learning methods have been applied to reduce various types of CBCT image artifacts arising from motion, metal objects, or low-dose acquisition, a comprehensive review summarizing the successes and shortcomings of these approaches, with a primary focus on the type of artifacts rather than the architecture of neural networks, is lacking in the literature. In this review, the data generation and <b>simulation</b> pipelines, and artifact reduction techniques are specifically investigated for each type of artifact. We provide an overview of deep learning techniques that have successfully been shown to reduce artifacts in 3D, as well as in time-resolved (4D) CBCT through the use of projection- and/or volume-domain optimizations, or by introducing neural networks directly within the CBCT reconstruction algorithms. Research gaps are identified to suggest avenues for future exploration. One of the key findings of this work is an observed trend towards the use of generative models including <b>GANs</b> and score-based or <b>diffusion</b> <b>models,</b> accompanied with the need for more diverse and open training datasets and <b>simulations.</b></p></p class="citation"></blockquote><h3 id=1882--101318-cosalpure-learning-concept-from-group-images-for-robust-co-saliency-detection-jiayi-zhu-et-al-2024>(18/82 | 101/318) CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection (Jiayi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu. (2024)<br><strong>CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection</strong><br><button class=copy-to-clipboard title="CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Object Detection, Text2image, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18554v1.pdf filename=2403.18554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Co-salient <b>object</b> <b>detection</b> (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some <b>adversarial</b> <b>perturbations,</b> leading to substantial accuracy reduction. The <b>adversarial</b> <b>perturbations</b> can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient <b>objects.</b> <b>In</b> this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient <b>objects</b> <b>based</b> on the input group images and then leveraging this concept to purify <b>adversarial</b> <b>perturbations,</b> which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided <b>diffusion</b> <b>purification.</b> For the first module, we adopt a pre-trained <b>text-to-image</b> <b>diffusion</b> <b>model</b> to learn the concept of co-salient <b>objects</b> <b>within</b> group images where the learned concept is robust to <b>adversarial</b> <b>examples.</b> For the second module, we map the <b>adversarial</b> <b>image</b> to the latent space and then perform <b>diffusion</b> <b>generation</b> by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA <b>adversarial</b> <b>attack</b> containing different <b>adversarial</b> <b>patterns,</b> including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly.</p></p class="citation"></blockquote><h3 id=1982--102318-language-plays-a-pivotal-role-in-the-object-attribute-compositional-generalization-of-clip-reza-abbasi-et-al-2024>(19/82 | 102/318) Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP (Reza Abbasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah. (2024)<br><strong>Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP</strong><br><button class=copy-to-clipboard title="Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Out-of-distribution, Supervised Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18525v1.pdf filename=2403.18525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> models, such as CLIP, have shown promising <b>Out-of-Distribution</b> (OoD) generalization under various types of <b>distribution</b> <b>shifts.</b> Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both <b>supervised</b> models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the scale and diversity of training data and language supervision play a key role in unlocking the compositional generalization abilities of <b>vision-language</b> models.</p></p class="citation"></blockquote><h3 id=2082--103318-diffstyler-diffusion-based-localized-image-style-transfer-shaoxu-li-2024>(20/82 | 103/318) DiffStyler: Diffusion-based Localized Image Style Transfer (Shaoxu Li, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoxu Li. (2024)<br><strong>DiffStyler: Diffusion-based Localized Image Style Transfer</strong><br><button class=copy-to-clipboard title="DiffStyler: Diffusion-based Localized Image Style Transfer" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Style Transfer, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18461v1.pdf filename=2403.18461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image <b>style</b> <b>transfer</b> aims to imbue digital imagery with the distinctive attributes of <b>style</b> <b>targets,</b> such as colors, brushstrokes, shapes, whilst concurrently preserving the semantic integrity of the content. Despite the advancements in arbitrary <b>style</b> <b>transfer</b> methods, a prevalent challenge remains the delicate equilibrium between content semantics and <b>style</b> <b>attributes.</b> Recent developments in large-scale <b>text-to-image</b> <b>diffusion</b> <b>models</b> have heralded unprecedented synthesis capabilities, albeit at the expense of relying on extensive and often imprecise textual descriptions to delineate artistic <b>styles.</b> <b>Addressing</b> these limitations, this paper introduces DiffStyler, a novel approach that facilitates efficient and precise arbitrary image <b>style</b> <b>transfer.</b> DiffStyler lies the utilization of a <b>text-to-image</b> Stable <b>Diffusion</b> <b>model-based</b> LoRA to encapsulate the essence of <b>style</b> <b>targets.</b> This approach, coupled with strategic cross-LoRA feature and attention injection, guides the <b>style</b> <b>transfer</b> process. The foundation of our methodology is rooted in the observation that LoRA maintains the spatial feature consistency of UNet, a discovery that further inspired the development of a mask-wise <b>style</b> <b>transfer</b> technique. This technique employs masks extracted through a pre-trained FastSAM model, utilizing mask <b>prompts</b> to facilitate feature fusion during the denoising process, thereby enabling localized <b>style</b> <b>transfer</b> that preserves the original image&rsquo;s unaffected regions. Moreover, our approach accommodates multiple <b>style</b> <b>targets</b> through the use of corresponding masks. Through extensive experimentation, we demonstrate that DiffStyler surpasses previous methods in achieving a more harmonious balance between content preservation and <b>style</b> <b>integration.</b></p></p class="citation"></blockquote><h3 id=2182--104318-mathrmf2depth-self-supervised-indoor-monocular-depth-estimation-via-optical-flow-consistency-and-feature-map-synthesis-xiaotong-guo-et-al-2024>(21/82 | 104/318) $\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation via Optical Flow Consistency and Feature Map Synthesis (Xiaotong Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaotong Guo, Huijie Zhao, Shuwei Shao, Xudong Li, Baochang Zhang. (2024)<br><strong>$\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation via Optical Flow Consistency and Feature Map Synthesis</strong><br><button class=copy-to-clipboard title="$\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation via Optical Flow Consistency and Feature Map Synthesis" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Self-supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18443v1.pdf filename=2403.18443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> monocular depth estimation methods have been increasingly given much attention due to the benefit of not requiring large, labelled datasets. Such <b>self-supervised</b> methods require high-quality salient features and consequently suffer from severe performance drop for indoor scenes, where low-textured regions dominant in the scenes are almost indiscriminative. To address the issue, we propose a <b>self-supervised</b> indoor monocular depth estimation framework called $\mathrm{F^2Depth}$. A <b>self-supervised</b> optical flow estimation network is introduced to supervise depth learning. To improve optical flow estimation performance in low-textured areas, only some patches of points with more discriminative features are adopted for <b>finetuning</b> based on our well-designed patch-based photometric loss. The <b>finetuned</b> optical flow estimation network generates high-accuracy optical flow as a supervisory signal for depth estimation. Correspondingly, an optical flow consistency loss is designed. Multi-scale feature maps produced by <b>finetuned</b> optical flow estimation network perform warping to compute feature map synthesis loss as another supervisory signal for depth learning. Experimental results on the NYU Depth V2 dataset demonstrate the effectiveness of the framework and our proposed losses. To evaluate the generalization ability of our $\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of approximately 1500 points selected from 99 images in 18 scenes. <b>Zero-shot</b> generalization experiments on 7-Scenes dataset and Campus Indoor achieve $\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show that our model can generalize well to monocular images captured in unknown indoor scenes.</p></p class="citation"></blockquote><h3 id=2282--105318-colour-and-brush-stroke-pattern-recognition-in-abstract-art-using-modified-deep-convolutional-generative-adversarial-networks-srinitish-srinivasan-et-al-2024>(22/82 | 105/318) Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks (Srinitish Srinivasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Srinitish Srinivasan, Varenya Pathak. (2024)<br><strong>Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks</strong><br><button class=copy-to-clipboard title="Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Generative Adversarial Network, Generative Adversarial Network, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18397v1.pdf filename=2403.18397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the <b>emotions</b> <b>of</b> an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and <b>emotion</b> <b>recognition</b> algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using <b>Generative</b> <b>Adversarial</b> <b>Neural</b> Networks(GAN). <b>GANs</b> have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient <b>GAN</b> architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns. The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing. Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of <b>GAN</b> training and compare its significant difference. These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem.</p></p class="citation"></blockquote><h3 id=2382--106318-ship-in-sight-diffusion-models-for-ship-image-super-resolution-luigi-sigillo-et-al-2024>(23/82 | 106/318) Ship in Sight: Diffusion Models for Ship-Image Super Resolution (Luigi Sigillo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello. (2024)<br><strong>Ship in Sight: Diffusion Models for Ship-Image Super Resolution</strong><br><button class=copy-to-clipboard title="Ship in Sight: Diffusion Models for Ship-Image Super Resolution" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Object Detection, Foundation Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18370v1.pdf filename=2403.18370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in <b>text-to-image</b> <b>diffusion</b> <b>models,</b> taking advantage of the prior knowledge that such <b>foundation</b> <b>models</b> have already learned. In particular, we present a <b>diffusion-model-based</b> <b>architecture</b> that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resoluted image. Since the specificity of this task and the scarcity availability of off-the-shelf data, we also introduce a large labeled ship dataset scraped from online ship images, mostly from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method achieves more robust results than other deep learning models previously employed for super resolution, as proven by the multiple experiments performed. Moreover, we investigate how this model can benefit downstream tasks, such as classification and <b>object</b> <b>detection,</b> thus emphasizing practical implementation in a real-world scenario. Experimental results show flexibility, reliability, and impressive performance of the proposed framework over state-of-the-art methods for different tasks. The code is available at: <a href=https://github.com/LuigiSigillo/ShipinSight>https://github.com/LuigiSigillo/ShipinSight</a> .</p></p class="citation"></blockquote><h3 id=2482--107318-toward-interactive-regional-understanding-in-vision-large-language-models-jungbeom-lee-et-al-2024>(24/82 | 107/318) Toward Interactive Regional Understanding in Vision-Large Language Models (Jungbeom Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jungbeom Lee, Sanghyuk Chun, Sangdoo Yun. (2024)<br><strong>Toward Interactive Regional Understanding in Vision-Large Language Models</strong><br><button class=copy-to-clipboard title="Toward Interactive Regional Understanding in Vision-Large Language Models" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Zero-shot, Dialogue System, Image2text, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18260v1.pdf filename=2403.18260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>Vision-Language</b> Pre-training (VLP) models have demonstrated significant advancements. Nevertheless, these models heavily rely on <b>image-text</b> pairs that capture only coarse and global information of an image, leading to a limitation in their regional understanding ability. In this work, we introduce \textbf{RegionVLM}, equipped with explicit regional modeling capabilities, allowing them to understand user-indicated image regions. To achieve this, we design a simple yet innovative architecture, requiring no modifications to the model architecture or objective function. Additionally, we leverage a dataset that contains a novel source of information, namely Localized Narratives, which has been overlooked in previous VLP research. Our experiments demonstrate that our single generalist model not only achieves an interactive <b>dialogue</b> <b>system</b> but also exhibits superior performance on various <b>zero-shot</b> region understanding tasks, without compromising its ability for global image understanding.</p></p class="citation"></blockquote><h3 id=2582--108318-benchmarking-object-detectors-with-coco-a-new-path-forward-shweta-singh-et-al-2024>(25/82 | 108/318) Benchmarking Object Detectors with COCO: A New Path Forward (Shweta Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shweta Singh, Aayan Yadav, Jitesh Jain, Humphrey Shi, Justin Johnson, Karan Desai. (2024)<br><strong>Benchmarking Object Detectors with COCO: A New Path Forward</strong><br><button class=copy-to-clipboard title="Benchmarking Object Detectors with COCO: A New Path Forward" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Object Detection, Benchmarking, Benchmarking, Stemming, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18819v1.pdf filename=2403.18819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Common <b>Objects</b> <b>in</b> Context (COCO) dataset has been instrumental in <b>benchmarking</b> <b>object</b> <b>detectors</b> over the past decade. Like every dataset, COCO contains subtle errors and imperfections <b>stemming</b> from its annotation procedure. With the advent of high-performing models, we ask whether these errors of COCO are hindering its utility in reliably <b>benchmarking</b> further progress. In search for an answer, we inspect thousands of masks from COCO (2017 version) and uncover different types of errors such as imprecise mask boundaries, non-exhaustively annotated instances, and mislabeled masks. Due to the prevalence of COCO, we choose to correct these errors to maintain continuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner set of annotations with visibly better mask quality than COCO-2017. We evaluate fifty <b>object</b> <b>detectors</b> and find that models that predict visually sharper masks score higher on COCO-ReM, affirming that they were being incorrectly penalized due to errors in COCO-2017. Moreover, our models trained using COCO-ReM converge faster and score higher than their larger variants trained using COCO-2017, highlighting the importance of data quality in improving <b>object</b> <b>detectors.</b> With these findings, we advocate using COCO-ReM for future <b>object</b> <b>detection</b> research. Our dataset is available at <a href=https://cocorem.xyz>https://cocorem.xyz</a></p></p class="citation"></blockquote><h3 id=2682--109318-middle-fusion-and-multi-stage-multi-form-prompts-for-robust-rgb-t-tracking-qiming-wang-et-al-2024>(26/82 | 109/318) Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T Tracking (Qiming Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiming Wang, Yongqiang Bai, Hongxing Song. (2024)<br><strong>Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T Tracking</strong><br><button class=copy-to-clipboard title="Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T Tracking" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18193v1.pdf filename=2403.18193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>RGB-T tracking, a vital downstream task of object tracking, has made remarkable progress in recent years. Yet, it remains hindered by two major challenges: 1) the trade-off between performance and efficiency; 2) the scarcity of training data. To address the latter challenge, some recent methods employ <b>prompts</b> <b>to</b> <b>fine-tune</b> pre-trained RGB tracking models and leverage upstream knowledge in a parameter-efficient manner. However, these methods inadequately explore modality-independent patterns and disregard the dynamic reliability of different modalities in open scenarios. We propose M3PT, a novel RGB-T <b>prompt</b> <b>tracking</b> method that leverages middle fusion and <b>multi-modal</b> and multi-stage visual <b>prompts</b> <b>to</b> overcome these challenges. We pioneer the use of the middle fusion framework for RGB-T tracking, which achieves a balance between performance and efficiency. Furthermore, we incorporate the pre-trained RGB tracking model into the framework and utilize multiple flexible <b>prompt</b> <b>strategies</b> to adapt the pre-trained model to the comprehensive exploration of uni-modal patterns and the improved modeling of fusion-modal features, harnessing the potential of <b>prompt</b> <b>learning</b> in RGB-T tracking. Our method outperforms the state-of-the-art methods on four challenging <b>benchmarks,</b> while attaining 46.1 fps inference speed.</p></p class="citation"></blockquote><h3 id=2782--110318-metacap-meta-learning-priors-from-multi-view-imagery-for-sparse-view-human-performance-capture-and-rendering-guoxing-sun-et-al-2024>(27/82 | 110/318) MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering (Guoxing Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoxing Sun, Rishabh Dabral, Pascal Fua, Christian Theobalt, Marc Habermann. (2024)<br><strong>MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering</strong><br><button class=copy-to-clipboard title="MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Fine-tuning, Fine-tuning, Geometry, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18820v1.pdf filename=2403.18820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Faithful human performance capture and free-view rendering from sparse RGB observations is a long-standing problem in Vision and Graphics. The main challenges are the lack of observations and the inherent ambiguities of the setting, e.g. occlusions and depth ambiguity. As a result, radiance fields, which have shown great promise in capturing high-frequency appearance and <b>geometry</b> details in dense setups, perform poorly when na"ively supervising them on sparse camera views, as the field simply overfits to the sparse-view inputs. To address this, we propose MetaCap, a method for efficient and high-quality <b>geometry</b> recovery and novel view synthesis given very sparse or even a single view of the human. Our key idea is to <b>meta-learn</b> <b>the</b> radiance field weights solely from potentially sparse multi-view videos, which can serve as a prior when <b>fine-tuning</b> them on sparse imagery depicting the human. This prior provides a good network weight initialization, thereby effectively addressing ambiguities in sparse-view capture. Due to the articulated structure of the human body and motion-induced surface deformations, learning such a prior is non-trivial. Therefore, we propose to <b>meta-learn</b> <b>the</b> field weights in a pose-canonicalized space, which reduces the spatial feature range and makes feature learning more effective. Consequently, one can <b>fine-tune</b> our field parameters to quickly generalize to unseen poses, novel illumination conditions as well as novel and sparse (even monocular) camera views. For evaluating our method under different scenarios, we collect a new dataset, WildDynaCap, which contains subjects captured in, both, a dense camera dome and in-the-wild sparse camera rigs, and demonstrate superior results compared to recent state-of-the-art methods on both public and WildDynaCap dataset.</p></p class="citation"></blockquote><h3 id=2882--111318-multi-scale-unified-network-for-image-classification-wenzhuo-liu-et-al-2024>(28/82 | 111/318) Multi-scale Unified Network for Image Classification (Wenzhuo Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu. (2024)<br><strong>Multi-scale Unified Network for Image Classification</strong><br><button class=copy-to-clipboard title="Multi-scale Unified Network for Image Classification" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18294v1.pdf filename=2403.18294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> have advanced significantly in visual <b>representation</b> <b>learning</b> and recognition. However, they face notable challenges in performance and computational efficiency when dealing with real-world, multi-scale image inputs. Conventional methods rescale all input images into a fixed size, wherein a larger fixed size favors performance but rescaling small size images to a larger size incurs digitization noise and increased computation cost. In this work, we carry out a comprehensive, layer-wise investigation of <b>CNN</b> models in response to scale variation, based on Centered Kernel Alignment (CKA) analysis. The observations reveal lower layers are more sensitive to input image scale variations than high-level layers. Inspired by this insight, we propose Multi-scale Unified Network (MUSN) consisting of multi-scale subnets, a unified network, and scale-invariant constraint. Our method divides the shallow layers into multi-scale subnets to enable feature extraction from multi-scale inputs, and the low-level features are unified in deep layers for extracting high-level semantic features. A scale-invariant constraint is posed to maintain feature consistency across different scales. Extensive experiments on ImageNet and other scale-diverse datasets, demonstrate that MSUN achieves significant improvements in both model performance and computational efficiency. Particularly, MSUN yields an accuracy increase up to 44.53% and diminishes FLOPs by 7.01-16.13% in multi-scale scenarios.</p></p class="citation"></blockquote><h3 id=2982--112318-duolando-follower-gpt-with-off-policy-reinforcement-learning-for-dance-accompaniment-li-siyao-et-al-2024>(29/82 | 112/318) Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment (Li Siyao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Siyao, Tianpei Gu, Zhitao Yang, Zhengyu Lin, Ziwei Liu, Henghui Ding, Lei Yang, Chen Change Loy. (2024)<br><strong>Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment</strong><br><button class=copy-to-clipboard title="Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-SD, cs.CV, eess-AS<br>Keyword Score: 33<br>Keywords: Benchmarking, Out-of-distribution, Reinforcement Learning, GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18811v1.pdf filename=2403.18811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel task within the field of 3D dance generation, termed dance accompaniment, which necessitates the generation of responsive movements from a dance partner, the &ldquo;follower&rdquo;, synchronized with the lead dancer&rsquo;s movements and the underlying musical rhythm. Unlike existing solo or group dance generation tasks, a duet dance scenario entails a heightened degree of interaction between the two participants, requiring delicate coordination in both pose and position. To support this task, we first build a large-scale and diverse duet interactive dance dataset, DD100, by recording about 117 minutes of professional dancers&rsquo; performances. To address the challenges inherent in this task, we propose a <b>GPT-based</b> model, Duolando, which autoregressively predicts the subsequent tokenized motion conditioned on the coordinated information of the music, the leader&rsquo;s and the follower&rsquo;s movements. To further enhance the <b>GPT&rsquo;s</b> capabilities of generating stable results on unseen conditions (music and leader motions), we devise an off-policy <b>reinforcement</b> <b>learning</b> strategy that allows the model to explore viable trajectories from <b>out-of-distribution</b> samplings, guided by human-defined rewards. Based on the collected dataset and proposed method, we establish a <b>benchmark</b> with several carefully designed metrics.</p></p class="citation"></blockquote><h3 id=3082--113318-rap-retrieval-augmented-planner-for-adaptive-procedure-planning-in-instructional-videos-ali-zare-et-al-2024>(30/82 | 113/318) RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos (Ali Zare et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang. (2024)<br><strong>RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos</strong><br><button class=copy-to-clipboard title="RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18600v1.pdf filename=2403.18600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states. Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies. (2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans. (3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets.In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the procedure length is not fixed or pre-determined. To address these challenges we introduce Retrieval-Augmented Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively determines the conclusion of actions using an auto-regressive model architecture. For temporal relation, RAP establishes an external memory module to explicitly retrieve the most relevant state-action pairs from the training videos and revises the generated procedures. To tackle high annotation cost, RAP utilizes a <b>weakly-supervised</b> <b>learning</b> <b>manner</b> to expand the training dataset to other task-relevant, unannotated videos by generating pseudo labels for action steps. Experiments on CrossTask and COIN <b>benchmarks</b> show the superiority of RAP over traditional fixed-length models, establishing it as a strong baseline solution for adaptive procedure planning.</p></p class="citation"></blockquote><h3 id=3182--114318-singulartrajectory-universal-trajectory-predictor-using-diffusion-model-inhwan-bae-et-al-2024>(31/82 | 114/318) SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model (Inhwan Bae et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Inhwan Bae, Young-Jae Park, Hae-Gon Jeon. (2024)<br><strong>SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model</strong><br><button class=copy-to-clipboard title="SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Few-shot, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18452v1.pdf filename=2403.18452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are five types of trajectory prediction tasks: deterministic, stochastic, <b>domain</b> <b>adaptation,</b> momentary observation, and <b>few-shot.</b> These associated tasks are defined by various factors, such as the length of input paths, data split and pre-processing methods. Interestingly, even though they commonly take sequential coordinates of observations as input and infer future paths in the same coordinates as output, designing specialized architectures for each task is still necessary. For the other task, generality issues can lead to sub-optimal performances. In this paper, we propose SingularTrajectory, a <b>diffusion-based</b> <b>universal</b> trajectory prediction framework to reduce the performance gap across the five tasks. The core of SingularTrajectory is to unify a variety of human dynamics representations on the associated tasks. To do this, we first build a Singular space to project all types of motion patterns from each task into one embedding space. We next propose an adaptive anchor working in the Singular space. Unlike traditional fixed anchor methods that sometimes yield unacceptable paths, our adaptive anchor enables correct anchors, which are put into a wrong location, based on a traversability map. Finally, we adopt a <b>diffusion-based</b> <b>predictor</b> to further enhance the prototype paths using a cascaded denoising process. Our unified framework ensures the generality across various <b>benchmark</b> settings such as input modality, and trajectory lengths. Extensive experiments on five public <b>benchmarks</b> demonstrate that SingularTrajectory substantially outperforms existing models, highlighting its effectiveness in estimating general dynamics of human movements. Code is publicly available at <a href=https://github.com/inhwanbae/SingularTrajectory>https://github.com/inhwanbae/SingularTrajectory</a> .</p></p class="citation"></blockquote><h3 id=3282--115318-efficient-test-time-adaptation-of-vision-language-models-adilbek-karmanov-et-al-2024>(32/82 | 115/318) Efficient Test-Time Adaptation of Vision-Language Models (Adilbek Karmanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, Eric Xing. (2024)<br><strong>Efficient Test-Time Adaptation of Vision-Language Models</strong><br><button class=copy-to-clipboard title="Efficient Test-Time Adaptation of Vision-Language Models" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift, Few-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18293v1.pdf filename=2403.18293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-time adaptation with pre-trained <b>vision-language</b> models has attracted increasing attention for tackling <b>distribution</b> <b>shifts</b> during the test time. Though prior studies have achieved very promising performance, they involve intensive computation which is severely unaligned with test-time adaptation. We design TDA, a training-free dynamic adapter that enables effective and efficient test-time adaptation with <b>vision-language</b> models. TDA works with a lightweight key-value cache that maintains a dynamic queue with <b>few-shot</b> pseudo labels as values and the corresponding test-sample features as keys. Leveraging the key-value cache, TDA allows adapting to test data gradually via progressive pseudo label refinement which is super-efficient without incurring any backpropagation. In addition, we introduce negative pseudo labeling that alleviates the adverse impact of pseudo label noises by assigning pseudo labels to certain negative classes when the model is uncertain about its pseudo label predictions. Extensive experiments over two <b>benchmarks</b> demonstrate TDA&rsquo;s superior effectiveness and efficiency as compared with the state-of-the-art. The code has been released in \url{https://kdiaaa.github.io/tda/}.</p></p class="citation"></blockquote><h3 id=3382--116318-image-deraining-via-self-supervised-reinforcement-learning-he-hao-liao-et-al-2024>(33/82 | 116/318) Image Deraining via Self-supervised Reinforcement Learning (He-Hao Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He-Hao Liao, Yan-Tsung Peng, Wen-Tao Chu, Ping-Chun Hsieh, Chung-Chi Tsai. (2024)<br><strong>Image Deraining via Self-supervised Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Image Deraining via Self-supervised Reinforcement Learning" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 33<br>Keywords: Benchmarking, Few-shot, Reinforcement Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18270v1.pdf filename=2403.18270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quality of images captured outdoors is often affected by the weather. One factor that interferes with sight is rain, which can obstruct the view of observers and computer vision applications that rely on those images. The work aims to recover rain images by removing rain streaks via <b>Self-supervised</b> <b>Reinforcement</b> <b>Learning</b> (RL) for image deraining (SRL-Derain). We locate rain streak pixels from the input rain image via dictionary learning and use pixel-wise RL agents to take multiple inpainting actions to remove rain progressively. To our knowledge, this work is the first attempt where <b>self-supervised</b> RL is applied to image deraining. Experimental results on several <b>benchmark</b> image-deraining datasets show that the proposed SRL-Derain performs favorably against state-of-the-art <b>few-shot</b> and <b>self-supervised</b> deraining and denoising methods.</p></p class="citation"></blockquote><h3 id=3482--117318-garment3dgen-3d-garment-stylization-and-texture-generation-nikolaos-sarafianos-et-al-2024>(34/82 | 117/318) Garment3DGen: 3D Garment Stylization and Texture Generation (Nikolaos Sarafianos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikolaos Sarafianos, Tuur Stuyck, Xiaoyu Xiang, Yilei Li, Jovan Popovic, Rakesh Ranjan. (2024)<br><strong>Garment3DGen: 3D Garment Stylization and Texture Generation</strong><br><button class=copy-to-clipboard title="Garment3DGen: 3D Garment Stylization and Texture Generation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18816v1.pdf filename=2403.18816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance. Our proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text <b>prompts.</b> The generated assets can be directly draped and simulated on human bodies. First, we leverage the recent progress of image to 3D diffusion methods to generate 3D garment geometries. However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target. Second, we introduce carefully designed losses that allow the input base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated. Finally, a texture estimation module generates high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets. With Garment3DGen users can generate the textured 3D garment of their choice without the need of artist intervention. One can provide a textual <b>prompt</b> describing the garment they desire to generate a <b>simulation-ready</b> 3D asset. We present a plethora of quantitative and qualitative comparisons on various assets both real and generated and provide use-cases of how one can generate <b>simulation-ready</b> 3D garments.</p></p class="citation"></blockquote><h3 id=3582--118318-ecodepth-effective-conditioning-of-diffusion-models-for-monocular-depth-estimation-suraj-patni-et-al-2024>(35/82 | 118/318) ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation (Suraj Patni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suraj Patni, Aradhye Agarwal, Chetan Arora. (2024)<br><strong>ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation</strong><br><button class=copy-to-clipboard title="ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Zero-shot, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18807v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18807v2.pdf filename=2403.18807v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based <b>text</b> <b>embeddings.</b> Based on this idea, we propose a new SIDE model using a <b>diffusion</b> <b>backbone</b> which is conditioned on ViT embeddings. Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GEDepth). For <b>zero-shot</b> transfer with a model trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%) over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoeDepth. The code is available at <a href=https://ecodepth-iitd.github.io>https://ecodepth-iitd.github.io</a></p></p class="citation"></blockquote><h3 id=3682--119318-i2ckd--intra--and-inter-class-knowledge-distillation-for-semantic-segmentation-ayoub-karine-et-al-2024>(36/82 | 119/318) I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic Segmentation (Ayoub Karine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayoub Karine, Thibault Napoléon, Maher Jridi. (2024)<br><strong>I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic Segmentation</strong><br><button class=copy-to-clipboard title="I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic Segmentation" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18490v1.pdf filename=2403.18490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a new <b>knowledge</b> <b>distillation</b> method tailored for image semantic segmentation, termed Intra- and Inter-Class <b>Knowledge</b> <b>Distillation</b> (I2CKD). The focus of this method is on capturing and transferring <b>knowledge</b> <b>between</b> the intermediate layers of teacher (cumbersome model) and student (compact model). For <b>knowledge</b> <b>extraction,</b> we exploit class prototypes derived from feature maps. To facilitate <b>knowledge</b> <b>transfer,</b> we employ a triplet loss in order to minimize intra-class variances and maximize inter-class variances between teacher and student prototypes. Consequently, I2CKD enables the student to better mimic the feature representation of the teacher for each class, thereby enhancing the segmentation performance of the compact network. Extensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal VOC and CamVid, using various teacher-student network pairs demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=3782--120318-doda-diffusion-for-object-detection-domain-adaptation-in-agriculture-shuai-xiang-et-al-2024>(37/82 | 120/318) DODA: Diffusion for Object-detection Domain Adaptation in Agriculture (Shuai Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Xiang, Pieter M. Blok, James Burridge, Haozhou Wang, Wei Guo. (2024)<br><strong>DODA: Diffusion for Object-detection Domain Adaptation in Agriculture</strong><br><button class=copy-to-clipboard title="DODA: Diffusion for Object-detection Domain Adaptation in Agriculture" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Object Detection, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18334v1.pdf filename=2403.18334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The diverse and high-quality content generated by recent generative models demonstrates the great potential of using synthetic data to train downstream models. However, in vision, especially in objection detection, related areas are not fully explored, the synthetic images are merely used to balance the long tails of existing datasets, and the accuracy of the generated labels is low, the full potential of generative models has not been exploited. In this paper, we propose DODA, a data synthesizer that can generate high-quality <b>object</b> <b>detection</b> data for new <b>domains</b> <b>in</b> agriculture. Specifically, we improve the controllability of layout-to-image through encoding layout as an image, thereby improving the quality of labels, and use a visual encoder to provide visual clues for the <b>diffusion</b> <b>model</b> to decouple visual features from the <b>diffusion</b> <b>model,</b> and empowering the model the ability to generate data in new <b>domains.</b> <b>On</b> the Global Wheat Head Detection (GWHD) Dataset, which is the largest dataset in agriculture and contains diverse <b>domains,</b> <b>using</b> the data synthesized by DODA improves the performance of the <b>object</b> <b>detector</b> by 12.74-17.76 AP$_{50}$ in the <b>domain</b> <b>that</b> was significantly shifted from the training data.</p></p class="citation"></blockquote><h3 id=3882--121318-unleashing-the-potential-of-sam-for-medical-adaptation-via-hierarchical-decoding-zhiheng-cheng-et-al-2024>(38/82 | 121/318) Unleashing the Potential of SAM for Medical Adaptation via Hierarchical Decoding (Zhiheng Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiheng Cheng, Qingyue Wei, Hongru Zhu, Yan Wang, Liangqiong Qu, Wei Shao, Yuyin Zhou. (2024)<br><strong>Unleashing the Potential of SAM for Medical Adaptation via Hierarchical Decoding</strong><br><button class=copy-to-clipboard title="Unleashing the Potential of SAM for Medical Adaptation via Hierarchical Decoding" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18271v1.pdf filename=2403.18271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Segment Anything Model (SAM) has garnered significant attention for its versatile segmentation abilities and intuitive <b>prompt-based</b> interface. However, its application in medical imaging presents challenges, requiring either substantial training costs and extensive medical datasets for full model <b>fine-tuning</b> or high-quality <b>prompts</b> for optimal performance. This paper introduces H-SAM: a <b>prompt-free</b> adaptation of SAM tailored for efficient <b>fine-tuning</b> of medical images via a two-stage hierarchical decoding procedure. In the initial stage, H-SAM employs SAM&rsquo;s original decoder to generate a prior probabilistic mask, guiding a more intricate decoding process in the second stage. Specifically, we propose two key designs: 1) A class-balanced, mask-guided <b>self-attention</b> mechanism addressing the unbalanced label distribution, enhancing image embedding; 2) A learnable mask cross-attention mechanism spatially modulating the interplay among different image regions based on the prior mask. Moreover, the inclusion of a hierarchical pixel decoder in H-SAM enhances its proficiency in capturing fine-grained and localized details. This approach enables SAM to effectively integrate learned medical priors, facilitating enhanced adaptation for medical image segmentation with limited samples. Our H-SAM demonstrates a 4.78% improvement in average Dice compared to existing <b>prompt-free</b> SAM variants for multi-organ segmentation using only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM even outperforms state-of-the-art semi-supervised models relying on extensive unlabeled training data across various medical datasets. Our code is available at <a href=https://github.com/Cccccczh404/H-SAM>https://github.com/Cccccczh404/H-SAM</a>.</p></p class="citation"></blockquote><h3 id=3982--122318-road-obstacle-detection-based-on-unknown-objectness-scores-chihiro-noguchi-et-al-2024>(39/82 | 122/318) Road Obstacle Detection based on Unknown Objectness Scores (Chihiro Noguchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chihiro Noguchi, Toshiaki Ohgushi, Masao Yamanaka. (2024)<br><strong>Road Obstacle Detection based on Unknown Objectness Scores</strong><br><button class=copy-to-clipboard title="Road Obstacle Detection based on Unknown Objectness Scores" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Anomaly Detection, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18207v1.pdf filename=2403.18207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The detection of unknown traffic obstacles is vital to ensure safe autonomous driving. The standard <b>object-detection</b> <b>methods</b> cannot identify unknown <b>objects</b> <b>that</b> are not included under predefined categories. This is because <b>object-detection</b> <b>methods</b> are trained to assign a background label to pixels corresponding to the presence of unknown <b>objects.</b> <b>To</b> address this problem, the pixel-wise <b>anomaly-detection</b> <b>approach</b> has attracted increased research attention. <b>Anomaly-detection</b> <b>techniques,</b> such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown <b>objects</b> <b>as</b> <b>out-of-distribution</b> (OoD) samples. However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance. The purpose of this study is to achieve stable performance for detecting unknown <b>objects</b> <b>by</b> incorporating the <b>object-detection</b> <b>fashions</b> into the pixel-wise <b>anomaly</b> <b>detection</b> methods. To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise <b>anomaly</b> <b>scores</b> and objectness scores. Our experimental results show that the objectness scores play an important role in improving the detection performance. Based on these results, we propose a novel <b>anomaly</b> <b>score</b> by integrating these two scores, which we term as unknown objectness score. Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets.</p></p class="citation"></blockquote><h3 id=4082--123318-few-shot-online-anomaly-detection-and-segmentation-shenxing-wei-et-al-2024>(40/82 | 123/318) Few-shot Online Anomaly Detection and Segmentation (Shenxing Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shenxing Wei, Xing Wei, Zhiheng Ma, Songlin Dong, Shaochen Zhang, Yihong Gong. (2024)<br><strong>Few-shot Online Anomaly Detection and Segmentation</strong><br><button class=copy-to-clipboard title="Few-shot Online Anomaly Detection and Segmentation" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Convolutional Neural Network, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18201v1.pdf filename=2403.18201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting <b>anomaly</b> <b>patterns</b> from images is a crucial artificial intelligence technique in industrial applications. Recent research in this domain has emphasized the necessity of a large volume of training data, overlooking the practical scenario where, post-deployment of the model, unlabeled data containing both normal and abnormal samples can be utilized to enhance the model&rsquo;s performance. Consequently, this paper focuses on addressing the challenging yet practical <b>few-shot</b> online <b>anomaly</b> <b>detection</b> and segmentation (FOADS) task. Under the FOADS framework, models are trained on a <b>few-shot</b> normal dataset, followed by inspection and improvement of their capabilities by leveraging unlabeled streaming data containing both normal and abnormal samples simultaneously. To tackle this issue, we propose modeling the feature distribution of normal images using a Neural Gas network, which offers the flexibility to adapt the topology structure to identify outliers in the data flow. In order to achieve improved performance with limited training samples, we employ multi-scale feature embedding extracted from a <b>CNN</b> pre-trained on ImageNet to obtain a robust representation. Furthermore, we introduce an algorithm that can incrementally update parameters without the need to store previous samples. Comprehensive experimental results demonstrate that our method can achieve substantial performance under the FOADS setting, while ensuring that the time complexity remains within an acceptable range on MVTec AD and BTAD datasets.</p></p class="citation"></blockquote><h3 id=4182--124318-multi-layer-dense-attention-decoder-for-polyp-segmentation-krushi-patel-et-al-2024>(41/82 | 124/318) Multi-Layer Dense Attention Decoder for Polyp Segmentation (Krushi Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Krushi Patel, Fengjun Li, Guanghui Wang. (2024)<br><strong>Multi-Layer Dense Attention Decoder for Polyp Segmentation</strong><br><button class=copy-to-clipboard title="Multi-Layer Dense Attention Decoder for Polyp Segmentation" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18180v1.pdf filename=2403.18180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting and segmenting polyps is crucial for expediting the diagnosis of colon cancer. This is a challenging task due to the large variations of polyps in color, texture, and lighting conditions, along with subtle differences between the polyp and its surrounding area. Recently, <b>vision</b> <b>Transformers</b> have shown robust abilities in modeling global context for polyp segmentation. However, they face two major limitations: the inability to learn local relations among multi-level layers and inadequate feature aggregation in the decoder. To address these issues, we propose a novel decoder architecture aimed at hierarchically aggregating locally enhanced multi-level dense features. Specifically, we introduce a novel module named Dense Attention Gate (DAG), which adaptively fuses all previous layers&rsquo; features to establish local feature relations among all layers. Furthermore, we propose a novel nested decoder architecture that hierarchically aggregates decoder features, thereby enhancing semantic features. We incorporate our novel dense decoder with the PVT backbone network and conduct evaluations on five polyp segmentation datasets: Kvasir, CVC-300, CVC-ColonDB, CVC-ClinicDB, and ETIS. Our experiments and comparisons with nine competing segmentation models demonstrate that the proposed architecture achieves state-of-the-art performance and outperforms the previous models on four datasets. The source code is available at: <a href=https://github.com/krushi1992/Dense-Decoder>https://github.com/krushi1992/Dense-Decoder</a>.</p></p class="citation"></blockquote><h3 id=4282--125318-envisioning-medclip-a-deep-dive-into-explainability-for-medical-vision-language-models-anees-ur-rehman-hashmi-et-al-2024>(42/82 | 125/318) Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models (Anees Ur Rehman Hashmi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anees Ur Rehman Hashmi, Dwarikanath Mahapatra, Mohammad Yaqub. (2024)<br><strong>Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models</strong><br><button class=copy-to-clipboard title="Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Explainable AI, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18996v1.pdf filename=2403.18996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explaining Deep Learning models is becoming increasingly important in the face of daily emerging <b>multimodal</b> models, particularly in safety-critical domains like medical imaging. However, the lack of detailed investigations into the performance of explainability methods on these models is widening the gap between their development and safe deployment. In this work, we analyze the performance of various <b>explainable</b> <b>AI</b> methods on a <b>vision-language</b> model, MedCLIP, to demystify its inner workings. We also provide a simple methodology to overcome the shortcomings of these methods. Our work offers a different new perspective on the explainability of a recent well-known VLM in the medical domain and our assessment method is generalizable to other current and possible future VLMs.</p></p class="citation"></blockquote><h3 id=4382--126318-imagenet-d-benchmarking-neural-network-robustness-on-diffusion-synthetic-object-chenshuang-zhang-et-al-2024>(43/82 | 126/318) ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object (Chenshuang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao. (2024)<br><strong>ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object</strong><br><button class=copy-to-clipboard title="ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Diffusion Model, Benchmarking, Benchmarking, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18775v1.pdf filename=2403.18775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We establish rigorous <b>benchmarks</b> for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness <b>benchmarks</b> are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that <b>benchmark</b> deep models&rsquo; robustness. Leveraging <b>diffusion</b> <b>models,</b> we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this <b>benchmark</b> as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest <b>foundation</b> <b>models</b> like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60%. Our work suggests that <b>diffusion</b> <b>models</b> can be an effective source to test vision models. The code and dataset are available at <a href=https://github.com/chenshuang-zhang/imagenet_d>https://github.com/chenshuang-zhang/imagenet_d</a>.</p></p class="citation"></blockquote><h3 id=4482--127318-pipnet3d-interpretable-detection-of-alzheimer-in-mri-scans-lisa-anita-de-santi-et-al-2024>(44/82 | 127/318) PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans (Lisa Anita De Santi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lisa Anita De Santi, Jörg Schlötterer, Michael Scheschenja, Joel Wessendorf, Meike Nauta, Vincenzo Positano, Christin Seifert. (2024)<br><strong>PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans</strong><br><button class=copy-to-clipboard title="PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Black Box, Unsupervised Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18328v1.pdf filename=2403.18328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Information from neuroimaging examinations (CT, MRI) is increasingly used to support diagnoses of dementia, e.g., Alzheimer&rsquo;s disease. While current clinical practice is mainly based on visual inspection and feature engineering, Deep Learning approaches can be used to automate the analysis and to discover new image-biomarkers. Part-prototype neural networks (PP-NN) are an alternative to standard blackbox models, and have shown promising results in general computer vision. PP-NN&rsquo;s base their <b>reasoning</b> on prototypical image regions that are learned fully <b>unsupervised,</b> and combined with a simple-to-understand decision layer. We present PIPNet3D, a PP-NN for volumetric images. We apply PIPNet3D to the clinical case study of Alzheimer&rsquo;s Disease diagnosis from structural Magnetic Resonance Imaging (sMRI). We assess the quality of prototypes under a systematic evaluation framework, propose new metrics to evaluate brain prototypes and perform an evaluation with domain experts. Our results show that PIPNet3D is an interpretable, compact model for Alzheimer&rsquo;s diagnosis with its <b>reasoning</b> well aligned to medical domain knowledge. Notably, PIPNet3D achieves the same accuracy as its blackbox counterpart; and removing the remaining clinically irrelevant prototypes from its decision process does not decrease predictive performance.</p></p class="citation"></blockquote><h3 id=4582--128318-towards-non-exemplar-semi-supervised-class-incremental-learning-wenzhuo-liu-et-al-2024>(45/82 | 128/318) Towards Non-Exemplar Semi-Supervised Class-Incremental Learning (Wenzhuo Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu. (2024)<br><strong>Towards Non-Exemplar Semi-Supervised Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="Towards Non-Exemplar Semi-Supervised Class-Incremental Learning" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Contrastive Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18291v1.pdf filename=2403.18291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks perform remarkably well in close-world scenarios. However, novel classes emerged continually in real applications, making it necessary to learn incrementally. Class-incremental learning (CIL) aims to gradually recognize new classes while maintaining the discriminability of old ones. Existing CIL methods have two limitations: a heavy reliance on preserving old data for forgetting mitigation and the need for vast labeled data for knowledge adaptation. To overcome these issues, we propose a non-exemplar semi-supervised CIL framework with <b>contrastive</b> <b>learning</b> and semi-supervised incremental prototype classifier (Semi-IPC). On the one hand, <b>contrastive</b> <b>learning</b> helps the model learn rich representations, easing the trade-off between learning representations of new classes and forgetting that of old classes. On the other hand, Semi-IPC learns a prototype for each class with <b>unsupervised</b> regularization, enabling the model to incrementally learn from partially labeled new data while maintaining the knowledge of old classes. Experiments on <b>benchmark</b> datasets demonstrate the strong performance of our method: without storing any old samples and only using less than 1% of labels, Semi-IPC outperforms advanced exemplar-based methods. We hope our work offers new insights for future CIL research. The code will be made publicly available.</p></p class="citation"></blockquote><h3 id=4682--129318-neusdfusion-a-spatial-aware-generative-model-for-3d-shape-completion-reconstruction-and-generation-ruikai-cui-et-al-2024>(46/82 | 129/318) NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation (Ruikai Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji. (2024)<br><strong>NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation</strong><br><button class=copy-to-clipboard title="NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Autoencoder, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18241v1.pdf filename=2403.18241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a <b>transformer-based</b> <b>autoencoder</b> structure, promoting the preservation of spatial relationships in the generated 3D shapes. This yields an algorithm that consistently outperforms state-of-the-art 3D shape generation methods on various tasks, including unconditional shape generation, <b>multi-modal</b> shape completion, single-view reconstruction, and text-to-shape synthesis.</p></p class="citation"></blockquote><h3 id=4782--130318-neuropictor-refining-fmri-to-image-reconstruction-via-multi-individual-pretraining-and-multi-level-modulation-jingyang-huo-et-al-2024>(47/82 | 130/318) NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation (Jingyang Huo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyang Huo, Yikai Wang, Xuelin Qian, Yun Wang, Chong Li, Jianfeng Feng, Yanwei Fu. (2024)<br><strong>NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation</strong><br><button class=copy-to-clipboard title="NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18211v1.pdf filename=2403.18211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained <b>diffusion</b> <b>models.</b> These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of <b>diffusion</b> <b>models</b> using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide <b>diffusion</b> <b>model</b> with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particular individual. NeuroPictor extracts high-level semantic features from fMRI signals that characterizing the visual stimulus and incrementally <b>fine-tunes</b> the <b>diffusion</b> <b>model</b> with a low-level manipulation network to provide precise structural instructions. By training with over 60,000 fMRI-image pairs from various individuals, our model enjoys superior fMRI-to-image decoding capacity, particularly in the within-subject setting, as evidenced in <b>benchmark</b> datasets. Project page: <a href=https://jingyanghuo.github.io/neuropictor/>https://jingyanghuo.github.io/neuropictor/</a>.</p></p class="citation"></blockquote><h3 id=4882--131318-lift3d-zero-shot-lifting-of-any-2d-vision-model-to-3d-mukund-varma-t-et-al-2024>(48/82 | 131/318) Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D (Mukund Varma T et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mukund Varma T, Peihao Wang, Zhiwen Fan, Zhangyang Wang, Hao Su, Ravi Ramamoorthi. (2024)<br><strong>Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D</strong><br><button class=copy-to-clipboard title="Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18922v1.pdf filename=2403.18922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been an explosion of 2D vision models for numerous tasks such as semantic segmentation, <b>style</b> <b>transfer</b> or scene editing, enabled by large-scale 2D image datasets. At the same time, there has been renewed interest in 3D scene representations such as neural radiance fields from multi-view images. However, the availability of 3D or multiview data is still substantially limited compared to 2D image datasets, making extending 2D vision models to 3D data highly desirable but also very challenging. Indeed, extending a single 2D vision operator like scene editing to 3D typically requires a highly creative method specialized to that task and often requires per-scene optimization. In this paper, we ask the question of whether any 2D vision model can be lifted to make 3D consistent predictions. We answer this question in the affirmative; our new Lift3D method trains to predict unseen views on feature spaces generated by a few visual models (i.e. DINO and CLIP), but then generalizes to novel vision operators and tasks, such as <b>style</b> <b>transfer,</b> super-resolution, open vocabulary segmentation and image colorization; for some of these tasks, there is no comparable previous 3D method. In many cases, we even outperform state-of-the-art methods specialized for the task in question. Moreover, Lift3D is a <b>zero-shot</b> method, in the sense that it requires no task-specific training, nor scene-specific optimization.</p></p class="citation"></blockquote><h3 id=4982--132318-gamba-marry-gaussian-splatting-with-mamba-for-single-view-3d-reconstruction-qiuhong-shen-et-al-2024>(49/82 | 132/318) Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction (Qiuhong Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang. (2024)<br><strong>Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction</strong><br><button class=copy-to-clipboard title="Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18795v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18795v2.pdf filename=2403.18795v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines. Previous methods primarily rely on Score <b>Distillation</b> Sampling (SDS) and Neural Radiance Fields (NeRF). Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent <b>reasoning</b> and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians. Gamba incorporates significant advancements in data preprocessing, regularization design, and training methodologies. We assessed Gamba against existing optimization-based and feed-forward 3D generation approaches using the real-world scanned OmniObject3D dataset. Here, Gamba demonstrates competitive generation capabilities, both qualitatively and quantitatively, while achieving remarkable speed, approximately 0.6 second on a single NVIDIA A100 GPU.</p></p class="citation"></blockquote><h3 id=5082--133318-density-guided-translator-boosts-synthetic-to-real-unsupervised-domain-adaptive-segmentation-of-3d-point-clouds-zhimin-yuan-et-al-2024>(50/82 | 133/318) Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds (Zhimin Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan Guo, Cheng Wang. (2024)<br><strong>Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds</strong><br><button class=copy-to-clipboard title="Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18469v1.pdf filename=2403.18469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D synthetic-to-real <b>unsupervised</b> domain adaptive segmentation is crucial to annotating new domains. Self-training is a competitive approach for this task, but its performance is limited by different sensor sampling patterns (i.e., variations in point density) and incomplete training strategies. In this work, we propose a density-guided translator (DGT), which translates point density between domains, and integrates it into a two-stage self-training pipeline named DGT-ST. First, in contrast to existing works that simultaneously conduct data generation and feature/output alignment within unstable <b>adversarial</b> <b>training,</b> we employ the non-learnable DGT to bridge the domain gap at the input level. Second, to provide a well-initialized model for self-training, we propose a category-level <b>adversarial</b> <b>network</b> in stage one that utilizes the prototype to prevent negative transfer. Finally, by leveraging the designs above, a domain-mixed self-training method with source-aware consistency loss is proposed in stage two to narrow the domain gap further. Experiments on two synthetic-to-real segmentation tasks (SynLiDAR $\rightarrow$ semanticKITTI and SynLiDAR $\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms state-of-the-art methods, achieving 9.4$%$ and 4.3$%$ mIoU improvements, respectively. Code is available at \url{https://github.com/yuan-zm/DGT-ST}.</p></p class="citation"></blockquote><h3 id=5182--134318-u-sketch-an-efficient-approach-for-sketch-to-image-diffusion-models-ilias-mitsouras-et-al-2024>(51/82 | 134/318) U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models (Ilias Mitsouras et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos. (2024)<br><strong>U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models</strong><br><button class=copy-to-clipboard title="U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18425v1.pdf filename=2403.18425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have demonstrated remarkable performance in <b>text-to-image</b> synthesis, producing realistic and high resolution images that faithfully adhere to the corresponding text-prompts. Despite their great success, they still fall behind in sketch-to-image synthesis tasks, where in addition to text-prompts, the spatial layout of the generated images has to closely follow the outlines of certain reference sketches. Employing an MLP latent edge predictor to guide the spatial layout of the synthesized image by predicting edge maps at each denoising step has been recently proposed. Despite yielding promising results, the pixel-wise operation of the MLP does not take into account the spatial layout as a whole, and demands numerous denoising iterations to produce satisfactory images, leading to time inefficiency. To this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge predictor, which is capable of efficiently capturing both local and global features, as well as spatial correlations between pixels. Moreover, we propose the addition of a sketch simplification network that offers the user the choice of preprocessing and simplifying input sketches for enhanced outputs. The experimental results, corroborated by user feedback, demonstrate that our proposed U-Net latent edge predictor leads to more realistic results, that are better aligned with the spatial outlines of the reference sketches, while drastically reducing the number of required denoising steps and, consequently, the overall execution time.</p></p class="citation"></blockquote><h3 id=5282--135318-ecnet-effective-controllable-text-to-image-diffusion-models-sicheng-li-et-al-2024>(52/82 | 135/318) ECNet: Effective Controllable Text-to-Image Diffusion Models (Sicheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sicheng Li, Keqiang Sun, Zhixin Lai, Xiaoshi Wu, Feng Qiu, Haoran Xie, Kazunori Miyata, Hongsheng Li. (2024)<br><strong>ECNet: Effective Controllable Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="ECNet: Effective Controllable Text-to-Image Diffusion Models" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18417v1.pdf filename=2403.18417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The conditional <b>text-to-image</b> <b>diffusion</b> <b>models</b> have garnered significant attention in recent years. However, the precision of these models is often compromised mainly for two reasons, ambiguous condition input and inadequate condition guidance over single denoising loss. To address the challenges, we introduce two innovative solutions. Firstly, we propose a Spatial Guidance Injector (SGI) which enhances conditional detail by encoding text inputs with precise annotation information. This method directly tackles the issue of ambiguous control inputs by providing clear, annotated guidance to the model. Secondly, to overcome the issue of limited conditional supervision, we introduce <b>Diffusion</b> <b>Consistency</b> Loss (DCL), which applies supervision on the denoised latent code at any given time step. This encourages consistency between the latent code at each time step and the input signal, thereby enhancing the robustness and accuracy of the output. The combination of SGI and DCL results in our Effective Controllable Network (ECNet), which offers a more accurate controllable end-to-end <b>text-to-image</b> generation framework with a more precise conditioning input and stronger controllable supervision. We validate our approach through extensive experiments on generation under various conditions, such as human body skeletons, facial landmarks, and sketches of general objects. The results consistently demonstrate that our method significantly enhances the controllability and robustness of the generated images, outperforming existing state-of-the-art controllable <b>text-to-image</b> models.</p></p class="citation"></blockquote><h3 id=5382--136318-bam-box-abstraction-monitors-for-real-time-ood-detection-in-object-detection-changshun-wu-et-al-2024>(53/82 | 136/318) BAM: Box Abstraction Monitors for Real-time OoD Detection in Object Detection (Changshun Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changshun Wu, Weicheng He, Chih-Hong Cheng, Xiaowei Huang, Saddek Bensalem. (2024)<br><strong>BAM: Box Abstraction Monitors for Real-time OoD Detection in Object Detection</strong><br><button class=copy-to-clipboard title="BAM: Box Abstraction Monitors for Real-time OoD Detection in Object Detection" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18373v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18373v1.pdf filename=2403.18373v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OoD) detection techniques for deep neural networks (DNNs) become crucial thanks to their filtering of abnormal inputs, especially when DNNs are used in safety-critical applications and interact with an open and dynamic environment. Nevertheless, integrating OoD detection into state-of-the-art (SOTA) <b>object</b> <b>detection</b> DNNs poses significant challenges, partly due to the complexity introduced by the SOTA OoD construction methods, which require the modification of DNN architecture and the introduction of complex loss functions. This paper proposes a simple, yet surprisingly effective, method that requires neither retraining nor architectural change in <b>object</b> <b>detection</b> DNN, called Box Abstraction-based Monitors (BAM). The novelty of BAM stems from using a finite union of convex box abstractions to capture the learned features of <b>objects</b> <b>for</b> in-distribution (ID) data, and an important observation that features from OoD data are more likely to fall outside of these boxes. The union of convex regions within the feature space allows the formation of non-convex and interpretable decision boundaries, overcoming the limitations of VOS-like detectors without sacrificing real-time performance. Experiments integrating BAM into Faster R-CNN-based <b>object</b> <b>detection</b> DNNs demonstrate a considerably improved performance against SOTA OoD detection techniques.</p></p class="citation"></blockquote><h3 id=5482--137318-tracking-assisted-object-detection-with-event-cameras-ting-kang-yen-et-al-2024>(54/82 | 137/318) Tracking-Assisted Object Detection with Event Cameras (Ting-Kang Yen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin, Jia-Fong Yeh, Hung-Ting Su, Winston Hsu. (2024)<br><strong>Tracking-Assisted Object Detection with Event Cameras</strong><br><button class=copy-to-clipboard title="Tracking-Assisted Object Detection with Event Cameras" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18330v1.pdf filename=2403.18330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event-based <b>object</b> <b>detection</b> has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur. However, feature asynchronism and sparsity cause invisible <b>objects</b> <b>due</b> to no relative motion to the camera, posing a significant challenge in the task. Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues. While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively. In this paper, we consider those invisible <b>objects</b> <b>as</b> pseudo-occluded <b>objects</b> <b>and</b> aim to reveal their features. Firstly, we introduce visibility attribute of <b>objects</b> <b>and</b> contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset. Secondly, we exploit tracking strategies for pseudo-occluded <b>objects</b> <b>to</b> maintain their permanence and retain their bounding boxes, even when features have not been available for a very long time. These strategies can be treated as an explicit-learned memory guided by the tracking objective to record the displacements of <b>objects</b> <b>across</b> frames. Lastly, we propose a spatio-temporal feature aggregation module to enrich the latent features and a consistency loss to increase the robustness of the overall pipeline. We conduct comprehensive experiments to verify our method&rsquo;s effectiveness where still <b>objects</b> <b>are</b> retained but real occluded <b>objects</b> <b>are</b> discarded. The results demonstrate that (1) the additional visibility labels can assist in <b>supervised</b> training, and (2) our method outperforms state-of-the-art approaches with a significant improvement of 7.9% absolute mAP.</p></p class="citation"></blockquote><h3 id=5582--138318-sgdm-static-guided-dynamic-module-make-stronger-visual-models-wenjie-xing-et-al-2024>(55/82 | 138/318) SGDM: Static-Guided Dynamic Module Make Stronger Visual Models (Wenjie Xing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjie Xing, Zhenchao Cui, Jing Qi. (2024)<br><strong>SGDM: Static-Guided Dynamic Module Make Stronger Visual Models</strong><br><button class=copy-to-clipboard title="SGDM: Static-Guided Dynamic Module Make Stronger Visual Models" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18282v1.pdf filename=2403.18282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The spatial attention mechanism has been widely used to improve <b>object</b> <b>detection</b> performance. However, its operation is currently limited to static <b>convolutions</b> lacking content-adaptive features. This paper innovatively approaches from the perspective of dynamic <b>convolution.</b> We propose Razor Dynamic <b>Convolution</b> (RDConv) to address thetwo flaws in dynamic weight <b>convolution,</b> making it hard to implement in spatial mechanism: 1) it is computation-heavy; 2) when generating weights, spatial information is disregarded. Firstly, by using Razor Operation to generate certain features, we vastly reduce the parameters of the entire dynamic <b>convolution</b> operation. Secondly, we added a spatial branch inside RDConv to generate <b>convolutional</b> kernel parameters with richer spatial information. Embedding dynamic <b>convolution</b> will also bring the problem of sensitivity to high-frequency noise. We propose the Static-Guided Dynamic Module (SGDM) to address this limitation. By using SGDM, we utilize a set of asymmetric static <b>convolution</b> kernel parameters to guide the construction of dynamic <b>convolution.</b> We introduce the mechanism of shared weights in static <b>convolution</b> to solve the problem of dynamic <b>convolution</b> being sensitive to high-frequency noise. Extensive experiments illustrate that multiple different <b>object</b> <b>detection</b> backbones equipped with SGDM achieve a highly competitive boost in performance(e.g., +4% mAP with YOLOv5n on VOC and +1.7% mAP with YOLOv8n on COCO) with negligible parameter increase(i.e., +0.33M on YOLOv5n and +0.19M on YOLOv8n).</p></p class="citation"></blockquote><h3 id=5682--139318-fourier-or-wavelet-bases-as-counterpart-self-attention-in-spikformer-for-efficient-visual-classification-qingyu-wang-et-al-2024>(56/82 | 139/318) Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification (Qingyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyu Wang, Duzhen Zhang, Tilelin Zhang, Bo Xu. (2024)<br><strong>Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification</strong><br><button class=copy-to-clipboard title="Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-NE, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18228v1.pdf filename=2403.18228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Energy-efficient spikformer has been proposed by integrating the biologically plausible spiking neural network (SNN) and artificial <b>Transformer,</b> whereby the Spiking <b>Self-Attention</b> (SSA) is used to achieve both higher accuracy and lower computational cost. However, it seems that <b>self-attention</b> is not always necessary, especially in sparse spike-form calculation manners. In this paper, we innovatively replace vanilla SSA (using dynamic bases calculating from Query and Key) with spike-form Fourier Transform, Wavelet Transform, and their combinations (using fixed triangular or wavelets bases), based on a key hypothesis that both of them use a set of basis functions for information transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is proposed and verified in visual classification tasks, including both static image and event-based video datasets. The FWformer can achieve comparable or even higher accuracies ($0.4%$-$1.5%$), higher running speed ($9%$-$51%$ for training and $19%$-$70%$ for inference), reduced theoretical energy consumption ($20%$-$25%$), and reduced GPU memory usage ($4%$-$26%$), compared to the standard spikformer. Our result indicates the continuous refinement of new <b>Transformers,</b> that are inspired either by biological discovery (spike-form), or information theory (Fourier or Wavelet Transform), is promising.</p></p class="citation"></blockquote><h3 id=5782--140318-mitigating-hallucinations-in-large-vision-language-models-with-instruction-contrastive-decoding-xintong-wang-et-al-2024>(57/82 | 140/318) Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding (Xintong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann. (2024)<br><strong>Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding</strong><br><button class=copy-to-clipboard title="Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-MM, cs.CV<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18715v1.pdf filename=2403.18715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>Vision-Language</b> Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in <b>multimodal</b> decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in <b>multimodal</b> fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative <b>benchmarks</b> (POPE and MME) and a generative <b>benchmark</b> (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations. Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs.</p></p class="citation"></blockquote><h3 id=5882--141318-bringing-textual-prompt-to-ai-generated-image-quality-assessment-bowen-qu-et-al-2024>(58/82 | 141/318) Bringing Textual Prompt to AI-Generated Image Quality Assessment (Bowen Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Qu, Haohui Li, Wei Gao. (2024)<br><strong>Bringing Textual Prompt to AI-Generated Image Quality Assessment</strong><br><button class=copy-to-clipboard title="Bringing Textual Prompt to AI-Generated Image Quality Assessment" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18714v1.pdf filename=2403.18714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI-Generated Images (AGIs) have inherent <b>multimodal</b> nature. Unlike traditional image quality assessment (IQA) on natural scenarios, AGIs quality assessment (AGIQA) takes the correspondence of image and its textual <b>prompt</b> into consideration. This is coupled in the ground truth score, which confuses the unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs Quality Assessment via Image and <b>Prompt),</b> a <b>multimodal</b> framework for AGIQA via corresponding image and <b>prompt</b> incorporation. Specifically, we propose a novel incremental pretraining task named Image2Prompt for better understanding of AGIs and their corresponding textual <b>prompts.</b> An effective and efficient image-prompt fusion module, along with a novel special [QA] token, are also applied. Both are plug-and-play and beneficial for the cooperation of image and its corresponding <b>prompt.</b> Experiments demonstrate that our IP-IQA achieves the state-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.</p></p class="citation"></blockquote><h3 id=5982--142318-object-pose-estimation-via-the-aggregation-of-diffusion-features-tianfu-wang-et-al-2024>(59/82 | 142/318) Object Pose Estimation via the Aggregation of Diffusion Features (Tianfu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianfu Wang, Guosheng Hu, Hongguang Wang. (2024)<br><strong>Object Pose Estimation via the Aggregation of Diffusion Features</strong><br><button class=copy-to-clipboard title="Object Pose Estimation via the Aggregation of Diffusion Features" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18791v1.pdf filename=2403.18791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large <b>benchmarks.</b> However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of <b>diffusion</b> <b>models,</b> e.g. Stable <b>Diffusion,</b> <b>which</b> hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these <b>diffusion</b> <b>features</b> for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate <b>diffusion</b> <b>features</b> of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular <b>benchmark</b> datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at <a href=https://github.com/Tianfu18/diff-feats-pose>https://github.com/Tianfu18/diff-feats-pose</a>.</p></p class="citation"></blockquote><h3 id=6082--143318-handbooster-boosting-3d-hand-mesh-reconstruction-by-conditional-synthesis-and-sampling-of-hand-object-interactions-hao-xu-et-al-2024>(60/82 | 143/318) HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions (Hao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu. (2024)<br><strong>HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions</strong><br><button class=copy-to-clipboard title="HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18575v1.pdf filename=2403.18575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a <b>diffusion</b> <b>model</b> to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB <b>benchmarks.</b> Our code will be released on <a href=https://github.com/hxwork/HandBooster_Pytorch>https://github.com/hxwork/HandBooster_Pytorch</a>.</p></p class="citation"></blockquote><h3 id=6182--144318-a-semi-supervised-nighttime-dehazing-baseline-with-spatial-frequency-aware-and-realistic-brightness-constraint-xiaofeng-cong-et-al-2024>(61/82 | 144/318) A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint (Xiaofeng Cong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaofeng Cong, Jie Gui, Jing Zhang, Junming Hou, Hao Shen. (2024)<br><strong>A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint</strong><br><button class=copy-to-clipboard title="A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Semi-Supervised Training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18548v1.pdf filename=2403.18548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing research based on deep learning has extensively explored the problem of daytime image dehazing. However, few studies have considered the characteristics of nighttime hazy scenes. There are two distinctions between nighttime and daytime haze. First, there may be multiple active colored light sources with lower illumination intensity in nighttime scenes, which may cause haze, glow and noise with localized, coupled and frequency inconsistent characteristics. Second, due to the domain discrepancy between simulated and real-world data, unrealistic brightness may occur when applying a dehazing model trained on simulated data to real-world data. To address the above two issues, we propose a <b>semi-supervised</b> <b>model</b> for real-world nighttime dehazing. First, the spatial attention and frequency spectrum filtering are implemented as a spatial-frequency domain information interaction module to handle the first issue. Second, a pseudo-label-based retraining strategy and a local window-based brightness loss for <b>semi-supervised</b> <b>training</b> process is designed to suppress haze and glow while achieving realistic brightness. Experiments on public <b>benchmarks</b> validate the effectiveness of the proposed method and its superiority over state-of-the-art methods. The source code and Supplementary Materials are placed in the <a href=https://github.com/Xiaofeng-life/SFSNiD>https://github.com/Xiaofeng-life/SFSNiD</a>.</p></p class="citation"></blockquote><h3 id=6282--145318-backpropagation-free-network-for-3d-test-time-adaptation-yanshuo-wang-et-al-2024>(62/82 | 145/318) Backpropagation-free Network for 3D Test-time Adaptation (Yanshuo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanshuo Wang, Ali Cheraghian, Zeeshan Hayder, Jie Hong, Sameera Ramasinghe, Shafin Rahman, David Ahmedt-Aristizabal, Xuesong Li, Lars Petersson, Mehrtash Harandi. (2024)<br><strong>Backpropagation-free Network for 3D Test-time Adaptation</strong><br><button class=copy-to-clipboard title="Backpropagation-free Network for 3D Test-time Adaptation" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18442v1.pdf filename=2403.18442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world systems often encounter new data over time, which leads to experiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods tend to apply computationally heavy and memory-intensive backpropagation-based approaches to handle this. Here, we propose a novel method that uses a backpropagation-free approach for TTA for the specific case of 3D data. Our model uses a two-stream architecture to maintain knowledge about the source domain as well as complementary target-domain-specific information. The backpropagation-free property of our model helps address the well-known forgetting problem and mitigates the error accumulation issue. The proposed method also eliminates the need for the usually noisy process of pseudo-labeling and reliance on costly <b>self-supervised</b> training. Moreover, our method leverages subspace learning, effectively reducing the distribution variance between the two domains. Furthermore, the source-domain-specific and the target-domain-specific streams are aligned using a novel entropy-based adaptive fusion strategy. Extensive experiments on popular <b>benchmarks</b> demonstrate the effectiveness of our method. The code will be available at <a href=https://github.com/abie-e/BFTT3D>https://github.com/abie-e/BFTT3D</a>.</p></p class="citation"></blockquote><h3 id=6382--146318-generative-multi-modal-models-are-good-class-incremental-learners-xusheng-cao-et-al-2024>(63/82 | 146/318) Generative Multi-modal Models are Good Class-Incremental Learners (Xusheng Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng. (2024)<br><strong>Generative Multi-modal Models are Good Class-Incremental Learners</strong><br><button class=copy-to-clipboard title="Generative Multi-modal Models are Good Class-Incremental Learners" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Few-shot, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18383v1.pdf filename=2403.18383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier&rsquo;s bias towards the current task has long posed a significant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative <b>multi-modal</b> models, we would explore replacing discriminative models with generative ones for CIL. However, transitioning from discriminative to generative models requires addressing two key challenges. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative <b>multi-modal</b> model (GMM) framework for class-incremental learning. Our approach directly generates labels for images using an adapted generative model. After obtaining the detailed text, we use a text encoder to extract text features and employ feature matching to determine the most similar label as the classification prediction. In the conventional CIL settings, we achieve significantly better results in long-sequence task scenarios. Under the <b>Few-shot</b> CIL setting, we have improved by at least 14% accuracy over all the current state-of-the-art methods with significantly less forgetting. Our code is available at \url{https://github.com/DoubleClass/GMM}.</p></p class="citation"></blockquote><h3 id=6482--147318-dont-look-into-the-dark-latent-codes-for-pluralistic-image-inpainting-haiwei-chen-et-al-2024>(64/82 | 147/318) Don&rsquo;t Look into the Dark: Latent Codes for Pluralistic Image Inpainting (Haiwei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiwei Chen, Yajie Zhao. (2024)<br><strong>Don&rsquo;t Look into the Dark: Latent Codes for Pluralistic Image Inpainting</strong><br><button class=copy-to-clipboard title="Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18186v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18186v1.pdf filename=2403.18186v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a method for large-mask pluralistic image inpainting based on the generative framework of discrete latent codes. Our method learns latent priors, discretized as tokens, by only performing computations at the visible locations of the image. This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional <b>transformer</b> that infers the missing labels by only looking at these tokens, and a dedicated synthesis network that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings. Experiments on public <b>benchmarks</b> validate our design choices as the proposed method outperforms strong baselines in both visual quality and diversity metrics.</p></p class="citation"></blockquote><h3 id=6582--148318-egocentric-scene-aware-human-trajectory-prediction-weizhuo-wang-et-al-2024>(65/82 | 148/318) Egocentric Scene-aware Human Trajectory Prediction (Weizhuo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weizhuo Wang, C. Karen Liu, Monroe Kennedy III. (2024)<br><strong>Egocentric Scene-aware Human Trajectory Prediction</strong><br><button class=copy-to-clipboard title="Egocentric Scene-aware Human Trajectory Prediction" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19026v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19026v2.pdf filename=2403.19026v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons. Such a robot needs to be able to predict the ego motion of the wearer based on egocentric vision and the surrounding scene. In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings. To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user&rsquo;s perspective. We present a method to predict human motion conditioning on the surrounding static scene. Our method leverages a <b>diffusion</b> <b>model</b> to produce a distribution of potential future trajectories, taking into account the user&rsquo;s observation of the environment. We introduce a compact representation to encode the user&rsquo;s visual memory of the surroundings, as well as an efficient sample-generating technique to speed up real-time inference of a <b>diffusion</b> <b>model.</b> We ablate our model and compare it to baselines, and results show that our model outperforms existing methods on key metrics of collision avoidance and trajectory mode coverage.</p></p class="citation"></blockquote><h3 id=6682--149318-cross-domain-fiber-cluster-shape-analysis-for-language-performance-cognitive-score-prediction-yui-lo-et-al-2024>(66/82 | 149/318) Cross-domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction (Yui Lo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yui Lo, Yuqian Chen, Dongnan Liu, Wan Liu, Leo Zekelman, Fan Zhang, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Weidong Cai, Lauren J. O&rsquo;Donnell. (2024)<br><strong>Cross-domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction</strong><br><button class=copy-to-clipboard title="Cross-domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, eess-IV, q-bio-NC<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19001v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19001v2.pdf filename=2403.19001v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shape plays an important role in computer graphics, offering informative features to convey an object&rsquo;s morphology and functionality. Shape analysis in brain imaging can help interpret structural and functionality correlations of the human brain. In this work, we investigate the shape of the brain&rsquo;s 3D white matter connections and its potential predictive relationship to human cognitive function. We reconstruct brain connections as sequences of 3D points using diffusion magnetic resonance imaging (dMRI) tractography. To describe each connection, we extract 12 shape descriptors in addition to traditional dMRI connectivity and tissue microstructure features. We introduce a novel framework, Shape&ndash;fused Fiber Cluster <b>Transformer</b> (SFFormer), that leverages a multi-head cross-attention feature fusion module to predict subject-specific language performance based on dMRI tractography. We assess the performance of the method on a large dataset including 1065 healthy young adults. The results demonstrate that both the <b>transformer-based</b> SFFormer model and its inter/intra feature fusion with shape, microstructure, and connectivity are informative, and together, they improve the prediction of subject-specific language performance scores. Overall, our results indicate that the shape of the brain&rsquo;s connections is predictive of human language function.</p></p class="citation"></blockquote><h3 id=6782--150318-unidepth-universal-monocular-metric-depth-estimation-luigi-piccinelli-et-al-2024>(67/82 | 150/318) UniDepth: Universal Monocular Metric Depth Estimation (Luigi Piccinelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, Fisher Yu. (2024)<br><strong>UniDepth: Universal Monocular Metric Depth Estimation</strong><br><button class=copy-to-clipboard title="UniDepth: Universal Monocular Metric Depth Estimation" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18913v1.pdf filename=2403.18913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate monocular metric depth estimation (MMDE) is crucial to solving downstream tasks in 3D perception and modeling. However, the remarkable accuracy of recent MMDE methods is confined to their training domains. These methods fail to generalize to unseen domains even in the presence of moderate domain gaps, which hinders their practical applicability. We propose a new model, UniDepth, capable of reconstructing metric 3D scenes from solely single images across domains. Departing from the existing MMDE methods, UniDepth directly predicts metric 3D points from the input image at inference time without any additional information, striving for a universal and flexible MMDE solution. In particular, UniDepth implements a self-promptable camera module predicting dense camera representation to condition depth features. Our model exploits a pseudo-spherical output representation, which disentangles camera and depth representations. In addition, we propose a geometric invariance loss that promotes the invariance of camera-prompted depth features. Thorough evaluations on ten datasets in a <b>zero-shot</b> regime consistently demonstrate the superior performance of UniDepth, even when compared with methods directly trained on the testing domains. Code and models are available at: <a href=https://github.com/lpiccinelli-eth/unidepth>https://github.com/lpiccinelli-eth/unidepth</a></p></p class="citation"></blockquote><h3 id=6882--151318-enhancing-multiple-object-tracking-accuracy-via-quantum-annealing-yasuyuki-ihara-2024>(68/82 | 151/318) Enhancing Multiple Object Tracking Accuracy via Quantum Annealing (Yasuyuki Ihara, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasuyuki Ihara. (2024)<br><strong>Enhancing Multiple Object Tracking Accuracy via Quantum Annealing</strong><br><button class=copy-to-clipboard title="Enhancing Multiple Object Tracking Accuracy via Quantum Annealing" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV, quant-ph<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18908v1.pdf filename=2403.18908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiple object tracking (MOT), a key task in image recognition, presents a persistent challenge in balancing processing speed and tracking accuracy. This study introduces a novel approach that leverages quantum annealing <b>(QA)</b> to expedite computation speed, while enhancing tracking accuracy through the ensembling of object tracking processes. A method to improve the matching integration process is also proposed. By utilizing the sequential nature of MOT, this study further augments the tracking method via reverse annealing (RA). Experimental validation confirms the maintenance of high accuracy with an annealing time of a mere 3 $\mu$s per tracking process. The proposed method holds significant potential for real-time MOT applications, including traffic flow measurement for urban traffic light control, collision prediction for autonomous robots and vehicles, and management of products mass-produced in factories.</p></p class="citation"></blockquote><h3 id=6982--152318-annolid-annotate-segment-and-track-anything-you-need-chen-yang-et-al-2024>(69/82 | 152/318) Annolid: Annotate, Segment, and Track Anything You Need (Chen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Yang, Thomas A. Cleland. (2024)<br><strong>Annolid: Annotate, Segment, and Track Anything You Need</strong><br><button class=copy-to-clipboard title="Annolid: Annotate, Segment, and Track Anything You Need" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18690v1.pdf filename=2403.18690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Annolid is a deep learning-based software package designed for the segmentation, labeling, and tracking of research targets within video files, focusing primarily on animal behavior analysis. Based on state-of-the-art instance segmentation methods, Annolid now harnesses the Cutie video object segmentation model to achieve resilient, markerless tracking of multiple animals from single annotated frames, even in environments in which they may be partially or entirely concealed by environmental features or by one another. Our integration of Segment Anything and <b>Grounding-DINO</b> strategies additionally enables the automatic masking and segmentation of recognizable animals and objects by text command, removing the need for manual annotation. Annolid&rsquo;s comprehensive approach to object segmentation flexibly accommodates a broad spectrum of behavior analysis applications, enabling the classification of diverse behavioral states such as freezing, digging, pup huddling, and social interactions in addition to the tracking of animals and their body parts.</p></p class="citation"></blockquote><h3 id=7082--153318-homogeneous-tokenizer-matters-homogeneous-visual-tokenizer-for-remote-sensing-image-understanding-run-shao-et-al-2024>(70/82 | 153/318) Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding (Run Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Run Shao, Zhaoyang Zhang, Chao Tao, Yunsheng Zhang, Chengli Peng, Haifeng Li. (2024)<br><strong>Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding</strong><br><button class=copy-to-clipboard title="Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18593v1.pdf filename=2403.18593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The tokenizer, as one of the fundamental components of <b>large</b> <b>models,</b> <b>has</b> long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the <b>large</b> <b>language</b> <b>model</b> is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language. Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision. We designed a simple HOmogeneous visual tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity, the OPM splits the image into 4*4 pixel seeds and then utilizes the attention mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds within the same SIR. To achieve adaptability, the OVM defines a variable number of learnable vectors as cross-attention queries, allowing for the adjustment of token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19 classification dataset, and GID5 segmentation dataset for sparse and dense tasks. The results demonstrate that the visual tokens obtained by HOOK correspond to individual objects, which demonstrates homogeneity. HOOK outperformed Patch Embed by 6% and 10% in the two tasks and achieved state-of-the-art performance compared to the baselines used for comparison. Compared to Patch Embed, which requires more than one hundred tokens for one image, HOOK requires only 6 and 8 tokens for sparse and dense tasks, respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The code is available at <a href=https://github.com/GeoX-Lab/Hook>https://github.com/GeoX-Lab/Hook</a>.</p></p class="citation"></blockquote><h3 id=7182--154318-attention-calibration-for-disentangled-text-to-image-personalization-yanbing-zhang-et-al-2024>(71/82 | 154/318) Attention Calibration for Disentangled Text-to-Image Personalization (Yanbing Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanbing Zhang, Mengping Yang, Qin Zhou, Zhe Wang. (2024)<br><strong>Attention Calibration for Disentangled Text-to-Image Personalization</strong><br><button class=copy-to-clipboard title="Attention Calibration for Disentangled Text-to-Image Personalization" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18551v1.pdf filename=2403.18551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent thrilling progress in large-scale <b>text-to-image</b> (T2I) models has unlocked unprecedented synthesis quality of AI-generated content (AIGC) including image generation, 3D and video composition. Further, personalized techniques enable appealing customized production of a novel concept given only several images as reference. However, an intriguing problem persists: Is it possible to capture multiple, novel concepts from one single reference image? In this paper, we identify that existing approaches fail to preserve visual consistency with the reference image and eliminate cross-influence from concepts. To alleviate this, we propose an attention calibration mechanism to improve the concept-level understanding of the T2I model. Specifically, we first introduce new learnable modifiers bound with classes to capture attributes of multiple concepts. Then, the classes are separated and strengthened following the activation of the cross-attention operation, ensuring comprehensive and self-contained concepts. Additionally, we suppress the attention activation of different classes to mitigate mutual influence among concepts. Together, our proposed method, dubbed DisenDiff, can learn disentangled multiple concepts from one single image and produce novel customized images with learned concepts. We demonstrate that our method outperforms the current state of the art in both qualitative and quantitative evaluations. More importantly, our proposed techniques are compatible with LoRA and inpainting pipelines, enabling more interactive experiences.</p></p class="citation"></blockquote><h3 id=7282--155318-diffusionface-towards-a-comprehensive-dataset-for-diffusion-based-face-forgery-analysis-zhongxi-chen-et-al-2024>(72/82 | 155/318) DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face Forgery Analysis (Zhongxi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongxi Chen, Ke Sun, Ziyin Zhou, Xianming Lin, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji. (2024)<br><strong>DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face Forgery Analysis</strong><br><button class=copy-to-clipboard title="DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face Forgery Analysis" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18471v1.pdf filename=2403.18471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid progress in deep learning has given rise to hyper-realistic facial forgery methods, leading to concerns related to misinformation and security risks. Existing face forgery datasets have limitations in generating high-quality facial images and addressing the challenges posed by evolving generative techniques. To combat this, we present DiffusionFace, the first <b>diffusion-based</b> <b>face</b> forgery dataset, covering various forgery categories, including unconditional and Text Guide facial image generation, Img2Img, Inpaint, and <b>Diffusion-based</b> <b>facial</b> exchange algorithms. Our DiffusionFace dataset stands out with its extensive collection of 11 <b>diffusion</b> <b>models</b> and the high-quality of the generated images, providing essential metadata and a real-world internet-sourced forgery facial image dataset for evaluation. Additionally, we provide an in-depth analysis of the data and introduce practical evaluation protocols to rigorously assess discriminative models&rsquo; effectiveness in detecting counterfeit facial images, aiming to enhance security in facial image authentication processes. The dataset is available for download at \url{https://github.com/Rapisurazurite/DiffFace}.</p></p class="citation"></blockquote><h3 id=7382--156318-aic-unet-anatomy-informed-cascaded-unet-for-robust-multi-organ-segmentation-young-seok-jeon-et-al-2024>(73/82 | 156/318) AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ Segmentation (Young Seok Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Young Seok Jeon, Hongfei Yang, Huazhu Fu, Mengling Feng. (2024)<br><strong>AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ Segmentation</strong><br><button class=copy-to-clipboard title="AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ Segmentation" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18878v1.pdf filename=2403.18878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imposing key anatomical features, such as the number of organs, their shapes, sizes, and relative positions, is crucial for building a robust multi-organ segmentation model. Current attempts to incorporate anatomical features include broadening effective receptive fields (ERF) size with resource- and data-intensive modules such as <b>self-attention</b> or introducing organ-specific topology regularizers, which may not scale to multi-organ segmentation problems where inter-organ relation also plays a huge role. We introduce a new approach to impose anatomical constraints on any existing encoder-decoder segmentation model by conditioning model prediction with learnable anatomy prior. More specifically, given an abdominal scan, a part of the encoder spatially warps a learnable prior to align with the given input scan using thin plate spline (TPS) grid interpolation. The warped prior is then integrated during the decoding phase to guide the model for more anatomy-informed predictions. Code is available at \hyperlink{https://anonymous.4open.science/r/AIC-UNet-7048}{https://anonymous.4open.science/r/AIC-UNet-7048}.</p></p class="citation"></blockquote><h3 id=7482--157318-a-channel-ensemble-approach-unbiased-and-low-variance-pseudo-labels-is-critical-for-semi-supervised-classification-jiaqi-wu-et-al-2024>(74/82 | 157/318) A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is Critical for Semi-supervised Classification (Jiaqi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Wu, Junbiao Pang, Baochang Zhang, Qingming Huang. (2024)<br><strong>A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is Critical for Semi-supervised Classification</strong><br><button class=copy-to-clipboard title="A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is Critical for Semi-supervised Classification" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18407v1.pdf filename=2403.18407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Semi-supervised</b> <b>learning</b> (SSL) is a practical challenge in computer vision. Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of The Art (SOTA) performances in SSL. These approaches employ a threshold-to-pseudo-label (T2L) process to generate PLs by truncating the confidence scores of unlabeled data predicted by the self-training method. However, self-trained models typically yield biased and high-variance predictions, especially in the scenarios when a little labeled data are supplied. To address this issue, we propose a lightweight channel-based ensemble method to effectively consolidate multiple inferior PLs into the theoretically guaranteed unbiased and low-variance one. Importantly, our approach can be readily extended to any SSL framework, such as FixMatch or FreeMatch. Experimental results demonstrate that our method significantly outperforms state-of-the-art techniques on CIFAR10/100 in terms of effectiveness and efficiency.</p></p class="citation"></blockquote><h3 id=7582--158318-uncertainty-aware-sar-atr-defending-against-adversarial-attacks-via-bayesian-neural-networks-tian-ye-et-al-2024>(75/82 | 158/318) Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via Bayesian Neural Networks (Tian Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Ye, Rajgopal Kannan, Viktor Prasanna, Carl Busart. (2024)<br><strong>Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via Bayesian Neural Networks</strong><br><button class=copy-to-clipboard title="Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via Bayesian Neural Networks" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18318v1.pdf filename=2403.18318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>attacks</b> have demonstrated the vulnerability of Machine Learning (ML) image classifiers in Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) systems. An <b>adversarial</b> <b>attack</b> can deceive the classifier into making incorrect predictions by perturbing the input SAR images, for example, with a few scatterers attached to the on-ground objects. Therefore, it is critical to develop robust SAR ATR systems that can detect potential <b>adversarial</b> <b>attacks</b> by leveraging the inherent uncertainty in ML classifiers, thereby effectively alerting human decision-makers. In this paper, we propose a novel uncertainty-aware SAR ATR for detecting <b>adversarial</b> <b>attacks.</b> Specifically, we leverage the capability of Bayesian Neural Networks (BNNs) in performing image classification with quantified epistemic uncertainty to measure the confidence for each input SAR image. By evaluating the uncertainty, our method alerts when the input SAR image is likely to be adversarially generated. Simultaneously, we also generate visual explanations that reveal the specific regions in the SAR image where the <b>adversarial</b> <b>scatterers</b> are likely to to be present, thus aiding human decision-making with hints of evidence of <b>adversarial</b> <b>attacks.</b> Experiments on the MSTAR dataset demonstrate that our approach can identify over 80% <b>adversarial</b> <b>SAR</b> images with fewer than 20% false alarms, and our visual explanations can identify up to over 90% of scatterers in an <b>adversarial</b> <b>SAR</b> image.</p></p class="citation"></blockquote><h3 id=7682--159318-enhancing-generative-class-incremental-learning-performance-with-model-forgetting-approach-taro-togo-et-al-2024>(76/82 | 159/318) Enhancing Generative Class Incremental Learning Performance with Model Forgetting Approach (Taro Togo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taro Togo, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama. (2024)<br><strong>Enhancing Generative Class Incremental Learning Performance with Model Forgetting Approach</strong><br><button class=copy-to-clipboard title="Enhancing Generative Class Incremental Learning Performance with Model Forgetting Approach" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18258v1.pdf filename=2403.18258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a novel approach to Generative Class Incremental Learning (GCIL) by introducing the forgetting mechanism, aimed at dynamically managing class information for better adaptation to streaming data. GCIL is one of the hot topics in the field of computer vision, and this is considered one of the crucial tasks in society, specifically the <b>continual</b> <b>learning</b> of generative models. The ability to forget is a crucial brain function that facilitates <b>continual</b> <b>learning</b> by selectively discarding less relevant information for humans. However, in the field of machine learning models, the concept of intentionally forgetting has not been extensively investigated. In this study we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL, thereby examining their impact on the models&rsquo; ability to learn in <b>continual</b> <b>learning.</b> Through our experiments, we have found that integrating the forgetting mechanisms significantly enhances the models&rsquo; performance in acquiring new knowledge, underscoring the positive role that strategic forgetting plays in the process of <b>continual</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=7782--160318-taformer-a-unified-target-aware-transformer-for-video-and-motion-joint-prediction-in-aerial-scenes-liangyu-xu-et-al-2024>(77/82 | 160/318) TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint Prediction in Aerial Scenes (Liangyu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangyu Xu, Wanxuan Lu, Hongfeng Yu, Yongqiang Mao, Hanbo Bi, Chenglong Liu, Xian Sun, Kun Fu. (2024)<br><strong>TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint Prediction in Aerial Scenes</strong><br><button class=copy-to-clipboard title="TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint Prediction in Aerial Scenes" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18238v1.pdf filename=2403.18238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As drone technology advances, using unmanned aerial vehicles for aerial surveys has become the dominant trend in modern low-altitude remote sensing. The surge in aerial video data necessitates accurate prediction for future scenarios and motion states of the interested target, particularly in applications like traffic management and disaster response. Existing video prediction methods focus solely on predicting future scenes (video frames), suffering from the neglect of explicitly modeling target&rsquo;s motion states, which is crucial for aerial video interpretation. To address this issue, we introduce a novel task called Target-Aware Aerial Video Prediction, aiming to simultaneously predict future scenes and motion states of the target. Further, we design a model specifically for this task, named TAFormer, which provides a unified modeling approach for both video and target motion states. Specifically, we introduce Spatiotemporal Attention (STA), which decouples the learning of video dynamics into spatial static attention and temporal dynamic attention, effectively modeling the scene appearance and motion. Additionally, we design an Information Sharing Mechanism (ISM), which elegantly unifies the modeling of video and target motion by facilitating information interaction through two sets of messenger tokens. Moreover, to alleviate the difficulty of distinguishing targets in blurry predictions, we introduce Target-Sensitive Gaussian Loss (TSGL), enhancing the model&rsquo;s sensitivity to both target&rsquo;s position and content. Extensive experiments on UAV123VP and VisDroneVP (derived from single-object tracking datasets) demonstrate the exceptional performance of TAFormer in target-aware video prediction, showcasing its adaptability to the additional requirements of aerial video interpretation for target awareness.</p></p class="citation"></blockquote><h3 id=7882--161318-towards-image-ambient-lighting-normalization-florin-alexandru-vasluianu-et-al-2024>(78/82 | 161/318) Towards Image Ambient Lighting Normalization (Florin-Alexandru Vasluianu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florin-Alexandru Vasluianu, Tim Seizinger, Zongwei Wu, Rakesh Ranjan, Radu Timofte. (2024)<br><strong>Towards Image Ambient Lighting Normalization</strong><br><button class=copy-to-clipboard title="Towards Image Ambient Lighting Normalization" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18730v1.pdf filename=2403.18730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lighting normalization is a crucial but underexplored restoration task with broad applications. However, existing works often simplify this task within the context of shadow removal, limiting the light sources to one and oversimplifying the scene, thus excluding complex self-shadows and restricting surface classes to smooth ones. Although promising, such simplifications hinder generalizability to more realistic settings encountered in daily use. In this paper, we propose a new challenging task termed Ambient Lighting Normalization (ALN), which enables the study of interactions between shadows, unifying image restoration and shadow removal in a broader context. To address the lack of appropriate datasets for ALN, we introduce the large-scale high-resolution dataset Ambient6K, comprising samples obtained from multiple light sources and including self-shadows resulting from complex geometries, which is the first of its kind. For <b>benchmarking,</b> we select various mainstream methods and rigorously evaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong baseline that maximizes Image-Frequency joint entropy to selectively restore local areas under different lighting conditions, without relying on shadow localization priors. Experiments show that IFBlend achieves SOTA scores on Ambient6K and exhibits competitive performance on conventional shadow removal <b>benchmarks</b> compared to shadow-specific models with mask priors. The dataset, <b>benchmark,</b> and code are available at <a href=https://github.com/fvasluianu97/IFBlend>https://github.com/fvasluianu97/IFBlend</a>.</p></p class="citation"></blockquote><h3 id=7982--162318-an-evolutionary-network-architecture-search-framework-with-adaptive-multimodal-fusion-for-hand-gesture-recognition-yizhang-xia-et-al-2024>(79/82 | 162/318) An Evolutionary Network Architecture Search Framework with Adaptive Multimodal Fusion for Hand Gesture Recognition (Yizhang Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhang Xia, Shihao Song, Zhanglu Hou, Junwen Xu, Juan Zou, Yuan Liu, Shengxiang Yang. (2024)<br><strong>An Evolutionary Network Architecture Search Framework with Adaptive Multimodal Fusion for Hand Gesture Recognition</strong><br><button class=copy-to-clipboard title="An Evolutionary Network Architecture Search Framework with Adaptive Multimodal Fusion for Hand Gesture Recognition" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-NE, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18208v1.pdf filename=2403.18208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hand gesture recognition (HGR) based on <b>multimodal</b> data has attracted considerable attention owing to its great potential in applications. Various manually designed <b>multimodal</b> deep networks have performed well in <b>multimodal</b> HGR (MHGR), but most of existing algorithms require a lot of expert experience and time-consuming manual trials. To address these issues, we propose an evolutionary network architecture search framework with the adaptive multimodel fusion (AMF-ENAS). Specifically, we design an encoding space that simultaneously considers fusion positions and ratios of the <b>multimodal</b> data, allowing for the automatic construction of <b>multimodal</b> networks with different architectures through decoding. Additionally, we consider three input streams corresponding to intra-modal surface electromyography (sEMG), intra-modal accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to various datasets, the ENAS framework is designed to automatically search a MHGR network with appropriate fusion positions and ratios. To the best of our knowledge, this is the first time that ENAS has been utilized in MHGR to tackle issues related to the fusion position and ratio of <b>multimodal</b> data. Experimental results demonstrate that AMF-ENAS achieves state-of-the-art performance on the Ninapro DB2, DB3, and DB7 datasets.</p></p class="citation"></blockquote><h3 id=8082--163318-splatface-gaussian-splat-face-reconstruction-leveraging-an-optimizable-surface-jiahao-luo-et-al-2024>(80/82 | 163/318) SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface (Jiahao Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Luo, Jing Liu, James Davis. (2024)<br><strong>SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface</strong><br><button class=copy-to-clipboard title="SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18784v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18784v2.pdf filename=2403.18784v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SplatFace, a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined <b>geometry.</b> Our method is designed to simultaneously deliver both high-quality novel view rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D Morphable Model (3DMM) to provide a surface geometric structure, making it possible to reconstruct faces with a limited set of input images. We introduce a joint optimization strategy that refines both the Gaussians and the morphable surface through a synergistic non-rigid alignment process. A novel distance metric, splat-to-surface, is proposed to improve alignment by considering both the Gaussian position and covariance. The surface information is also utilized to incorporate a world-space densification process, resulting in superior reconstruction quality. Our experimental analysis demonstrates that the proposed method is competitive with both other Gaussian splatting techniques in novel view synthesis and other 3D reconstruction methods in producing 3D face meshes with high geometric precision.</p></p class="citation"></blockquote><h3 id=8182--164318-parco-part-coordinating-text-to-motion-synthesis-qiran-zou-et-al-2024>(81/82 | 164/318) ParCo: Part-Coordinating Text-to-Motion Synthesis (Qiran Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiran Zou, Shangyuan Yuan, Shian Du, Yu Wang, Chang Liu, Yi Xu, Jie Chen, Xiangyang Ji. (2024)<br><strong>ParCo: Part-Coordinating Text-to-Motion Synthesis</strong><br><button class=copy-to-clipboard title="ParCo: Part-Coordinating Text-to-Motion Synthesis" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18512v1.pdf filename=2403.18512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a challenging task: text-to-motion synthesis, aiming to generate motions that align with textual descriptions and exhibit coordinated movements. Currently, the part-based methods introduce part partition into the motion synthesis process to achieve finer-grained generation. However, these methods encounter challenges such as the lack of coordination between different part motions and difficulties for networks to understand part concepts. Moreover, introducing finer-grained part concepts poses computational complexity challenges. In this paper, we propose Part-Coordinating Text-to-Motion Synthesis (ParCo), endowed with enhanced capabilities for understanding part motions and communication among different part motion generators, ensuring a coordinated and fined-grained motion synthesis. Specifically, we discretize whole-body motion into multiple part motions to establish the prior concept of different parts. Afterward, we employ multiple lightweight generators designed to synthesize different part motions and coordinate them through our part coordination module. Our approach demonstrates superior performance on common <b>benchmarks</b> with economic computations, including HumanML3D and KIT-ML, providing substantial evidence of its effectiveness. Code is available at <a href=https://github.com/qrzou/ParCo>https://github.com/qrzou/ParCo</a> .</p></p class="citation"></blockquote><h3 id=8282--165318-dvlo-deep-visual-lidar-odometry-with-local-to-global-feature-fusion-and-bi-directional-structure-alignment-jiuming-liu-et-al-2024>(82/82 | 165/318) DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and Bi-Directional Structure Alignment (Jiuming Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiuming Liu, Dong Zhuo, Zhiheng Feng, Siting Zhu, Chensheng Peng, Zhe Liu, Hesheng Wang. (2024)<br><strong>DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and Bi-Directional Structure Alignment</strong><br><button class=copy-to-clipboard title="DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and Bi-Directional Structure Alignment" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18274v1.pdf filename=2403.18274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Information inside visual and LiDAR data is well complementary derived from the fine-grained texture of images and massive geometric information in point clouds. However, it remains challenging to explore effective visual-LiDAR fusion, mainly due to the intrinsic data structure inconsistency between two modalities: Images are regular and dense, but LiDAR points are unordered and sparse. To address the problem, we propose a local-to-global fusion network with bi-directional structure alignment. To obtain locally fused features, we project points onto image plane as cluster centers and cluster image pixels around each center. Image pixels are pre-organized as pseudo points for image-to-point structure alignment. Then, we convert points to pseudo images by cylindrical projection (point-to-image structure alignment) and perform adaptive global feature fusion between point features with local fused features. Our method achieves state-of-the-art performance on KITTI odometry and FlyingThings3D scene flow datasets compared to both single-modal and <b>multi-modal</b> methods. Codes will be released later.</p></p class="citation"></blockquote><h2 id=cslg-48>cs.LG (48)</h2><h3 id=148--166318-physics-informed-graph-neural-networks-for-water-distribution-systems-inaam-ashraf-et-al-2024>(1/48 | 166/318) Physics-Informed Graph Neural Networks for Water Distribution Systems (Inaam Ashraf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer. (2024)<br><strong>Physics-Informed Graph Neural Networks for Water Distribution Systems</strong><br><button class=copy-to-clipboard title="Physics-Informed Graph Neural Networks for Water Distribution Systems" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 83<br>Keywords: Graph Convolutional Network, Message-Passing, Graph, Graph Neural Network, Convolution, Convolutional Neural Network, Simulation, Simulator, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18570v1.pdf filename=2403.18570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world&rsquo;s population will likely live in urban environments in 2050, efficient <b>simulation</b> and planning tools for WDS play a crucial role in reaching UN&rsquo;s sustainable developmental goal (SDG) 6 - &ldquo;Clean water and sanitation for all&rdquo;. In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few <b>graph</b> <b>convolutional</b> <b>neural</b> <b>network</b> <b>(GCN)</b> layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an <b>unsupervised</b> manner. To the best of our knowledge, this is the first DL approach to emulate the popular hydraulic simulator EPANET, utilizing no additional information. Like most DL models and unlike the hydraulic simulator, our model demonstrates vastly faster emulation times that do not increase drastically with the size of the WDS. Moreover, we achieve high accuracy on the ground truth and very similar results compared to the hydraulic simulator as demonstrated through experiments on five real-world WDS datasets.</p></p class="citation"></blockquote><h3 id=248--167318-energy-guided-data-sampling-for-traffic-prediction-with-mini-training-datasets-zhaohui-yang-et-al-2024>(2/48 | 167/318) Energy-Guided Data Sampling for Traffic Prediction with Mini Training Datasets (Zhaohui Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaohui Yang, Kshitij Jerath. (2024)<br><strong>Energy-Guided Data Sampling for Traffic Prediction with Mini Training Datasets</strong><br><button class=copy-to-clipboard title="Energy-Guided Data Sampling for Traffic Prediction with Mini Training Datasets" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Simulation, Simulator, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18710v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18710v2.pdf filename=2403.18710v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent endeavors aimed at forecasting future traffic flow states through deep learning encounter various challenges and yield diverse outcomes. A notable obstacle arises from the substantial data requirements of deep learning models, a resource often scarce in traffic flow systems. Despite the abundance of domain knowledge concerning traffic flow dynamics, prevailing deep learning methodologies frequently fail to fully exploit it. To address these issues, we propose an innovative solution that merges <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> with <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> architecture to enhance the prediction of traffic flow dynamics. A key revelation of our research is the feasibility of sampling training data for large traffic systems from <b>simulations</b> conducted on smaller traffic systems. This insight suggests the potential for referencing a macroscopic-level distribution to inform the sampling of microscopic data. Such sampling is facilitated by the observed scale invariance in the normalized energy distribution of the statistical mechanics model, thereby streamlining the data generation process for large-scale traffic systems. Our <b>simulations</b> demonstrate promising agreement between predicted and actual traffic flow dynamics, underscoring the efficacy of our proposed approach.</p></p class="citation"></blockquote><h3 id=348--168318-self-expansion-of-pre-trained-models-with-mixture-of-adapters-for-continual-learning-huiyi-wang-et-al-2024>(3/48 | 168/318) Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning (Huiyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huiyi Wang, Haodong Lu, Lina Yao, Dong Gong. (2024)<br><strong>Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning</strong><br><button class=copy-to-clipboard title="Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Vision Transformer, Autoencoder, Continual Learning, Distribution Shift, Distribution Shift, Fine-tuning, Prompt, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18886v1.pdf filename=2403.18886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> aims to learn from a stream of continuously arriving data with minimum forgetting of previously learned knowledge. While previous works have explored the effectiveness of leveraging the generalizable knowledge from pre-trained models in <b>continual</b> <b>learning,</b> existing parameter-efficient <b>fine-tuning</b> approaches focus on the use of a predetermined or task-wise set of adapters or <b>prompts.</b> However, these approaches still suffer from forgetting due to task interference on jointly used parameters or restricted flexibility. The reliance on a static model architecture may lead to the allocation of excessive parameters that are not essential or, conversely, inadequate adaptation for downstream tasks, given that the scale and <b>distribution</b> <b>of</b> incoming data are unpredictable in <b>continual</b> <b>learning.</b> We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel <b>fine-tuning</b> approach which automatically decides to reuse or add adapter modules on demand in <b>continual</b> <b>learning,</b> depending on whether drastic <b>distribution</b> <b>shift</b> that could not be handled by existing modules is detected at different representation levels. We design each adapter module to consist of an adapter and a representation descriptor, specifically, implemented as an <b>autoencoder.</b> The representation descriptor functions as a <b>distributional</b> <b>shift</b> indicator during training and triggers adapter expansion. For better usage of the adapters, an expandable weighting router is learned jointly for mixture of adapter outputs. By comparing with <b>vision-transformer-based</b> <b>continual</b> <b>learning</b> adaptation methods, we demonstrate that the proposed framework outperforms the state-of-the-art without memory rehearsal.</p></p class="citation"></blockquote><h3 id=448--169318-the-topos-of-transformer-networks-mattia-jacopo-villani-et-al-2024>(4/48 | 169/318) The Topos of Transformer Networks (Mattia Jacopo Villani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mattia Jacopo Villani, Peter McBurney. (2024)<br><strong>The Topos of Transformer Networks</strong><br><button class=copy-to-clipboard title="The Topos of Transformer Networks" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-CT<br>Keyword Score: 53<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18415v1.pdf filename=2403.18415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>transformer</b> neural network has significantly out-shined all other neural network architectures as the engine behind <b>large</b> <b>language</b> <b>models.</b> We provide a theoretical analysis of the expressivity of the <b>transformer</b> architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the <b>convolutional,</b> <b>recurrent</b> and <b>graph</b> <b>convolutional</b> <b>networks,</b> can be embedded in a pretopos of piecewise-linear functions, but that the <b>transformer</b> necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas <b>transformers</b> are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.</p></p class="citation"></blockquote><h3 id=548--170318-pdnnet-pdn-aware-gnn-cnn-heterogeneous-network-for-dynamic-ir-drop-prediction-yuxiang-zhao-et-al-2024>(5/48 | 170/318) PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction (Yuxiang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang. (2024)<br><strong>PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction</strong><br><button class=copy-to-clipboard title="PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18569v1.pdf filename=2403.18569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>IR drop on the power delivery network (PDN) is closely related to PDN&rsquo;s configuration and cell current consumption. As the integrated circuit (IC) design is growing larger, dynamic IR drop <b>simulation</b> becomes computationally unaffordable and machine learning based IR drop prediction has been explored as a promising solution. Although <b>CNN-based</b> methods have been adapted to IR drop prediction task in several works, the shortcomings of overlooking PDN configuration is non-negligible. In this paper, we consider not only how to properly represent cell-PDN relation, but also how to model IR drop following its physical nature in the feature aggregation procedure. Thus, we propose a novel <b>graph</b> structure, PDNGraph, to unify the representations of the PDN structure and the fine-grained cell-PDN relation. We further propose a dual-branch heterogeneous network, PDNNet, incorporating two parallel <b>GNN-CNN</b> branches to favorably capture the above features during the learning process. Several key designs are presented to make the dynamic IR drop prediction highly effective and interpretable. We are the first work to apply <b>graph</b> structure to deep-learning based dynamic IR drop prediction method. Experiments show that PDNNet outperforms the state-of-the-art <b>CNN-based</b> methods by up to 39.3% reduction in prediction error and achieves 545x speedup compared to the commercial tool, which demonstrates the superiority of our method.</p></p class="citation"></blockquote><h3 id=648--171318-multi-modal-contrastive-learning-for-online-clinical-time-series-applications-fabian-baldenweg-et-al-2024>(6/48 | 171/318) Multi-Modal Contrastive Learning for Online Clinical Time-Series Applications (Fabian Baldenweg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabian Baldenweg, Manuel Burger, Gunnar Rätsch, Rita Kuznetsova. (2024)<br><strong>Multi-Modal Contrastive Learning for Online Clinical Time-Series Applications</strong><br><button class=copy-to-clipboard title="Multi-Modal Contrastive Learning for Online Clinical Time-Series Applications" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Contrastive Learning, Multi-modal, Self-supervised Learning, Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18316v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18316v1.pdf filename=2403.18316v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electronic Health Record (EHR) datasets from Intensive Care Units (ICU) contain a diverse set of data modalities. While prior works have successfully leveraged multiple modalities in <b>supervised</b> settings, we apply advanced <b>self-supervised</b> <b>multi-modal</b> <b>contrastive</b> <b>learning</b> techniques to ICU data, specifically focusing on clinical notes and time-series for clinically relevant online prediction tasks. We introduce a loss function <b>Multi-Modal</b> Neighborhood <b>Contrastive</b> <b>Loss</b> (MM-NCL), a soft neighborhood function, and showcase the excellent linear probe and <b>zero-shot</b> performance of our approach.</p></p class="citation"></blockquote><h3 id=748--172318-branch-tuning-balancing-stability-and-plasticity-for-continual-self-supervised-learning-wenzhuo-liu-et-al-2024>(7/48 | 172/318) Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning (Wenzhuo Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu. (2024)<br><strong>Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Continual Learning, Convolution, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18266v1.pdf filename=2403.18266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) has emerged as an effective paradigm for deriving general representations from vast amounts of unlabeled data. However, as real-world applications continually integrate new content, the high computational and resource demands of SSL necessitate <b>continual</b> <b>learning</b> rather than complete retraining. This poses a challenge in striking a balance between stability and plasticity when adapting to new information. In this paper, we employ Centered Kernel Alignment for quantitatively analyzing model stability and plasticity, revealing the critical roles of batch normalization layers for stability and <b>convolutional</b> layers for plasticity. Motivated by this, we propose Branch-tuning, an efficient and straightforward method that achieves a balance between stability and plasticity in <b>continual</b> <b>SSL.</b> Branch-tuning consists of branch expansion and compression, and can be easily applied to various SSL methods without the need of modifying the original methods, retaining old data or models. We validate our method through incremental experiments on various <b>benchmark</b> datasets, demonstrating its effectiveness and practical value in real-world scenarios. We hope our work offers new insights for future <b>continual</b> <b>self-supervised</b> <b>learning</b> research. The code will be made publicly available.</p></p class="citation"></blockquote><h3 id=848--173318-computationally-and-memory-efficient-robust-predictive-analytics-using-big-data-daniel-menges-et-al-2024>(8/48 | 173/318) Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data (Daniel Menges et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Menges, Adil Rasheed. (2024)<br><strong>Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data</strong><br><button class=copy-to-clipboard title="Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-IV<br>Keyword Score: 40<br>Keywords: LSTM, LSTM, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19721v1.pdf filename=2403.19721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the current data-intensive era, big data has become a significant asset for Artificial Intelligence (AI), serving as a foundation for developing data-driven models and providing insight into various unknown fields. This study navigates through the challenges of data uncertainties, storage limitations, and predictive data-driven modeling using big data. We utilize Robust Principal Component Analysis (RPCA) for effective noise reduction and outlier elimination, and Optimal Sensor Placement (OSP) for efficient data compression and storage. The proposed OSP technique enables data compression without substantial information loss while simultaneously reducing storage needs. While RPCA offers an enhanced alternative to traditional Principal Component Analysis (PCA) for high-dimensional data management, the scope of this work extends its utilization, focusing on robust, data-driven modeling applicable to huge data sets in real-time. For that purpose, <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> networks, a type of <b>recurrent</b> <b>neural</b> <b>network,</b> are applied to model and predict data based on a low-dimensional subset obtained from OSP, leading to a crucial acceleration of the training phase. <b>LSTMs</b> are feasible for capturing <b>long-term</b> <b>dependencies</b> <b>in</b> <b>time</b> series data, making them particularly suited for predicting the future states of physical systems on historical data. All the presented algorithms are not only theorized but also simulated and validated using real thermal imaging data mapping a ship&rsquo;s engine.</p></p class="citation"></blockquote><h3 id=948--174318-contrastive-learning-with-orthonormal-anchors-cloa-huanran-li-et-al-2024>(9/48 | 174/318) Contrastive Learning with Orthonormal Anchors (CLOA) (Huanran Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huanran Li, Daniel Pimentel-Alarcón. (2024)<br><strong>Contrastive Learning with Orthonormal Anchors (CLOA)</strong><br><button class=copy-to-clipboard title="Contrastive Learning with Orthonormal Anchors (CLOA)" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Fine-tuning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18699v1.pdf filename=2403.18699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study focuses on addressing the instability issues prevalent in <b>contrastive</b> <b>learning,</b> specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This &ldquo;over-fusion&rdquo; effect detrimentally affects classification accuracy in subsequent <b>supervised-learning</b> <b>tasks.</b> Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the <b>fine-tuning</b> phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding while simultaneously ensuring their aggregation into dense, well-defined clusters. Our method demonstrates remarkable improvements with just a fraction of the conventional label requirements, as evidenced by our results on CIFAR10 and CIFAR100 datasets.</p></p class="citation"></blockquote><h3 id=1048--175318-safe-and-robust-reinforcement-learning-principles-and-practice-taku-yamagata-et-al-2024>(10/48 | 175/318) Safe and Robust Reinforcement Learning: Principles and Practice (Taku Yamagata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taku Yamagata, Raul Santos-Rodriguez. (2024)<br><strong>Safe and Robust Reinforcement Learning: Principles and Practice</strong><br><button class=copy-to-clipboard title="Safe and Robust Reinforcement Learning: Principles and Practice" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Reinforcement Learning, Domain Adaptation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18539v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18539v2.pdf filename=2403.18539v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL) has shown remarkable success in solving relatively complex tasks, yet the deployment of RL systems in real-world scenarios poses significant challenges related to safety and robustness. This paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust RL landscape, encompassing algorithmic, ethical, and practical considerations. We conduct a comprehensive review of methodologies and open problems that <b>summarizes</b> the efforts in recent years to address the inherent risks associated with RL applications. After discussing and proposing definitions for both safe and robust RL, the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of RL agents. We examine techniques such as uncertainty estimation, optimisation methodologies, exploration-exploitation trade-offs, and <b>adversarial</b> <b>training.</b> Environmental factors, including sim-to-real transfer and <b>domain</b> <b>adaptation,</b> are also scrutinized to understand how RL systems can adapt to diverse and dynamic surroundings. Moreover, human involvement is an integral ingredient of the analysis, acknowledging the broad set of roles that humans can take in this context. Importantly, to aid practitioners in navigating the complexities of safe and robust RL implementation, this paper introduces a practical checklist derived from the synthesized literature. The checklist encompasses critical aspects of algorithm design, training environment considerations, and ethical guidelines. It will serve as a resource for developers and policymakers alike to ensure the responsible deployment of RL systems in many application domains.</p></p class="citation"></blockquote><h3 id=1148--176318-robustness-and-visual-explanation-for-black-box-image-video-and-ecg-signal-classification-with-reinforcement-learning-soumyendu-sarkar-et-al-2024>(11/48 | 176/318) Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning (Soumyendu Sarkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Avisek Naug, Sahand Ghorbanpour. (2024)<br><strong>Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs-MA, cs.LG<br>Keyword Score: 35<br>Keywords: Adversarial Learning, Black Box, Reinforcement Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18985v1.pdf filename=2403.18985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a generic <b>Reinforcement</b> <b>Learning</b> (RL) framework optimized for crafting <b>adversarial</b> <b>attacks</b> on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with <b>adversarial</b> <b>training</b> and transparency across varied applications and data types.</p></p class="citation"></blockquote><h3 id=1248--177318-selective-mixup-fine-tuning-for-optimizing-non-decomposable-objectives-shrinivas-ramasubramanian-et-al-2024>(12/48 | 177/318) Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives (Shrinivas Ramasubramanian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan. (2024)<br><strong>Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives</strong><br><button class=copy-to-clipboard title="Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 33<br>Keywords: Benchmarking, Fairness, Fine-tuning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18301v1.pdf filename=2403.18301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various <b>supervised</b> and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as <b>fairness.</b> We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive <b>fine-tuning</b> technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a mixup of features between samples from particular classes such that it optimizes the given objective. We comprehensively evaluate our technique against the existing empirical and theoretically principled methods on standard <b>benchmark</b> datasets for imbalanced classification. We find that proposed SelMix <b>fine-tuning</b> significantly improves the performance for various practical non-decomposable objectives across <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1348--178318-genet-a-graph-neural-network-based-anti-noise-task-oriented-semantic-communication-paradigm-chunhang-zheng-et-al-2024>(13/48 | 178/318) GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm (Chunhang Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunhang Zheng, Kechao Cai. (2024)<br><strong>GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm</strong><br><button class=copy-to-clipboard title="GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18296v1.pdf filename=2403.18296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)-based</b> paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input <b>data</b> <b>image</b> into <b>graph</b> <b>structures.</b> <b>Then</b> we leverage a <b>GNN-based</b> encoder to extract semantic information from the source <b>data.</b> <b>This</b> extracted semantic information is then transmitted through the channel. At the receiver&rsquo;s end, a <b>GNN-based</b> decoder is utilized to reconstruct the relevant semantic information from the source <b>data</b> <b>for</b> TOC. Through experimental evaluation, we show GeNet&rsquo;s effectiveness in anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet&rsquo;s performance by varying the number of nodes, revealing its versatility as a new paradigm for semantic communication. Additionally, we show GeNet&rsquo;s robustness to geometric transformations by testing it with different rotation angles, without resorting to <b>data</b> <b>augmentation.</b></p></p class="citation"></blockquote><h3 id=1448--179318-corast-towards-foundation-model-powered-correlated-data-analysis-in-resource-constrained-cps-and-iot-yi-hu-et-al-2024>(14/48 | 179/318) CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT (Yi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong. (2024)<br><strong>CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT</strong><br><button class=copy-to-clipboard title="CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 31<br>Keywords: Federated Learning, Foundation Model, Multi-modal, Multi-modal, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18451v1.pdf filename=2403.18451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> (FMs) emerge as a promising solution to harness distributed and diverse environmental data by leveraging prior knowledge to understand the complicated temporal and spatial correlations within heterogeneous datasets. Unlike distributed learning frameworks such as <b>federated</b> <b>learning,</b> which often struggle with <b>multimodal</b> data, FMs can transform diverse inputs into embeddings. This process facilitates the integration of information from various modalities and the application of prior learning to new domains. However, deploying FMs in resource-constrained edge systems poses significant challenges. To this end, we introduce CoRAST, a novel learning framework that utilizes FMs for enhanced analysis of distributed, correlated heterogeneous data. Utilizing a server-based FM, CoRAST can exploit existing environment information to extract temporal, spatial, and cross-modal correlations among sensor data. This enables CoRAST to offer context-aware insights for localized client tasks through FM-powered global <b>representation</b> <b>learning.</b> Our evaluation on real-world weather dataset demonstrates CoRAST&rsquo;s ability to exploit correlated heterogeneous data through environmental <b>representation</b> <b>learning</b> to reduce the forecast errors by up to 50.3% compared to the baselines.</p></p class="citation"></blockquote><h3 id=1548--180318-transfusion-contrastive-learning-with-transformers-huanran-li-et-al-2024>(15/48 | 180/318) TransFusion: Contrastive Learning with Transformers (Huanran Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huanran Li, Daniel Pimentel-Alarcón. (2024)<br><strong>TransFusion: Contrastive Learning with Transformers</strong><br><button class=copy-to-clipboard title="TransFusion: Contrastive Learning with Transformers" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Data Augmentation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18681v1.pdf filename=2403.18681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel framework, TransFusion, designed to make the process of <b>contrastive</b> <b>learning</b> more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block&rsquo;s weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of <b>data</b> <b>augmentation</b> and the minimum batch size required for effective <b>contrastive</b> <b>learning.</b> Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world <b>data,</b> <b>leading</b> to improved classification accuracy in downstream tasks.</p></p class="citation"></blockquote><h3 id=1648--181318-fusion-approaches-for-emotion-recognition-from-speech-using-acoustic-and-text-based-features-leonardo-pepino-et-al-2024>(16/48 | 181/318) Fusion approaches for emotion recognition from speech using acoustic and text-based features (Leonardo Pepino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonardo Pepino, Pablo Riera, Luciana Ferrer, Agustin Gravano. (2024)<br><strong>Fusion approaches for emotion recognition from speech using acoustic and text-based features</strong><br><button class=copy-to-clipboard title="Fusion approaches for emotion recognition from speech using acoustic and text-based features" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 30<br>Keywords: BERT, Emotion Recognition, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18635v1.pdf filename=2403.18635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study different approaches for classifying <b>emotions</b> <b>from</b> speech using acoustic and text-based features. We propose to obtain contextualized <b>word</b> <b>embeddings</b> with <b>BERT</b> to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings. We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results. In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works may overestimate the advantage of incorporating transcriptions.</p></p class="citation"></blockquote><h3 id=1748--182318-scalable-lipschitz-estimation-for-cnns-yusuf-sulehman-et-al-2024>(17/48 | 182/318) Scalable Lipschitz Estimation for CNNs (Yusuf Sulehman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusuf Sulehman, Tingting Mu. (2024)<br><strong>Scalable Lipschitz Estimation for CNNs</strong><br><button class=copy-to-clipboard title="Scalable Lipschitz Estimation for CNNs" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18613v1.pdf filename=2403.18613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating the Lipschitz constant of deep neural networks is of growing interest as it is useful for informing on generalisability and adversarial robustness. <b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> in particular, underpin much of the recent success in computer vision related applications. However, although existing methods for estimating the Lipschitz constant can be tight, they have limited scalability when applied to <b>CNNs.</b> To tackle this, we propose a novel method to accelerate Lipschitz constant estimation for <b>CNNs.</b> The core idea is to divide a large <b>convolutional</b> <b>block</b> <b>via</b> a joint layer and width-wise partition, into a collection of smaller blocks. We prove an upper-bound on the Lipschitz constant of the larger block in terms of the Lipschitz constants of the smaller blocks. Through varying the partition factor, the resulting method can be adjusted to prioritise either accuracy or scalability and permits parallelisation. We demonstrate an enhanced scalability and comparable accuracy to existing baselines through a range of experiments.</p></p class="citation"></blockquote><h3 id=1848--183318-improving-line-search-methods-for-large-scale-neural-network-training-philip-kenneweg-et-al-2024>(18/48 | 183/318) Improving Line Search Methods for Large Scale Neural Network Training (Philip Kenneweg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Kenneweg, Tristan Kenneweg, Barbara Hammer. (2024)<br><strong>Improving Line Search Methods for Large Scale Neural Network Training</strong><br><button class=copy-to-clipboard title="Improving Line Search Methods for Large Scale Neural Network Training" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Stochastic Gradient Descent, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18519v1.pdf filename=2403.18519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent studies, line search methods have shown significant improvements in the performance of traditional <b>stochastic</b> <b>gradient</b> <b>descent</b> techniques, eliminating the need for a specific learning rate schedule. In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness. We test these methods on larger datasets and more complex data domains than before. Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods. Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam. Our evaluation focuses on <b>Transformers</b> and <b>CNNs</b> in the domains of NLP and image data. Our work is publicly available as a Python package, which provides a hyperparameter free Pytorch optimizer.</p></p class="citation"></blockquote><h3 id=1948--184318-faster-convergence-for-transformer-fine-tuning-with-line-search-methods-philip-kenneweg-et-al-2024>(19/48 | 184/318) Faster Convergence for Transformer Fine-tuning with Line Search Methods (Philip Kenneweg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer. (2024)<br><strong>Faster Convergence for Transformer Fine-tuning with Line Search Methods</strong><br><button class=copy-to-clipboard title="Faster Convergence for Transformer Fine-tuning with Line Search Methods" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Stochastic Gradient Descent, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18506v1.pdf filename=2403.18506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works have shown that line search methods greatly increase performance of traditional <b>stochastic</b> <b>gradient</b> <b>descent</b> methods on a variety of datasets and architectures [1], [2]. In this work we succeed in extending line search methods to the novel and highly popular <b>Transformer</b> architecture and dataset domains in natural language processing. More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units. Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases. Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures.</p></p class="citation"></blockquote><h3 id=2048--185318-collaborative-active-learning-in-conditional-trust-environment-zan-kai-chong-et-al-2024>(20/48 | 185/318) Collaborative Active Learning in Conditional Trust Environment (Zan-Kai Chong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng. (2024)<br><strong>Collaborative Active Learning in Conditional Trust Environment</strong><br><button class=copy-to-clipboard title="Collaborative Active Learning in Conditional Trust Environment" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Active Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18436v1.pdf filename=2403.18436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate collaborative <b>active</b> <b>learning,</b> a paradigm in which multiple collaborators explore a new domain by leveraging their combined machine learning capabilities without disclosing their existing data and models. Instead, the collaborators share prediction results from the new domain and newly acquired labels. This collaboration offers several advantages: (a) it addresses privacy and security concerns by eliminating the need for direct model and data disclosure; (b) it enables the use of different data sources and insights without direct data exchange; and (c) it promotes cost-effectiveness and resource efficiency through shared labeling costs. To realize these benefits, we introduce a collaborative <b>active</b> <b>learning</b> framework designed to fulfill the aforementioned objectives. We validate the effectiveness of the proposed framework through <b>simulations.</b> The results demonstrate that collaboration leads to higher AUC scores compared to independent efforts, highlighting the framework&rsquo;s ability to overcome the limitations of individual models. These findings support the use of collaborative approaches in <b>active</b> <b>learning,</b> emphasizing their potential to enhance outcomes through collective expertise and shared resources. Our work provides a foundation for further research on collaborative <b>active</b> <b>learning</b> and its practical applications in various domains where data privacy, cost efficiency, and model performance are critical considerations.</p></p class="citation"></blockquote><h3 id=2148--186318-on-spectrogram-analysis-in-a-multiple-classifier-fusion-framework-for-power-grid-classification-using-electric-network-frequency-georgios-tzolopoulos-et-al-2024>(21/48 | 186/318) On Spectrogram Analysis in a Multiple Classifier Fusion Framework for Power Grid Classification Using Electric Network Frequency (Georgios Tzolopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgios Tzolopoulos, Christos Korgialas, Constantine Kotropoulos. (2024)<br><strong>On Spectrogram Analysis in a Multiple Classifier Fusion Framework for Power Grid Classification Using Electric Network Frequency</strong><br><button class=copy-to-clipboard title="On Spectrogram Analysis in a Multiple Classifier Fusion Framework for Power Grid Classification Using Electric Network Frequency" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18402v1.pdf filename=2403.18402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Electric Network Frequency (ENF) serves as a unique signature inherent to power distribution systems. Here, a novel approach for power grid classification is developed, leveraging ENF. Spectrograms are generated from audio and power recordings across different grids, revealing distinctive ENF patterns that aid in grid classification through a fusion of classifiers. Four traditional machine learning classifiers plus a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN),</b> optimized using Neural Architecture Search, are developed for One-vs-All classification. This process generates numerous predictions per sample, which are then compiled and used to train a shallow multi-label neural network specifically designed to model the fusion process, ultimately leading to the conclusive class prediction for each sample. Experimental findings reveal that both validation and testing accuracy outperform those of current state-of-the-art classifiers, underlining the effectiveness and robustness of the proposed methodology.</p></p class="citation"></blockquote><h3 id=2248--187318-a-geometric-explanation-of-the-likelihood-ood-detection-paradox-hamidreza-kamkari-et-al-2024>(22/48 | 187/318) A Geometric Explanation of the Likelihood OOD Detection Paradox (Hamidreza Kamkari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamidreza Kamkari, Brendan Leigh Ross, Jesse C. Cresswell, Anthony L. Caterini, Rahul G. Krishnan, Gabriel Loaiza-Ganem. (2024)<br><strong>A Geometric Explanation of the Likelihood OOD Detection Paradox</strong><br><button class=copy-to-clipboard title="A Geometric Explanation of the Likelihood OOD Detection Paradox" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18910v1.pdf filename=2403.18910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to <b>out-of-distribution</b> (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can be applied to normalizing flows and score-based <b>diffusion</b> <b>models,</b> and obtains results which match or surpass state-of-the-art OOD detection <b>benchmarks</b> using the same DGM backbones. Our code is available at <a href=https://github.com/layer6ai-labs/dgm_ood_detection>https://github.com/layer6ai-labs/dgm_ood_detection</a>.</p></p class="citation"></blockquote><h3 id=2348--188318-looking-beyond-what-you-see-an-empirical-analysis-on-subgroup-intersectional-fairness-for-multi-label-chest-x-ray-classification-using-social-determinants-of-racial-health-inequities-dana-moukheiber-et-al-2024>(23/48 | 188/318) Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities (Dana Moukheiber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dana Moukheiber, Saurabh Mahindre, Lama Moukheiber, Mira Moukheiber, Mingchen Gao. (2024)<br><strong>Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities</strong><br><button class=copy-to-clipboard title="Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-CY, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Fairness, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18196v1.pdf filename=2403.18196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure <b>fairness</b> across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular <b>benchmark</b> and evaluation of <b>fairness.</b> We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for <b>fairness</b> constraints and integrate class-balanced <b>fine-tuning</b> for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that our framework achieves an optimal tradeoff between accuracy and <b>fairness</b> compared to baseline methods.</p></p class="citation"></blockquote><h3 id=2448--189318-detecting-generative-parroting-through-overfitting-masked-autoencoders-saeid-asgari-taghanaki-et-al-2024>(24/48 | 189/318) Detecting Generative Parroting through Overfitting Masked Autoencoders (Saeid Asgari Taghanaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeid Asgari Taghanaki, Joseph Lambourne. (2024)<br><strong>Detecting Generative Parroting through Overfitting Masked Autoencoders</strong><br><button class=copy-to-clipboard title="Detecting Generative Parroting through Overfitting Masked Autoencoders" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Autoencoder, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19050v1.pdf filename=2403.19050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>generative</b> <b>AI</b> models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to <b>generative</b> <b>parroting,</b> where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked <b>Autoencoder</b> (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method&rsquo;s potential to ensure ethical use and enhance the legal compliance of <b>generative</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=2548--190318-understanding-the-learning-dynamics-of-alignment-with-human-feedback-shawn-im-et-al-2024>(25/48 | 190/318) Understanding the Learning Dynamics of Alignment with Human Feedback (Shawn Im et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shawn Im, Yixuan Li. (2024)<br><strong>Understanding the Learning Dynamics of Alignment with Human Feedback</strong><br><button class=copy-to-clipboard title="Understanding the Learning Dynamics of Alignment with Human Feedback" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18742v1.pdf filename=2403.18742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary <b>LLMs</b> and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potentially offensive text; reader discretion is advised.</p></p class="citation"></blockquote><h3 id=2648--191318-semi-supervised-learning-for-deep-causal-generative-models-yasin-ibrahim-et-al-2024>(26/48 | 191/318) Semi-Supervised Learning for Deep Causal Generative Models (Yasin Ibrahim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas. (2024)<br><strong>Semi-Supervised Learning for Deep Causal Generative Models</strong><br><button class=copy-to-clipboard title="Semi-Supervised Learning for Deep Causal Generative Models" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Counter-factual, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18717v1.pdf filename=2403.18717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing models that can answer questions of the form &ldquo;How would $x$ change if $y$ had been $z$?&rdquo; is fundamental for advancing medical image analysis. Training causal generative models that address such <b>counterfactual</b> questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a <b>semi-supervised</b> <b>deep</b> causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference to infer missing values and subsequently generate realistic <b>counterfactuals,</b> even for samples with incomplete labels.</p></p class="citation"></blockquote><h3 id=2748--192318-fresco-federated-reinforcement-energy-system-for-cooperative-optimization-nicolas-mauricio-cuadrado-et-al-2024>(27/48 | 192/318) FRESCO: Federated Reinforcement Energy System for Cooperative Optimization (Nicolas Mauricio Cuadrado et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Mauricio Cuadrado, Roberto Alejandro Gutierrez, Martin Takáč. (2024)<br><strong>FRESCO: Federated Reinforcement Energy System for Cooperative Optimization</strong><br><button class=copy-to-clipboard title="FRESCO: Federated Reinforcement Energy System for Cooperative Optimization" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18444v1.pdf filename=2403.18444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise in renewable energy is creating new dynamics in the energy grid that promise to create a cleaner and more participative energy grid, where technology plays a crucial part in making the required flexibility to achieve the vision of the next-generation grid. This work presents FRESCO, a framework that aims to ease the implementation of energy markets using a hierarchical control architecture of <b>reinforcement</b> <b>learning</b> agents trained using <b>federated</b> <b>learning.</b> The core concept we are proving is that having greedy agents subject to changing conditions from a higher level agent creates a cooperative setup that will allow for fulfilling all the individual objectives. This paper presents a general overview of the framework, the current progress, and some insights we obtained from the recent results.</p></p class="citation"></blockquote><h3 id=2848--193318-the-artificial-neural-twin----process-optimization-and-continual-learning-in-distributed-process-chains-johannes-emmert-et-al-2024>(28/48 | 193/318) The Artificial Neural Twin &ndash; Process Optimization and Continual Learning in Distributed Process Chains (Johannes Emmert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Emmert, Ronald Mendez, Houman Mirzaalian Dastjerdi, Christopher Syben, Andreas Maier. (2024)<br><strong>The Artificial Neural Twin &ndash; Process Optimization and Continual Learning in Distributed Process Chains</strong><br><button class=copy-to-clipboard title="The Artificial Neural Twin -- Process Optimization and Continual Learning in Distributed Process Chains" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-11; J-2; F-2-2, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Continual Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18343v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18343v1.pdf filename=2403.18343v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industrial process optimization and control is crucial to increase economic and ecologic efficiency. However, data sovereignty, differing goals, or the required expert knowledge for implementation impede holistic implementation. Further, the increasing use of data-driven AI-methods in process models and industrial sensory often requires regular <b>fine-tuning</b> to accommodate distribution drifts. We propose the Artificial Neural Twin, which combines concepts from model predictive control, deep learning, and sensor networks to address these issues. Our approach introduces differentiable data fusion to estimate the state of distributed process steps and their dependence on input data. By treating the interconnected process steps as a quasi neural-network, we can backpropagate loss gradients for process optimization or model <b>fine-tuning</b> to process parameters or AI models respectively. The concept is demonstrated on a virtual machine park simulated in Unity, consisting of bulk material processes in plastic recycling.</p></p class="citation"></blockquote><h3 id=2948--194318-macroscale-fracture-surface-segmentation-via-semi-supervised-learning-considering-the-structural-similarity-johannes-rosenberger-et-al-2024>(29/48 | 194/318) Macroscale fracture surface segmentation via semi-supervised learning considering the structural similarity (Johannes Rosenberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Rosenberger, Johannes Tlatlik, Sebastian Münstermann. (2024)<br><strong>Macroscale fracture surface segmentation via semi-supervised learning considering the structural similarity</strong><br><button class=copy-to-clipboard title="Macroscale fracture surface segmentation via semi-supervised learning considering the structural similarity" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-m, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Semi-Supervised Learning, Semi-Supervised Training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18337v1.pdf filename=2403.18337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To this date the safety assessment of materials, used for example in the nuclear power sector, commonly relies on a fracture mechanical analysis utilizing macroscopic concepts, where a global load quantity K or J is compared to the materials fracture toughness curve. Part of the experimental effort involved in these concepts is dedicated to the quantitative analysis of fracture surfaces. Within the scope of this study a methodology for the <b>semi-supervised</b> <b>training</b> of deep learning models for fracture surface segmentation on a macroscopic level was established. Therefore, three distinct and unique datasets were created to analyze the influence of structural similarity on the segmentation capability. The structural similarity differs due to the assessed materials and specimen, as well as imaging-induced variance due to fluctuations in image acquisition in different laboratories. The datasets correspond to typical isolated laboratory conditions, complex real-world circumstances, and a curated subset of the two. We implemented a weak-to-strong consistency regularization for <b>semi-supervised</b> <b>learning.</b> On the heterogeneous dataset we were able to train robust and well-generalizing models that learned feature representations from images across different domains without observing a significant drop in prediction quality. Furthermore, our approach reduced the number of labeled images required for training by a factor of 6. To demonstrate the success of our method and the benefit of our approach for the fracture mechanics assessment, we utilized the models for initial crack size measurements with the area average method. For the laboratory setting, the deep learning assisted measurements proved to have the same quality as manual measurements. For models trained on the heterogeneous dataset, very good measurement accuracies with mean deviations smaller than 1 % could be achieved&mldr;</p></p class="citation"></blockquote><h3 id=3048--195318-a-thermodynamically-consistent-physics-informed-deep-learning-material-model-for-short-fiberpolymer-nanocomposites-betim-bahtiri-et-al-2024>(30/48 | 195/318) A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites (Betim Bahtiri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes. (2024)<br><strong>A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites</strong><br><button class=copy-to-clipboard title="A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CE, cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 20<br>Keywords: LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18310v1.pdf filename=2403.18310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work proposes a physics-informed deep learning (PIDL)-based constitutive model for investigating the viscoelastic-viscoplastic behavior of short fiber-reinforced nanoparticle-filled epoxies under various ambient conditions. The deep-learning model is trained to enforce thermodynamic principles, leading to a thermodynamically consistent constitutive model. To accomplish this, a <b>long</b> <b>short-term</b> <b>memory</b> <b>network</b> is combined with a feed-forward neural network to predict internal variables required for characterizing the internal dissipation of the nanocomposite materials. In addition, another feed-forward neural network is used to indicate the free-energy function, which enables defining the thermodynamic state of the entire system. The PIDL model is initially developed for the three-dimensional case by generating synthetic data from a classical constitutive model. The model is then trained by extracting the data directly from cyclic loading-unloading experimental tests. Numerical examples show that the PIDL model can accurately predict the mechanical behavior of epoxy-based nanocomposites for different volume fractions of fibers and nanoparticles under various hygrothermal conditions.</p></p class="citation"></blockquote><h3 id=3148--196318-dsf-gan-downstream-feedback-generative-adversarial-network-oriel-perets-et-al-2024>(31/48 | 196/318) DSF-GAN: DownStream Feedback Generative Adversarial Network (Oriel Perets et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oriel Perets, Nadav Rappoport. (2024)<br><strong>DSF-GAN: DownStream Feedback Generative Adversarial Network</strong><br><button class=copy-to-clipboard title="DSF-GAN: DownStream Feedback Generative Adversarial Network" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2, cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18267v1.pdf filename=2403.18267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Utility and privacy are two crucial measurements of the quality of synthetic tabular data. While significant advancements have been made in privacy measures, generating synthetic samples with high utility remains challenging. To enhance the utility of synthetic samples, we propose a novel architecture called the DownStream Feedback <b>Generative</b> <b>Adversarial</b> <b>Network</b> (DSF-GAN). This approach incorporates feedback from a downstream prediction model during training to augment the generator&rsquo;s loss function with valuable information. Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of synthetic samples. To evaluate our method, we tested it using two popular datasets. Our experiments demonstrate improved model performance when training on synthetic samples generated by DSF-GAN, compared to those generated by the same <b>GAN</b> architecture without feedback. The evaluation was conducted on the same validation set comprising real samples. All code and datasets used in this research will be made openly available for ease of reproduction.</p></p class="citation"></blockquote><h3 id=3248--197318-from-two-dimensional-to-three-dimensional-environment-with-q-learning-modeling-autonomous-navigation-with-reinforcement-learning-and-no-libraries-ergon-cugler-de-moraes-silva-2024>(32/48 | 197/318) From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries (Ergon Cugler de Moraes Silva, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ergon Cugler de Moraes Silva. (2024)<br><strong>From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries</strong><br><button class=copy-to-clipboard title="From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-CO<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18219v1.pdf filename=2403.18219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) algorithms have become indispensable tools in artificial intelligence, empowering agents to acquire optimal decision-making policies through interactions with their environment and feedback mechanisms. This study explores the performance of RL agents in both two-dimensional (2D) and three-dimensional (3D) environments, aiming to research the dynamics of learning across different spatial dimensions. A key aspect of this investigation is the absence of pre-made libraries for learning, with the algorithm developed exclusively through computational mathematics. The methodological framework centers on RL principles, employing a Q-learning agent class and distinct environment classes tailored to each spatial dimension. The research aims to address the question: How do <b>reinforcement</b> <b>learning</b> agents adapt and perform in environments of varying spatial dimensions, particularly in 2D and 3D settings? Through empirical analysis, the study evaluates agents&rsquo; learning trajectories and adaptation processes, revealing insights into the efficacy of RL algorithms in navigating complex, multi-dimensional spaces. Reflections on the findings <b>prompt</b> considerations for future research, particularly in understanding the dynamics of learning in higher-dimensional environments.</p></p class="citation"></blockquote><h3 id=3348--198318-compression-of-the-koopman-matrix-for-nonlinear-physical-models-via-hierarchical-clustering-tomoya-nishikata-et-al-2024>(33/48 | 198/318) Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering (Tomoya Nishikata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomoya Nishikata, Jun Ohkubo. (2024)<br><strong>Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering</strong><br><button class=copy-to-clipboard title="Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-DS<br>Keyword Score: 13<br>Keywords: Clustering, Hierarchical Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18181v1.pdf filename=2403.18181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning methods allow the prediction of nonlinear dynamical systems from data alone. The Koopman operator is one of them, which enables us to employ linear analysis for nonlinear dynamical systems. The linear characteristics of the Koopman operator are hopeful to understand the nonlinear dynamics and perform rapid predictions. The extended dynamic mode decomposition (EDMD) is one of the methods to approximate the Koopman operator as a finite-dimensional matrix. In this work, we propose a method to compress the Koopman matrix using <b>hierarchical</b> <b>clustering.</b> Numerical demonstrations for the cart-pole model and comparisons with the conventional singular value decomposition (SVD) are shown; the results indicate that the <b>hierarchical</b> <b>clustering</b> performs better than the naive SVD compressions.</p></p class="citation"></blockquote><h3 id=3448--199318-equity-in-healthcare-analyzing-disparities-in-machine-learning-predictions-of-diabetic-patient-readmissions-zainab-al-zanbouri-et-al-2024>(34/48 | 199/318) Equity in Healthcare: Analyzing Disparities in Machine Learning Predictions of Diabetic Patient Readmissions (Zainab Al-Zanbouri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zainab Al-Zanbouri, Gauri Sharma, Shaina Raza. (2024)<br><strong>Equity in Healthcare: Analyzing Disparities in Machine Learning Predictions of Diabetic Patient Readmissions</strong><br><button class=copy-to-clipboard title="Equity in Healthcare: Analyzing Disparities in Machine Learning Predictions of Diabetic Patient Readmissions" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19057v1.pdf filename=2403.19057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates how machine learning (ML) models can predict hospital readmissions for diabetic patients fairly and accurately across different demographics (age, gender, race). We compared models like Deep Learning, Generalized Linear Models, Gradient Boosting Machines (GBM), and Naive Bayes. GBM stood out with an F1-score of 84.3% and accuracy of 82.2%, accurately predicting readmissions across demographics. A <b>fairness</b> analysis was conducted across all the models. GBM minimized disparities in predictions, achieving balanced results across genders and races. It showed low False Discovery Rates (FDR) (6-7%) and False Positive Rates (FPR) (5%) for both genders. Additionally, FDRs remained low for racial groups, such as African Americans (8%) and Asians (7%). Similarly, FPRs were consistent across age groups (4%) for both patients under 40 and those above 40, indicating its precision and ability to reduce bias. These findings emphasize the importance of choosing ML models carefully to ensure both accuracy and <b>fairness</b> for all patients. By showcasing effectiveness of various models with <b>fairness</b> metrics, this study promotes personalized medicine and the need for fair ML algorithms in healthcare. This can ultimately reduce disparities and improve outcomes for diabetic patients of all backgrounds.</p></p class="citation"></blockquote><h3 id=3548--200318-exploiting-symmetry-in-dynamics-for-model-based-reinforcement-learning-with-asymmetric-rewards-yasin-sonmez-et-al-2024>(35/48 | 200/318) Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards (Yasin Sonmez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasin Sonmez, Neelay Junnarkar, Murat Arcak. (2024)<br><strong>Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards</strong><br><button class=copy-to-clipboard title="Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19024v1.pdf filename=2403.19024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work in <b>reinforcement</b> <b>learning</b> has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in <b>reinforcement</b> <b>learning</b> and learning in control theory where symmetry techniques can be applied. We use Cartan&rsquo;s moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.</p></p class="citation"></blockquote><h3 id=3648--201318-thelxinoë-recognizing-human-emotions-using-pupillometry-and-machine-learning-darlene-barker-et-al-2024>(36/48 | 201/318) Thelxinoë: Recognizing Human Emotions Using Pupillometry and Machine Learning (Darlene Barker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Darlene Barker, Haim Levkowitz. (2024)<br><strong>Thelxinoë: Recognizing Human Emotions Using Pupillometry and Machine Learning</strong><br><button class=copy-to-clipboard title="Thelxinoë: Recognizing Human Emotions Using Pupillometry and Machine Learning" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19014v1.pdf filename=2403.19014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we present a method for <b>emotion</b> <b>recognition</b> in Virtual Reality (VR) using pupillometry. We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VR generated data. Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR). By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it. This research contributes significantly to the Thelxino"e framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions. Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for future advancements in virtual touch technology.</p></p class="citation"></blockquote><h3 id=3748--202318-conditional-wasserstein-distances-with-applications-in-bayesian-ot-flow-matching-jannis-chemseddine-et-al-2024>(37/48 | 202/318) Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching (Jannis Chemseddine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jannis Chemseddine, Paul Hagemann, Christian Wald, Gabriele Steidl. (2024)<br><strong>Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching</strong><br><button class=copy-to-clipboard title="Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18705v1.pdf filename=2403.18705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback&ndash;Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein <b>GAN</b> literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance. Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation.</p></p class="citation"></blockquote><h3 id=3848--203318-learning-in-pinns-phase-transition-total-diffusion-and-generalization-sokratis-j-anagnostopoulos-et-al-2024>(38/48 | 203/318) Learning in PINNs: Phase transition, total diffusion, and generalization (Sokratis J. Anagnostopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, George Em Karniadakis. (2024)<br><strong>Learning in PINNs: Phase transition, total diffusion, and generalization</strong><br><button class=copy-to-clipboard title="Learning in PINNs: Phase transition, total diffusion, and generalization" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Information Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18494v1.pdf filename=2403.18494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR), examining the behavior of first-order optimizers like Adam in non-convex objectives. By interpreting the drift/diffusion phases in the <b>information</b> <b>bottleneck</b> theory, focusing on gradient homogeneity, we identify a third phase termed ``total diffusion", characterized by equilibrium in the learning rates and homogeneous gradients. This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence. We propose a residual-based re-weighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization. We also explore the <b>information</b> <b>compression</b> phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible <b>information</b> <b>loss.</b> Supported by experimental data on physics-informed neural networks (PINNs), which underscore the importance of gradient homogeneity due to their PDE-based sample inter-dependence, our findings suggest that recognizing phase transitions could refine ML optimization strategies for improved generalization.</p></p class="citation"></blockquote><h3 id=3948--204318-synthesizing-eeg-signals-from-event-related-potential-paradigms-with-conditional-diffusion-models-guido-klein-et-al-2024>(39/48 | 204/318) Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models (Guido Klein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann. (2024)<br><strong>Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models</strong><br><button class=copy-to-clipboard title="Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6; G-3; I-5-4; J-3, cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18486v1.pdf filename=2403.18486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically <b>diffusion</b> <b>models.</b> While <b>diffusion</b> <b>models</b> have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t.~sampling or require alternative representations of the EEG data. To overcome these limitations, we introduce a novel approach to conditional <b>diffusion</b> <b>models</b> that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data. In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples. The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class.</p></p class="citation"></blockquote><h3 id=4048--205318-generalized-policy-learning-for-smart-grids-fl-trpo-approach-yunxiang-li-et-al-2024>(40/48 | 205/318) Generalized Policy Learning for Smart Grids: FL TRPO Approach (Yunxiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunxiang Li, Nicolas Mauricio Cuadrado, Samuel Horváth, Martin Takáč. (2024)<br><strong>Generalized Policy Learning for Smart Grids: FL TRPO Approach</strong><br><button class=copy-to-clipboard title="Generalized Policy Learning for Smart Grids: FL TRPO Approach" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18439v1.pdf filename=2403.18439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The smart grid domain requires bolstering the capabilities of existing energy management systems; <b>Federated</b> <b>Learning</b> (FL) aligns with this goal as it demonstrates a remarkable ability to train models on heterogeneous datasets while maintaining data privacy, making it suitable for smart grid applications, which often involve disparate data distributions and interdependencies among features that hinder the suitability of linear models. This paper introduces a framework that combines FL with a Trust Region Policy Optimization (FL TRPO) aiming to reduce energy-associated emissions and costs. Our approach reveals latent interconnections and employs personalized encoding methods to capture unique insights, understanding the relationships between features and optimal strategies, allowing our model to generalize to previously unseen data. Experimental results validate the robustness of our approach, affirming its proficiency in effectively learning policy models for smart grid challenges.</p></p class="citation"></blockquote><h3 id=4148--206318-global-vegetation-modeling-with-pre-trained-weather-transformers-pascal-janetzky-et-al-2024>(41/48 | 206/318) Global Vegetation Modeling with Pre-Trained Weather Transformers (Pascal Janetzky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pascal Janetzky, Florian Gallusser, Simon Hentschel, Andreas Hotho, Anna Krause. (2024)<br><strong>Global Vegetation Modeling with Pre-Trained Weather Transformers</strong><br><button class=copy-to-clipboard title="Global Vegetation Modeling with Pre-Trained Weather Transformers" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18438v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18438v1.pdf filename=2403.18438v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate vegetation models can produce further insights into the complex interaction between vegetation activity and ecosystem processes. Previous research has established that long-term trends and short-term variability of temperature and precipitation affect vegetation activity. Motivated by the recent success of <b>Transformer-based</b> Deep Learning models for medium-range weather forecasting, we adapt the publicly available pre-trained FourCastNet to model vegetation activity while accounting for the short-term dynamics of climate variability. We investigate how the learned global representation of the atmosphere&rsquo;s state can be transferred to model the normalized difference vegetation index (NDVI). Our model globally estimates vegetation activity at a resolution of \SI{0.25}{\degree} while relying only on meteorological data. We demonstrate that leveraging pre-trained weather models improves the NDVI estimates compared to learning an NDVI model from scratch. Additionally, we compare our results to other recent data-driven NDVI modeling approaches from machine learning and ecology literature. We further provide experimental evidence on how much data and training time is necessary to turn FourCastNet into an effective vegetation model. Code and models will be made available upon publication.</p></p class="citation"></blockquote><h3 id=4248--207318-iip-mixerintra-inter-patch-mixing-architecture-for-battery-remaining-useful-life-prediction-guangzai-ye-et-al-2024>(42/48 | 207/318) IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction (Guangzai Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen. (2024)<br><strong>IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction</strong><br><button class=copy-to-clipboard title="IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18379v1.pdf filename=2403.18379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately estimating the Remaining Useful Life (RUL) of lithium-ion batteries is crucial for maintaining the safe and stable operation of rechargeable battery management systems. However, this task is often challenging due to the complex temporal dynamics involved. Recently, attention-based networks, such as <b>Transformers</b> and Informer, have been the popular architecture in time series forecasting. Despite their effectiveness, these models with abundant parameters necessitate substantial training time to unravel temporal patterns. To tackle these challenges, we propose a simple MLP-Mixer-based architecture named &lsquo;Intra-Inter Patch Mixer&rsquo; (IIP-Mixer), which is an architecture based exclusively on multi-layer perceptrons (MLPs), extracting information by mixing operations along both intra-patch and inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer comprises parallel dual-head mixer layers: the intra-patch mixing MLP, capturing local temporal patterns in the short-term period, and the inter-patch mixing MLP, capturing global temporal patterns in the long-term period. Notably, to address the varying importance of features in RUL prediction, we introduce a weighted loss function in the MLP-Mixer-based architecture, marking the first time such an approach has been employed. Our experiments demonstrate that IIP-Mixer achieves competitive performance in battery RUL prediction, outperforming other popular time-series frameworks</p></p class="citation"></blockquote><h3 id=4348--208318-stragglers-aware-low-latency-synchronous-federated-learning-via-layer-wise-model-updates-natalie-lang-et-al-2024>(43/48 | 208/318) Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates (Natalie Lang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Natalie Lang, Alejandro Cohen, Nir Shlezinger. (2024)<br><strong>Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates</strong><br><button class=copy-to-clipboard title="Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18375v1.pdf filename=2403.18375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synchronous <b>federated</b> <b>learning</b> (FL) is a popular paradigm for collaborative edge learning. It typically involves a set of heterogeneous devices locally training neural network (NN) models in parallel with periodic centralized aggregations. As some of the devices may have limited computational resources and varying availability, FL latency is highly sensitive to stragglers. Conventional approaches discard incomplete intra-model updates done by stragglers, alter the amount of local workload and architecture, or resort to asynchronous settings; which all affect the trained model performance under tight training latency constraints. In this work, we propose straggler-aware layer-wise <b>federated</b> <b>learning</b> (SALF) that leverages the optimization procedure of NNs via backpropagation to update the global model in a layer-wise fashion. SALF allows stragglers to synchronously convey partial gradients, having each layer of the global model be updated independently with a different contributing set of users. We provide a theoretical analysis, establishing convergence guarantees for the global model under mild assumptions on the distribution of the participating devices, revealing that SALF converges at the same asymptotic rate as FL with no timing limitations. This insight is matched with empirical observations, demonstrating the performance gains of SALF compared to alternative mechanisms mitigating the device heterogeneity gap in FL.</p></p class="citation"></blockquote><h3 id=4448--209318-long-and-short-term-constraints-driven-safe-reinforcement-learning-for-autonomous-driving-xuemin-hu-et-al-2024>(44/48 | 209/318) Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving (Xuemin Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuemin Hu, Pan Chen, Yijun Wen, Bo Tang, Long Chen. (2024)<br><strong>Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving</strong><br><button class=copy-to-clipboard title="Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18209v1.pdf filename=2403.18209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent&rsquo;s safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the vehicle throughout the decision-making process. In addition, we develop a safe RL method with dual-constraint optimization based on the Lagrange multiplier to optimize the training process for end-to-end autonomous driving. Comprehensive experiments were conducted on the MetaDrive simulator. Experimental results demonstrate that the proposed method achieves higher safety in continuous state and action tasks, and exhibits higher exploration performance in long-distance decision-making tasks compared with state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=4548--210318-superior-parallel-big-data-clustering-through-competitive-stochastic-sample-size-optimization-in-big-means-rustam-mussabayev-et-al-2024>(45/48 | 210/318) Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means (Rustam Mussabayev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rustam Mussabayev, Ravil Mussabayev. (2024)<br><strong>Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means</strong><br><button class=copy-to-clipboard title="Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-IR, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Clustering, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18766v1.pdf filename=2403.18766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel K-means <b>clustering</b> algorithm, an advancement on the conventional Big-means methodology. The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications. It addresses scalability and computation time challenges typically faced with traditional techniques. The algorithm adjusts <b>sample</b> <b>sizes</b> dynamically for each worker during execution, optimizing performance. Data from these <b>sample</b> <b>sizes</b> are continually analyzed, facilitating the identification of the most efficient configuration. By incorporating a competitive element among workers using different <b>sample</b> <b>sizes,</b> efficiency within the Big-means algorithm is further stimulated. In essence, the algorithm balances computational time and <b>clustering</b> quality by employing a stochastic, competitive sampling strategy in a parallel computing setting.</p></p class="citation"></blockquote><h3 id=4648--211318-tensor-based-graph-learning-with-consistency-and-specificity-for-multi-view-clustering-long-shi-et-al-2024>(46/48 | 211/318) Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering (Long Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Shi, Lei Cao, Yunshan Ye, Yu Zhao, Badong Chen. (2024)<br><strong>Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering</strong><br><button class=copy-to-clipboard title="Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18393v1.pdf filename=2403.18393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> learning is widely recognized as a crucial technique in multi-view <b>clustering.</b> Existing <b>graph</b> learning methods typically involve constructing an adaptive neighbor <b>graph</b> based on probabilistic neighbors and then learning a consensus <b>graph</b> to for <b>clustering,</b> however, they are confronted with two limitations. Firstly, they often rely on Euclidean distance to measure similarity when constructing the adaptive neighbor <b>graph,</b> which proves inadequate in capturing the intrinsic structure among data points in many real-world scenarios. Secondly, most of these methods focus solely on consensus <b>graph,</b> ignoring view-specific <b>graph</b> information. In response to the aforementioned drawbacks, we in this paper propose a novel tensor-based <b>graph</b> learning framework that simultaneously considers consistency and specificity for multi-view <b>clustering.</b> Specifically, we calculate the similarity distance on the Stiefel manifold to preserve the intrinsic structure among data points. By making an assumption that the learned neighbor <b>graph</b> of each view comprises both a consistent <b>graph</b> and a view-specific <b>graph,</b> we formulate a new tensor-based target <b>graph</b> learning paradigm. Owing to the benefits of tensor singular value decomposition (t-SVD) in uncovering high-order correlations, this model is capable of achieving a complete understanding of the target <b>graph.</b> Furthermore, we develop an iterative algorithm to solve the proposed objective optimization problem. Experiments conducted on real-world datasets have demonstrated the superior performance of the proposed method over some state-of-the-art multi-view <b>clustering</b> methods. The source code has been released on <a href=https://github.com/lshi91/CSTGL-Code>https://github.com/lshi91/CSTGL-Code</a>.</p></p class="citation"></blockquote><h3 id=4748--212318-nonlinear-model-reduction-for-operator-learning-hamidreza-eivazi-et-al-2024>(47/48 | 212/318) Nonlinear model reduction for operator learning (Hamidreza Eivazi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamidreza Eivazi, Stefan Wittek, Andreas Rausch. (2024)<br><strong>Nonlinear model reduction for operator learning</strong><br><button class=copy-to-clipboard title="Nonlinear model reduction for operator learning" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18735v1.pdf filename=2403.18735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Operator learning provides methods to approximate mappings between infinite-dimensional function spaces. Deep operator networks (DeepONets) are a notable architecture in this field. Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several <b>benchmark</b> tests. We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning. Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet.</p></p class="citation"></blockquote><h3 id=4848--213318-multi-label-adaptive-batch-selection-by-highlighting-hard-and-imbalanced-samples-ao-zhou-et-al-2024>(48/48 | 213/318) Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples (Ao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ao Zhou, Bin Liu, Jin Wang, Grigorios Tsoumakas. (2024)<br><strong>Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples</strong><br><button class=copy-to-clipboard title="Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18192v1.pdf filename=2403.18192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural network models have demonstrated their effectiveness in classifying multi-label data from various domains. Typically, they employ a training mode that combines mini-batches with optimizers, where each sample is randomly selected with equal probability when constructing mini-batches. However, the intrinsic class imbalance in multi-label data may bias the model towards majority labels, since samples relevant to minority labels may be underrepresented in each mini-batch. Meanwhile, during the training process, we observe that instances associated with minority labels tend to induce greater losses. Existing heuristic batch selection methods, such as priority selection of samples with high contribution to the objective function, i.e., samples with high loss, have been proven to accelerate convergence while reducing the loss and test error in single-label data. However, batch selection methods have not yet been applied and validated in multi-label data. In this study, we introduce a simple yet effective adaptive batch selection algorithm tailored to multi-label deep learning models. It adaptively selects each batch by prioritizing hard samples related to minority labels. A variant of our method also takes informative label correlations into consideration. Comprehensive experiments combining five multi-label deep learning models on thirteen <b>benchmark</b> datasets show that our method converges faster and performs better than random batch selection.</p></p class="citation"></blockquote><h2 id=eessiv-9>eess.IV (9)</h2><h3 id=19--214318-generative-medical-segmentation-jiayu-huo-et-al-2024>(1/9 | 214/318) Generative Medical Segmentation (Jiayu Huo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayu Huo, Xi Ouyang, Sébastien Ourselin, Rachel Sparks. (2024)<br><strong>Generative Medical Segmentation</strong><br><button class=copy-to-clipboard title="Generative Medical Segmentation" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 83<br>Keywords: Vision Transformer, Autoencoder, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Variational Autoencoder, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18198v1.pdf filename=2403.18198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rapid advancements in medical image segmentation performance have been significantly driven by the development of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and <b>Vision</b> <b>Transformers</b> (ViTs). However, these models introduce high computational demands and often have limited ability to generalize across diverse medical imaging datasets. In this manuscript, we introduce Generative Medical Segmentation (GMS), a novel approach leveraging a generative model for image segmentation. Concretely, GMS employs a robust pre-trained <b>Variational</b> <b>Autoencoder</b> (VAE) to derive latent representations of both images and masks, followed by a mapping model that learns the transition from image to mask in the latent space. This process culminates in generating a precise segmentation mask within the image space using the pre-trained VAE decoder. The design of GMS leads to fewer learnable parameters in the model, resulting in a reduced computational burden and enhanced generalization capability. Our extensive experimental analysis across five public datasets in different medical imaging domains demonstrates GMS outperforms existing discriminative segmentation models and has remarkable domain generalization. Our experiments suggest GMS could set a new <b>benchmark</b> for medical image segmentation, offering a scalable and effective solution. GMS implementation and model weights are available at <a href=https://github.com/King-HAW/GMS>https://github.com/King-HAW/GMS</a>.</p></p class="citation"></blockquote><h3 id=29--215318-hemit-he-to-multiplex-immunohistochemistry-image-translation-with-dual-branch-pix2pix-generator-chang-bian-et-al-2024>(2/9 | 215/318) HEMIT: H&amp;E to Multiplex-immunohistochemistry Image Translation with Dual-Branch Pix2pix Generator (Chang Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chang Bian, Beth Philips, Tim Cootes, Martin Fergie. (2024)<br><strong>HEMIT: H&amp;E to Multiplex-immunohistochemistry Image Translation with Dual-Branch Pix2pix Generator</strong><br><button class=copy-to-clipboard title="HEMIT: H&E to Multiplex-immunohistochemistry Image Translation with Dual-Branch Pix2pix Generator" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 63<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Supervised Learning, Transformer, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18501v1.pdf filename=2403.18501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational analysis of multiplexed immunofluorescence histology data is emerging as an important method for understanding the tumour micro-environment in cancer. This work presents HEMIT, a dataset designed for translating Hematoxylin and Eosin (H&amp;E) sections to multiplex-immunohistochemistry (mIHC) <b>images,</b> <b>featuring</b> DAPI, CD3, and panCK markers. Distinctively, HEMIT&rsquo;s mIHC <b>images</b> <b>are</b> multi-component and cellular-level aligned with H&amp;E, enriching <b>supervised</b> stain translation tasks. To our knowledge, HEMIT is the first publicly available cellular-level aligned dataset that enables H&amp;E to multi-target mIHC <b>image</b> <b>translation.</b> This dataset provides the computer vision community with a valuable resource to develop novel computational methods which have the potential to gain new insights from H&amp;E slide archives. We also propose a new dual-branch generator architecture, using residual <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and Swin <b>Transformers</b> which achieves better translation outcomes than other popular algorithms. When evaluated on HEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the highest overall score on key metrics including the Structural Similarity Index Measure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio (PSNR). Additionally, downstream analysis has been used to further validate the quality of the generated mIHC <b>images.</b> <b>These</b> results set a new <b>benchmark</b> in the field of stain translation tasks.</p></p class="citation"></blockquote><h3 id=39--216318-benchmarking-image-transformers-for-prostate-cancer-detection-from-ultrasound-data-mohamed-harmanani-et-al-2024>(3/9 | 216/318) Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data (Mohamed Harmanani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar, Amoon Jamzad, Mahdi Gilany, Minh Nguyen Nhat To, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi. (2024)<br><strong>Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data</strong><br><button class=copy-to-clipboard title="Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV, q-bio-TO<br>Keyword Score: 56<br>Keywords: Benchmarking, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Multiple Instance Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18233v1.pdf filename=2403.18233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in ultrasound images typically employ <b>convolutional</b> <b>networks</b> <b>(CNNs)</b> to detect cancer in small regions of interest (ROI) along a needle trace region. However, this approach suffers from weak labelling, since the ground-truth histopathology labels do not describe the properties of individual ROIs. Recently, multi-scale approaches have sought to mitigate this issue by combining the context awareness of <b>transformers</b> with a <b>CNN</b> feature extractor to detect cancer from <b>multiple</b> <b>ROIs</b> <b>using</b> <b>multiple-instance</b> <b>learning</b> <b>(MIL).</b> In this work, we present a detailed study of several image <b>transformer</b> architectures for both ROI-scale and multi-scale classification, and a comparison of the performance of <b>CNNs</b> and <b>transformers</b> for ultrasound-based prostate cancer classification. We also design a novel multi-objective learning strategy that combines both ROI and core predictions to further mitigate label noise. METHODS: We evaluate 3 image <b>transformers</b> on ROI-scale cancer classification, then use the strongest model to tune a multi-scale classifier with MIL. We train our MIL models using our novel multi-objective learning strategy and compare our results to existing baselines. RESULTS: We find that for both ROI-scale and multi-scale PCa detection, image <b>transformer</b> backbones lag behind their <b>CNN</b> counterparts. This deficit in performance is even more noticeable for larger models. When using multi-objective learning, we can improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a specificity of 66.3%. CONCLUSION: <b>Convolutional</b> <b>networks</b> are better suited for modelling sparse datasets of prostate ultrasounds, producing more robust features than <b>transformers</b> in PCa detection. Multi-scale methods remain the best architecture for this task, with multi-objective learning presenting an effective way to improve performance.</p></p class="citation"></blockquote><h3 id=49--217318-ct-3dflow--leveraging-3d-normalizing-flows-for-unsupervised-detection-of-pathological-pulmonary-ct-scans-aissam-djahnine-et-al-2024>(4/9 | 217/318) CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection of Pathological Pulmonary CT scans (Aissam Djahnine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aissam Djahnine, Alexandre Popoff, Emilien Jupin-Delevaux, Vincent Cottin, Olivier Nempont, Loic Boussel. (2024)<br><strong>CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection of Pathological Pulmonary CT scans</strong><br><button class=copy-to-clipboard title="CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection of Pathological Pulmonary CT scans" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Convolutional Neural Network, Generative Adversarial Network, Out-of-distribution, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18514v1.pdf filename=2403.18514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> pathology detection can be implemented by training a model on healthy data only and measuring the deviation from the training set upon inference, for example with <b>CNN-based</b> feature extraction and one-class classifiers, or reconstruction-score-based methods such as AEs, <b>GANs</b> and <b>Diffusion</b> <b>models.</b> Normalizing Flows (NF) have the ability to directly learn the probability distribution of training examples through an invertible architecture. We leverage this property in a novel 3D NF-based model named CT-3DFlow, specifically tailored for patient-level pulmonary pathology detection in chest CT data. Our model is trained <b>unsupervised</b> on healthy 3D pulmonary CT patches, and detects deviations from its log-likelihood distribution as anomalies. We aggregate patches-level likelihood values from a patient&rsquo;s CT scan to provide a patient-level &rsquo;normal&rsquo;/&lsquo;abnormal&rsquo; prediction. <b>Out-of-distribution</b> detection performance is evaluated using expert annotations on a separate chest CT test dataset, outperforming other state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=59--218318-a-vascular-synthetic-model-for-improved-aneurysm-segmentation-and-detection-via-deep-neural-networks-rafic-nader-et-al-2024>(5/9 | 218/318) A vascular synthetic model for improved aneurysm segmentation and detection via Deep Neural Networks (Rafic Nader et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rafic Nader, Florent Autrusseau, Vincent L&rsquo;Allinec, Romain Bourcier. (2024)<br><strong>A vascular synthetic model for improved aneurysm segmentation and detection via Deep Neural Networks</strong><br><button class=copy-to-clipboard title="A vascular synthetic model for improved aneurysm segmentation and detection via Deep Neural Networks" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 45<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18734v1.pdf filename=2403.18734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We hereby present a full synthetic model, able to mimic the various constituents of the cerebral vascular tree: the cerebral arteries, the bifurcations and the intracranial aneurysms. By building this model, our goal was to provide a substantial dataset of brain arteries which could be used by a 3D <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> to either segment or detect/recognize various vascular diseases (such as artery dissection/thrombosis) or even some portions of the cerebral vasculature, such as the bifurcations or aneurysms. In this study, we will particularly focus on Intra-Cranial Aneurysm (ICA) detection and segmentation. The cerebral aneurysms most often occur on a particular structure of the vascular tree named the Circle of Willis. Various studies have been conducted to detect and monitor the ICAs and those based on Deep Learning (DL) achieve the best performances. Specifically, in this work, we propose a full synthetic 3D model able to mimic the brain vasculature as acquired by Magnetic Resonance Angiography (MRA), and more particularly the Time Of Flight (TOF) principle. Among the various MRI modalities, the MRA-TOF allows to have a relatively good rendering of the blood vessels and is non-invasive (no contrast liquid injection). Our model has been designed to simultaneously mimic the arteries <b>geometry,</b> the ICA shape and the background noise. The <b>geometry</b> of the vascular tree is modeled thanks to an interpolation with 3D Spline functions, and the statistical properties of the background MRI noise is collected from MRA acquisitions and reproduced within the model. In this work, we thoroughly describe the synthetic vasculature model, we build up a neural network designed for ICA segmentation and detection, and finally, we carry out an in-depth evaluation of the performance gap gained thanks to the synthetic model <b>data</b> <b>augmentation.</b></p></p class="citation"></blockquote><h3 id=69--219318-theoretical-bound-guided-hierarchical-vae-for-neural-image-codecs-yichi-zhang-et-al-2024>(6/9 | 219/318) Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs (Yichi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu. (2024)<br><strong>Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs</strong><br><button class=copy-to-clipboard title="Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18535v1.pdf filename=2403.18535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies reveal a significant theoretical link between <b>variational</b> <b>autoencoders</b> (VAEs) and rate-distortion theory, notably in utilizing VAEs to estimate the theoretical upper bound of the information rate-distortion function of images. Such estimated theoretical bounds substantially exceed the performance of existing neural image codecs (NICs). To narrow this gap, we propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The proposed BG-VAE leverages the theoretical bound to guide the NIC model towards enhanced performance. We implement the BG-VAE using Hierarchical VAEs and demonstrate its effectiveness through extensive experiments. Along with advanced neural network blocks, we provide a versatile, variable-rate NIC that outperforms existing methods when considering both rate-distortion performance and computational complexity. The code is available at BG-VAE.</p></p class="citation"></blockquote><h3 id=79--220318-deep-learning-segmentation-and-classification-of-red-blood-cells-using-a-large-multi-scanner-dataset-mohamed-elmanna-et-al-2024>(7/9 | 220/318) Deep Learning Segmentation and Classification of Red Blood Cells Using a Large Multi-Scanner Dataset (Mohamed Elmanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Elmanna, Ahmed Elsafty, Yomna Ahmed, Muhammad Rushdi, Ahmed Morsy. (2024)<br><strong>Deep Learning Segmentation and Classification of Red Blood Cells Using a Large Multi-Scanner Dataset</strong><br><button class=copy-to-clipboard title="Deep Learning Segmentation and Classification of Red Blood Cells Using a Large Multi-Scanner Dataset" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18468v1.pdf filename=2403.18468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital pathology has recently been revolutionized by advancements in artificial intelligence, deep learning, and high-performance computing. With its advanced tools, digital pathology can help improve and speed up the diagnostic process, reduce human errors, and streamline the reporting step. In this paper, we report a new large red blood cell (RBC) image dataset and propose a two-stage deep learning framework for RBC image segmentation and classification. The dataset is a highly diverse dataset of more than 100K RBCs containing eight different classes. The dataset, which is considerably larger than any publicly available hematopathology dataset, was labeled independently by two hematopathologists who also manually created masks for RBC cell segmentation. Subsequently, in the proposed framework, first, a U-Net model was trained to achieve automatic RBC image segmentation. Second, an EfficientNetB0 model was trained to classify RBC images into one of the eight classes using a <b>transfer</b> <b>learning</b> approach with a 5X2 cross-validation scheme. An IoU of 98.03% and an average classification accuracy of 96.5% were attained on the test set. Moreover, we have performed experimental comparisons against several prominent <b>CNN</b> models. These comparisons show the superiority of the proposed model with a good balance between performance and computational cost.</p></p class="citation"></blockquote><h3 id=89--221318-transformers-based-architectures-for-stroke-segmentation-a-review-yalda-zafari-ghadim-et-al-2024>(8/9 | 221/318) Transformers-based architectures for stroke segmentation: A review (Yalda Zafari-Ghadim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok. (2024)<br><strong>Transformers-based architectures for stroke segmentation: A review</strong><br><button class=copy-to-clipboard title="Transformers-based architectures for stroke segmentation: A review" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18637v1.pdf filename=2403.18637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stroke remains a significant global health concern, necessitating precise and efficient diagnostic tools for timely intervention and improved patient outcomes. The emergence of deep learning methodologies has transformed the landscape of medical image analysis. Recently, <b>Transformers,</b> initially designed for natural language processing, have exhibited remarkable capabilities in various computer vision applications, including medical image analysis. This comprehensive review aims to provide an in-depth exploration of the cutting-edge <b>Transformer-based</b> architectures applied in the context of stroke segmentation. It commences with an exploration of stroke pathology, imaging modalities, and the challenges associated with accurate diagnosis and segmentation. Subsequently, the review delves into the fundamental ideas of <b>Transformers,</b> offering detailed insights into their architectural intricacies and the underlying mechanisms that empower them to effectively capture complex spatial information within medical images. The existing literature is systematically categorized and analyzed, discussing various approaches that leverage <b>Transformers</b> for stroke segmentation. A critical assessment is provided, highlighting the strengths and limitations of these methods, including considerations of performance and computational efficiency. Additionally, this review explores potential avenues for future research and development</p></p class="citation"></blockquote><h3 id=99--222318-h2aseg-hierarchical-adaptive-interaction-and-weighting-network-for-tumor-segmentation-in-petct-images-jinpeng-lu-et-al-2024>(9/9 | 222/318) H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for Tumor Segmentation in PET/CT Images (Jinpeng Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinpeng Lu, Jingyun Chen, Linghan Cai, Songhan Jiang, Yongbing Zhang. (2024)<br><strong>H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for Tumor Segmentation in PET/CT Images</strong><br><button class=copy-to-clipboard title="H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for Tumor Segmentation in PET/CT Images" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18339v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18339v2.pdf filename=2403.18339v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Positron emission tomography (PET) combined with computed tomography (CT) imaging is routinely used in cancer diagnosis and prognosis by providing complementary information. Automatically segmenting tumors in PET/CT images can significantly improve examination efficiency. Traditional <b>multi-modal</b> segmentation solutions mainly rely on concatenation operations for modality fusion, which fail to effectively model the non-linear dependencies between PET and CT modalities. Recent studies have investigated various approaches to optimize the fusion of modality-specific features for enhancing joint representations. However, modality-specific encoders used in these methods operate independently, inadequately leveraging the synergistic relationships inherent in PET and CT modalities, for example, the complementarity between semantics and structure. To address these issues, we propose a Hierarchical Adaptive Interaction and Weighting Network termed H2ASeg to explore the intrinsic cross-modal correlations and transfer potential complementary information. Specifically, we design a Modality-Cooperative Spatial Attention (MCSA) module that performs intra- and inter-modal interactions globally and locally. Additionally, a Target-Aware Modality Weighting (TAMW) module is developed to highlight tumor-related features within <b>multi-modal</b> features, thereby refining tumor segmentation. By embedding these modules across different layers, H2ASeg can hierarchically model cross-modal correlations, enabling a nuanced understanding of both semantic and structural tumor features. Extensive experiments demonstrate the superiority of H2ASeg, outperforming state-of-the-art methods on AutoPet-II and Hecktor2022 <b>benchmarks.</b> The code is released at <a href=https://github.com/JinPLu/H2ASeg>https://github.com/JinPLu/H2ASeg</a>.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--223318-noise-robust-keyword-spotting-through-self-supervised-pretraining-jacob-mørk-et-al-2024>(1/3 | 223/318) Noise-Robust Keyword Spotting through Self-supervised Pretraining (Jacob Mørk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Mørk, Holger Severin Bovbjerg, Gergely Kiss, Zheng-Hua Tan. (2024)<br><strong>Noise-Robust Keyword Spotting through Self-supervised Pretraining</strong><br><button class=copy-to-clipboard title="Noise-Robust Keyword Spotting through Self-supervised Pretraining" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: 68T10, I-2-6, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Self-supervised Learning, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18560v1.pdf filename=2403.18560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Voice assistants are now widely available, and to activate them a keyword spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using <b>supervised</b> <b>learning</b> methods and require a large amount of labelled data to achieve a good performance. Leveraging unlabelled data through <b>self-supervised</b> <b>learning</b> (SSL) has been shown to increase the accuracy in clean conditions. This paper explores how SSL pretraining such as Data2Vec can be used to enhance the robustness of KWS models in noisy conditions, which is under-explored. Models of three different sizes are pretrained using different pretraining approaches and then <b>fine-tuned</b> for KWS. These models are then tested and compared to models trained using two baseline <b>supervised</b> <b>learning</b> methods, one being standard training using clean data and the other one being multi-style training (MTR). The results show that pretraining and <b>fine-tuning</b> on clean data is superior to <b>supervised</b> <b>learning</b> on clean data across all testing conditions, and superior to <b>supervised</b> <b>MTR</b> for testing conditions of SNR above 5 dB. This indicates that pretraining alone can increase the model&rsquo;s robustness. Finally, it is found that using noisy data for pretraining models, especially with the Data2Vec-denoising approach, significantly enhances the robustness of KWS models in noisy conditions.</p></p class="citation"></blockquote><h3 id=23--224318-a-diffusion-based-generative-equalizer-for-music-restoration-eloi-moliner-et-al-2024>(2/3 | 224/318) A Diffusion-Based Generative Equalizer for Music Restoration (Eloi Moliner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eloi Moliner, Maija Turunen, Filip Elvander, Vesa Välimäki. (2024)<br><strong>A Diffusion-Based Generative Equalizer for Music Restoration</strong><br><button class=copy-to-clipboard title="A Diffusion-Based Generative Equalizer for Music Restoration" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18636v1.pdf filename=2403.18636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach to audio restoration, focusing on the enhancement of low-quality music recordings, and in particular historical ones. Building upon a previous algorithm called BABE, or Blind Audio Bandwidth Extension, we introduce BABE-2, which presents a series of significant improvements. This research broadens the concept of bandwidth extension to \emph{generative equalization}, a novel task that, to the best of our knowledge, has not been explicitly addressed in previous studies. BABE-2 is built around an optimization algorithm utilizing priors from <b>diffusion</b> <b>models,</b> which are trained or <b>fine-tuned</b> using a curated set of high-quality music tracks. The algorithm simultaneously performs two critical tasks: estimation of the filter degradation magnitude response and hallucination of the restored audio. The proposed method is objectively evaluated on historical piano recordings, showing a marked enhancement over the prior version. The method yields similarly impressive results in rejuvenating the works of renowned vocalists Enrico Caruso and Nellie Melba. This research represents an advancement in the practical restoration of historical music.</p></p class="citation"></blockquote><h3 id=33--225318-dual-path-mamba-short-and-long-term-bidirectional-selective-structured-state-space-models-for-speech-separation-xilin-jiang-et-al-2024>(3/3 | 225/318) Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation (Xilin Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xilin Jiang, Cong Han, Nima Mesgarani. (2024)<br><strong>Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation</strong><br><button class=copy-to-clipboard title="Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18257v1.pdf filename=2403.18257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have been the most successful architecture for various speech modeling tasks, including speech separation. However, the <b>self-attention</b> mechanism in <b>transformers</b> with quadratic complexity is inefficient in computation and memory. Recent models incorporate new layers and modules along with <b>transformers</b> for better performance but also introduce extra model complexity. In this work, we replace <b>transformers</b> with Mamba, a selective state space model, for speech separation. We propose dual-path Mamba, which models short-term and long-term forward and backward dependency of speech signals using selective state spaces. Our experimental results on the WSJ0-2mix data show that our dual-path Mamba models match or outperform dual-path <b>transformer</b> models Sepformer with only 60% of its parameters, and the QDPN with only 30% of its parameters. Our large model also reaches a new state-of-the-art SI-SNRi of 24.4 dB.</p></p class="citation"></blockquote><h2 id=csai-14>cs.AI (14)</h2><h3 id=114--226318-lc-llm-explainable-lane-change-intention-and-trajectory-predictions-with-large-language-models-mingxing-peng-et-al-2024>(1/14 | 226/318) LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models (Mingxing Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu, Kehua Chen, Hao, Yang, Xuesong Wang, Yinhai Wang. (2024)<br><strong>LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models</strong><br><button class=copy-to-clipboard title="LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 70<br>Keywords: Fine-tuning, Supervised Learning, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18344v1.pdf filename=2403.18344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict the lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong <b>reasoning</b> capabilities and self-explanation abilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information in natural language as <b>prompts</b> for input into the <b>LLM</b> and employing a <b>supervised</b> <b>fine-tuning</b> technique to tailor the <b>LLM</b> specifically for our lane change prediction task. This allows us to utilize the <b>LLM&rsquo;s</b> powerful common sense <b>reasoning</b> abilities to understand complex interactive information, thereby improving the accuracy of long-term predictions. Furthermore, we incorporate explanatory requirements into the <b>prompts</b> in the inference stage. Therefore, our LC-LLM model not only can predict lane change intentions and trajectories but also provides explanations for its predictions, enhancing the interpretability. Extensive experiments on the <b>large-scale</b> <b>highD</b> <b>dataset</b> demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize <b>LLMs</b> for predicting lane change behavior. Our study shows that <b>LLMs</b> can encode comprehensive interaction information for driving behavior understanding.</p></p class="citation"></blockquote><h3 id=214--227318-boosting-conversational-question-answering-with-fine-grained-retrieval-augmentation-and-self-check-linhao-ye-et-al-2024>(2/14 | 227/318) Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check (Linhao Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He. (2024)<br><strong>Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check</strong><br><button class=copy-to-clipboard title="Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 70<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Large Language Model, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18243v1.pdf filename=2403.18243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> aims to generate more reliable and accurate responses, by augmenting <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with the external vast and dynamic knowledge. Most previous work focuses on using <b>RAG</b> for single-round <b>question</b> <b>answering,</b> while how to adapt <b>RAG</b> to the complex conversational setting wherein the <b>question</b> <b>is</b> interdependent on the preceding context is not well studied. In this paper, we propose a conversation-level <b>RAG</b> approach, which incorporates fine-grained <b>retrieval</b> <b>augmentation</b> <b>and</b> self-check for conversational <b>question</b> <b>answering</b> (CQA). In particular, our approach consists of three components, namely conversational <b>question</b> <b>refiner,</b> fine-grained retriever and self-check based response generator, which work collaboratively for <b>question</b> <b>understanding</b> and relevant information acquisition in conversational settings. Extensive experiments demonstrate the great advantages of our approach over the state-of-the-art baselines. Moreover, we also release a Chinese CQA dataset with new features including reformulated <b>question,</b> <b>extracted</b> keyword, retrieved paragraphs and their helpfulness, which facilitates further researches in <b>RAG</b> enhanced CQA.</p></p class="citation"></blockquote><h3 id=314--228318-large-language-models-need-consultants-for-reasoning-becoming-an-expert-in-a-complex-human-system-through-behavior-simulation-chuwen-wang-et-al-2024>(3/14 | 228/318) Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation (Chuwen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuwen Wang, Shirong Zeng, Cheng Wang. (2024)<br><strong>Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation</strong><br><button class=copy-to-clipboard title="Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18230v1.pdf filename=2403.18230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> in conjunction with various <b>reasoning</b> reinforcement methodologies, have demonstrated remarkable capabilities comparable to humans in fields such as mathematics, law, coding, common sense, and world knowledge. In this paper, we delve into the <b>reasoning</b> abilities of <b>LLMs</b> within complex human systems. We propose a novel <b>reasoning</b> framework, termed <code>Mosaic Expert Observation Wall'' (MEOW) exploiting generative-agents-based &lt;b>simulation&lt;/b> technique. In the MEOW framework, simulated data are utilized to train an expert model concentrating </code>experience&rsquo;&rsquo; about a specific task in each independent time of <b>simulation.</b> It is the accumulated ``experience&rsquo;&rsquo; through the <b>simulation</b> that makes for an expert on a task in a complex human system. We conduct the experiments within a communication game that mirrors real-world security scenarios. The results indicate that our proposed methodology can cooperate with existing methodologies to enhance the <b>reasoning</b> abilities of <b>LLMs</b> in complex human systems.</p></p class="citation"></blockquote><h3 id=414--229318-leveraging-large-language-models-for-relevance-judgments-in-legal-case-retrieval-shengjie-ma-et-al-2024>(4/14 | 229/318) Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval (Shengjie Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengjie Ma, Chong Chen, Qi Chu, Jiaxin Mao. (2024)<br><strong>Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Few-shot, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18405v1.pdf filename=2403.18405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task. Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridical judgments. With the advent of advanced <b>large</b> <b>language</b> <b>models,</b> some recent studies have suggested that it is promising to use <b>LLMs</b> for relevance judgment. Nonetheless, the method of employing a general <b>large</b> <b>language</b> <b>model</b> for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored. To fill this research gap, we devise a novel <b>few-shot</b> workflow tailored to the relevant judgment of legal cases. The proposed workflow breaks down the annotation process into a series of stages, imitating the process employed by human annotators and enabling a flexible integration of expert <b>reasoning</b> to enhance the accuracy of relevance judgments. By comparing the relevance judgments of <b>LLMs</b> and human experts, we empirically show that we can obtain reliable relevance judgments with the proposed workflow. Furthermore, we demonstrate the capacity to augment existing legal case retrieval models through the synthesis of data generated by the <b>large</b> <b>language</b> <b>model.</b></p></p class="citation"></blockquote><h3 id=514--230318-malbert-is-a-compact-multilingual-bert-model-still-worth-it-christophe-servan-et-al-2024>(5/14 | 230/318) mALBERT: Is a Compact Multilingual BERT Model Still Worth It? (Christophe Servan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christophe Servan, Sahar Ghannay, Sophie Rosset. (2024)<br><strong>mALBERT: Is a Compact Multilingual BERT Model Still Worth It?</strong><br><button class=copy-to-clipboard title="mALBERT: Is a Compact Multilingual BERT Model Still Worth It?" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: BERT, Natural Language Understanding, Question Answering, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18338v1.pdf filename=2403.18338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Within the current trend of Pretained Language Models <b>(PLM),</b> emerge more and more criticisms about the ethical andecological impact of such models. In this article, considering these critical remarks, we propose to focus on smallermodels, such as compact models like ALBERT, which are more ecologically virtuous than these <b>PLM.</b> However,PLMs enable huge breakthroughs in <b>Natural</b> <b>Language</b> <b>Processing</b> tasks, such as Spoken and <b>Natural</b> <b>LanguageUnderstanding,</b> <b>classification,</b> <b>Question&ndash;Answering</b> <b>tasks.</b> <b>PLMs</b> also have the advantage of being multilingual, and,as far as we know, a multilingual version of compact ALBERT models does not exist. Considering these facts, wepropose the free release of the first version of a multilingual compact ALBERT model, pre-trained using Wikipediadata, which complies with the ethical aspect of such a language model. We also evaluate the model against classicalmultilingual <b>PLMs</b> in classical NLP tasks. Finally, this paper proposes a rare study on the subword tokenizationimpact on language performances.</p></p class="citation"></blockquote><h3 id=614--231318-probabilistic-model-checking-of-stochastic-reinforcement-learning-policies-dennis-gross-et-al-2024>(6/14 | 231/318) Probabilistic Model Checking of Stochastic Reinforcement Learning Policies (Dennis Gross et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Gross, Helge Spieker. (2024)<br><strong>Probabilistic Model Checking of Stochastic Reinforcement Learning Policies</strong><br><button class=copy-to-clipboard title="Probabilistic Model Checking of Stochastic Reinforcement Learning Policies" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 33<br>Keywords: Benchmarking, Markov Decision Process, Probabilistic Model, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18725v1.pdf filename=2403.18725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a method to verify stochastic <b>reinforcement</b> <b>learning</b> (RL) policies. This approach is compatible with any RL algorithm as long as the algorithm and its corresponding environment collectively adhere to the <b>Markov</b> <b>property.</b> <b>In</b> this setting, the future state of the environment should depend solely on its current state and the action executed, independent of any previous states or actions. Our method integrates a verification technique, referred to as model checking, with RL, leveraging a <b>Markov</b> <b>decision</b> <b>process,</b> a trained RL policy, and a <b>probabilistic</b> <b>computation</b> tree logic (PCTL) formula to build a formal model that can be subsequently verified via the model checker Storm. We demonstrate our method&rsquo;s applicability across multiple <b>benchmarks,</b> comparing it to baseline methods called deterministic safety estimates and naive monolithic model checking. Our results show that our method is suited to verify stochastic RL policies.</p></p class="citation"></blockquote><h3 id=714--232318-neural-architecture-search-for-sentence-classification-with-bert-philip-kenneweg-et-al-2024>(7/14 | 232/318) Neural Architecture Search for Sentence Classification with BERT (Philip Kenneweg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Kenneweg, Sarah Schröder, Barbara Hammer. (2024)<br><strong>Neural Architecture Search for Sentence Classification with BERT</strong><br><button class=copy-to-clipboard title="Neural Architecture Search for Sentence Classification with BERT" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, BERT, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18547v1.pdf filename=2403.18547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre training of language models on large text corpora is common practice in Natural Language Processing. Following, fine tuning of these models is performed to achieve the best results on a variety of tasks. In this paper we question the common practice of only adding a single output layer as a classification head on top of the network. We perform an AutoML search to find architectures that outperform the current single layer at only a small compute cost. We validate our classification architecture on a variety of NLP <b>benchmarks</b> from the <b>GLUE</b> dataset.</p></p class="citation"></blockquote><h3 id=814--233318-identification-and-uses-of-deep-learning-backbones-via-pattern-mining-michael-livanos-et-al-2024>(8/14 | 233/318) Identification and Uses of Deep Learning Backbones via Pattern Mining (Michael Livanos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Livanos, Ian Davidson. (2024)<br><strong>Identification and Uses of Deep Learning Backbones via Pattern Mining</strong><br><button class=copy-to-clipboard title="Identification and Uses of Deep Learning Backbones via Pattern Mining" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 25<br>Keywords: MNIST, Black Box, Heuristic Approach<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18278v1.pdf filename=2403.18278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning is extensively used in many areas of data mining as a <b>black-box</b> <b>method</b> with impressive results. However, understanding the core mechanism of how deep learning makes predictions is a relatively understudied problem. Here we explore the notion of identifying a backbone of deep learning for a given group of instances. A group here can be instances of the same class or even misclassified instances of the same class. We view each instance for a given group as activating a subset of neurons and attempt to find a subgraph of neurons associated with a given concept/group. We formulate this problem as a set cover style problem and show it is intractable and presents a highly constrained integer linear programming (ILP) formulation. As an alternative, we explore a coverage-based <b>heuristic</b> <b>approach</b> related to pattern mining, and show it converges to a Pareto equilibrium point of the ILP formulation. Experimentally we explore these backbones to identify mistakes and improve performance, explanation, and visualization. We demonstrate application-based results using several challenging data sets, including Bird Audio Detection (BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic <b>MNIST</b> data.</p></p class="citation"></blockquote><h3 id=914--234318-a-path-towards-legal-autonomy-an-interoperable-and-explainable-approach-to-extracting-transforming-loading-and-computing-legal-information-using-large-language-models-expert-systems-and-bayesian-networks-axel-constant-et-al-2024>(9/14 | 234/318) A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks (Axel Constant et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Axel Constant, Hannes Westermann, Bryan Wilson, Alex Kiefer, Ines Hipolito, Sylvain Pronovost, Steven Swanson, Mahault Albarracin, Maxwell J. D. Ramstead. (2024)<br><strong>A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks</strong><br><button class=copy-to-clipboard title="A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs-LO, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18537v1.pdf filename=2403.18537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Legal autonomy - the lawful activity of artificial intelligence agents - can be achieved in one of two ways. It can be achieved either by imposing constraints on AI actors such as developers, deployers and users, and on AI resources such as data, or by imposing constraints on the range and scope of the impact that AI agents can have on the environment. The latter approach involves encoding extant rules concerning AI driven devices into the software of AI agents controlling those devices (e.g., encoding rules about limitations on zones of operations into the agent software of an autonomous drone device). This is a challenge since the effectivity of such an approach requires a method of extracting, loading, transforming and computing legal information that would be both explainable and legally interoperable, and that would enable AI agents to reason about the law. In this paper, we sketch a proof of principle for such a method using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> expert legal systems known as legal decision paths, and Bayesian networks. We then show how the proposed method could be applied to extant regulation in matters of autonomous cars, such as the California Vehicle Code.</p></p class="citation"></blockquote><h3 id=1014--235318-leveraging-large-language-models-for-fuzzy-string-matching-in-political-science-yu-wang-2024>(10/14 | 235/318) Leveraging Large Language Models for Fuzzy String Matching in Political Science (Yu Wang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Wang. (2024)<br><strong>Leveraging Large Language Models for Fuzzy String Matching in Political Science</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Models for Fuzzy String Matching in Political Science" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18218v1.pdf filename=2403.18218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fuzzy string matching remains a key issue when political scientists combine data from different sources. Existing matching methods invariably rely on string distances, such as Levenshtein distance and cosine similarity. As such, they are inherently incapable of matching strings that refer to the same entity with different names such as &lsquo;&lsquo;JP Morgan&rsquo;&rsquo; and &lsquo;&lsquo;Chase Bank&rsquo;&rsquo;, &lsquo;&lsquo;DPRK&rsquo;&rsquo; and &lsquo;&lsquo;North Korea&rsquo;&rsquo;, &lsquo;&lsquo;Chuck Fleischmann (R)&rsquo;&rsquo; and &lsquo;&lsquo;Charles Fleischmann (R)&rsquo;&rsquo;. In this letter, we propose to use <b>large</b> <b>language</b> <b>models</b> to entirely sidestep this problem in an easy and intuitive manner. Extensive experiments show that our proposed methods can improve the state of the art by as much as 39% in terms of average precision while being substantially easier and more intuitive to use by political scientists. Moreover, our results are robust against various temperatures. We further note that enhanced <b>prompting</b> can lead to additional performance improvements.</p></p class="citation"></blockquote><h3 id=1114--236318-exploring-the-privacy-protection-capabilities-of-chinese-large-language-models-yuqi-yang-et-al-2024>(11/14 | 236/318) Exploring the Privacy Protection Capabilities of Chinese Large Language Models (Yuqi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Yang, Xiaowen Huang, Jitao Sang. (2024)<br><strong>Exploring the Privacy Protection Capabilities of Chinese Large Language Models</strong><br><button class=copy-to-clipboard title="Exploring the Privacy Protection Capabilities of Chinese Large Language Models" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18205v1.pdf filename=2403.18205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> renowned for their impressive capabilities in various tasks, have significantly advanced artificial intelligence. Yet, these advancements have raised growing concerns about privacy and security implications. To address these issues and explain the risks inherent in these models, we have devised a three-tiered progressive framework tailored for evaluating privacy in language systems. This framework consists of progressively complex and in-depth privacy test tasks at each tier. Our primary objective is to comprehensively evaluate the sensitivity of <b>large</b> <b>language</b> <b>models</b> to private information, examining how effectively they discern, manage, and safeguard sensitive data in diverse scenarios. This systematic evaluation helps us understand the degree to which these models comply with privacy protection guidelines and the effectiveness of their inherent safeguards against privacy breaches. Our observations indicate that existing Chinese <b>large</b> <b>language</b> <b>models</b> universally show privacy protection shortcomings. It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models.</p></p class="citation"></blockquote><h3 id=1214--237318-enhancing-manufacturing-quality-prediction-models-through-the-integration-of-explainability-methods-dennis-gross-et-al-2024>(12/14 | 237/318) Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods (Dennis Gross et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch. (2024)<br><strong>Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods</strong><br><button class=copy-to-clipboard title="Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs-CY, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18731v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18731v1.pdf filename=2403.18731v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research presents a method that utilizes explainability techniques to amplify the performance of machine learning (ML) models in forecasting the quality of milling processes, as demonstrated in this paper through a manufacturing use case. The methodology entails the initial training of ML models, followed by a <b>fine-tuning</b> phase where irrelevant features identified through explainability methods are eliminated. This procedural refinement results in performance enhancements, paving the way for potential reductions in manufacturing costs and a better understanding of the trained ML models. This study highlights the usefulness of explainability techniques in both explaining and optimizing predictive models in the manufacturing realm.</p></p class="citation"></blockquote><h3 id=1314--238318-ftbc-forward-temporal-bias-correction-for-optimizing-ann-snn-conversion-xiaofeng-wu-et-al-2024>(13/14 | 238/318) FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion (Xiaofeng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaofeng Wu, Velibor Bojkovic, Bin Gu, Kun Suo, Kai Zou. (2024)<br><strong>FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion</strong><br><button class=copy-to-clipboard title="FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs.AI<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18388v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18388v1.pdf filename=2403.18388v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient computing compared with Artificial Neural Networks (ANNs), closely mirroring biological neural processes. However, this potential comes with inherent challenges in directly training SNNs through spatio-temporal backpropagation &ndash; <b>stemming</b> from the temporal dynamics of spiking neurons and their discrete signal processing &ndash; which necessitates alternative ways of training, most notably through ANN-SNN conversion. In this work, we introduce a lightweight Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing conversion accuracy without the computational overhead. We ground our method on provided theoretical findings that through proper temporal bias calibration the expected error of ANN-SNN conversion can be reduced to be zero after each time step. We further propose a heuristic algorithm for finding the temporal bias only in the forward pass, thus eliminating the computational burden of backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet datasets, achieving a notable increase in accuracy on all datasets. Codes are released at a GitHub repository.</p></p class="citation"></blockquote><h3 id=1414--239318-endtoendml-an-open-source-end-to-end-pipeline-for-machine-learning-applications-nisha-pillai-et-al-2024>(14/14 | 239/318) EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications (Nisha Pillai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nisha Pillai, Athish Ram Das, Moses Ayoola, Ganga Gireesan, Bindu Nanduri, Mahalingam Ramkumar. (2024)<br><strong>EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications</strong><br><button class=copy-to-clipboard title="EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 6<br>Keywords: Clustering, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18203v1.pdf filename=2403.18203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence (AI) techniques are widely applied in the life sciences. However, applying innovative AI techniques to understand and deconvolute biological complexity is hindered by the learning curve for life science scientists to understand and use computing languages. An open-source, user-friendly interface for AI models, that does not require programming skills to analyze complex biological data will be extremely valuable to the bioinformatics community. With easy access to different sequencing technologies and increased interest in different &lsquo;omics&rsquo; studies, the number of biological datasets being generated has increased and analyzing these high-throughput datasets is computationally demanding. The majority of AI libraries today require advanced programming skills as well as machine learning, data preprocessing, and visualization skills. In this research, we propose a web-based end-to-end pipeline that is capable of preprocessing, training, evaluating, and visualizing machine learning (ML) models without manual intervention or coding expertise. By integrating traditional machine learning and deep neural network models with visualizations, our library assists in recognizing, classifying, <b>clustering,</b> and predicting a wide range of <b>multi-modal,</b> multi-sensor datasets, including images, languages, and one-dimensional numerical data, for drug discovery, pathogen classification, and medical diagnostics.</p></p class="citation"></blockquote><h2 id=cscr-11>cs.CR (11)</h2><h3 id=111--240318-cpr-retrieval-augmented-generation-for-copyright-protection-aditya-golatkar-et-al-2024>(1/11 | 240/318) CPR: Retrieval Augmented Generation for Copyright Protection (Aditya Golatkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Golatkar, Alessandro Achille, Luca Zancato, Yu-Xiang Wang, Ashwin Swaminathan, Stefano Soatto. (2024)<br><strong>CPR: Retrieval Augmented Generation for Copyright Protection</strong><br><button class=copy-to-clipboard title="CPR: Retrieval Augmented Generation for Copyright Protection" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-CV, cs.CR<br>Keyword Score: 63<br>Keywords: Diffusion Model, Benchmarking, Machine Unlearning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18920v1.pdf filename=2403.18920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> is emerging as a flexible and robust technique to adapt models to private users data without training, to handle credit attribution, and to allow efficient <b>machine</b> <b>unlearning</b> at scale. However, <b>RAG</b> techniques for image generation may lead to parts of the retrieved samples being copied in the model&rsquo;s output. To reduce risks of leaking private information contained in the retrieved set, we introduce Copy-Protected generation with <b>Retrieval</b> <b>(CPR),</b> <b>a</b> new method for <b>RAG</b> with strong copyright protection guarantees in a mixed-private setting for <b>diffusion</b> <b>models.CPR</b> allows to condition the output of <b>diffusion</b> <b>models</b> on a set of retrieved images, while also guaranteeing that unique identifiable information about those example is not exposed in the generated outputs. In particular, it does so by sampling from a mixture of public (safe) distribution and private (user) distribution by merging their <b>diffusion</b> <b>scores</b> at inference. We prove that CPR satisfies Near Access Freeness (NAF) which bounds the amount of information an attacker may be able to extract from the generated images. We provide two algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously proposed rejection-sampling-based NAF methods, our methods enable efficient copyright-protected sampling with a single run of backward <b>diffusion.</b> <b>We</b> show that our method can be applied to any pre-trained conditional <b>diffusion</b> <b>model,</b> such as Stable <b>Diffusion</b> <b>or</b> unCLIP. In particular, we empirically show that applying CPR on top of unCLIP improves quality and <b>text-to-image</b> alignment of the generated results (81.4 to 83.17 on TIFA <b>benchmark),</b> while enabling credit attribution, copy-right protection, and deterministic, constant time, unlearning.</p></p class="citation"></blockquote><h3 id=211--241318-foc-figure-out-the-cryptographic-functions-in-stripped-binaries-with-llms-guoqiang-chen-et-al-2024>(2/11 | 241/318) FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs (Guoqiang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoqiang Chen, Xiuwei Shang, Shaoyin Cheng, Yanming Zhang, Weiming Zhang, Nenghai Yu. (2024)<br><strong>FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs</strong><br><button class=copy-to-clipboard title="FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: ChatGPT, Large Language Model, Large Language Model, Rouge, Rouge-L, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18403v1.pdf filename=2403.18403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task. Cryptographic algorithms exhibit greater logical complexity compared to typical code, yet their analysis is unavoidable in areas such as virus analysis and legacy code inspection. Existing methods often rely on data or structural pattern matching, leading to suboptimal generalizability and suffering from manual work. In this paper, we propose a novel framework called FoC to Figure out the Cryptographic functions in stripped binaries. In FoC, we first build a binary <b>large</b> <b>language</b> <b>model</b> (FoCBinLLM) to <b>summarize</b> the semantics of cryptographic functions in natural language. The prediction of FoC-BinLLM is insensitive to minor changes, such as vulnerability patches. To mitigate it, we further build a binary code similarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive representations and use it to retrieve similar implementations of unknown cryptographic functions in a database. In addition, we construct a cryptographic binary dataset for evaluation and to facilitate further research in this domain. And an automated method is devised to create semantic labels for extensive binary functions. Evaluation results demonstrate that FoC-BinLLM outperforms <b>ChatGPT</b> by 14.61% on the <b>ROUGE-L</b> score. FoC-Sim outperforms the previous best methods with a 52% higher Recall@1. Furthermore, our method also shows practical ability in virus analysis and 1-day vulnerability detection.</p></p class="citation"></blockquote><h3 id=311--242318-misguide--defense-against-data-free-deep-learning-model-extraction-mahendra-gurve-et-al-2024>(3/11 | 242/318) MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction (Mahendra Gurve et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahendra Gurve, Sankar Behera, Satyadev Ahlawat, Yamuna Prasad. (2024)<br><strong>MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction</strong><br><button class=copy-to-clipboard title="MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: Under Review, cs-CR, cs.CR<br>Keyword Score: 48<br>Keywords: Vision Transformer, Benchmarking, Black Box, Transformer, Vision Transformer, Model Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18580v1.pdf filename=2403.18580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of Machine Learning as a Service (MLaaS) has led to the widespread deployment of machine learning <b>models</b> <b>trained</b> on diverse datasets. These <b>models</b> <b>are</b> employed for predictive services through APIs, raising concerns about the security and confidentiality of the <b>models</b> <b>due</b> to emerging vulnerabilities in prediction APIs. Of particular concern are <b>model</b> <b>cloning</b> attacks, where individuals with limited data and no knowledge of the training dataset manage to replicate a victim <b>model&rsquo;s</b> <b>functionality</b> through <b>black-box</b> <b>query</b> access. This commonly entails generating adversarial queries to query the victim <b>model,</b> <b>thereby</b> creating a labeled dataset. This paper proposes &ldquo;MisGUIDE&rdquo;, a two-step defense framework for Deep Learning <b>models</b> <b>that</b> disrupts the adversarial sample generation process by providing a probabilistic response when the query is deemed OOD. The first step employs a <b>Vision</b> <b>Transformer-based</b> framework to identify OOD queries, while the second step perturbs the response for such queries, introducing a probabilistic loss function to MisGUIDE the attackers. The aim of the proposed defense method is to reduce the accuracy of the cloned <b>model</b> <b>while</b> maintaining accuracy on authentic queries. Extensive experiments conducted on two <b>benchmark</b> datasets demonstrate that the proposed framework significantly enhances the resistance against state-of-the-art data-free <b>model</b> <b>extraction</b> in <b>black-box</b> <b>settings.</b></p></p class="citation"></blockquote><h3 id=411--243318-bayesian-learned-models-can-detect-adversarial-malware-for-free-bao-gia-doan-et-al-2024>(4/11 | 243/318) Bayesian Learned Models Can Detect Adversarial Malware For Free (Bao Gia Doan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bao Gia Doan, Dang Quang Nguyen, Paul Montague, Tamas Abraham, Olivier De Vel, Seyit Camtepe, Salil S. Kanhere, Ehsan Abbasnejad, Damith C. Ranasinghe. (2024)<br><strong>Bayesian Learned Models Can Detect Adversarial Malware For Free</strong><br><button class=copy-to-clipboard title="Bayesian Learned Models Can Detect Adversarial Malware For Free" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Mutual Information, Prompt, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18309v1.pdf filename=2403.18309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The vulnerability of machine learning-based malware detectors to <b>adversarial</b> <b>attacks</b> has <b>prompted</b> the need for robust solutions. <b>Adversarial</b> <b>training</b> is an effective method but is computationally expensive to scale up to large datasets and comes at the cost of sacrificing model performance for robustness. We hypothesize that <b>adversarial</b> <b>malware</b> exploits the low-confidence regions of models and can be identified using epistemic uncertainty of ML approaches &ndash; epistemic uncertainty in a machine learning-based malware detector is a result of a lack of similar training samples in regions of the problem space. In particular, a Bayesian formulation can capture the model parameters&rsquo; distribution and quantify epistemic uncertainty without sacrificing model performance. To verify our hypothesis, we consider Bayesian learning approaches with a <b>mutual</b> <b>information-based</b> formulation to quantify uncertainty and detect <b>adversarial</b> <b>malware</b> in Android, Windows domains and PDF malware. We found, quantifying uncertainty through Bayesian learning methods can defend against <b>adversarial</b> <b>malware.</b> In particular, Bayesian models: (1) are generally capable of identifying <b>adversarial</b> <b>malware</b> in both feature and problem space, (2) can detect concept drift by measuring uncertainty, and (3) with a diversity-promoting approach (or better posterior approximations) lead to parameter instances from the posterior to significantly enhance a detectors&rsquo; ability.</p></p class="citation"></blockquote><h3 id=511--244318-privacy-preserving-distributed-nonnegative-matrix-factorization-ehsan-lari-et-al-2024>(5/11 | 244/318) Privacy-Preserving Distributed Nonnegative Matrix Factorization (Ehsan Lari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Lari, Reza Arablouei, Stefan Werner. (2024)<br><strong>Privacy-Preserving Distributed Nonnegative Matrix Factorization</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Distributed Nonnegative Matrix Factorization" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DC, cs-LG, cs.CR, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18326v1.pdf filename=2403.18326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nonnegative matrix factorization (NMF) is an effective data representation tool with numerous applications in signal processing and machine learning. However, deploying NMF in a decentralized manner over ad-hoc networks introduces privacy concerns due to the conventional approach of sharing raw data among network agents. To address this, we propose a privacy-preserving algorithm for fully-distributed NMF that decomposes a distributed large data matrix into left and right matrix factors while safeguarding each agent&rsquo;s local data privacy. It facilitates collaborative estimation of the left matrix factor among agents and enables them to estimate their respective right factors without exposing raw data. To ensure data privacy, we secure information exchanges between neighboring agents utilizing the Paillier cryptosystem, a probabilistic asymmetric algorithm for public-key cryptography that allows computations on encrypted data without decryption. <b>Simulation</b> results conducted on synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc networks.</p></p class="citation"></blockquote><h3 id=611--245318-a-transformer-based-framework-for-payload-malware-detection-and-classification-kyle-stein-et-al-2024>(6/11 | 245/318) A Transformer-Based Framework for Payload Malware Detection and Classification (Kyle Stein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh. (2024)<br><strong>A Transformer-Based Framework for Payload Malware Detection and Classification</strong><br><button class=copy-to-clipboard title="A Transformer-Based Framework for Payload Malware Detection and Classification" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18223v1.pdf filename=2403.18223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats. IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity. Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network. In this paper, we propose a revolutionary DPI algorithm based on <b>transformers</b> adapted for the purpose of detecting malicious traffic with a classifier head. <b>Transformers</b> learn the complex content of sequence data and generalize them well to similar scenarios thanks to their <b>self-attention</b> mechanism. Our proposed method uses the raw payload bytes that represent the packet contents and is deployed as man-in-the-middle. The payload bytes are used to detect malicious packets and classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23 datasets demonstrate that our <b>transformer-based</b> model is effective in distinguishing malicious from benign traffic in the test dataset, attaining an average accuracy of 79% using binary classification and 72% on the multi-classification experiment, both using solely payload bytes.</p></p class="citation"></blockquote><h3 id=711--246318-dealing-with-imbalanced-classes-in-bot-iot-dataset-jesse-atuhurra-et-al-2024>(7/11 | 246/318) Dealing with Imbalanced Classes in Bot-IoT Dataset (Jesse Atuhurra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jesse Atuhurra, Takanori Hara, Yuanyu Zhang, Masahiro Sasabe, Shoji Kasahara. (2024)<br><strong>Dealing with Imbalanced Classes in Bot-IoT Dataset</strong><br><button class=copy-to-clipboard title="Dealing with Imbalanced Classes in Bot-IoT Dataset" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18989v1.pdf filename=2403.18989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapidly spreading usage of Internet of Things (IoT) devices, a network intrusion detection system (NIDS) plays an important role in detecting and protecting various types of attacks in the IoT network. To evaluate the robustness of the NIDS in the IoT network, the existing work proposed a realistic botnet dataset in the IoT network (Bot-IoT dataset) and applied it to machine learning-based <b>anomaly</b> <b>detection.</b> This dataset contains imbalanced normal and attack packets because the number of normal packets is much smaller than that of attack ones. The nature of imbalanced data may make it difficult to identify the minority class correctly. In this thesis, to address the class imbalance problem in the Bot-IoT dataset, we propose a binary classification method with synthetic minority over-sampling techniques (SMOTE). The proposed classifier aims to detect attack packets and overcome the class imbalance problem using the SMOTE algorithm. Through numerical results, we demonstrate the proposed classifier&rsquo;s fundamental characteristics and the impact of imbalanced data on its performance.</p></p class="citation"></blockquote><h3 id=811--247318-spikewhisper-temporal-spike-backdoor-attacks-on-federated-neuromorphic-learning-over-low-power-devices-hanqing-fu-et-al-2024>(8/11 | 247/318) Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices (Hanqing Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu. (2024)<br><strong>Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices</strong><br><button class=copy-to-clipboard title="Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR, eess-SP<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18607v1.pdf filename=2403.18607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>neuromorphic</b> learning (FedNL) leverages event-driven spiking neural networks and <b>federated</b> <b>learning</b> frameworks to effectively execute intelligent analysis tasks over amounts of distributed low-power devices but also perform vulnerability to poisoning attacks. The threat of backdoor attacks on traditional deep neural networks typically comes from time-invariant data. However, in FedNL, unknown threats may be hidden in time-varying spike signals. In this paper, we start to explore a novel vulnerability of FedNL-based systems with the concept of time division multiplexing, termed Spikewhisper, which allows attackers to evade detection as much as possible, as multiple malicious clients can imperceptibly poison with different triggers at different timeslices. In particular, the stealthiness of Spikewhisper is derived from the time-domain divisibility of global triggers, in which each malicious client pastes only one local trigger to a certain timeslice in the neuromorphic sample, and also the polarity and motion of each local trigger can be configured by attackers. Extensive experiments based on two different neuromorphic datasets demonstrate that the attack success rate of Spikewispher is higher than the temporally centralized attacks. Besides, it is validated that the effect of Spikewispher is sensitive to the trigger duration.</p></p class="citation"></blockquote><h3 id=911--248318-the-impact-of-uniform-inputs-on-activation-sparsity-and-energy-latency-attacks-in-computer-vision-andreas-müller-et-al-2024>(9/11 | 248/318) The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency Attacks in Computer Vision (Andreas Müller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Müller, Erwin Quiring. (2024)<br><strong>The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency Attacks in Computer Vision</strong><br><button class=copy-to-clipboard title="The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency Attacks in Computer Vision" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18587v1.pdf filename=2403.18587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Resource efficiency plays an important role for machine learning nowadays. The energy and decision latency are two critical aspects to ensure a sustainable and practical application. Unfortunately, the energy consumption and decision latency are not robust against adversaries. Researchers have recently demonstrated that attackers can compute and submit so-called sponge examples at inference time to increase the energy consumption and decision latency of neural networks. In computer vision, the proposed strategy crafts inputs with less activation sparsity which could otherwise be used to accelerate the computation. In this paper, we analyze the mechanism how these energy-latency attacks reduce activation sparsity. In particular, we find that input uniformity is a key enabler. A uniform image, that is, an image with mostly flat, uniformly colored surfaces, triggers more activations due to a specific interplay of <b>convolution,</b> batch normalization, and ReLU activation. Based on these insights, we propose two new simple, yet effective strategies for crafting sponge examples: sampling images from a probability distribution and identifying dense, yet inconspicuous inputs in natural datasets. We empirically examine our findings in a comprehensive evaluation with multiple image classification models and show that our attack achieves the same sparsity effect as prior sponge-example methods, but at a fraction of computation effort. We also show that our sponge examples transfer between different neural networks. Finally, we discuss applications of our findings for the good by improving efficiency by increasing sparsity.</p></p class="citation"></blockquote><h3 id=1011--249318-statistical-testing-of-random-number-generators-and-their-improvement-using-randomness-extraction-cameron-foreman-et-al-2024>(10/11 | 249/318) Statistical testing of random number generators and their improvement using randomness extraction (Cameron Foreman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cameron Foreman, Richie Yeung, Florian J. Curchod. (2024)<br><strong>Statistical testing of random number generators and their improvement using randomness extraction</strong><br><button class=copy-to-clipboard title="Statistical testing of random number generators and their improvement using randomness extraction" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR, quant-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18716v1.pdf filename=2403.18716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random number generators (RNGs) are notoriously hard to build and test, especially in a cryptographic setting. Although one cannot conclusively determine the quality of an RNG by testing the statistical properties of its output alone, running numerical tests is both a powerful verification tool and the only universally applicable method. In this work, we present and make available a comprehensive statistical testing environment (STE) that is based on existing statistical test suites. The STE can be parameterised to run lightweight (i.e. fast) all the way to intensive testing, which goes far beyond what is required by certification bodies. With it, we <b>benchmark</b> the statistical properties of several RNGs, comparing them against each other. We then present and implement a variety of post-processing methods, in the form of randomness extractors, which improve the RNG&rsquo;s output quality under different sets of assumptions and analyse their impact through numerical testing with the STE.</p></p class="citation"></blockquote><h3 id=1111--250318-optimizing-cyber-response-time-on-temporal-active-directory-networks-using-decoys-huy-q-ngo-et-al-2024>(11/11 | 250/318) Optimizing Cyber Response Time on Temporal Active Directory Networks Using Decoys (Huy Q. Ngo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huy Q. Ngo, Mingyu Guo, Hung Nguyen. (2024)<br><strong>Optimizing Cyber Response Time on Temporal Active Directory Networks Using Decoys</strong><br><button class=copy-to-clipboard title="Optimizing Cyber Response Time on Temporal Active Directory Networks Using Decoys" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-GT, cs-NE, cs.CR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18162v1.pdf filename=2403.18162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microsoft Active Directory (AD) is the default security management system for Window domain network. We study the problem of placing decoys in AD network to detect potential attacks. We model the problem as a Stackelberg game between an attacker and a defender on AD attack <b>graphs</b> where the defender employs a set of decoys to detect the attacker on their way to Domain Admin (DA). Contrary to previous works, we consider time-varying (temporal) attack <b>graphs.</b> We proposed a novel metric called response time, to measure the effectiveness of our decoy placement in temporal attack <b>graphs.</b> Response time is defined as the duration from the moment attackers trigger the first decoy to when they compromise the DA. Our goal is to maximize the defender&rsquo;s response time to the worst-case attack paths. We establish the NP-hard nature of the defender&rsquo;s optimization problem, leading us to develop Evolutionary Diversity Optimization (EDO) algorithms. EDO algorithms identify diverse sets of high-quality solutions for the optimization problem. Despite the polynomial nature of the fitness function, it proves experimentally slow for larger <b>graphs.</b> To enhance scalability, we proposed an algorithm that exploits the static nature of AD infrastructure in the temporal setting. Then, we introduce tailored repair operations, ensuring the convergence to better results while maintaining scalability for larger <b>graphs.</b></p></p class="citation"></blockquote><h2 id=csse-6>cs.SE (6)</h2><h3 id=16--251318-few-shot-cross-system-anomaly-trace-classification-for-microservice-based-systems-yuqing-wang-et-al-2024>(1/6 | 251/318) Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems (Yuqing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqing Wang, Mika V. Mantylä, Serge Demeyer, Mutlu Beyazit, Joanna Kisaakye, Jesse Nyyssölä. (2024)<br><strong>Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems</strong><br><button class=copy-to-clipboard title="Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-SE, cs.SE<br>Keyword Score: 60<br>Keywords: Anomaly Detection, Autoencoder, Few-shot, Few-shot Learning, Meta Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18998v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18998v2.pdf filename=2403.18998v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based <b>anomaly</b> <b>detection</b> and root cause analysis. In this paper, we propose a novel framework for <b>few-shot</b> <b>abnormal</b> trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention <b>Autoencoder</b> for constructing system-specific trace representations, which enables (2) <b>Transformer</b> Encoder-based Model-Agnostic <b>Meta-Learning</b> <b>to</b> perform effective and efficient <b>few-shot</b> <b>learning</b> for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the different MSS. Within the same MSS, our framework achieves an average accuracy of 93.26% and 85.2% across 50 <b>meta-testing</b> <b>tasks</b> for Trainticket and OnlineBoutique, respectively, when provided with 10 instances for each task. In a cross-system context, our framework gets an average accuracy of 92.19% and 84.77% for the same <b>meta-testing</b> <b>tasks</b> of the respective system, also with 10 instances provided for each task. Our work demonstrates the applicability of achieving <b>few-shot</b> <b>abnormal</b> trace classification for MSS and shows how it can enable cross-system adaptability. This opens an avenue for building more generalized AIOps tools that require less system-specific data labeling for <b>anomaly</b> <b>detection</b> and root cause analysis.</p></p class="citation"></blockquote><h3 id=26--252318-vulnerability-detection-with-code-language-models-how-far-are-we-yangruibo-ding-et-al-2024>(2/6 | 252/318) Vulnerability Detection with Code Language Models: How Far Are We? (Yangruibo Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yangruibo Ding, Yanjun Fu, Omniyyah Ibrahim, Chawin Sitawarin, Xinyun Chen, Basel Alomair, David Wagner, Baishakhi Ray, Yizheng Chen. (2024)<br><strong>Vulnerability Detection with Code Language Models: How Far Are We?</strong><br><button class=copy-to-clipboard title="Vulnerability Detection with Code Language Models: How Far Are We?" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18624v1.pdf filename=2403.18624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection. To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified <b>benchmarks</b> while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs&rsquo; performance in real-world conditions. Evaluating code LMs on PrimeVul reveals that existing <b>benchmarks</b> significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like <b>GPT-3.5</b> and <b>GPT-4</b> were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.</p></p class="citation"></blockquote><h3 id=36--253318-a-state-of-the-practice-release-readiness-checklist-for-generative-ai-based-software-products-harsh-patel-et-al-2024>(3/6 | 253/318) A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products (Harsh Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Patel, Dominique Boucher, Emad Fallahzadeh, Ahmed E. Hassan, Bram Adams. (2024)<br><strong>A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products</strong><br><button class=copy-to-clipboard title="A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Fine-tuning, Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18958v1.pdf filename=2403.18958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the complexities of integrating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into software products, with a focus on the challenges encountered for determining their readiness for release. Our systematic review of grey literature identifies common challenges in deploying <b>LLMs,</b> ranging from pre-training and <b>fine-tuning</b> to user experience considerations. The study introduces a comprehensive checklist designed to guide practitioners in evaluating key release readiness aspects such as performance, monitoring, and deployment strategies, aiming to enhance the reliability and effectiveness of <b>LLM-based</b> applications in real-world settings.</p></p class="citation"></blockquote><h3 id=46--254318-an-exploratory-study-on-upper-level-computing-students-use-of-large-language-models-as-tools-in-a-semester-long-project-ben-arie-tanay-et-al-2024>(4/6 | 254/318) An Exploratory Study on Upper-Level Computing Students&rsquo; Use of Large Language Models as Tools in a Semester-Long Project (Ben Arie Tanay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Arie Tanay, Lexy Arinze, Siddhant S. Joshi, Kirsten A. Davis, James C. Davis. (2024)<br><strong>An Exploratory Study on Upper-Level Computing Students&rsquo; Use of Large Language Models as Tools in a Semester-Long Project</strong><br><button class=copy-to-clipboard title="An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-HC, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18679v1.pdf filename=2403.18679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> such as <b>ChatGPT</b> and CoPilot are influencing software engineering practice. Software engineering educators must teach future software engineers how to use such tools well. As of yet, there have been few studies that report on the use of <b>LLMs</b> in the classroom. It is, therefore, important to evaluate students&rsquo; perception of <b>LLMs</b> and possible ways of adapting the computing curriculum to these shifting paradigms. Purpose: The purpose of this study is to explore computing students&rsquo; experiences and approaches to using <b>LLMs</b> during a semester-long software engineering project. Design/Method: We collected data from a senior-level software engineering course at Purdue University. This course uses a project-based learning (PBL) design. The students used <b>LLMs</b> such as <b>ChatGPT</b> and Copilot in their projects. A sample of these student teams were interviewed to understand (1) how they used <b>LLMs</b> in their projects; and (2) whether and how their perspectives on <b>LLMs</b> changed over the course of the semester. We analyzed the data to identify themes related to students&rsquo; usage patterns and learning outcomes. Results/Discussion: When computing students utilize <b>LLMs</b> within a project, their use cases cover both technical and professional applications. In addition, these students perceive <b>LLMs</b> to be efficient tools in obtaining information and completion of tasks. However, there were concerns about the responsible use of <b>LLMs</b> without being detrimental to their own learning outcomes. Based on our findings, we recommend future research to investigate the usage of <b>LLM&rsquo;s</b> in lower-level computer engineering courses to understand whether and how <b>LLMs</b> can be integrated as a learning aid without hurting the learning outcomes.</p></p class="citation"></blockquote><h3 id=56--255318-cycle-learning-to-self-refine-the-code-generation-yangruibo-ding-et-al-2024>(5/6 | 255/318) CYCLE: Learning to Self-Refine the Code Generation (Yangruibo Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yangruibo Ding, Marcus J. Min, Gail Kaiser, Baishakhi Ray. (2024)<br><strong>CYCLE: Learning to Self-Refine the Code Generation</strong><br><button class=copy-to-clipboard title="CYCLE: Learning to Self-Refine the Code Generation" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 13<br>Keywords: Benchmarking, Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18746v1.pdf filename=2403.18746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained <b>code</b> <b>language</b> models have achieved promising performance in <b>code</b> <b>generation</b> and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of <b>code</b> <b>LMs,</b> which focus only on the accuracy of the one-time prediction. For the cases when <b>code</b> <b>LMs</b> fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that <b>code</b> <b>LMs</b> cannot efficiently self-refine their faulty generations as well. In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular <b>code</b> <b>generation</b> <b>benchmarks,</b> HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintains, sometimes improves, the quality of one-time <b>code</b> <b>generation,</b> while significantly improving the self-refinement capability of <b>code</b> <b>LMs.</b> We implement four variants of CYCLE with varied numbers of parameters across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently boosts the <b>code</b> <b>generation</b> performance, by up to 63.5%, across <b>benchmarks</b> and varied model sizes. We also notice that CYCLE outperforms <b>code</b> <b>LMs</b> that have 3$\times$ more parameters in self-refinement.</p></p class="citation"></blockquote><h3 id=66--256318-algorithmic-details-behind-the-predator-shape-analyser-kamil-dudka-et-al-2024>(6/6 | 256/318) Algorithmic Details behind the Predator Shape Analyser (Kamil Dudka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kamil Dudka, Petr Muller, Petr Peringer, Veronika Šoková, Tomáš Vojnar. (2024)<br><strong>Algorithmic Details behind the Predator Shape Analyser</strong><br><button class=copy-to-clipboard title="Algorithmic Details behind the Predator Shape Analyser" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-PL, cs-SE, cs.SE<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18491v1.pdf filename=2403.18491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This chapter, which is an extended and revised version of the conference paper &lsquo;Predator: Byte-Precise Verification of Low-Level List Manipulation&rsquo;, concentrates on a detailed description of the algorithms behind the Predator shape analyser based on abstract interpretation and symbolic memory <b>graphs.</b> Predator is particularly suited for formal analysis and verification of sequential non-recursive C code that uses low-level pointer operations to manipulate various kinds of linked lists of unbounded size as well as various other kinds of pointer structures of bounded size. The tool supports practically relevant forms of pointer arithmetic, block operations, address alignment, or memory reinterpretation. We present the overall architecture of the tool, along with selected implementation details of the tool as well as its extension into so-called Predator Hunting Party, which utilises multiple concurrently-running Predator analysers with various restrictions on their behaviour. Results of experiments with Predator within the SV-COMP competition as well as on our own <b>benchmarks</b> are provided.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--257318-moderating-illicit-online-image-promotion-for-unsafe-user-generated-content-games-using-large-vision-language-models-keyan-guo-et-al-2024>(1/1 | 257/318) Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models (Keyan Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keyan Guo, Ayush Utkarsh, Wenbo Ding, Isabelle Ondracek, Ziming Zhao, Guo Freeman, Nishant Vishwamitra, Hongxin Hu. (2024)<br><strong>Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs-LG, cs-SI, cs.CY<br>Keyword Score: 50<br>Keywords: Zero-shot, Reasoning, Domain Adaptation, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18957v1.pdf filename=2403.18957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online user-generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment. However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents. Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users. This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content. In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs. We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators. Our in-depth studies reveal a new understanding of this problem and the urgent need for automatically flagging illicit UGCG promotions. We additionally create a cutting-edge system, UGCG-Guard, designed to aid social media platforms in effectively identifying images used for illicit UGCG promotions. This system leverages recently introduced large <b>vision-language</b> models (VLMs) and employs a novel conditional <b>prompting</b> strategy for <b>zero-shot</b> <b>domain</b> <b>adaptation,</b> along with chain-of-thought (CoT) <b>reasoning</b> for contextual identification. UGCG-Guard achieves outstanding results, with an accuracy rate of 94% in detecting these images used for the illicit promotion of such games in real-world scenarios.</p></p class="citation"></blockquote><h2 id=csdc-5>cs.DC (5)</h2><h3 id=15--258318-resource-allocation-in-large-language-model-integrated-6g-vehicular-networks-chang-liu-et-al-2024>(1/5 | 258/318) Resource Allocation in Large Language Model Integrated 6G Vehicular Networks (Chang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chang Liu, Jun Zhao. (2024)<br><strong>Resource Allocation in Large Language Model Integrated 6G Vehicular Networks</strong><br><button class=copy-to-clipboard title="Resource Allocation in Large Language Model Integrated 6G Vehicular Networks" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-SY, cs.DC, eess-SP, eess-SY, math-OC<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19016v1.pdf filename=2403.19016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the upcoming 6G era, vehicular networks are shifting from simple Vehicle-to-Vehicle (V2V) communication to the more complex Vehicle-to-Everything (V2X) connectivity. At the forefront of this shift is the incorporation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into vehicles. Known for their sophisticated natural language processing abilities, <b>LLMs</b> change how users interact with their vehicles. This integration facilitates voice-driven commands and interactions, departing from the conventional manual control systems. However, integrating <b>LLMs</b> into vehicular systems presents notable challenges. The substantial computational demands and energy requirements of <b>LLMs</b> pose significant challenges, especially in the constrained environment of a vehicle. Additionally, the time-sensitive nature of tasks in vehicular networks adds another layer of complexity. In this paper, we consider an edge computing system where vehicles process the initial layers of <b>LLM</b> computations locally, and offload the remaining <b>LLM</b> computation tasks to the Roadside Units (RSUs), envisioning a vehicular ecosystem where <b>LLM</b> computations seamlessly interact with the ultra-low latency and high-bandwidth capabilities of 6G networks. To balance the trade-off between completion time and energy consumption, we formulate a multi-objective optimization problem to minimize the total cost of the vehicles and RSUs. The problem is then decomposed into two sub-problems, which are solved by sequential quadratic programming (SQP) method and fractional programming technique. The <b>simulation</b> results clearly indicate that the algorithm we have proposed is highly effective in reducing both the completion time and energy consumption of the system.</p></p class="citation"></blockquote><h3 id=25--259318-optimizing-communication-for-latency-sensitive-hpc-applications-on-up-to-48-fpgas-using-accl-marius-meyer-et-al-2024>(2/5 | 259/318) Optimizing Communication for Latency Sensitive HPC Applications on up to 48 FPGAs Using ACCL (Marius Meyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marius Meyer, Tobias Kenter, Lucian Petrica, Kenneth O&rsquo;Brien, Michaela Blott, Christian Pessl. (2024)<br><strong>Optimizing Communication for Latency Sensitive HPC Applications on up to 48 FPGAs Using ACCL</strong><br><button class=copy-to-clipboard title="Optimizing Communication for Latency Sensitive HPC Applications on up to 48 FPGAs Using ACCL" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AR, cs-DC, cs.DC<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18374v1.pdf filename=2403.18374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most FPGA boards in the HPC domain are well-suited for parallel scaling because of the direct integration of versatile and high-throughput network ports. However, the utilization of their network capabilities is often challenging and error-prone because the whole network stack and communication patterns have to be implemented and managed on the FPGAs. Also, this approach conceptually involves a trade-off between the performance potential of improved communication and the impact of resource consumption for communication infrastructure, since the utilized resources on the FPGAs could otherwise be used for computations. In this work, we investigate this trade-off, firstly, by using synthetic <b>benchmarks</b> to evaluate the different configuration options of the communication framework ACCL and their impact on communication latency and throughput. Finally, we use our findings to implement a shallow water <b>simulation</b> whose scalability heavily depends on low-latency communication. With a suitable configuration of ACCL, good scaling behavior can be shown to all 48 FPGAs installed in the system. Overall, the results show that the availability of inter-FPGA communication frameworks as well as the configurability of framework and network stack are crucial to achieve the best application performance with low latency communication.</p></p class="citation"></blockquote><h3 id=35--260318-distributed-maximum-consensus-over-noisy-links-ehsan-lari-et-al-2024>(3/5 | 260/318) Distributed Maximum Consensus over Noisy Links (Ehsan Lari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Lari, Reza Arablouei, Naveen K. D. Venkategowda, Stefan Werner. (2024)<br><strong>Distributed Maximum Consensus over Noisy Links</strong><br><button class=copy-to-clipboard title="Distributed Maximum Consensus over Noisy Links" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18509v1.pdf filename=2403.18509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a distributed algorithm, termed noise-robust distributed maximum consensus (RD-MC), for estimating the maximum value within a multi-agent network in the presence of noisy communication links. Our approach entails redefining the maximum consensus problem as a distributed optimization problem, allowing a solution using the alternating direction method of multipliers. Unlike existing algorithms that rely on multiple sets of noise-corrupted estimates, RD-MC employs a single set, enhancing both robustness and efficiency. To further mitigate the effects of link noise and improve robustness, we apply moving averaging to the local estimates. Through extensive <b>simulations,</b> we demonstrate that RD-MC is significantly more robust to communication link noise compared to existing maximum-consensus algorithms.</p></p class="citation"></blockquote><h3 id=45--261318-optimal-resource-efficiency-with-fairness-in-heterogeneous-gpu-clusters-zizhao-mo-et-al-2024>(4/5 | 261/318) Optimal Resource Efficiency with Fairness in Heterogeneous GPU Clusters (Zizhao Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zizhao Mo, Huanle Xu, Wing Cheong Lau. (2024)<br><strong>Optimal Resource Efficiency with Fairness in Heterogeneous GPU Clusters</strong><br><button class=copy-to-clipboard title="Optimal Resource Efficiency with Fairness in Heterogeneous GPU Clusters" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18545v1.pdf filename=2403.18545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring the highest training throughput to maximize resource efficiency, while maintaining <b>fairness</b> among users, is critical for deep learning (DL) training in heterogeneous GPU clusters. However, current DL schedulers provide only limited <b>fairness</b> properties and suboptimal training throughput, impeding tenants from effectively leveraging heterogeneous resources. The underlying design challenge stems from inherent conflicts between efficiency and <b>fairness</b> properties. In this paper, we introduce OEF, a new resource allocation framework specifically developed for achieving optimal resource efficiency and ensuring diverse <b>fairness</b> properties in heterogeneous GPU clusters. By integrating resource efficiency and <b>fairness</b> within a global optimization framework, OEF is capable of providing users with maximized overall efficiency, as well as various guarantees of <b>fairness,</b> in both cooperative and non-cooperative environments. We have implemented OEF in a cluster resource manager and conducted large-scale experiments, showing that OEF can improve the overall training throughput by up to 32% while improving <b>fairness</b> compared to state-of-the-art heterogeneity-aware schedulers.</p></p class="citation"></blockquote><h3 id=55--262318-enhanced-openmp-algorithm-to-compute-all-pairs-shortest-path-on-x86-architectures-sergio-calderón-et-al-2024>(5/5 | 262/318) Enhanced OpenMP Algorithm to Compute All-Pairs Shortest Path on x86 Architectures (Sergio Calderón et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergio Calderón, Enzo Rucci, Franco Chichizola. (2024)<br><strong>Enhanced OpenMP Algorithm to Compute All-Pairs Shortest Path on x86 Architectures</strong><br><button class=copy-to-clipboard title="Enhanced OpenMP Algorithm to Compute All-Pairs Shortest Path on x86 Architectures" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18619v1.pdf filename=2403.18619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graphs</b> have become a key tool when modeling and solving problems in different areas. The Floyd-Warshall (FW) algorithm computes the shortest path between all pairs of vertices in a <b>graph</b> and is employed in areas like communication networking, traffic routing, bioinformatics, among others. However, FW is computationally and spatially expensive since it requires O(n^3) operations and O(n^2) memory space. As the <b>graph</b> gets larger, parallel computing becomes necessary to provide a solution in an acceptable time range. In this paper, we studied a FW code developed for Xeon Phi KNL processors and adapted it to run on any Intel x86 processors, losing the specificity of the former. To do so, we verified one by one the optimizations proposed by the original code, making adjustments to the base code where necessary, and analyzing its performance on two Intel servers under different test scenarios. In addition, a new optimization was proposed to increase the concurrency degree of the parallel algorithm, which was implemented using two different synchronization mechanisms. The experimental results show that all optimizations were beneficial on the two x86 platforms selected. Last, the new optimization proposal improved performance by up to 23%.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--263318-smof-streaming-modern-cnns-on-fpgas-with-smart-off-chip-eviction-petros-toupas-et-al-2024>(1/2 | 263/318) SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction (Petros Toupas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Petros Toupas, Zhewen Yu, Christos-Savvas Bouganis, Dimitrios Tzovaras. (2024)<br><strong>SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction</strong><br><button class=copy-to-clipboard title="SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-CV, cs-LG, cs.AR<br>Keyword Score: 40<br>Keywords: Yolo, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18921v1.pdf filename=2403.18921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> have demonstrated their effectiveness in numerous vision tasks. However, their high processing requirements necessitate efficient hardware acceleration to meet the application&rsquo;s performance targets. In the space of FPGAs, streaming-based dataflow architectures are often adopted by users, as significant performance gains can be achieved through layer-wise pipelining and reduced off-chip memory access by retaining data on-chip. However, modern topologies, such as the UNet, <b>YOLO,</b> and X3D models, utilise long skip connections, requiring significant on-chip storage and thus limiting the performance achieved by such system architectures. The paper addresses the above limitation by introducing weight and activation eviction mechanisms to off-chip memory along the computational pipeline, taking into account the available compute and memory resources. The proposed mechanism is incorporated into an existing toolflow, expanding the design space by utilising off-chip memory as a buffer. This enables the mapping of such modern <b>CNNs</b> to devices with limited on-chip memory, under the streaming architecture design approach. SMOF has demonstrated the capacity to deliver competitive and, in some cases, state-of-the-art performance across a spectrum of computer vision tasks, achieving up to 10.65 X throughput improvement compared to previous works.</p></p class="citation"></blockquote><h3 id=22--264318-merits-of-time-domain-computing-for-vmm----a-quantitative-comparison-florian-freye-et-al-2024>(2/2 | 264/318) Merits of Time-Domain Computing for VMM &ndash; A Quantitative Comparison (Florian Freye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Freye, Jie Lou, Christian Lanius, Tobias Gemmeke. (2024)<br><strong>Merits of Time-Domain Computing for VMM &ndash; A Quantitative Comparison</strong><br><button class=copy-to-clipboard title="Merits of Time-Domain Computing for VMM -- A Quantitative Comparison" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18367v1.pdf filename=2403.18367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vector-matrix-multiplication (VMM) accel-erators have gained a lot of traction, especially due to therise of <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and the desireto compute them on the edge. Besides the classical digitalapproach, analog computing has gone through a renais-sance to push energy efficiency further. A more recent ap-proach is called time-domain (TD) computing. In contrastto analog computing, TD computing permits easy technol-ogy as well as voltage scaling. As it has received limitedresearch attention, it is not yet clear which scenarios aremost suitable to be computed in the TD. In this work, weinvestigate these scenarios, focussing on energy efficiencyconsidering approximative computations that preserve ac-curacy. Both goals are addressed by a novel efficiency met-ric, which is used to find a baseline design. We use SPICEsimulation data which is fed into a python framework toevaluate how performance scales for VMM computation.We see that TD computing offers best energy efficiency forsmall to medium sized arrays. With throughput and sili-con footprint we investigate two additional metrics, givinga holistic comparison.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--265318-real-acoustic-fields-an-audio-visual-room-acoustics-dataset-and-benchmark-ziyang-chen-et-al-2024>(1/2 | 265/318) Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark (Ziyang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, Alexander Richard. (2024)<br><strong>Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark</strong><br><button class=copy-to-clipboard title="Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CV, cs-MM, cs-SD, cs.SD, eess-AS<br>Keyword Score: 33<br>Keywords: Benchmarking, Few-shot, Few-shot Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18821v1.pdf filename=2403.18821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new dataset called Real Acoustic Fields (RAF) that captures real acoustic room data from multiple modalities. The dataset includes high-quality and densely captured room impulse response data paired with multi-view images, and precise 6DoF pose tracking data for sound emitters and listeners in the rooms. We used this dataset to evaluate existing methods for novel-view acoustic synthesis and impulse response generation which previously relied on synthetic data. In our evaluation, we thoroughly assessed existing audio and audio-visual models against multiple criteria and proposed settings to enhance their performance on real-world data. We also conducted experiments to investigate the impact of incorporating visual data (i.e., images and depth) into neural acoustic field models. Additionally, we demonstrated the effectiveness of a simple sim2real approach, where a model is pre-trained with simulated data and <b>fine-tuned</b> with sparse real-world data, resulting in significant improvements in the <b>few-shot</b> <b>learning</b> approach. RAF is the first dataset to provide densely captured room acoustic data, making it an ideal resource for researchers working on audio and audio-visual neural acoustic field modeling techniques. Demos and datasets are available on our project page: <a href=https://facebookresearch.github.io/real-acoustic-fields/>https://facebookresearch.github.io/real-acoustic-fields/</a></p></p class="citation"></blockquote><h3 id=22--266318-aces-evaluating-automated-audio-captioning-models-on-the-semantics-of-sounds-gijs-wijngaard-et-al-2024>(2/2 | 266/318) ACES: Evaluating Automated Audio Captioning Models on the Semantics of Sounds (Gijs Wijngaard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gijs Wijngaard, Elia Formisano, Bruno L. Giordano, Michel Dumontier. (2024)<br><strong>ACES: Evaluating Automated Audio Captioning Models on the Semantics of Sounds</strong><br><button class=copy-to-clipboard title="ACES: Evaluating Automated Audio Captioning Models on the Semantics of Sounds" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18572v1.pdf filename=2403.18572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated Audio Captioning is a <b>multimodal</b> task that aims to convert audio content into natural language. The assessment of audio captioning systems is typically based on quantitative metrics applied to text data. Previous studies have employed metrics derived from <b>machine</b> <b>translation</b> and image captioning to evaluate the quality of generated audio captions. Drawing inspiration from auditory cognitive neuroscience research, we introduce a novel metric approach &ndash; Audio Captioning Evaluation on Semantics of Sound (ACES). ACES takes into account how human listeners parse semantic information from sounds, providing a novel and comprehensive evaluation perspective for automated audio captioning systems. ACES combines semantic similarities and semantic entity labeling. ACES outperforms similar automated audio captioning metrics on the Clotho-Eval FENSE <b>benchmark</b> in two evaluation categories.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=14--267318-intent-aware-drl-based-uplink-dynamic-scheduler-for-5g-nr-salwa-mostafa-et-al-2024>(1/4 | 267/318) Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR (Salwa Mostafa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis. (2024)<br><strong>Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR</strong><br><button class=copy-to-clipboard title="Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-AI, cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 33<br>Keywords: Graph, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18364v1.pdf filename=2403.18364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the problem of supporting Industrial Internet of Things user equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and random traffic arrival. A deep <b>reinforcement</b> <b>learning</b> (DRL) based centralized dynamic scheduler for time-frequency resources is proposed to learn how to schedule the available communication resources among the IIoT UEs. The proposed scheduler leverages an RL framework to adapt to the dynamic changes in the wireless communication system and traffic arrivals. Moreover, a <b>graph-based</b> reduction scheme is proposed to reduce the state and action space of the RL framework to allow fast convergence and a better learning strategy. <b>Simulation</b> results demonstrate the effectiveness of the proposed intelligent scheduler in guaranteeing the expressed intent of IIoT UEs compared to several traditional scheduling schemes, such as round-robin, semi-static, and heuristic approaches. The proposed scheduler also outperforms the contention-free and contention-based schemes in maximizing the number of successfully computed tasks.</p></p class="citation"></blockquote><h3 id=24--268318-mutual-information-optimization-for-sim-based-holographic-mimo-systems-nemanja-stefan-perović-et-al-2024>(2/4 | 268/318) Mutual Information Optimization for SIM-Based Holographic MIMO Systems (Nemanja Stefan Perović et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nemanja Stefan Perović, Le-Nam Tran. (2024)<br><strong>Mutual Information Optimization for SIM-Based Holographic MIMO Systems</strong><br><button class=copy-to-clipboard title="Mutual Information Optimization for SIM-Based Holographic MIMO Systems" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Mutual Information, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18307v1.pdf filename=2403.18307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of emerging stacked intelligent metasurface (SIM)-based holographic MIMO (HMIMO) systems, a fundamental problem is to study the <b>mutual</b> <b>information</b> (MI) between transmitted and received signals to establish their capacity. However, direct optimization or analytical evaluation of the MI, particularly for discrete signaling, is often intractable. To address this challenge, we adopt the channel cutoff rate (CR) as an alternative optimization metric for the MI maximization. In this regard, we propose an alternating projected gradient method (APGM), which optimizes the CR of a SIM-based HMIMO system by adjusting signal precoding and the phase shifts across the transmit and receive SIMs in a layer-by-layer basis. <b>Simulation</b> results indicate that the proposed algorithm significantly enhances the CR, achieving substantial gains proportional to those observed for the corresponding MI. This justifies the effectiveness of using the channel CR for the MI optimization. Moreover, we demonstrate that the integration of digital precoding, even on a modest scale, has a significant impact on the ultimate performance of SIM-aided systems.</p></p class="citation"></blockquote><h3 id=34--269318-representatividad-muestral-en-la-incertidumbre-simétrica-multivariada-para-la-selección-de-atributos-gustavo-sosa-cabrera-2024>(3/4 | 269/318) Representatividad Muestral en la Incertidumbre Simétrica Multivariada para la Selección de Atributos (Gustavo Sosa-Cabrera, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gustavo Sosa-Cabrera. (2024)<br><strong>Representatividad Muestral en la Incertidumbre Simétrica Multivariada para la Selección de Atributos</strong><br><button class=copy-to-clipboard title="Representatividad Muestral en la Incertidumbre Simétrica Multivariada para la Selección de Atributos" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT, math-ST, stat-TH<br>Keyword Score: 23<br>Keywords: Sample Size, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18685v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18685v1.pdf filename=2403.18685v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical <b>simulation</b> techniques under various mixes of informative and non-informative randomly generated features. Experiments show how the number of attributes, their cardinalities, and the <b>sample</b> <b>size</b> affect the MSU. In this thesis, through observation of results, it is proposed an heuristic condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction. &ndash; En el presente trabajo hemos analizado el comportamiento de una versi'on multivariada de la incertidumbre sim'etrica a trav'es de t'ecnicas de simulaci'on estad'isticas sobre varias combinaciones de atributos informativos y no-informativos generados de forma aleatoria. Los experimentos muestran como el n'umero de atributos, sus cardinalidades y el tama~no muestral afectan al MSU como medida. En esta tesis, mediante la observaci'on de resultados hemos propuesto una condici'on que preserva una buena calidad en el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci'on de dimensionalidad.</p></p class="citation"></blockquote><h3 id=44--270318-the-dimensions-of-the-hulls-of-conorm-codes-from-algebraic-geometry-codes-junmin-an-et-al-2024>(4/4 | 270/318) The Dimensions of the Hulls of Conorm Codes from Algebraic Geometry Codes (Junmin An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junmin An, Jon-Lark Kim. (2024)<br><strong>The Dimensions of the Hulls of Conorm Codes from Algebraic Geometry Codes</strong><br><button class=copy-to-clipboard title="The Dimensions of the Hulls of Conorm Codes from Algebraic Geometry Codes" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: 94B27, cs-IT, cs.IT, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18231v1.pdf filename=2403.18231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chara et al. introduced conorm codes defined over algebraic <b>geometry</b> codes, but the hulls of conorm codes were not determined yet. In this paper, we study the dimension of the hull of conorm codes using the method introduced by Camps et al. For an algebraic <b>geometry</b> code $\mathcal{C}:=C_\mathscr{L}(D, G)$, we consider the divisor $\gcd(G, H)$, where $H$ is the divisor satisfying [C_\mathscr{L}(D, G)^\perp=C_\mathscr{L}(D, H).] Given an extension $F&rsquo;/\mathbb{F}_{q^t}$ of an algebraic function field $F/\mathbb{F}_q$, we assume that the divisor $\gcd(G, H)$ is non-special. If the degree of $\gcd(G, H)$ is greater than $2g-2+{t\over [F&rsquo;:F]}\deg\text{Diff}(F&rsquo;/F)$, then we have determined the exact dimension of the hull of the conorm of $\mathcal{C}$. If not, we have determined the lower bound of the dimension of the hull of the conorm of $\mathcal{C}$. We provide some examples for the dimension of the hull of certain conorm codes of AG codes defined over a rational function field.</p></p class="citation"></blockquote><h2 id=eesssy-10>eess.SY (10)</h2><h3 id=110--271318-differentially-private-distributed-nonconvex-stochastic-optimization-with-quantized-communications-jialong-chen-et-al-2024>(1/10 | 271/318) Differentially Private Distributed Nonconvex Stochastic Optimization with Quantized Communications (Jialong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialong Chen, Jimin Wang, Ji-Feng Zhang. (2024)<br><strong>Differentially Private Distributed Nonconvex Stochastic Optimization with Quantized Communications</strong><br><button class=copy-to-clipboard title="Differentially Private Distributed Nonconvex Stochastic Optimization with Quantized Communications" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 33<br>Keywords: MNIST, Quantization, Sample Size, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18254v1.pdf filename=2403.18254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a new distributed nonconvex stochastic optimization algorithm that can achieve privacy protection, communication efficiency and convergence simultaneously. Specifically, each node adds time-varying privacy noises to its local state to avoid information leakage, and then <b>quantizes</b> its noise-perturbed state before transmitting to improve communication efficiency. By employing the subsampling method controlled through the <b>sample-size</b> <b>parameter,</b> the proposed algorithm reduces the impact of privacy noises, and enhances the <b>differential</b> <b>privacy</b> level. When the global cost function satisfies the Polyak-Lojasiewicz condition, the mean and high-probability convergence rate and the oracle complexity of the proposed algorithm are given. Importantly, the proposed algorithm achieves both the mean convergence and a finite cumulative <b>differential</b> <b>privacy</b> budget over infinite iterations as the <b>sample-size</b> <b>goes</b> to infinity. A numerical example of the distributed training on the <b>&ldquo;MNIST&rdquo;</b> dataset is given to show the effectiveness of the algorithm.</p></p class="citation"></blockquote><h3 id=210--272318-differentially-private-dual-gradient-tracking-for-distributed-resource-allocation-wei-huo-et-al-2024>(2/10 | 272/318) Differentially Private Dual Gradient Tracking for Distributed Resource Allocation (Wei Huo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Huo, Xiaomeng Chen, Lingying Huang, Karl Henrik Johansson, Ling Shi. (2024)<br><strong>Differentially Private Dual Gradient Tracking for Distributed Resource Allocation</strong><br><button class=copy-to-clipboard title="Differentially Private Dual Gradient Tracking for Distributed Resource Allocation" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18275v1.pdf filename=2403.18275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates privacy issues in distributed resource allocation over directed networks, where each agent holds a private cost function and optimizes its decision subject to a global coupling constraint through local interaction with other agents. Conventional methods for resource allocation over directed networks require all agents to transmit their original data to neighbors, which poses the risk of disclosing sensitive and private information. To address this issue, we propose an algorithm called differentially private dual gradient tracking (DP-DGT) for distributed resource allocation, which obfuscates the exchanged messages using independent Laplacian noise. Our algorithm ensures that the agents&rsquo; decisions converge to a neighborhood of the optimal solution almost surely. Furthermore, without the assumption of bounded gradients, we prove that the cumulative <b>differential</b> <b>privacy</b> loss under the proposed algorithm is finite even when the number of iterations goes to infinity. To the best of our knowledge, we are the first to simultaneously achieve these two goals in distributed resource allocation problems over directed networks. Finally, numerical <b>simulations</b> on economic dispatch problems within the IEEE 14-bus system illustrate the effectiveness of our proposed algorithm.</p></p class="citation"></blockquote><h3 id=310--273318-nonlinear-model-predictive-control-for-enhanced-navigation-of-autonomous-surface-vessels-daniel-menges-et-al-2024>(3/10 | 273/318) Nonlinear Model Predictive Control for Enhanced Navigation of Autonomous Surface Vessels (Daniel Menges et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Menges, Trym Tengesdal, Adil Rasheed. (2024)<br><strong>Nonlinear Model Predictive Control for Enhanced Navigation of Autonomous Surface Vessels</strong><br><button class=copy-to-clipboard title="Nonlinear Model Predictive Control for Enhanced Navigation of Autonomous Surface Vessels" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19028v1.pdf filename=2403.19028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article proposes an approach for collision avoidance, path following, and anti-grounding of autonomous surface vessels under consideration of environmental forces based on Nonlinear Model Predictive Control (NMPC). Artificial Potential Fields (APFs) set the foundation for the cost function of the optimal control problem in terms of collision avoidance and anti-grounding. Depending on the risk of a collision given by the resulting force of the APFs, the controller optimizes regarding an adapted heading and travel speed by additionally following a desired path. For this purpose, nonlinear vessel dynamics are used for the NMPC. To extend the situational awareness concerning environmental disturbances impacted by wind, waves, and sea currents, a nonlinear disturbance observer is coupled to the entire NMPC scheme, allowing for the correction of an incorrect vessel motion due to external forces. In addition, the most essential rules according to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) are considered. The results of the <b>simulations</b> show that the proposed framework can control an autonomous surface vessel under various challenging scenarios, including environmental disturbances, to avoid collisions and follow desired paths.</p></p class="citation"></blockquote><h3 id=410--274318-genesis-rl-generating-natural-edge-cases-with-systematic-integration-of-safety-considerations-and-reinforcement-learning-hsin-jung-yang-et-al-2024>(4/10 | 274/318) GENESIS-RL: GEnerating Natural Edge-cases with Systematic Integration of Safety considerations and Reinforcement Learning (Hsin-Jung Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hsin-Jung Yang, Joe Beck, Md Zahid Hasan, Ekin Beyazit, Subhadeep Chakraborty, Tichakorn Wongpiromsarn, Soumik Sarkar. (2024)<br><strong>GENESIS-RL: GEnerating Natural Edge-cases with Systematic Integration of Safety considerations and Reinforcement Learning</strong><br><button class=copy-to-clipboard title="GENESIS-RL: GEnerating Natural Edge-cases with Systematic Integration of Safety considerations and Reinforcement Learning" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19062v1.pdf filename=2403.19062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving field of autonomous systems, the safety and reliability of the system components are fundamental requirements. These components are often vulnerable to complex and unforeseen environments, making natural edge-case generation essential for enhancing system resilience. This paper presents GENESIS-RL, a novel framework that leverages system-level safety considerations and <b>reinforcement</b> <b>learning</b> techniques to systematically generate naturalistic edge cases. By simulating challenging conditions that mimic the real-world situations, our framework aims to rigorously test entire system&rsquo;s safety and reliability. Although demonstrated within the autonomous driving application, our methodology is adaptable across diverse autonomous systems. Our experimental validation, conducted on high-fidelity simulator underscores the overall effectiveness of this framework.</p></p class="citation"></blockquote><h3 id=510--275318-fpga-based-neural-thrust-controller-for-uavs-sharif-azem-et-al-2024>(5/10 | 275/318) FPGA-Based Neural Thrust Controller for UAVs (Sharif Azem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui, Christian Hochberger, Heinz Koeppl. (2024)<br><strong>FPGA-Based Neural Thrust Controller for UAVs</strong><br><button class=copy-to-clipboard title="FPGA-Based Neural Thrust Controller for UAVs" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18703v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18703v2.pdf filename=2403.18703v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of unmanned aerial vehicles (UAVs) has improved a variety of fields by providing a versatile, cost-effective and accessible platform for implementing state-of-the-art algorithms. To accomplish a broader range of tasks, there is a growing need for enhanced on-board computing to cope with increasing complexity and dynamic environmental conditions. Recent advances have seen the application of Deep Neural Networks (DNNs), particularly in combination with <b>Reinforcement</b> <b>Learning</b> (RL), to improve the adaptability and performance of UAVs, especially in unknown environments. However, the computational requirements of DNNs pose a challenge to the limited computing resources available on many UAVs. This work explores the use of Field Programmable Gate Arrays (FPGAs) as a viable solution to this challenge, offering flexibility, high performance, energy and time efficiency. We propose a novel hardware board equipped with an Artix-7 FPGA for a popular open-source micro-UAV platform. We successfully validate its functionality by implementing an RL-based low-level controller using real-world experiments.</p></p class="citation"></blockquote><h3 id=610--276318-optimal-control-synthesis-of-markov-decision-processes-for-efficiency-with-surveillance-tasks-yu-chen-et-al-2024>(6/10 | 276/318) Optimal Control Synthesis of Markov Decision Processes for Efficiency with Surveillance Tasks (Yu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Chen, Xuanyuan Yin, Shaoyuan Li, Xiang Yin. (2024)<br><strong>Optimal Control Synthesis of Markov Decision Processes for Efficiency with Surveillance Tasks</strong><br><button class=copy-to-clipboard title="Optimal Control Synthesis of Markov Decision Processes for Efficiency with Surveillance Tasks" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18632v1.pdf filename=2403.18632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the problem of optimal control synthesis for Markov Decision Processes <b>(MDPs),</b> addressing both qualitative and quantitative objectives. Specifically, we require the system to fulfill a qualitative surveillance task in the sense that a specific region of interest can be visited infinitely often with probability one. Furthermore, to quantify the performance of the system, we consider the concept of efficiency, which is defined as the ratio between rewards and costs. This measure is more general than the standard long-run average reward metric as it aims to maximize the reward obtained per unit cost. Our objective is to synthesize a control policy that ensures the surveillance task while maximizes the efficiency. We provide an effective approach to synthesize a stationary control policy achieving $\epsilon$-optimality by integrating state classifications of <b>MDPs</b> and perturbation analysis in a novel manner. Our results generalize existing works on efficiency-optimal control synthesis for MDP by incorporating qualitative surveillance tasks. A robot motion planning case study is provided to illustrate the proposed algorithm.</p></p class="citation"></blockquote><h3 id=710--277318-feedback-linearizable-discretizations-of-second-order-mechanical-systems-using-retraction-maps-shreyas-n-b-et-al-2024>(7/10 | 277/318) Feedback Linearizable Discretizations of Second Order Mechanical Systems using Retraction Maps (Shreyas N. B. et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shreyas N. B., David Martin Diego, Ravi Banavar. (2024)<br><strong>Feedback Linearizable Discretizations of Second Order Mechanical Systems using Retraction Maps</strong><br><button class=copy-to-clipboard title="Feedback Linearizable Discretizations of Second Order Mechanical Systems using Retraction Maps" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18422v1.pdf filename=2403.18422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mechanical systems, in nature, are often described by a set of <b>continuous-time,</b> <b>nonlinear,</b> second-order differential equations (SODEs). This has motivated designs of various control laws implemented on digital controllers, consequently requiring numerical discretization schemes. Feedback linearizability of such sampled systems depends on the discretization scheme or map choice. In this article, we utilize retraction maps and their lifts to construct feedback linearizable discretizations for SODEs, which can be applied to various mechanical systems.</p></p class="citation"></blockquote><h3 id=810--278318-linear-hybrid-asymmetrical-load-modulated-balanced-amplifier-with-multi-band-reconfigurability-and-antenna-vswr-resilience-jiachen-guo-et-al-2024>(8/10 | 278/318) Linear Hybrid Asymmetrical Load-Modulated Balanced Amplifier with Multi-Band Reconfigurability and Antenna-VSWR Resilience (Jiachen Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiachen Guo, Yuchen Cao, Kenle Chen. (2024)<br><strong>Linear Hybrid Asymmetrical Load-Modulated Balanced Amplifier with Multi-Band Reconfigurability and Antenna-VSWR Resilience</strong><br><button class=copy-to-clipboard title="Linear Hybrid Asymmetrical Load-Modulated Balanced Amplifier with Multi-Band Reconfigurability and Antenna-VSWR Resilience" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18250v1.pdf filename=2403.18250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the first-ever highly linear and load-insensitive three-way load-modulation power amplifier (PA) based on reconfigurable hybrid asymmetrical load modulated balanced amplifier (H-ALMBA). Through proper amplitude and phase controls, the carrier, control amplifier (CA), and two peaking balanced amplifiers (BA1 and BA2) can form a linear high-order load modulation over wide bandwidth. Moreover, it is theoretically unveiled that the load modulation behavior of H-ALMBA can be insensitive to load mismatch by leveraging bias reconfiguration and the intrinsic load-insensitivity of balanced topology. Specifically, the PA&rsquo;s linearity and efficiency profiles can be maintained against arbitrary load mismatch through $Z_\mathrm{L}$-dependent reconfiguration of CA supply voltage ($V_\mathrm{DD,CA}$) and turning-on sequence of BA1 and BA2. Based on the proposed theory, an RF-input linear H-ALMBA is developed with <b>GaN</b> transistors and wideband quadrature hybrids. Over the design bandwidth from $1.7$-$2.9$ GHz, an efficiency of $56.8%$$-$$72.9%$ at peak power and $49.8%$$-$$61.2%$ at $10$-dB PBO are measured together with linear AMAM and AMPM responses. In modulated evaluation with 4G LTE signal, an EVM of $3.1%$, ACPR of $-39$ dB, and average efficiency of up to $52%$ are measured. Moreover, the reconfigurable H-ALMBA experimentally maintains an excellent average efficiency and linearity against arbitrary load mismatch at $2:1$ VSWR, and this mismatch-resilient operation can be achieved at any in-band frequencies. The overall measured performance favorably outperforms the state-of-the-art.</p></p class="citation"></blockquote><h3 id=910--279318-a-dynamic-programming-approach-for-road-traffic-estimation-mattia-laurini-et-al-2024>(9/10 | 279/318) A Dynamic Programming Approach for Road Traffic Estimation (Mattia Laurini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mattia Laurini, Irene Saccani, Stefano Ardizzoni, Luca Consolini, Marco Locatelli. (2024)<br><strong>A Dynamic Programming Approach for Road Traffic Estimation</strong><br><button class=copy-to-clipboard title="A Dynamic Programming Approach for Road Traffic Estimation" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18561v1.pdf filename=2403.18561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a road network represented by a directed <b>graph.</b> We assume to collect many measurements of traffic flows on all the network arcs, or on a subset of them. We assume that the users are divided into different groups. Each group follows a different path. The flows of all user groups are modeled as a set of independent Poisson processes. Our focus is estimating the paths followed by each user group, and the means of the associated Poisson processes. We present a possible solution based on a Dynamic Programming algorithm. The method relies on the knowledge of high order cumulants. We discuss the theoretical properties of the introduced method. Finally, we present some numerical tests on well-known <b>benchmark</b> networks, using synthetic data.</p></p class="citation"></blockquote><h3 id=1010--280318-incentive-compatible-vertiport-reservation-in-advanced-air-mobility-an-auction-based-approach-pan-yang-su-et-al-2024>(10/10 | 280/318) Incentive-Compatible Vertiport Reservation in Advanced Air Mobility: An Auction-Based Approach (Pan-Yang Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pan-Yang Su, Chinmay Maheshwari, Victoria Tuck, Shankar Sastry. (2024)<br><strong>Incentive-Compatible Vertiport Reservation in Advanced Air Mobility: An Auction-Based Approach</strong><br><button class=copy-to-clipboard title="Incentive-Compatible Vertiport Reservation in Advanced Air Mobility: An Auction-Based Approach" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: 91B03, 91A68, 90B06, 90C27, cs-MA, cs-SY, econ-TH, eess-SY, eess.SY, math-OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18166v1.pdf filename=2403.18166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of advanced air mobility (AAM) is expected to become a multibillion-dollar industry in the near future. Market-based mechanisms are touted to be an integral part of AAM operations, which comprise heterogeneous operators with private valuations. In this work, we study the problem of designing a mechanism to coordinate the movement of electric vertical take-off and landing (eVTOL) aircraft, operated by multiple operators each having heterogeneous valuations associated with their fleet, between vertiports, while enforcing the arrival, departure, and parking constraints at vertiports. Particularly, we propose an incentive-compatible and individually rational vertiport reservation mechanism that maximizes a social welfare metric, which encapsulates the objective of maximizing the overall valuations of all operators while minimizing the congestion at vertiports. Additionally, we improve the computational tractability of designing the reservation mechanism by proposing a mixed binary linear programming approach that is based on constructing network flow <b>graph</b> corresponding to the underlying problem.</p></p class="citation"></blockquote><h2 id=mathna-7>math.NA (7)</h2><h3 id=17--281318-higher-order-multi-dimension-reduction-methods-via-einstein-product-alaeddine-zahir-et-al-2024>(1/7 | 281/318) Higher order multi-dimension reduction methods via Einstein-product (Alaeddine Zahir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alaeddine Zahir, Khalide Jbilou, Ahmed Ratnani. (2024)<br><strong>Higher order multi-dimension reduction methods via Einstein-product</strong><br><button class=copy-to-clipboard title="Higher order multi-dimension reduction methods via Einstein-product" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 33<br>Keywords: Graph, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18171v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18171v2.pdf filename=2403.18171v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the extension of dimension reduction (DR) techniques to the multi-dimension case by using the Einstein product. Our focus lies on <b>graph-based</b> methods, encompassing both linear and nonlinear approaches, within both <b>supervised</b> and <b>unsupervised</b> <b>learning</b> paradigms. Additionally, we investigate variants such as repulsion <b>graphs</b> and kernel methods for linear approaches. Furthermore, we present two generalizations for each method, based on single or multiple weights. We demonstrate the straightforward nature of these generalizations and provide theoretical insights. Numerical experiments are conducted, and results are compared with original methods, highlighting the efficiency of our proposed methods, particularly in handling high-dimensional data such as color images.</p></p class="citation"></blockquote><h3 id=27--282318-fractional-variational-integrators-based-on-convolution-quadrature-khaled-hariz-et-al-2024>(2/7 | 282/318) Fractional variational integrators based on convolution quadrature (Khaled Hariz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khaled Hariz, Fernando Jiménez, Sina Ober-Blöbaum. (2024)<br><strong>Fractional variational integrators based on convolution quadrature</strong><br><button class=copy-to-clipboard title="Fractional variational integrators based on convolution quadrature" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18362v1.pdf filename=2403.18362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fractional dissipation is a powerful tool to study non-local physical phenomena such as damping models. The design of geometric, in particular, variational integrators for the numerical <b>simulation</b> of such systems relies on a variational formulation of the model. In [19], a new approach is proposed to deal with dissipative systems including fractionally damped systems in a variational way for both, the continuous and discrete setting. It is based on the doubling of variables and their fractional derivatives. The aim of this work is to derive higher-order fractional variational integrators by means of <b>convolution</b> quadrature (CQ) based on backward difference formulas. We then provide numerical methods that are of order 2 improving a previous result in [19]. The convergence properties of the fractional variational integrators and saturation effects due to the approximation of the fractional derivatives by CQ are studied numerically.</p></p class="citation"></blockquote><h3 id=37--283318-generalized-convergence-of-the-deep-bsde-method-a-step-towards-fully-coupled-fbsdes-and-applications-in-stochastic-control-balint-negyesi-et-al-2024>(3/7 | 283/318) Generalized convergence of the deep BSDE method: a step towards fully-coupled FBSDEs and applications in stochastic control (Balint Negyesi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Balint Negyesi, Zhipeng Huang, Cornelis W. Oosterlee. (2024)<br><strong>Generalized convergence of the deep BSDE method: a step towards fully-coupled FBSDEs and applications in stochastic control</strong><br><button class=copy-to-clipboard title="Generalized convergence of the deep BSDE method: a step towards fully-coupled FBSDEs and applications in stochastic control" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65C05, 65C30, 93E20, cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18552v1.pdf filename=2403.18552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We are concerned with high-dimensional coupled FBSDE systems approximated by the deep BSDE method of Han et al. (2018). It was shown by Han and Long (2020) that the errors induced by the deep BSDE method admit a posteriori estimate depending on the loss function, whenever the backward equation only couples into the forward diffusion through the Y process. We generalize this result to fully-coupled drift coefficients, and give sufficient conditions for convergence under standard assumptions. The resulting conditions are directly verifiable for any equation. Consequently, unlike in earlier theory, our convergence analysis enables the treatment of FBSDEs <b>stemming</b> from stochastic optimal control problems. In particular, we provide a theoretical justification for the non-convergence of the deep BSDE method observed in recent literature, and present direct guidelines for when convergence can be guaranteed in practice. Our theoretical findings are supported by several numerical experiments in high-dimensional settings.</p></p class="citation"></blockquote><h3 id=47--284318-concurrent-level-set-and-fiber-orientation-optimization-of-composite-structures-m-mokhtarzadeh-et-al-2024>(4/7 | 284/318) Concurrent level set and fiber orientation optimization of composite structures (M. Mokhtarzadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. Mokhtarzadeh, F Lopez Jimenez, K. Maute. (2024)<br><strong>Concurrent level set and fiber orientation optimization of composite structures</strong><br><button class=copy-to-clipboard title="Concurrent level set and fiber orientation optimization of composite structures" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18971v1.pdf filename=2403.18971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By adjusting both the structural shape and fiber orientation, this research aims to optimize the design of Fiber Reinforced Composite structures. The structural <b>geometry</b> is represented by a level set function, which is approximated by quadratic B-spline functions. The fiber orientation field is parameterized with quadratic/cubic B-splines on hierarchically refined meshes. Different levels for B-spline mesh refinement for the level set and fiber orientation fields are studied to obtain a smooth fiber layout. To facilitate FRC manufacturing, the parallel alignment, and smoothness of fiber paths are enforced by introducing penalty terms referred to as &ldquo;misalignment penalty and curvature penalty&rdquo;, which are incorporated into the optimization process. A geometric interpretation of the penalties is provided. The material behavior of the FRCs is modeled by the Mori-Tanaka homogenization scheme and the macroscopic structure response is modeled by linear elasticity under static mutiloading conditions. The Governing equations are discretized by a Heaviside-enriched eXtended IsoGeometric Analysis to avoid the need to generate conformal meshes. Instabilities in XIGA are mitigated by the facet-oriented ghost stabilization technique. This work considers mass and strain energy in the formulation of the optimization objective, along with misalignment and curvature penalties and additional regularization terms. Constraints are imposed on the volume of the structure. The resulting optimization problems are solved by a gradient-based algorithm. The design sensitivities are computed by the adjoint method. Numerical examples demonstrate with two-dimensional and three-dimensional configurations that the proposed method is efficient in simultaneously optimizing the macroscopic shape and the fiber layout while improving manufacturability by promoting parallel and smooth fiber paths.</p></p class="citation"></blockquote><h3 id=57--285318-robust-numerical-algebraic-geometry-emma-r-cobian-et-al-2024>(5/7 | 285/318) Robust Numerical Algebraic Geometry (Emma R. Cobian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emma R. Cobian, Jonathan D. Hauenstein, Charles W. Wampler. (2024)<br><strong>Robust Numerical Algebraic Geometry</strong><br><button class=copy-to-clipboard title="Robust Numerical Algebraic Geometry" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18749v1.pdf filename=2403.18749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of numerical algebraic <b>geometry</b> consists of algorithms for numerically solving systems of polynomial equations. When the system is exact, such as having rational coefficients, the solution set is well-defined. However, for a member of a parameterized family of polynomial systems where the parameter values may be measured with imprecision or arise from prior numerical computations, uncertainty may arise in the structure of the solution set, including the number of isolated solutions, the existence of higher dimensional solution components, and the number of irreducible components along with their multiplicities. The loci where these structures change form a stratification of exceptional algebraic sets in the space of parameters. We describe methodologies for making the interpretation of numerical results more robust by searching for nearby parameter values on an exceptional set. We demonstrate these techniques on several illustrative examples and then treat several more substantial problems arising from the kinematics of mechanisms and robots.</p></p class="citation"></blockquote><h3 id=67--286318-global-convergence-of-iterative-solvers-for-problems-of-nonlinear-magnetostatics-herbert-egger-et-al-2024>(6/7 | 286/318) Global convergence of iterative solvers for problems of nonlinear magnetostatics (Herbert Egger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Herbert Egger, Felix Engertsberger, Bogdan Radu. (2024)<br><strong>Global convergence of iterative solvers for problems of nonlinear magnetostatics</strong><br><button class=copy-to-clipboard title="Global convergence of iterative solvers for problems of nonlinear magnetostatics" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18520v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18520v1.pdf filename=2403.18520v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the convergence of iterative solvers for problems of nonlinear magnetostatics. Using the equivalence to an underlying minimization problem, we can establish global linear convergence of a large class of methods, including the damped Newton-method, fixed-point iteration, and the Kacanov iteration, which can all be interpreted as generalized gradient descent methods. Armijo backtracking isconsidered for an adaptive choice of the stepsize. The general assumptions required for our analysis cover inhomogeneous, nonlinear, and anisotropic materials, as well as permanent magnets. The main results are proven on the continuous level, but they carry over almost verbatim to various approximation schemes, including finite elements and isogeometric analysis, leading to bounds on the iteration numbers, which are independent of the particular discretization. The theoretical results are illustrated by numerical tests for a typical <b>benchmark</b> problem.</p></p class="citation"></blockquote><h3 id=77--287318-stability-and-convergence-of-the-penalty-formulation-for-nonlinear-magnetostatics-herbert-egger-et-al-2024>(7/7 | 287/318) Stability and convergence of the penalty formulation for nonlinear magnetostatics (Herbert Egger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Herbert Egger, Felix Engertsberger, Klaus Roppert. (2024)<br><strong>Stability and convergence of the penalty formulation for nonlinear magnetostatics</strong><br><button class=copy-to-clipboard title="Stability and convergence of the penalty formulation for nonlinear magnetostatics" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18285v1.pdf filename=2403.18285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The magnetostatic field distribution in a nonlinear medium amounts to the unique minimizer of the magnetic coenergy over all fields that can be generated by the same current. This is a nonlinear saddlepoint problem whose numerical solution can in principle be achieved by mixed finite element methods and appropriate nonlinear solvers. The saddlepoint structure, however, makes the solution cumbersome. A remedy is to split the magnetic field into a known source field and the gradient of a scalar potential which is governed by a convex minimization problem. The penalty approach avoids the use of artificial potentials and Lagrange multipliers and leads to an unconstrained convex minimization problem involving a large parameter. We provide a rigorous justification of the penalty approach by deriving error estimates for the approximation due to penalization. We further highlight the close connections to the Lagrange-multiplier and scalar potential approach. The theoretical results are illustrated by numerical tests for a typical <b>benchmark</b> problem</p></p class="citation"></blockquote><h2 id=mathst-1>math.ST (1)</h2><h3 id=11--288318-meta-learning-with-generalized-ridge-regression-high-dimensional-asymptotics-optimality-and-hyper-covariance-estimation-yanhao-jin-et-al-2024>(1/1 | 288/318) Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation (Yanhao Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanhao Jin, Krishnakumar Balasubramanian, Debashis Paul. (2024)<br><strong>Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation</strong><br><button class=copy-to-clipboard title="Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-LG, math-ST, math.ST, stat-ML, stat-TH<br>Keyword Score: 30<br>Keywords: Meta Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19720v1.pdf filename=2403.19720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Meta-learning</b> <b>involves</b> training models on a variety of training tasks in a way that enables them to generalize well on new, unseen test tasks. In this work, we consider <b>meta-learning</b> <b>within</b> the framework of high-dimensional multivariate random-effects linear models and study generalized ridge-regression based predictions. The statistical intuition of using generalized ridge regression in this setting is that the covariance structure of the random regression coefficients could be leveraged to make better predictions on new tasks. Accordingly, we first characterize the precise asymptotic behavior of the predictive risk for a new test task when the data dimension grows proportionally to the number of samples per task. We next show that this predictive risk is optimal when the weight matrix in generalized ridge regression is chosen to be the inverse of the covariance matrix of random coefficients. Finally, we propose and analyze an estimator of the inverse covariance matrix of random regression coefficients based on data from the training tasks. As opposed to intractable MLE-type estimators, the proposed estimators could be computed efficiently as they could be obtained by solving (global) geodesically-convex optimization problems. Our analysis and methodology use tools from random matrix theory and Riemannian optimization. <b>Simulation</b> results demonstrate the improved generalization performance of the proposed method on new unseen test tasks within the considered framework.</p></p class="citation"></blockquote><h2 id=astro-phsr-1>astro-ph.SR (1)</h2><h3 id=11--289318-super-resolution-of-sohomdi-magnetograms-of-solar-active-regions-using-sdohmi-data-and-an-attention-aided-convolutional-neural-network-chunhui-xu-et-al-2024>(1/1 | 289/318) Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using SDO/HMI Data and an Attention-Aided Convolutional Neural Network (Chunhui Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunhui Xu, Jason T. L. Wang, Haimin Wang, Haodi Jiang, Qin Li, Yasser Abduallah, Yan Xu. (2024)<br><strong>Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using SDO/HMI Data and an Attention-Aided Convolutional Neural Network</strong><br><button class=copy-to-clipboard title="Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using SDO/HMI Data and an Attention-Aided Convolutional Neural Network" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.SR<br>Categories: astro-ph-SR, astro-ph.SR, cs-LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18302v1.pdf filename=2403.18302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image super-resolution has been an important subject in image processing and recognition. Here, we present an attention-aided <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> for solar image super-resolution. Our method, named SolarCNN, aims to enhance the quality of line-of-sight (LOS) magnetograms of solar active regions (ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and Heliospheric Observatory (SOHO). The ground-truth labels used for training SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist of strong magnetic fields in which magnetic energy can suddenly be released to produce extreme space weather events, such as solar flares, coronal mass ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI magnetograms allow for better understanding and forecasting of violent events of space weather. Experimental results show that SolarCNN improves the quality of SOHO/MDI magnetograms in terms of the structural similarity index measure (SSIM), Pearson&rsquo;s correlation coefficient (PCC), and the peak signal-to-noise ratio (PSNR).</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--290318-a-study-of-three-influencer-archetypes-for-the-control-of-opinion-spread-in-time-varying-social-networks-michael-debuse-et-al-2024>(1/2 | 290/318) A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks (Michael DeBuse et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael DeBuse, Sean Warnick. (2024)<br><strong>A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks</strong><br><button class=copy-to-clipboard title="A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs-SY, cs.SI, eess-SY, physics-soc-ph<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18163v1.pdf filename=2403.18163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we consider the impact of information spread in time-varying social networks, where agents request to follow other agents with aligned opinions while dropping ties to neighbors whose posts are too dissimilar to their own views. Opinion control and rhetorical influence has a very long history, employing various methods including education, persuasion, propaganda, marketing, and manipulation through mis-, dis-, and mal-information. The automation of opinion controllers, however, has only recently become easily deployable at a wide scale, with the advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>generative</b> <b>AI</b> that can translate the quantified commands from opinion controllers into actual content with the appropriate nuance. Automated agents in social networks can be deployed for various purposes, such as breaking up echo chambers, bridging valuable new connections between agents, or shaping the opinions of a target population &ndash; and all of these raise important ethical concerns that deserve serious attention and thoughtful discussion and debate. This paper attempts to contribute to this discussion by considering three archetypal influencing styles observed by human drivers in these settings, comparing and contrasting the impact of these different control methods on the opinions of agents in the network. We will demonstrate the efficacy of current <b>generative</b> <b>AI</b> for generating nuanced content consistent with the command signal from automatic opinion controllers like these, and we will report on frameworks for approaching the relevant ethical considerations.</p></p class="citation"></blockquote><h3 id=22--291318-the-process-of-polarisation-as-a-loss-of-dimensionality-measuring-changes-in-polarisation-using-singular-value-decomposition-of-network-graphs-sage-anastasi-et-al-2024>(2/2 | 291/318) The process of polarisation as a loss of dimensionality: measuring changes in polarisation using Singular Value Decomposition of network graphs (Sage Anastasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sage Anastasi, Giulio Dalla Riva. (2024)<br><strong>The process of polarisation as a loss of dimensionality: measuring changes in polarisation using Singular Value Decomposition of network graphs</strong><br><button class=copy-to-clipboard title="The process of polarisation as a loss of dimensionality: measuring changes in polarisation using Singular Value Decomposition of network graphs" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: 62P25, cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18191v1.pdf filename=2403.18191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing polarisation in our societies is a major international concern. Current approaches to defining and detecting polarisation largely rely on finding evidence of bimodality in social networks or voter opinion surveys. It is difficult to detect temporal trends in polarisation, as the results usually fall into a binary of polarised or non-polarised, which cannot robustly show that subsequent increases in bimodality are statistically significant. Our work is aligned with Baldassari and Gelman&rsquo;s theory that polarisation should be defined as increasing correlation between positions in the ideological field. We also draw from post-structuralist work which argues that polarisation is the process of both the ideological and material layers of society being segregated into two poles, as in cases of apartheid. Thus, in order to measure the polarisation in a society, it would be beneficial to be able to assess social networks directly. In this paper we use Random Dot Product <b>Graphs</b> to embed social networks in metric spaces. In the case of a social network, the embedded dimensionality corresponds to the number of reasons any two people may form a social connection. A decrease in the optimal dimensionality for the embedding of the network <b>graph,</b> as measured using truncated Singular Value Decomposition of the <b>graph</b> adjacency matrix, indicates increasing polarisation in the network. We apply this method to two different Twitter networks based on discussions of climate change, and show that our methods agree with other researchers&rsquo; detection of polarisation in this space. We also use networks generated by stochastic block models to explore how an increase of the isolation between distinct communities in a network, or the increase in the predominance of one community over the other, are identifiable as polarisation processes.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--292318-tessellation-and-interactive-visualization-of-four-dimensional-spacetime-geometries-philip-claude-caplan-2024>(1/2 | 292/318) Tessellation and interactive visualization of four-dimensional spacetime geometries (Philip Claude Caplan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Claude Caplan. (2024)<br><strong>Tessellation and interactive visualization of four-dimensional spacetime geometries</strong><br><button class=copy-to-clipboard title="Tessellation and interactive visualization of four-dimensional spacetime geometries" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-CG, cs.CE<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19036v1.pdf filename=2403.19036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses two problems needed to support four-dimensional ($3d + t$) spacetime numerical <b>simulations.</b> The first contribution is a general algorithm for producing conforming spacetime meshes of moving geometries. Here, the surface points of the <b>geometry</b> are embedded in a four-dimensional space as the <b>geometry</b> moves in time. The <b>geometry</b> is first tessellated at prescribed time steps and then these tessellations are connected in the parameter space of each <b>geometry</b> entity to form tetrahedra. In contrast to previous work, this approach allows the resolution of the <b>geometry</b> to be controlled at each time step. The only restriction on the algorithm is the requirement that no topological changes to the <b>geometry</b> are made (i.e. the hierarchical relations between all <b>geometry</b> entities are maintained) as the <b>geometry</b> moves in time. The validity of the final mesh topology is verified by ensuring the tetrahedralizations represent a closed 3-manifold. For some analytic problems, the $4d$ volume of the tetrahedralization is also verified. The second problem addressed in this paper is the design of a system to interactively visualize four-dimensional meshes, including tetrahedra (embedded in $4d$) and pentatopes. Algorithms that either include or exclude a <b>geometry</b> shader are described, and the efficiency of each approach is then compared. Overall, the results suggest that visualizing tetrahedra (either those bounding the domain, or extracted from a pentatopal mesh) using a <b>geometry</b> shader achieves the highest frame rate, in the range of $20-30$ frames per second for meshes with about $50$ million tetrahedra.</p></p class="citation"></blockquote><h3 id=22--293318-sliced-online-model-checking-for-optimizing-the-beam-scheduling-problem-in-robotic-radiation-therapy-lars-beckers-et-al-2024>(2/2 | 293/318) Sliced Online Model Checking for Optimizing the Beam Scheduling Problem in Robotic Radiation Therapy (Lars Beckers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lars Beckers, Stefan Gerlach, Ole Lübke, Alexander Schlaefer, Sibylle Schupp. (2024)<br><strong>Sliced Online Model Checking for Optimizing the Beam Scheduling Problem in Robotic Radiation Therapy</strong><br><button class=copy-to-clipboard title="Sliced Online Model Checking for Optimizing the Beam Scheduling Problem in Robotic Radiation Therapy" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18918v1.pdf filename=2403.18918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided by pausing the beam delivery, the treatment time would increase. Typically, the beams are delivered in an order which minimizes the robot motion and thereby the overall treatment time. However, this order can be changed, i.e., instead of pausing beams, other feasible beam could be delivered. We address this problem of dynamically ordering the beams by applying a model checking paradigm to select feasible beams. Since breathing patterns are complex and change rapidly, any offline model would be too imprecise. Thus, model checking must be conducted online, predicting the patient&rsquo;s current breathing pattern for a short amount of time and checking which beams can be delivered safely. Monitoring the treatment delivery online provides the option to reschedule beams dynamically in order to avoid pausing and hence to reduce treatment time. While human breathing patterns are complex and may change rapidly, we need a model which can be verified quickly and use approximation by a superposition of sine curves. Further, we simplify the 3D breathing motion into separate 1D models. We compensate the simplification by adding noise inside the model itself. In turn, we synchronize between the multiple models representing the different spatial directions, the treatment <b>simulation,</b> and corresponding verification queries. Our preliminary results show a 16.02 % to 37.21 % mean improvement on the idle time compared to a static beam schedule, depending on an additional safety margin. Note that an additional safety margin around the ultrasound robot can decrease idle times but also compromises plan quality by limiting the range of available beam directions. In contrast, the approach using online model checking maintains the plan quality. Further, we compare to a naive machine learning approach that does not achieve its goals while being harder to reason about.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--294318-how-to-cache-important-contents-for-multi-modal-service-in-dynamic-networks-a-drl-based-caching-scheme-zhe-zhang-et-al-2024>(1/1 | 294/318) How to Cache Important Contents for Multi-modal Service in Dynamic Networks: A DRL-based Caching Scheme (Zhe Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Zhang, Marc St-Hilaire, Xin Wei, Haiwei Dong, Abdulmotaleb El Saddik. (2024)<br><strong>How to Cache Important Contents for Multi-modal Service in Dynamic Networks: A DRL-based Caching Scheme</strong><br><button class=copy-to-clipboard title="How to Cache Important Contents for Multi-modal Service in Dynamic Networks: A DRL-based Caching Scheme" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-MM, cs-NI, cs.NI<br>Keyword Score: 23<br>Keywords: Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18323v1.pdf filename=2403.18323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the continuous evolution of networking technologies, <b>multi-modal</b> services that involve video, audio, and haptic contents are expected to become the dominant multimedia service in the near future. Edge caching is a key technology that can significantly reduce network load and content transmission latency, which is critical for the delivery of <b>multi-modal</b> contents. However, existing caching approaches only rely on a limited number of factors, e.g., popularity, to evaluate their importance for caching, which is inefficient for caching <b>multi-modal</b> contents, especially in dynamic network environments. To overcome this issue, we propose a content importance-based caching scheme which consists of a content importance evaluation model and a caching model. By leveraging dueling double deep Q networks (D3QN) model, the content importance evaluation model can adaptively evaluate contents&rsquo; importance in dynamic networks. Based on the evaluated contents&rsquo; importance, the caching model can easily cache and evict proper contents to improve caching efficiency. The <b>simulation</b> results show that the proposed content importance-based caching scheme outperforms existing caching schemes in terms of caching hit ratio (at least 15% higher), reduced network load (up to 22% reduction), average number of hops (up to 27% lower), and unsatisfied requests ratio (more than 47% reduction).</p></p class="citation"></blockquote><h2 id=q-bioqm-2>q-bio.QM (2)</h2><h3 id=12--295318-sequential-inference-of-hospitalization-electronichealth-records-using-probabilistic-models-alan-d-kaplan-et-al-2024>(1/2 | 295/318) Sequential Inference of Hospitalization ElectronicHealth Records Using Probabilistic Models (Alan D. Kaplan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alan D. Kaplan, Priyadip Ray, John D. Greene, Vincent X. Liu. (2024)<br><strong>Sequential Inference of Hospitalization ElectronicHealth Records Using Probabilistic Models</strong><br><button class=copy-to-clipboard title="Sequential Inference of Hospitalization ElectronicHealth Records Using Probabilistic Models" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19011v1.pdf filename=2403.19011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the dynamic hospital setting, decision support can be a valuable tool for improving patient outcomes. Data-driven inference of future outcomes is challenging in this dynamic setting, where long sequences such as laboratory tests and medications are updated frequently. This is due in part to heterogeneity of data types and mixed-sequence types contained in variable length sequences. In this work we design a <b>probabilistic</b> <b>unsupervised</b> model for multiple arbitrary-length sequences contained in hospitalization Electronic Health Record (EHR) data. The model uses a latent variable structure and captures complex relationships between medications, diagnoses, laboratory tests, neurological assessments, and medications. It can be trained on original data, without requiring any lossy transformations or time binning. Inference algorithms are derived that use partial data to infer properties of the complete sequences, including their length and presence of specific values. We train this model on data from subjects receiving medical care in the Kaiser Permanente Northern California integrated healthcare delivery system. The results are evaluated against held-out data for predicting the length of sequences and presence of Intensive Care Unit (ICU) in hospitalization bed sequences. Our method outperforms a baseline approach, showing that in these experiments the trained model captures information in the sequences that is informative of their future values.</p></p class="citation"></blockquote><h3 id=22--296318-a-python-library-for-efficient-computation-of-molecular-fingerprints-michał-szafarczyk-et-al-2024>(2/2 | 296/318) A Python library for efficient computation of molecular fingerprints (Michał Szafarczyk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michał Szafarczyk, Piotr Ludynia, Przemysław Kukla. (2024)<br><strong>A Python library for efficient computation of molecular fingerprints</strong><br><button class=copy-to-clipboard title="A Python library for efficient computation of molecular fingerprints" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19718v1.pdf filename=2403.19718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning solutions are very popular in the field of chemoinformatics, where they have numerous applications, such as novel drug discovery or molecular property prediction. Molecular fingerprints are algorithms commonly used for vectorizing chemical molecules as a part of preprocessing in this kind of solution. However, despite their popularity, there are no libraries that implement them efficiently for large datasets, utilizing modern, multicore architectures. On top of that, most of them do not provide the user with an intuitive interface, or one that would be compatible with other machine learning tools. In this project, we created a Python library that computes molecular fingerprints efficiently and delivers an interface that is comprehensive and enables the user to easily incorporate the library into their existing machine learning workflow. The library enables the user to perform computation on large datasets using parallelism. Because of that, it is possible to perform such tasks as hyperparameter tuning in a reasonable time. We describe tools used in implementation of the library and asses its time performance on example <b>benchmark</b> datasets. Additionally, we show that using molecular fingerprints we can achieve results comparable to state-of-the-art ML solutions even with very simple models.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--297318-using-quantum-computing-to-infer-dynamic-behaviors-of-biological-and-artificial-neural-networks-gabriel-a-silva-2024>(1/1 | 297/318) Using Quantum Computing to Infer Dynamic Behaviors of Biological and Artificial Neural Networks (Gabriel A. Silva, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel A. Silva. (2024)<br><strong>Using Quantum Computing to Infer Dynamic Behaviors of Biological and Artificial Neural Networks</strong><br><button class=copy-to-clipboard title="Using Quantum Computing to Infer Dynamic Behaviors of Biological and Artificial Neural Networks" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, q-bio-NC, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18963v1.pdf filename=2403.18963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exploration of new problem classes for quantum computation is an active area of research. An essentially completely unexplored topic is the use of quantum algorithms and computing to explore and ask questions \textit{about} the functional dynamics of neural networks. This is a component of the still-nascent topic of applying quantum computing to the modeling and <b>simulations</b> of biological and artificial neural networks. In this work, we show how a carefully constructed set of conditions can use two foundational quantum algorithms, Grover and Deutsch-Josza, in such a way that the output measurements admit an interpretation that guarantees we can infer if a simple representation of a neural network (which applies to both biological and artificial networks) after some period of time has the potential to continue sustaining dynamic activity. Or whether the dynamics are guaranteed to stop either through &rsquo;epileptic&rsquo; dynamics or quiescence.</p></p class="citation"></blockquote><h2 id=cssy-1>cs.SY (1)</h2><h3 id=11--298318-mpc-cbf-with-adaptive-safety-margins-for-safety-critical-teleoperation-over-imperfect-network-connections-riccardo-periotto-et-al-2024>(1/1 | 298/318) MPC-CBF with Adaptive Safety Margins for Safety-critical Teleoperation over Imperfect Network Connections (Riccardo Periotto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Periotto, Mina Ferizbegovic, Fernando S. Barbosa, Roberto C. Sundin. (2024)<br><strong>MPC-CBF with Adaptive Safety Margins for Safety-critical Teleoperation over Imperfect Network Connections</strong><br><button class=copy-to-clipboard title="MPC-CBF with Adaptive Safety Margins for Safety-critical Teleoperation over Imperfect Network Connections" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SY<br>Categories: cs-SY, cs.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18650v1.pdf filename=2403.18650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper focuses on the design of a control strategy for safety-critical remote teleoperation. The main goal is to make the controlled system track the desired velocity specified by an operator while avoiding obstacles despite communication delays. Control Barrier Functions (CBFs) are used to define the safety constraints that the system has to respect to avoid obstacles, while Model Predictive Control (MPC) provides the framework for adjusting the desired input, taking the constraints into account. The resulting input is sent to the remote system, where appropriate low-level velocity controllers translate it into system-specific commands. The main novelty of the paper is a method to make the CBFs robust against the uncertainties caused by the network delays affecting the system&rsquo;s state and do so in a less conservative manner. The results show how the proposed method successfully solves the safety-critical teleoperation problem, making the controlled systems avoid obstacles with different types of network delay. The controller has also been tested in <b>simulation</b> and on a real manipulator, demonstrating its general applicability when reliable low-level velocity controllers are available.</p></p class="citation"></blockquote><h2 id=hep-ph-1>hep-ph (1)</h2><h3 id=11--299318-one-flow-to-correct-them-all-improving-simulations-in-high-energy-physics-with-a-single-normalising-flow-and-a-switch-caio-cesar-daumann-et-al-2024>(1/1 | 299/318) One flow to correct them all: improving simulations in high-energy physics with a single normalising flow and a switch (Caio Cesar Daumann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Caio Cesar Daumann, Mauro Donega, Johannes Erdmann, Massimiliano Galli, Jan Lukas Späh, Davide Valsecchi. (2024)<br><strong>One flow to correct them all: improving simulations in high-energy physics with a single normalising flow and a switch</strong><br><button class=copy-to-clipboard title="One flow to correct them all: improving simulations in high-energy physics with a single normalising flow and a switch" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ph<br>Categories: cs-LG, hep-ex, hep-ph, hep-ph, physics-data-an<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18582v1.pdf filename=2403.18582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simulated events are key ingredients in almost all high-energy physics analyses. However, imperfections in the <b>simulation</b> can lead to sizeable differences between the observed data and simulated events. The effects of such mismodelling on relevant observables must be corrected either effectively via scale factors, with weights or by modifying the distributions of the observables and their correlations. We introduce a correction method that transforms one multidimensional distribution <b>(simulation)</b> into another one (data) using a simple architecture based on a single normalising flow with a boolean condition. We demonstrate the effectiveness of the method on a physics-inspired toy dataset with non-trivial mismodelling of several observables and their correlations.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--300318-the-metric-distortion-of-randomized-social-choice-functions-c1-maximal-lottery-rules-and-simulations-fabian-frank-et-al-2024>(1/1 | 300/318) The Metric Distortion of Randomized Social Choice Functions: C1 Maximal Lottery Rules and Simulations (Fabian Frank et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabian Frank, Patrick Lederer. (2024)<br><strong>The Metric Distortion of Randomized Social Choice Functions: C1 Maximal Lottery Rules and Simulations</strong><br><button class=copy-to-clipboard title="The Metric Distortion of Randomized Social Choice Functions: C1 Maximal Lottery Rules and Simulations" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18340v1.pdf filename=2403.18340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The metric distortion of a randomized social choice function (RSCF) quantifies its worst-case approximation ratio of the optimal social cost when the voters&rsquo; costs for alternatives are given by distances in a metric space. This notion has recently attracted significant attention as numerous RSCFs that aim to minimize the metric distortion have been suggested. However, such tailored voting rules usually have little appeal other than their low metric distortion. In this paper, we will thus study the metric distortion of well-established RSCFs. In more detail, we first show that C1 maximal lottery rules, a well-known class of RSCFs, have a metric distortion of $4$ and furthermore prove that this is optimal within the class of majoritarian RSCFs (which only depend on the majority relation). As our second contribution, we perform extensive computer experiments on the metric distortion of established RSCFs to obtain insights into their average-case performance. These computer experiments are based on a new linear program for computing the metric distortion of a lottery on a given profile and reveal that some classical RSCFs perform almost as well as the currently best known RSCF with respect to the metric distortion on randomly sampled profiles.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--301318-many-objective-evolutionary-influence-maximization-balancing-spread-budget-fairness-and-time-elia-cunegatti-et-al-2024>(1/1 | 301/318) Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time (Elia Cunegatti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca. (2024)<br><strong>Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time</strong><br><button class=copy-to-clipboard title="Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs-SI, cs.NE<br>Keyword Score: 13<br>Keywords: Graph, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18755v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18755v2.pdf filename=2403.18755v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Influence Maximization (IM) problem seeks to discover the set of nodes in a <b>graph</b> that can spread the information propagation at most. This problem is known to be NP-hard, and it is usually studied by maximizing the influence (spread) and, optionally, optimizing a second objective, such as minimizing the seed set size or maximizing the influence <b>fairness.</b> However, in many practical scenarios multiple aspects of the IM problem must be optimized at the same time. In this work, we propose a first case study where several IM-specific objective functions, namely budget, <b>fairness,</b> communities, and time, are optimized on top of the maximization of influence and minimization of the seed set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm (MOEA) based on NSGA-II incorporating <b>graph-aware</b> operators and a smart initialization. We compare MOEIM in two experimental settings, including a total of nine <b>graph</b> datasets, two heuristic methods, a related MOEA, and a state-of-the-art Deep Learning approach. The experiments show that MOEIM overall outperforms the competitors in most of the tested many-objective settings. To conclude, we also investigate the correlation between the objectives, leading to novel insights into the topic. The codebase is available at <a href=https://github.com/eliacunegatti/MOEIM>https://github.com/eliacunegatti/MOEIM</a>.</p></p class="citation"></blockquote><h2 id=cslo-2>cs.LO (2)</h2><h3 id=12--302318-algebraic-reasoning-meets-automata-in-solving-linear-integer-arithmetic-peter-habermehl-et-al-2024>(1/2 | 302/318) Algebraic Reasoning Meets Automata in Solving Linear Integer Arithmetic (Peter Habermehl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Habermehl, Michal Hečko, Vojtěch Havlena, Lukáš Holík, Ondřej Lengál. (2024)<br><strong>Algebraic Reasoning Meets Automata in Solving Linear Integer Arithmetic</strong><br><button class=copy-to-clipboard title="Algebraic Reasoning Meets Automata in Solving Linear Integer Arithmetic" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18995v1.pdf filename=2403.18995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new angle on solving quantified linear integer arithmetic based on combining the automata-based approach, where numbers are understood as bitvectors, with ideas from (nowadays prevalent) algebraic approaches, which work directly with numbers. This combination is enabled by a fine-grained version of the duality between automata and arithmetic formulae. In particular, we employ a construction where states of automaton are obtained as derivatives of arithmetic formulae: then every state corresponds to a formula. Optimizations based on techniques and ideas transferred from the world of algebraic methods are used on thousands of automata states, which dramatically amplifies their effect. The merit of this combination of automata with algebraic methods is demonstrated by our prototype implementation being competitive to and even superior to state-of-the-art SMT solvers.</p></p class="citation"></blockquote><h3 id=22--303318-on-propositional-dynamic-logic-and-concurrency-matteo-acclavio-et-al-2024>(2/2 | 303/318) On Propositional Dynamic Logic and Concurrency (Matteo Acclavio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Acclavio, Fabrizio Montesi, Marco Peressotti. (2024)<br><strong>On Propositional Dynamic Logic and Concurrency</strong><br><button class=copy-to-clipboard title="On Propositional Dynamic Logic and Concurrency" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18508v1.pdf filename=2403.18508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic logic in the setting of concurrency has proved problematic because of the challenge of capturing interleaving. This challenge stems from the fact that the operational semantics for programs considered in these logics is tailored on trace <b>reasoning</b> for sequential programs. In this work, we generalise propositional dynamic logic (PDL) to a logic framework we call operational propositional dynamic logic (OPDL) in which we are able to reason on sets of programs provided with arbitrary operational semantics. We prove cut-elimination and adequacy of a sequent calculus for PDL and we extend these results to OPDL. We conclude by discussing OPDL for Milner&rsquo;s CCS and Choreographic Programming.</p></p class="citation"></blockquote><h2 id=csds-5>cs.DS (5)</h2><h3 id=15--304318-on-the-communication-complexity-of-approximate-pattern-matching-tomasz-kociumaka-et-al-2024>(1/5 | 304/318) On the Communication Complexity of Approximate Pattern Matching (Tomasz Kociumaka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomasz Kociumaka, Jakob Nogler, Philip Wellnitz. (2024)<br><strong>On the Communication Complexity of Approximate Pattern Matching</strong><br><button class=copy-to-clipboard title="On the Communication Complexity of Approximate Pattern Matching" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, quant-ph<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18812v1.pdf filename=2403.18812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The decades-old Pattern Matching with Edits problem, given a length-$n$ string $T$ (the text), a length-$m$ string $P$ (the pattern), and a positive integer $k$ (the threshold), asks to list all fragments of $T$ that are at edit distance at most $k$ from $P$. The one-way communication complexity of this problem is the minimum amount of space needed to encode the answer so that it can be retrieved without accessing the input strings $P$ and $T$. The closely related Pattern Matching with Mismatches problem (defined in terms of the Hamming distance instead of the edit distance) is already well understood from the communication complexity perspective: Clifford, Kociumaka, and Porat [SODA 2019] proved that $\Omega(n/m \cdot k \log(m/k))$ bits are necessary and $O(n/m \cdot k\log (m|\Sigma|/k))$ bits are sufficient; the upper bound allows encoding not only the occurrences of $P$ in $T$ with at most $k$ mismatches but also the substitutions needed to make each $k$-mismatch occurrence exact. Despite recent improvements in the running time [Charalampopoulos, Kociumaka, and Wellnitz; FOCS 2020 and 2022], the communication complexity of Pattern Matching with Edits remained unexplored, with a lower bound of $\Omega(n/m \cdot k\log(m/k))$ bits and an upper bound of $O(n/m \cdot k^3\log m)$ bits <b>stemming</b> from previous research. In this work, we prove an upper bound of $O(n/m \cdot k \log^2 m)$ bits, thus establishing the optimal communication complexity up to logarithmic factors. We also show that $O(n/m \cdot k \log m \log (m|\Sigma|))$ bits allow encoding, for each $k$-error occurrence of $P$ in $T$, the shortest sequence of edits needed to make the occurrence exact. We leverage the techniques behind our new result on the communication complexity to obtain quantum algorithms for Pattern Matching with Edits.</p></p class="citation"></blockquote><h3 id=25--305318-hypergraph-unreliability-in-quasi-polynomial-time-ruoxu-cen-et-al-2024>(2/5 | 305/318) Hypergraph Unreliability in Quasi-Polynomial Time (Ruoxu Cen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoxu Cen, Jason Li, Debmalya Panigrahi. (2024)<br><strong>Hypergraph Unreliability in Quasi-Polynomial Time</strong><br><button class=copy-to-clipboard title="Hypergraph Unreliability in Quasi-Polynomial Time" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18781v1.pdf filename=2403.18781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The hypergraph unreliability problem asks for the probability that a hypergraph gets disconnected when every hyperedge fails independently with a given probability. For <b>graphs,</b> the unreliability problem has been studied over many decades, and multiple fully polynomial-time approximation schemes are known starting with the work of Karger (STOC 1995). In contrast, prior to this work, no non-trivial result was known for hypergraphs (of arbitrary rank). In this paper, we give quasi-polynomial time approximation schemes for the hypergraph unreliability problem. For any fixed $\varepsilon \in (0, 1)$, we first give a $(1+\varepsilon)$-approximation algorithm that runs in $m^{O(\log n)}$ time on an $m$-hyperedge, $n$-vertex hypergraph. Then, we improve the running time to $m\cdot n^{O(\log^2 n)}$ with an additional exponentially small additive term in the approximation.</p></p class="citation"></blockquote><h3 id=35--306318-new-graph-and-hypergraph-container-lemmas-with-applications-in-property-testing-eric-blais-et-al-2024>(3/5 | 306/318) New Graph and Hypergraph Container Lemmas with Applications in Property Testing (Eric Blais et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Blais, Cameron Seth. (2024)<br><strong>New Graph and Hypergraph Container Lemmas with Applications in Property Testing</strong><br><button class=copy-to-clipboard title="New Graph and Hypergraph Container Lemmas with Applications in Property Testing" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18777v1.pdf filename=2403.18777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>graph</b> and hypergraph container methods are powerful tools with a wide range of applications across combinatorics. Recently, Blais and Seth (FOCS 2023) showed that the <b>graph</b> container method is particularly well-suited for the analysis of the natural canonical tester for two fundamental <b>graph</b> properties: having a large independent set and $k$-colorability. In this work, we show that the connection between the container method and property testing extends further along two different directions. First, we show that the container method can be used to analyze the canonical tester for many other properties of <b>graphs</b> and hypergraphs. We introduce a new hypergraph container lemma and use it to give an upper bound of $\widetilde{O}(kq^3/\epsilon)$ on the sample complexity of $\epsilon$-testing satisfiability, where $q$ is the number of variables per constraint and $k$ is the size of the alphabet. This is the first upper bound for the problem that is polynomial in all of $k$, $q$ and $1/\epsilon$. As a corollary, we get new upper bounds on the sample complexity of the canonical testers for hypergraph colorability and for every semi-homogeneous <b>graph</b> partition property. Second, we show that the container method can also be used to study the query complexity of (non-canonical) <b>graph</b> property testers. This result is obtained by introducing a new container lemma for the class of all independent set stars, a strict superset of the class of all independent sets. We use this container lemma to give a new upper bound of $\widetilde{O}(\rho^5/\epsilon^{7/2})$ on the query complexity of $\epsilon$-testing the $\rho$-independent set property. This establishes for the first time the non-optimality of the canonical tester for a non-homogeneous <b>graph</b> partition property.</p></p class="citation"></blockquote><h3 id=45--307318-realizing-temporal-transportation-trees-george-b-mertzios-et-al-2024>(4/5 | 307/318) Realizing temporal transportation trees (George B. Mertzios et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George B. Mertzios, Hendrik Molter, Paul G. Spirakis. (2024)<br><strong>Realizing temporal transportation trees</strong><br><button class=copy-to-clipboard title="Realizing temporal transportation trees" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18513v1.pdf filename=2403.18513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the complexity of the \textit{periodic temporal <b>graph</b> realization} problem with respect to upper bounds on the fastest path durations among its vertices. This constraint with respect to upper bounds appears naturally in transportation network design applications where, for example, a road network is given, and the goal is to appropriately schedule periodic travel routes, while not exceeding some desired upper bounds on the travel times. This approach is in contrast to verification applications of the <b>graph</b> realization problems, where exact values for the distances (respectively, fastest travel times) are given, following some kind of precise measurement. In our work, we focus only on underlying tree topologies, which are fundamental in many transportation network applications. As it turns out, the periodic upper-bounded temporal tree realization problem (TTR) has a very different computational complexity behavior than both (i) the classic <b>graph</b> realization problem with respect to shortest path distances in static <b>graphs</b> and (ii) the periodic temporal <b>graph</b> realization problem with exact given fastest travel times (which was recently introduced). First, we prove that, surprisingly, TTR is NP-hard, even for a constant period $\Delta$ and when the input tree $G$ satisfies at least one of the following conditions: (a) $G$ has a constant diameter, or (b) $G$ has constant maximum degree. In contrast, when we are given exact values of the fastest travel delays, the problem is known to be solvable in polynomial time. Second, we prove that TTR is fixed-parameter tractable (FPT) with respect to the number of leaves in the input tree $G$, via a novel combination of techniques for totally unimodular matrices and mixed integer linear programming.</p></p class="citation"></blockquote><h3 id=55--308318-minimum-sum-vertex-cover-kernelization-and-parameterized-algorithms-yixin-cao-et-al-2024>(5/5 | 308/318) Minimum sum vertex cover: kernelization and parameterized algorithms (Yixin Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixin Cao, Jingyi Liu, Jianxin Wang. (2024)<br><strong>Minimum sum vertex cover: kernelization and parameterized algorithms</strong><br><button class=copy-to-clipboard title="Minimum sum vertex cover: kernelization and parameterized algorithms" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18497v1.pdf filename=2403.18497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given an ordering of the vertices of a <b>graph,</b> the cost of covering an edge is the smaller number of its two ends. The minimum sum vertex cover problem asks for an ordering that minimizes the total cost of covering all edges. We consider parameterized complexity of this problem, using the largest cost~$k$ of covering a single edge as the parameter. Note that the first $k$ vertices form a (not necessarily minimal) vertex cover of the <b>graph,</b> and ordering of vertices after $k$ is irrelevant. We present a $(2k^2 + 3k)$-vertex kernel and an $O(|E(G)| + 2^kk! k^4)$-time algorithm for the minimum sum vertex cover problem.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--309318-constraintflow-a-dsl-for-specification-and-verification-of-neural-network-analyses-avaljot-singh-et-al-2024>(1/1 | 309/318) ConstraintFlow: A DSL for Specification and Verification of Neural Network Analyses (Avaljot Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avaljot Singh, Yasmin Sarita, Charith Mendis, Gagandeep Singh. (2024)<br><strong>ConstraintFlow: A DSL for Specification and Verification of Neural Network Analyses</strong><br><button class=copy-to-clipboard title="ConstraintFlow: A DSL for Specification and Verification of Neural Network Analyses" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18729v1.pdf filename=2403.18729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The uninterpretability of DNNs hinders their deployment to safety-critical applications. Recent works have shown that Abstract-Interpretation-based formal certification techniques provide promising avenues for building trust in DNNs to some extent. The intricate mathematical background of Abstract Interpretation poses two challenges: (i) easily designing the algorithms that capture the intricate DNN behavior by balancing cost vs. precision tradeoff, and (ii) maintaining the over-approximation-based soundness of these certifiers. General-purpose programming languages like C++ provide extensive functionality, however, verifying the soundness of the algorithms written in them can be impractical. The most commonly used DNN certification libraries like auto_LiRPA and ERAN prove the correctness of their analyses. However, they consist of only a few hard-coded abstract domains and abstract <b>transformers</b> (or transfer functions) and do not allow the user to define new analyses. Further, these libraries can handle only specific DNN architectures. To address these issues, we develop a declarative DSL &ndash; ConstraintFlow &ndash; that can be used to specify Abstract Interpretation-based DNN certifiers. In ConstraintFlow, programmers can easily define various existing and new abstract domains and <b>transformers,</b> all within just a few 10s of Lines of Code as opposed to 1000s of LOCs of existing libraries. We also provide lightweight automatic verification, which can be used to ensure the over-approximation-based soundness of the certifier code written in ConstraintFlow for arbitrary (but bounded) DNN architectures. Using this automated verification procedure, for the first time, we can verify the soundness of state-of-the-art DNN certifiers for arbitrary DNN architectures, all within a few minutes. We prove the soundness of our verification procedure and the completeness of a subset of ConstraintFlow.</p></p class="citation"></blockquote><h2 id=statml-6>stat.ML (6)</h2><h3 id=16--310318-neural-network-based-piecewise-survival-models-olov-holmer-et-al-2024>(1/6 | 310/318) Neural Network-Based Piecewise Survival Models (Olov Holmer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olov Holmer, Erik Frisk, Mattias Krysander. (2024)<br><strong>Neural Network-Based Piecewise Survival Models</strong><br><button class=copy-to-clipboard title="Neural Network-Based Piecewise Survival Models" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, cs-SY, eess-SY, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18664v1.pdf filename=2403.18664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a family of neural network-based survival models is presented. The models are specified based on piecewise definitions of the hazard function and the density function on a partitioning of the time; both constant and linear piecewise definitions are presented, resulting in a family of four models. The models can be seen as an extension of the commonly used <b>discrete-time</b> <b>and</b> piecewise exponential models and thereby add flexibility to this set of standard models. Using a simulated dataset the models are shown to perform well compared to the highly expressive, state-of-the-art energy-based model, while only requiring a fraction of the computation time.</p></p class="citation"></blockquote><h3 id=26--311318-supervised-multiple-kernel-learning-approaches-for-multi-omics-data-integration-mitja-briscik-et-al-2024>(2/6 | 311/318) Supervised Multiple Kernel Learning approaches for multi-omics data integration (Mitja Briscik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mitja Briscik, Gabriele Tazza, Marie-Agnes Dillies, László Vidács, Sébastien Dejean. (2024)<br><strong>Supervised Multiple Kernel Learning approaches for multi-omics data integration</strong><br><button class=copy-to-clipboard title="Supervised Multiple Kernel Learning approaches for multi-omics data integration" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-AP, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18355v1.pdf filename=2403.18355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in high-throughput technologies have originated an ever-increasing availability of omics datasets. The integration of multiple heterogeneous data sources is currently an issue for biology and bioinformatics. Multiple kernel learning (MKL) has shown to be a flexible and valid approach to consider the diverse nature of multi-omics inputs, despite being an underused tool in genomic data mining.We provide novel MKL approaches based on different kernel fusion strategies.To learn from the meta-kernel of input kernels, we adaptedunsupervised integration algorithms for <b>supervised</b> tasks with support vector machines.We also tested deep learning architectures for kernel fusion and classification.The results show that MKL-based models can compete with more complex, state-of-the-art, <b>supervised</b> multi-omics integrative approaches. Multiple kernel learning offers a natural framework for predictive models in multi-omics genomic data. Our results offer a direction for bio-data mining research and further development of methods for heterogeneous data integration.</p></p class="citation"></blockquote><h3 id=36--312318-minimax-optimal-fair-classification-with-bounded-demographic-disparity-xianli-zeng-et-al-2024>(3/6 | 312/318) Minimax Optimal Fair Classification with Bounded Demographic Disparity (Xianli Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianli Zeng, Guang Cheng, Edgar Dobriban. (2024)<br><strong>Minimax Optimal Fair Classification with Bounded Demographic Disparity</strong><br><button class=copy-to-clipboard title="Minimax Optimal Fair Classification with Bounded Demographic Disparity" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CY, cs-LG, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18216v1.pdf filename=2403.18216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mitigating the disparate impact of statistical machine learning methods is crucial for ensuring <b>fairness.</b> While extensive research aims to reduce disparity, the effect of using a \emph{finite dataset} &ndash; as opposed to the entire population &ndash; remains unclear. This paper explores the statistical foundations of fair binary classification with two protected groups, focusing on controlling demographic disparity, defined as the difference in acceptance rates between the groups. Although <b>fairness</b> may come at the cost of accuracy even with infinite data, we show that using a finite sample incurs additional costs due to the need to estimate group-specific acceptance thresholds. We study the minimax optimal classification error while constraining demographic disparity to a user-specified threshold. To quantify the impact of <b>fairness</b> constraints, we introduce a novel measure called \emph{fairness-aware excess risk} and derive a minimax lower bound on this measure that all classifiers must satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding method with an offset that we show attains the minimax lower bound. Our lower bound proofs involve several innovations. Experiments support that FairBayes-DDP+ controls disparity at the user-specified level, while being faster and having a more favorable <b>fairness-accuracy</b> tradeoff than several baselines.</p></p class="citation"></blockquote><h3 id=46--313318-steingen-generating-fidelitous-and-diverse-graph-samples-gesine-reinert-et-al-2024>(4/6 | 313/318) SteinGen: Generating Fidelitous and Diverse Graph Samples (Gesine Reinert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gesine Reinert, Wenkai Xu. (2024)<br><strong>SteinGen: Generating Fidelitous and Diverse Graph Samples</strong><br><button class=copy-to-clipboard title="SteinGen: Generating Fidelitous and Diverse Graph Samples" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18578v1.pdf filename=2403.18578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating <b>graphs</b> that preserve characteristic structures while promoting sample diversity can be challenging, especially when the number of <b>graph</b> observations is small. Here, we tackle the problem of <b>graph</b> generation from only one observed <b>graph.</b> The classical approach of <b>graph</b> generation from parametric models relies on the estimation of parameters, which can be inconsistent or expensive to compute due to intractable normalisation constants. Generative modelling based on machine learning techniques to generate high-quality <b>graph</b> samples avoids parameter estimation but usually requires abundant training samples. Our proposed generating procedure, SteinGen, which is phrased in the setting of <b>graphs</b> as realisations of exponential random <b>graph</b> models, combines ideas from Stein&rsquo;s method and MCMC by employing Markovian dynamics which are based on a Stein operator for the target model. SteinGen uses the Glauber dynamics associated with an estimated Stein operator to generate a sample, and re-estimates the Stein operator from the sample after every sampling step. We show that on a class of exponential random <b>graph</b> models this novel &ldquo;estimation and re-estimation&rdquo; generation strategy yields high distributional similarity (high fidelity) to the original data, combined with high sample diversity.</p></p class="citation"></blockquote><h3 id=56--314318-skscope-fast-sparsity-constrained-optimization-in-python-zezhi-wang-et-al-2024>(5/6 | 314/318) skscope: Fast Sparsity-Constrained Optimization in Python (Zezhi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zezhi Wang, Jin Zhu, Peng Chen, Huiyang Peng, Xiaoke Zhang, Anran Wang, Yu Zheng, Junxian Zhu, Xueqin Wang. (2024)<br><strong>skscope: Fast Sparsity-Constrained Optimization in Python</strong><br><button class=copy-to-clipboard title="skscope: Fast Sparsity-Constrained Optimization in Python" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-CO, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18540v1.pdf filename=2403.18540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Applying iterative solvers on sparsity-constrained optimization (SCO) requires tedious mathematical deduction and careful programming/debugging that hinders these solvers&rsquo; broad impact. In the paper, the library skscope is introduced to overcome such an obstacle. With skscope, users can solve the SCO by just programming the objective function. The convenience of skscope is demonstrated through two examples in the paper, where sparse linear regression and trend filtering are addressed with just four lines of code. More importantly, skscope&rsquo;s efficient implementation allows state-of-the-art solvers to quickly attain the sparse solution regardless of the high dimensionality of parameter space. Numerical experiments reveal the available solvers in skscope can achieve up to 80x speedup on the competing relaxation solutions obtained via the <b>benchmarked</b> convex solver. skscope is published on the Python Package Index (PyPI) and Conda, and its source code is available at: <a href=https://github.com/abess-team/skscope>https://github.com/abess-team/skscope</a>.</p></p class="citation"></blockquote><h3 id=66--315318-clustering-change-sign-detection-by-fusing-mixture-complexity-kento-urano-et-al-2024>(6/6 | 315/318) Clustering Change Sign Detection by Fusing Mixture Complexity (Kento Urano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kento Urano, Ryo Yuki, Kenji Yamanishi. (2024)<br><strong>Clustering Change Sign Detection by Fusing Mixture Complexity</strong><br><button class=copy-to-clipboard title="Clustering Change Sign Detection by Fusing Mixture Complexity" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-IT, cs-LG, math-IT, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18269v1.pdf filename=2403.18269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an early detection method for cluster structural changes. Cluster structure refers to discrete structural characteristics, such as the number of clusters, when data are represented using finite mixture models, such as Gaussian mixture models. We focused on scenarios in which the cluster structure gradually changed over time. For finite mixture models, the concept of mixture complexity (MC) measures the continuous cluster size by considering the cluster proportion bias and overlap between clusters. In this paper, we propose MC fusion as an extension of MC to handle situations in which multiple mixture numbers are possible in a finite mixture model. By incorporating the fusion of multiple models, our approach accurately captured the cluster structure during transitional periods of gradual change. Moreover, we introduce a method for detecting changes in the cluster structure by examining the transition of MC fusion. We demonstrate the effectiveness of our method through empirical analysis using both artificial and real-world datasets.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--316318-stability-properties-of-the-impulsive-goodwins-oscillator-in-1-cycle-anton-v-proskurnikov-et-al-2024>(1/1 | 316/318) Stability Properties of the Impulsive Goodwin&rsquo;s Oscillator in 1-cycle (Anton V. Proskurnikov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anton V. Proskurnikov, Alexander Medvedev. (2024)<br><strong>Stability Properties of the Impulsive Goodwin&rsquo;s Oscillator in 1-cycle</strong><br><button class=copy-to-clipboard title="Stability Properties of the Impulsive Goodwin's Oscillator in 1-cycle" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18557v1.pdf filename=2403.18557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Impulsive Goodwin&rsquo;s Oscillator (IGO) is a mathematical model of a hybrid closed-loop system. It arises by closing a special kind of continuous linear positive time-invariant system with impulsive feedback, which employs both amplitude and frequency pulse modulation. The structure of IGO precludes the existence of equilibria, and all its solutions are oscillatory. With its origin in mathematical biology, the IGO also presents a control paradigm useful in a wide range of applications, in particular dosing of chemicals and medicines. Since the pulse modulation feedback mechanism introduces significant nonlinearity and non-smoothness in the closedloop dynamics, conventional controller design methods fail to apply. However, the hybrid dynamics of IGO reduce to a nonlinear, time-invariant <b>discrete-time</b> <b>system,</b> exhibiting a one-to-one correspondence between periodic solutions of the original IGO and those of the <b>discrete-time</b> <b>system.</b> The paper proposes a design approach that leverages the linearization of the equivalent <b>discrete-time</b> <b>dynamics</b> in the vicinity of a fixed point. A simple and efficient local stability condition of the 1-cycle in terms of the characteristics of the amplitude and frequency modulation functions is obtained.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--317318-a-census-of-graph-drawing-algorithms-based-on-generalized-transversal-structures-olivier-bernardi-et-al-2024>(1/1 | 317/318) A census of graph-drawing algorithms based on generalized transversal structures (Olivier Bernardi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olivier Bernardi, Éric Fusy, Shizhe Liang. (2024)<br><strong>A census of graph-drawing algorithms based on generalized transversal structures</strong><br><button class=copy-to-clipboard title="A census of graph-drawing algorithms based on generalized transversal structures" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-CG, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18980v1.pdf filename=2403.18980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We define <b>graph</b> drawing algorithms which simultaneously generalize several classical ones. More precisely, we consider the following algorithms: (a) Fusy&rsquo;s algorithm for the straight-line grid drawing of planar triangulations, based on transversal structures, (b) Barri`ere and Huemmer&rsquo;s algorithm for the straight-line grid drawing of planar quadrangulations, based on separating decompositions, (c) He&rsquo;s algorithm for the orthogonal drawing of 3-valent planar maps, based on transversal structures, (d) Bernardi & Fusy &rsquo;s algorithm for the orthogonal drawing of 4-valent planar maps, based on 2-orientations. We present an algorithm generalizing (a) and (b) which produces a straight line grid drawing for planar maps with faces of degree at most 4, and we present an algorithm generalizing (c) and (d) which produces an orthogonal drawing for planar maps with vertices of degree at most 4. Our two algorithms are based on a class of combinatorial structures called grand-Schnyder woods.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--318318-instructbrush-learning-attention-based-instruction-optimization-for-image-editing-ruoyu-zhao-et-al-2024>(1/1 | 318/318) InstructBrush: Learning Attention-based Instruction Optimization for Image Editing (Ruoyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoyu Zhao, Qingnan Fan, Fei Kou, Shuai Qin, Hong Gu, Wei Wu, Pengcheng Xu, Mingrui Zhu, Nannan Wang, Xinbo Gao. (2024)<br><strong>InstructBrush: Learning Attention-based Instruction Optimization for Image Editing</strong><br><button class=copy-to-clipboard title="InstructBrush: Learning Attention-based Instruction Optimization for Image Editing" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-CV, cs-GR, cs.GR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18660v1.pdf filename=2403.18660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, instruction-based image editing methods have garnered significant attention in image editing. However, despite encompassing a wide range of editing priors, these methods are helpless when handling editing tasks that are challenging to accurately describe through language. We propose InstructBrush, an inversion method for instruction-based image editing methods to bridge this gap. It extracts editing effects from exemplar image pairs as editing instructions, which are further applied for image editing. Two key techniques are introduced into InstructBrush, Attention-based Instruction Optimization and Transformation-oriented Instruction Initialization, to address the limitations of the previous method in terms of inversion effects and instruction generalization. To explore the ability of instruction inversion methods to guide image editing in open scenarios, we establish a TransformationOriented Paired <b>Benchmark</b> (TOP-Bench), which contains a rich set of scenes and editing types. The creation of this <b>benchmark</b> paves the way for further exploration of instruction inversion. Quantitatively and qualitatively, our approach achieves superior performance in editing and is more semantically consistent with the target editing effects.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.28</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.30</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-42>cs.CL (42)</a><ul><li><a href=#142--1318-evaluating-large-language-models-for-health-related-text-classification-tasks-with-public-social-media-data-yuting-guo-et-al-2024>(1/42 | 1/318) Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data (Yuting Guo et al., 2024)</a></li><li><a href=#242--2318-reshaping-free-text-radiology-notes-into-structured-reports-with-generative-transformers-laura-bergomi-et-al-2024>(2/42 | 2/318) Reshaping Free-Text Radiology Notes Into Structured Reports With Generative Transformers (Laura Bergomi et al., 2024)</a></li><li><a href=#342--3318-long-form-factuality-in-large-language-models-jerry-wei-et-al-2024>(3/42 | 3/318) Long-form factuality in large language models (Jerry Wei et al., 2024)</a></li><li><a href=#442--4318-is-modularity-transferable-a-case-study-through-the-lens-of-knowledge-distillation-mateusz-klimaszewski-et-al-2024>(4/42 | 4/318) Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation (Mateusz Klimaszewski et al., 2024)</a></li><li><a href=#542--5318-blade-enhancing-black-box-large-language-models-with-small-domain-specific-models-haitao-li-et-al-2024>(5/42 | 5/318) BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models (Haitao Li et al., 2024)</a></li><li><a href=#642--6318-quantifying-and-mitigating-unimodal-biases-in-multimodal-large-language-models-a-causal-perspective-meiqi-chen-et-al-2024>(6/42 | 6/318) Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective (Meiqi Chen et al., 2024)</a></li><li><a href=#742--7318-cause-counterfactual-assessment-of-user-satisfaction-estimation-in-task-oriented-dialogue-systems-amin-abolghasemi-et-al-2024>(7/42 | 7/318) CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems (Amin Abolghasemi et al., 2024)</a></li><li><a href=#842--8318-dual-instruction-tuning-with-large-language-models-for-mathematical-reasoning-yongwei-zhou-et-al-2024>(8/42 | 8/318) Dual Instruction Tuning with Large Language Models for Mathematical Reasoning (Yongwei Zhou et al., 2024)</a></li><li><a href=#942--9318-md-pk-metaphor-detection-via-prompt-learning-and-knowledge-distillation-kaidi-jia-et-al-2024>(9/42 | 9/318) MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation (Kaidi Jia et al., 2024)</a></li><li><a href=#1042--10318-exploring-the-deceptive-power-of-llm-generated-fake-news-a-study-of-real-world-detection-challenges-yanshen-sun-et-al-2024>(10/42 | 10/318) Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges (Yanshen Sun et al., 2024)</a></li><li><a href=#1142--11318-can-language-beat-numerical-regression-language-based-multimodal-trajectory-prediction-inhwan-bae-et-al-2024>(11/42 | 11/318) Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction (Inhwan Bae et al., 2024)</a></li><li><a href=#1242--12318-projective-methods-for-mitigating-gender-bias-in-pre-trained-language-models-hillary-dawkins-et-al-2024>(12/42 | 12/318) Projective Methods for Mitigating Gender Bias in Pre-trained Language Models (Hillary Dawkins et al., 2024)</a></li><li><a href=#1342--13318-a-survey-on-large-language-models-from-concept-to-implementation-chen-wang-et-al-2024>(13/42 | 13/318) A Survey on Large Language Models from Concept to Implementation (Chen Wang et al., 2024)</a></li><li><a href=#1442--14318-acted-automatic-acquisition-of-typical-event-duration-for-semi-supervised-temporal-commonsense-qa-felix-virgo-et-al-2024>(14/42 | 14/318) AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA (Felix Virgo et al., 2024)</a></li><li><a href=#1542--15318-semrode-macro-adversarial-training-to-learn-representations-that-are-robust-to-word-level-attacks-brian-formento-et-al-2024>(15/42 | 15/318) SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks (Brian Formento et al., 2024)</a></li><li><a href=#1642--16318-biomedlm-a-27b-parameter-language-model-trained-on-biomedical-text-elliot-bolton-et-al-2024>(16/42 | 16/318) BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text (Elliot Bolton et al., 2024)</a></li><li><a href=#1742--17318-zaebuc-spoken-a-multilingual-multidialectal-arabic-english-speech-corpus-injy-hamed-et-al-2024>(17/42 | 17/318) ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus (Injy Hamed et al., 2024)</a></li><li><a href=#1842--18318-iteralign-iterative-constitutional-alignment-of-large-language-models-xiusi-chen-et-al-2024>(18/42 | 18/318) IterAlign: Iterative Constitutional Alignment of Large Language Models (Xiusi Chen et al., 2024)</a></li><li><a href=#1942--19318-sorry-come-again-prompting----enhancing-comprehension-and-diminishing-hallucination-with-pause-injected-optimal-paraphrasing-vipula-rawte-et-al-2024>(19/42 | 19/318) &lsquo;Sorry, Come Again?&rsquo; Prompting &ndash; Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing (Vipula Rawte et al., 2024)</a></li><li><a href=#2042--20318-fact-checking-beyond-training-set-payam-karisani-et-al-2024>(20/42 | 20/318) Fact Checking Beyond Training Set (Payam Karisani et al., 2024)</a></li><li><a href=#2142--21318-nl-iti-optimizing-probing-and-intervention-for-improvement-of-iti-method-jakub-hoscilowicz-et-al-2024>(21/42 | 21/318) NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method (Jakub Hoscilowicz et al., 2024)</a></li><li><a href=#2242--22318-evaluation-of-semantic-search-and-its-role-in-retrieved-augmented-generation-rag-for-arabic-language-ali-mahboub-et-al-2024>(22/42 | 22/318) Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language (Ali Mahboub et al., 2024)</a></li><li><a href=#2342--23318-a-novel-corpus-of-annotated-medical-imaging-reports-and-information-extraction-results-using-bert-based-language-models-namu-park-et-al-2024>(23/42 | 23/318) A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models (Namu Park et al., 2024)</a></li><li><a href=#2442--24318-sdsat-accelerating-llm-inference-through-speculative-decoding-with-semantic-adaptive-tokens-chengbo-liu-et-al-2024>(24/42 | 24/318) SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens (Chengbo Liu et al., 2024)</a></li><li><a href=#2542--25318-attention-aware-semantic-relevance-predicting-chinese-sentence-reading-kun-sun-2024>(25/42 | 25/318) Attention-aware semantic relevance predicting Chinese sentence reading (Kun Sun, 2024)</a></li><li><a href=#2642--26318-triviahg-a-dataset-for-automatic-hint-generation-from-factoid-questions-jamshid-mozafari-et-al-2024>(26/42 | 26/318) TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions (Jamshid Mozafari et al., 2024)</a></li><li><a href=#2742--27318-improving-attributed-text-generation-of-large-language-models-via-preference-learning-dongfang-li-et-al-2024>(27/42 | 27/318) Improving Attributed Text Generation of Large Language Models via Preference Learning (Dongfang Li et al., 2024)</a></li><li><a href=#2842--28318-rejection-improves-reliability-training-llms-to-refuse-unknown-questions-using-rl-from-knowledge-feedback-hongshen-xu-et-al-2024>(28/42 | 28/318) Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback (Hongshen Xu et al., 2024)</a></li><li><a href=#2942--29318-reflectsumm-a-benchmark-for-course-reflection-summarization-yang-zhong-et-al-2024>(29/42 | 29/318) ReflectSumm: A Benchmark for Course Reflection Summarization (Yang Zhong et al., 2024)</a></li><li><a href=#3042--30318-measuring-political-bias-in-large-language-models-what-is-said-and-how-it-is-said-yejin-bang-et-al-2024>(30/42 | 30/318) Measuring Political Bias in Large Language Models: What Is Said and How It Is Said (Yejin Bang et al., 2024)</a></li><li><a href=#3142--31318-capability-aware-prompt-reformulation-learning-for-text-to-image-generation-jingtao-zhan-et-al-2024>(31/42 | 31/318) Capability-aware Prompt Reformulation Learning for Text-to-Image Generation (Jingtao Zhan et al., 2024)</a></li><li><a href=#3242--32318-checkeval-robust-evaluation-framework-using-large-language-model-via-checklist-yukyung-lee-et-al-2024>(32/42 | 32/318) CheckEval: Robust Evaluation Framework using Large Language Model via Checklist (Yukyung Lee et al., 2024)</a></li><li><a href=#3342--33318-the-invalsi-benchmark-measuring-language-models-mathematical-and-language-understanding-in-italian-andrea-esuli-et-al-2024>(33/42 | 33/318) The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian (Andrea Esuli et al., 2024)</a></li><li><a href=#3442--34318-conformal-intent-classification-and-clarification-for-fast-and-accurate-intent-recognition-floris-den-hengst-et-al-2024>(34/42 | 34/318) Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition (Floris den Hengst et al., 2024)</a></li><li><a href=#3542--35318-semeval-task-1-semantic-textual-relatedness-for-african-and-asian-languages-nedjma-ousidhoum-et-al-2024>(35/42 | 35/318) SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages (Nedjma Ousidhoum et al., 2024)</a></li><li><a href=#3642--36318-improved-neural-protoform-reconstruction-via-reflex-prediction-liang-lu-et-al-2024>(36/42 | 36/318) Improved Neural Protoform Reconstruction via Reflex Prediction (Liang Lu et al., 2024)</a></li><li><a href=#3742--37318-debiasing-sentence-embedders-through-contrastive-word-pairs-philip-kenneweg-et-al-2024>(37/42 | 37/318) Debiasing Sentence Embedders through Contrastive Word Pairs (Philip Kenneweg et al., 2024)</a></li><li><a href=#3842--38318-can-llms-converse-formally-automatically-assessing-llms-in-translating-and-interpreting-formal-specifications-rushang-karia-et-al-2024>(38/42 | 38/318) Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications (Rushang Karia et al., 2024)</a></li><li><a href=#3942--39318-few-shot-recalibration-of-language-models-xiang-lisa-li-et-al-2024>(39/42 | 39/318) Few-Shot Recalibration of Language Models (Xiang Lisa Li et al., 2024)</a></li><li><a href=#4042--40318-blendx-complex-multi-intent-detection-with-blended-patterns-yejin-yoon-et-al-2024>(40/42 | 40/318) BlendX: Complex Multi-Intent Detection with Blended Patterns (Yejin Yoon et al., 2024)</a></li><li><a href=#4142--41318-chinese-offensive-language-detectioncurrent-status-and-future-directions-yunze-xiao-et-al-2024>(41/42 | 41/318) Chinese Offensive Language Detection:Current Status and Future Directions (Yunze Xiao et al., 2024)</a></li><li><a href=#4242--42318-since-the-scientific-literature-is-multilingual-our-models-should-be-too-abteen-ebrahimi-et-al-2024>(42/42 | 42/318) Since the Scientific Literature Is Multilingual, Our Models Should Be Too (Abteen Ebrahimi et al., 2024)</a></li></ul></li><li><a href=#csir-18>cs.IR (18)</a><ul><li><a href=#118--43318-to-recommend-or-not-recommendability-identification-in-conversations-with-pre-trained-language-models-zhefan-wang-et-al-2024>(1/18 | 43/318) To Recommend or Not: Recommendability Identification in Conversations with Pre-trained Language Models (Zhefan Wang et al., 2024)</a></li><li><a href=#218--44318-towards-llm-recsys-alignment-with-textual-id-learning-juntao-tan-et-al-2024>(2/18 | 44/318) Towards LLM-RecSys Alignment with Textual ID Learning (Juntao Tan et al., 2024)</a></li><li><a href=#318--45318-sequential-recommendation-with-latent-relations-based-on-large-language-model-shenghao-yang-et-al-2024>(3/18 | 45/318) Sequential Recommendation with Latent Relations based on Large Language Model (Shenghao Yang et al., 2024)</a></li><li><a href=#418--46318-lightweight-embeddings-for-graph-collaborative-filtering-xurong-liang-et-al-2024>(4/18 | 46/318) Lightweight Embeddings for Graph Collaborative Filtering (Xurong Liang et al., 2024)</a></li><li><a href=#518--47318-scaling-laws-for-dense-retrieval-yan-fang-et-al-2024>(5/18 | 47/318) Scaling Laws For Dense Retrieval (Yan Fang et al., 2024)</a></li><li><a href=#618--48318-common-sense-enhanced-knowledge-based-recommendation-with-large-language-model-shenghao-yang-et-al-2024>(6/18 | 48/318) Common Sense Enhanced Knowledge-based Recommendation with Large Language Model (Shenghao Yang et al., 2024)</a></li><li><a href=#718--49318-a-novel-behavior-based-recommendation-system-for-e-commerce-reza-barzegar-nozari-et-al-2024>(7/18 | 49/318) A Novel Behavior-Based Recommendation System for E-commerce (Reza Barzegar Nozari et al., 2024)</a></li><li><a href=#818--50318-rankmamba-benchmarking-mambas-document-ranking-performance-in-the-era-of-transformers-zhichao-xu-2024>(8/18 | 50/318) RankMamba, Benchmarking Mamba&rsquo;s Document Ranking Performance in the Era of Transformers (Zhichao Xu, 2024)</a></li><li><a href=#918--51318-modeling-sustainable-city-trips-integrating-co2-emissions-popularity-and-seasonality-into-tourism-recommender-systems-ashmi-banerjee-et-al-2024>(9/18 | 51/318) Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity, and Seasonality into Tourism Recommender Systems (Ashmi Banerjee et al., 2024)</a></li><li><a href=#1018--52318-improving-content-recommendation-knowledge-graph-based-semantic-contrastive-learning-for-diversity-and-cold-start-users-yejin-kim-et-al-2024>(10/18 | 52/318) Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users (Yejin Kim et al., 2024)</a></li><li><a href=#1118--53318-enhanced-generative-recommendation-via-content-and-collaboration-integration-yidan-wang-et-al-2024>(11/18 | 53/318) Enhanced Generative Recommendation via Content and Collaboration Integration (Yidan Wang et al., 2024)</a></li><li><a href=#1218--54318-delta-pre-train-a-discriminative-encoder-for-legal-case-retrieval-via-structural-word-alignment-haitao-li-et-al-2024>(12/18 | 54/318) DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via Structural Word Alignment (Haitao Li et al., 2024)</a></li><li><a href=#1318--55318-a-recommender-system-for-nft-collectibles-with-item-feature-minjoo-choi-et-al-2024>(13/18 | 55/318) A Recommender System for NFT Collectibles with Item Feature (Minjoo Choi et al., 2024)</a></li><li><a href=#1418--56318-high-recall-small-data-the-challenges-of-within-system-evaluation-in-a-live-legal-search-system-gineke-wiggers-et-al-2024>(14/18 | 56/318) High Recall, Small Data: The Challenges of Within-System Evaluation in a Live Legal Search System (Gineke Wiggers et al., 2024)</a></li><li><a href=#1518--57318-a-situation-aware-enhancer-for-personalized-recommendation-jiayu-li-et-al-2024>(15/18 | 57/318) A Situation-aware Enhancer for Personalized Recommendation (Jiayu Li et al., 2024)</a></li><li><a href=#1618--58318-decoy-effect-in-search-interaction-understanding-user-behavior-and-measuring-system-vulnerability-nuo-chen-et-al-2024>(16/18 | 58/318) Decoy Effect In Search Interaction: Understanding User Behavior and Measuring System Vulnerability (Nuo Chen et al., 2024)</a></li><li><a href=#1718--59318-improving-out-of-vocabulary-handling-in-recommendation-systems-william-shiao-et-al-2024>(17/18 | 59/318) Improving Out-of-Vocabulary Handling in Recommendation Systems (William Shiao et al., 2024)</a></li><li><a href=#1818--60318-one-backpropagation-in-two-tower-recommendation-models-erjia-chen-et-al-2024>(18/18 | 60/318) One Backpropagation in Two Tower Recommendation Models (Erjia Chen et al., 2024)</a></li></ul></li><li><a href=#csro-20>cs.RO (20)</a><ul><li><a href=#120--61318-physicsassistant-an-llm-powered-interactive-learning-robot-for-physics-lab-investigations-ehsan-latif-et-al-2024>(1/20 | 61/318) PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics Lab Investigations (Ehsan Latif et al., 2024)</a></li><li><a href=#220--62318-3p-llm-probabilistic-path-planning-using-large-language-model-for-autonomous-robot-navigation-ehsan-latif-2024>(2/20 | 62/318) 3P-LLM: Probabilistic Path Planning using Large Language Model for Autonomous Robot Navigation (Ehsan Latif, 2024)</a></li><li><a href=#320--63318-multi-agv-path-planning-method-via-reinforcement-learning-and-particle-filters-shao-shuo-2024>(3/20 | 63/318) Multi-AGV Path Planning Method via Reinforcement Learning and Particle Filters (Shao Shuo, 2024)</a></li><li><a href=#420--64318-vision-based-force-estimation-for-minimally-invasive-telesurgery-through-contact-detection-and-local-stiffness-models-shuyuan-yang-et-al-2024>(4/20 | 64/318) Vision-Based Force Estimation for Minimally Invasive Telesurgery Through Contact Detection and Local Stiffness Models (Shuyuan Yang et al., 2024)</a></li><li><a href=#520--65318-gaussian-process-based-traversability-analysis-for-terrain-mapless-navigation-abe-leininger-et-al-2024>(5/20 | 65/318) Gaussian Process-based Traversability Analysis for Terrain Mapless Navigation (Abe Leininger et al., 2024)</a></li><li><a href=#620--66318-mldt-multi-level-decomposition-for-complex-long-horizon-robotic-task-planning-with-open-source-large-language-model-yike-wu-et-al-2024>(6/20 | 66/318) MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model (Yike Wu et al., 2024)</a></li><li><a href=#720--67318-inverse-kinematics-learning-of-a-continuum-manipulator-using-limited-real-time-data-alok-ranjan-sahoo-et-al-2024>(7/20 | 67/318) Inverse kinematics learning of a continuum manipulator using limited real time data (Alok Ranjan Sahoo et al., 2024)</a></li><li><a href=#820--68318-lord-large-models-based-opposite-reward-design-for-autonomous-driving-xin-ye-et-al-2024>(8/20 | 68/318) LORD: Large Models based Opposite Reward Design for Autonomous Driving (Xin Ye et al., 2024)</a></li><li><a href=#920--69318-cobos-constraint-based-online-scheduler-for-human-robot-collaboration-marina-ionova-et-al-2024>(9/20 | 69/318) CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration (Marina Ionova et al., 2024)</a></li><li><a href=#1020--70318-hyrrt-connect-a-bidirectional-rapidly-exploring-random-trees-motion-planning-algorithm-for-hybrid-systems-nan-wang-et-al-2024>(10/20 | 70/318) HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion Planning Algorithm for Hybrid Systems (Nan Wang et al., 2024)</a></li><li><a href=#1120--71318-uncertainty-aware-deployment-of-pre-trained-language-conditioned-imitation-learning-policies-bo-wu-et-al-2024>(11/20 | 71/318) Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies (Bo Wu et al., 2024)</a></li><li><a href=#1220--72318-online-embedding-multi-scale-clip-features-into-3d-maps-shun-taguchi-et-al-2024>(12/20 | 72/318) Online Embedding Multi-Scale CLIP Features into 3D Maps (Shun Taguchi et al., 2024)</a></li><li><a href=#1320--73318-towards-human-centered-construction-robotics-an-rl-driven-companion-robot-for-contextually-assisting-carpentry-workers-yuning-wu-et-al-2024>(13/20 | 73/318) Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers (Yuning Wu et al., 2024)</a></li><li><a href=#1420--74318-cat-constraints-as-terminations-for-legged-locomotion-reinforcement-learning-elliot-chane-sane-et-al-2024>(14/20 | 74/318) CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning (Elliot Chane-Sane et al., 2024)</a></li><li><a href=#1520--75318-bridging-the-gap-regularized-reinforcement-learning-for-improved-classical-motion-planning-with-safety-modules-elias-goldsztejn-et-al-2024>(15/20 | 75/318) Bridging the Gap: Regularized Reinforcement Learning for Improved Classical Motion Planning with Safety Modules (Elias Goldsztejn et al., 2024)</a></li><li><a href=#1620--76318-imaging-radar-and-lidar-image-translation-for-3-dof-extrinsic-calibration-sangwoo-jung-et-al-2024>(16/20 | 76/318) Imaging radar and LiDAR image translation for 3-DOF extrinsic calibration (Sangwoo Jung et al., 2024)</a></li><li><a href=#1720--77318-robokeygen-robot-pose-and-joint-angles-estimation-via-diffusion-based-3d-keypoint-generation-yang-tian-et-al-2024>(17/20 | 77/318) RoboKeyGen: Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation (Yang Tian et al., 2024)</a></li><li><a href=#1820--78318-preference-based-planning-in-stochastic-environments-from-partially-ordered-temporal-goals-to-most-preferred-policies-hazhar-rahmani-et-al-2024>(18/20 | 78/318) Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies (Hazhar Rahmani et al., 2024)</a></li><li><a href=#1920--79318-sampling-based-motion-planning-with-online-racing-line-generation-for-autonomous-driving-on-three-dimensional-race-tracks-levent-ögretmen-et-al-2024>(19/20 | 79/318) Sampling-Based Motion Planning with Online Racing Line Generation for Autonomous Driving on Three-Dimensional Race Tracks (Levent Ögretmen et al., 2024)</a></li><li><a href=#2020--80318-manipulating-neural-path-planners-via-slight-perturbations-zikang-xiong-et-al-2024>(20/20 | 80/318) Manipulating Neural Path Planners via Slight Perturbations (Zikang Xiong et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--81318-llms-in-hci-data-work-bridging-the-gap-between-information-retrieval-and-responsible-research-practices-neda-taghizadeh-serajeh-et-al-2024>(1/3 | 81/318) LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices (Neda Taghizadeh Serajeh et al., 2024)</a></li><li><a href=#23--82318-iface-hand-over-face-gesture-recognition-leveraging-impedance-sensing-mengxi-liu-et-al-2024>(2/3 | 82/318) iFace: Hand-Over-Face Gesture Recognition Leveraging Impedance Sensing (Mengxi Liu et al., 2024)</a></li><li><a href=#33--83318-will-you-participate-exploring-the-potential-of-robotics-competitions-on-human-centric-topics-yuchong-zhang-et-al-2024>(3/3 | 83/318) Will You Participate? Exploring the Potential of Robotics Competitions on Human-centric Topics (Yuchong Zhang et al., 2024)</a></li></ul></li><li><a href=#cscv-82>cs.CV (82)</a><ul><li><a href=#182--84318-mini-gemini-mining-the-potential-of-multi-modality-vision-language-models-yanwei-li-et-al-2024>(1/82 | 84/318) Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models (Yanwei Li et al., 2024)</a></li><li><a href=#282--85318-an-image-grid-can-be-worth-a-video-zero-shot-video-question-answering-using-a-vlm-wonkyun-kim-et-al-2024>(2/82 | 85/318) An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM (Wonkyun Kim et al., 2024)</a></li><li><a href=#382--86318-illicit-object-detection-in-x-ray-images-using-vision-transformers-jorgen-cani-et-al-2024>(3/82 | 86/318) Illicit object detection in X-ray images using Vision Transformers (Jorgen Cani et al., 2024)</a></li><li><a href=#482--87318-dense-vision-transformer-compression-with-few-samples-hanxiao-zhang-et-al-2024>(4/82 | 87/318) Dense Vision Transformer Compression with Few Samples (Hanxiao Zhang et al., 2024)</a></li><li><a href=#582--88318-learning-cnn-on-vit-a-hybrid-model-to-explicitly-class-specific-boundaries-for-domain-adaptation-ba-hung-ngo-et-al-2024>(5/82 | 88/318) Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation (Ba Hung Ngo et al., 2024)</a></li><li><a href=#682--89318-textcraftor-your-text-encoder-can-be-image-quality-controller-yanyu-li-et-al-2024>(6/82 | 89/318) TextCraftor: Your Text Encoder Can be Image Quality Controller (Yanyu Li et al., 2024)</a></li><li><a href=#782--90318-beyond-embeddings-the-promise-of-visual-table-in-multi-modal-models-yiwu-zhong-et-al-2024>(7/82 | 90/318) Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models (Yiwu Zhong et al., 2024)</a></li><li><a href=#882--91318-vitar-vision-transformer-with-any-resolution-qihang-fan-et-al-2024>(8/82 | 91/318) ViTAR: Vision Transformer with Any Resolution (Qihang Fan et al., 2024)</a></li><li><a href=#982--92318-lita-language-instructed-temporal-localization-assistant-de-an-huang-et-al-2024>(9/82 | 92/318) LITA: Language Instructed Temporal-Localization Assistant (De-An Huang et al., 2024)</a></li><li><a href=#1082--93318-versat2i-improving-text-to-image-models-with-versatile-reward-jianshu-guo-et-al-2024>(10/82 | 93/318) VersaT2I: Improving Text-to-Image Models with Versatile Reward (Jianshu Guo et al., 2024)</a></li><li><a href=#1182--94318-orco-towards-better-generalization-via-orthogonality-and-contrast-for-few-shot-class-incremental-learning-noor-ahmed-et-al-2024>(11/82 | 94/318) OrCo: Towards Better Generalization via Orthogonality and Contrast for Few-Shot Class-Incremental Learning (Noor Ahmed et al., 2024)</a></li><li><a href=#1282--95318-direct-mineral-content-prediction-from-drill-core-images-via-transfer-learning-romana-boiger-et-al-2024>(12/82 | 95/318) Direct mineral content prediction from drill core images via transfer learning (Romana Boiger et al., 2024)</a></li><li><a href=#1382--96318-scaling-vision-and-language-navigation-with-offline-rl-valay-bundele-et-al-2024>(13/82 | 96/318) Scaling Vision-and-Language Navigation With Offline RL (Valay Bundele et al., 2024)</a></li><li><a href=#1482--97318-plot-tal----prompt-learning-with-optimal-transport-for-few-shot-temporal-action-localization-edward-fish-et-al-2024>(14/82 | 97/318) PLOT-TAL &ndash; Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization (Edward Fish et al., 2024)</a></li><li><a href=#1582--98318-objectdrop-bootstrapping-counterfactuals-for-photorealistic-object-removal-and-insertion-daniel-winter-et-al-2024>(15/82 | 98/318) ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion (Daniel Winter et al., 2024)</a></li><li><a href=#1682--99318-deep-learning-for-robust-and-explainable-models-in-computer-vision-mohammadreza-amirian-2024>(16/82 | 99/318) Deep Learning for Robust and Explainable Models in Computer Vision (Mohammadreza Amirian, 2024)</a></li><li><a href=#1782--100318-artifact-reduction-in-3d-and-4d-cone-beam-computed-tomography-images-with-deep-learning----a-review-mohammadreza-amirian-et-al-2024>(17/82 | 100/318) Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images with Deep Learning &ndash; A Review (Mohammadreza Amirian et al., 2024)</a></li><li><a href=#1882--101318-cosalpure-learning-concept-from-group-images-for-robust-co-saliency-detection-jiayi-zhu-et-al-2024>(18/82 | 101/318) CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection (Jiayi Zhu et al., 2024)</a></li><li><a href=#1982--102318-language-plays-a-pivotal-role-in-the-object-attribute-compositional-generalization-of-clip-reza-abbasi-et-al-2024>(19/82 | 102/318) Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP (Reza Abbasi et al., 2024)</a></li><li><a href=#2082--103318-diffstyler-diffusion-based-localized-image-style-transfer-shaoxu-li-2024>(20/82 | 103/318) DiffStyler: Diffusion-based Localized Image Style Transfer (Shaoxu Li, 2024)</a></li><li><a href=#2182--104318-mathrmf2depth-self-supervised-indoor-monocular-depth-estimation-via-optical-flow-consistency-and-feature-map-synthesis-xiaotong-guo-et-al-2024>(21/82 | 104/318) $\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation via Optical Flow Consistency and Feature Map Synthesis (Xiaotong Guo et al., 2024)</a></li><li><a href=#2282--105318-colour-and-brush-stroke-pattern-recognition-in-abstract-art-using-modified-deep-convolutional-generative-adversarial-networks-srinitish-srinivasan-et-al-2024>(22/82 | 105/318) Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks (Srinitish Srinivasan et al., 2024)</a></li><li><a href=#2382--106318-ship-in-sight-diffusion-models-for-ship-image-super-resolution-luigi-sigillo-et-al-2024>(23/82 | 106/318) Ship in Sight: Diffusion Models for Ship-Image Super Resolution (Luigi Sigillo et al., 2024)</a></li><li><a href=#2482--107318-toward-interactive-regional-understanding-in-vision-large-language-models-jungbeom-lee-et-al-2024>(24/82 | 107/318) Toward Interactive Regional Understanding in Vision-Large Language Models (Jungbeom Lee et al., 2024)</a></li><li><a href=#2582--108318-benchmarking-object-detectors-with-coco-a-new-path-forward-shweta-singh-et-al-2024>(25/82 | 108/318) Benchmarking Object Detectors with COCO: A New Path Forward (Shweta Singh et al., 2024)</a></li><li><a href=#2682--109318-middle-fusion-and-multi-stage-multi-form-prompts-for-robust-rgb-t-tracking-qiming-wang-et-al-2024>(26/82 | 109/318) Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T Tracking (Qiming Wang et al., 2024)</a></li><li><a href=#2782--110318-metacap-meta-learning-priors-from-multi-view-imagery-for-sparse-view-human-performance-capture-and-rendering-guoxing-sun-et-al-2024>(27/82 | 110/318) MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering (Guoxing Sun et al., 2024)</a></li><li><a href=#2882--111318-multi-scale-unified-network-for-image-classification-wenzhuo-liu-et-al-2024>(28/82 | 111/318) Multi-scale Unified Network for Image Classification (Wenzhuo Liu et al., 2024)</a></li><li><a href=#2982--112318-duolando-follower-gpt-with-off-policy-reinforcement-learning-for-dance-accompaniment-li-siyao-et-al-2024>(29/82 | 112/318) Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment (Li Siyao et al., 2024)</a></li><li><a href=#3082--113318-rap-retrieval-augmented-planner-for-adaptive-procedure-planning-in-instructional-videos-ali-zare-et-al-2024>(30/82 | 113/318) RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos (Ali Zare et al., 2024)</a></li><li><a href=#3182--114318-singulartrajectory-universal-trajectory-predictor-using-diffusion-model-inhwan-bae-et-al-2024>(31/82 | 114/318) SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model (Inhwan Bae et al., 2024)</a></li><li><a href=#3282--115318-efficient-test-time-adaptation-of-vision-language-models-adilbek-karmanov-et-al-2024>(32/82 | 115/318) Efficient Test-Time Adaptation of Vision-Language Models (Adilbek Karmanov et al., 2024)</a></li><li><a href=#3382--116318-image-deraining-via-self-supervised-reinforcement-learning-he-hao-liao-et-al-2024>(33/82 | 116/318) Image Deraining via Self-supervised Reinforcement Learning (He-Hao Liao et al., 2024)</a></li><li><a href=#3482--117318-garment3dgen-3d-garment-stylization-and-texture-generation-nikolaos-sarafianos-et-al-2024>(34/82 | 117/318) Garment3DGen: 3D Garment Stylization and Texture Generation (Nikolaos Sarafianos et al., 2024)</a></li><li><a href=#3582--118318-ecodepth-effective-conditioning-of-diffusion-models-for-monocular-depth-estimation-suraj-patni-et-al-2024>(35/82 | 118/318) ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation (Suraj Patni et al., 2024)</a></li><li><a href=#3682--119318-i2ckd--intra--and-inter-class-knowledge-distillation-for-semantic-segmentation-ayoub-karine-et-al-2024>(36/82 | 119/318) I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic Segmentation (Ayoub Karine et al., 2024)</a></li><li><a href=#3782--120318-doda-diffusion-for-object-detection-domain-adaptation-in-agriculture-shuai-xiang-et-al-2024>(37/82 | 120/318) DODA: Diffusion for Object-detection Domain Adaptation in Agriculture (Shuai Xiang et al., 2024)</a></li><li><a href=#3882--121318-unleashing-the-potential-of-sam-for-medical-adaptation-via-hierarchical-decoding-zhiheng-cheng-et-al-2024>(38/82 | 121/318) Unleashing the Potential of SAM for Medical Adaptation via Hierarchical Decoding (Zhiheng Cheng et al., 2024)</a></li><li><a href=#3982--122318-road-obstacle-detection-based-on-unknown-objectness-scores-chihiro-noguchi-et-al-2024>(39/82 | 122/318) Road Obstacle Detection based on Unknown Objectness Scores (Chihiro Noguchi et al., 2024)</a></li><li><a href=#4082--123318-few-shot-online-anomaly-detection-and-segmentation-shenxing-wei-et-al-2024>(40/82 | 123/318) Few-shot Online Anomaly Detection and Segmentation (Shenxing Wei et al., 2024)</a></li><li><a href=#4182--124318-multi-layer-dense-attention-decoder-for-polyp-segmentation-krushi-patel-et-al-2024>(41/82 | 124/318) Multi-Layer Dense Attention Decoder for Polyp Segmentation (Krushi Patel et al., 2024)</a></li><li><a href=#4282--125318-envisioning-medclip-a-deep-dive-into-explainability-for-medical-vision-language-models-anees-ur-rehman-hashmi-et-al-2024>(42/82 | 125/318) Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models (Anees Ur Rehman Hashmi et al., 2024)</a></li><li><a href=#4382--126318-imagenet-d-benchmarking-neural-network-robustness-on-diffusion-synthetic-object-chenshuang-zhang-et-al-2024>(43/82 | 126/318) ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object (Chenshuang Zhang et al., 2024)</a></li><li><a href=#4482--127318-pipnet3d-interpretable-detection-of-alzheimer-in-mri-scans-lisa-anita-de-santi-et-al-2024>(44/82 | 127/318) PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans (Lisa Anita De Santi et al., 2024)</a></li><li><a href=#4582--128318-towards-non-exemplar-semi-supervised-class-incremental-learning-wenzhuo-liu-et-al-2024>(45/82 | 128/318) Towards Non-Exemplar Semi-Supervised Class-Incremental Learning (Wenzhuo Liu et al., 2024)</a></li><li><a href=#4682--129318-neusdfusion-a-spatial-aware-generative-model-for-3d-shape-completion-reconstruction-and-generation-ruikai-cui-et-al-2024>(46/82 | 129/318) NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation (Ruikai Cui et al., 2024)</a></li><li><a href=#4782--130318-neuropictor-refining-fmri-to-image-reconstruction-via-multi-individual-pretraining-and-multi-level-modulation-jingyang-huo-et-al-2024>(47/82 | 130/318) NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation (Jingyang Huo et al., 2024)</a></li><li><a href=#4882--131318-lift3d-zero-shot-lifting-of-any-2d-vision-model-to-3d-mukund-varma-t-et-al-2024>(48/82 | 131/318) Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D (Mukund Varma T et al., 2024)</a></li><li><a href=#4982--132318-gamba-marry-gaussian-splatting-with-mamba-for-single-view-3d-reconstruction-qiuhong-shen-et-al-2024>(49/82 | 132/318) Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction (Qiuhong Shen et al., 2024)</a></li><li><a href=#5082--133318-density-guided-translator-boosts-synthetic-to-real-unsupervised-domain-adaptive-segmentation-of-3d-point-clouds-zhimin-yuan-et-al-2024>(50/82 | 133/318) Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds (Zhimin Yuan et al., 2024)</a></li><li><a href=#5182--134318-u-sketch-an-efficient-approach-for-sketch-to-image-diffusion-models-ilias-mitsouras-et-al-2024>(51/82 | 134/318) U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models (Ilias Mitsouras et al., 2024)</a></li><li><a href=#5282--135318-ecnet-effective-controllable-text-to-image-diffusion-models-sicheng-li-et-al-2024>(52/82 | 135/318) ECNet: Effective Controllable Text-to-Image Diffusion Models (Sicheng Li et al., 2024)</a></li><li><a href=#5382--136318-bam-box-abstraction-monitors-for-real-time-ood-detection-in-object-detection-changshun-wu-et-al-2024>(53/82 | 136/318) BAM: Box Abstraction Monitors for Real-time OoD Detection in Object Detection (Changshun Wu et al., 2024)</a></li><li><a href=#5482--137318-tracking-assisted-object-detection-with-event-cameras-ting-kang-yen-et-al-2024>(54/82 | 137/318) Tracking-Assisted Object Detection with Event Cameras (Ting-Kang Yen et al., 2024)</a></li><li><a href=#5582--138318-sgdm-static-guided-dynamic-module-make-stronger-visual-models-wenjie-xing-et-al-2024>(55/82 | 138/318) SGDM: Static-Guided Dynamic Module Make Stronger Visual Models (Wenjie Xing et al., 2024)</a></li><li><a href=#5682--139318-fourier-or-wavelet-bases-as-counterpart-self-attention-in-spikformer-for-efficient-visual-classification-qingyu-wang-et-al-2024>(56/82 | 139/318) Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification (Qingyu Wang et al., 2024)</a></li><li><a href=#5782--140318-mitigating-hallucinations-in-large-vision-language-models-with-instruction-contrastive-decoding-xintong-wang-et-al-2024>(57/82 | 140/318) Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding (Xintong Wang et al., 2024)</a></li><li><a href=#5882--141318-bringing-textual-prompt-to-ai-generated-image-quality-assessment-bowen-qu-et-al-2024>(58/82 | 141/318) Bringing Textual Prompt to AI-Generated Image Quality Assessment (Bowen Qu et al., 2024)</a></li><li><a href=#5982--142318-object-pose-estimation-via-the-aggregation-of-diffusion-features-tianfu-wang-et-al-2024>(59/82 | 142/318) Object Pose Estimation via the Aggregation of Diffusion Features (Tianfu Wang et al., 2024)</a></li><li><a href=#6082--143318-handbooster-boosting-3d-hand-mesh-reconstruction-by-conditional-synthesis-and-sampling-of-hand-object-interactions-hao-xu-et-al-2024>(60/82 | 143/318) HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions (Hao Xu et al., 2024)</a></li><li><a href=#6182--144318-a-semi-supervised-nighttime-dehazing-baseline-with-spatial-frequency-aware-and-realistic-brightness-constraint-xiaofeng-cong-et-al-2024>(61/82 | 144/318) A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint (Xiaofeng Cong et al., 2024)</a></li><li><a href=#6282--145318-backpropagation-free-network-for-3d-test-time-adaptation-yanshuo-wang-et-al-2024>(62/82 | 145/318) Backpropagation-free Network for 3D Test-time Adaptation (Yanshuo Wang et al., 2024)</a></li><li><a href=#6382--146318-generative-multi-modal-models-are-good-class-incremental-learners-xusheng-cao-et-al-2024>(63/82 | 146/318) Generative Multi-modal Models are Good Class-Incremental Learners (Xusheng Cao et al., 2024)</a></li><li><a href=#6482--147318-dont-look-into-the-dark-latent-codes-for-pluralistic-image-inpainting-haiwei-chen-et-al-2024>(64/82 | 147/318) Don&rsquo;t Look into the Dark: Latent Codes for Pluralistic Image Inpainting (Haiwei Chen et al., 2024)</a></li><li><a href=#6582--148318-egocentric-scene-aware-human-trajectory-prediction-weizhuo-wang-et-al-2024>(65/82 | 148/318) Egocentric Scene-aware Human Trajectory Prediction (Weizhuo Wang et al., 2024)</a></li><li><a href=#6682--149318-cross-domain-fiber-cluster-shape-analysis-for-language-performance-cognitive-score-prediction-yui-lo-et-al-2024>(66/82 | 149/318) Cross-domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction (Yui Lo et al., 2024)</a></li><li><a href=#6782--150318-unidepth-universal-monocular-metric-depth-estimation-luigi-piccinelli-et-al-2024>(67/82 | 150/318) UniDepth: Universal Monocular Metric Depth Estimation (Luigi Piccinelli et al., 2024)</a></li><li><a href=#6882--151318-enhancing-multiple-object-tracking-accuracy-via-quantum-annealing-yasuyuki-ihara-2024>(68/82 | 151/318) Enhancing Multiple Object Tracking Accuracy via Quantum Annealing (Yasuyuki Ihara, 2024)</a></li><li><a href=#6982--152318-annolid-annotate-segment-and-track-anything-you-need-chen-yang-et-al-2024>(69/82 | 152/318) Annolid: Annotate, Segment, and Track Anything You Need (Chen Yang et al., 2024)</a></li><li><a href=#7082--153318-homogeneous-tokenizer-matters-homogeneous-visual-tokenizer-for-remote-sensing-image-understanding-run-shao-et-al-2024>(70/82 | 153/318) Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding (Run Shao et al., 2024)</a></li><li><a href=#7182--154318-attention-calibration-for-disentangled-text-to-image-personalization-yanbing-zhang-et-al-2024>(71/82 | 154/318) Attention Calibration for Disentangled Text-to-Image Personalization (Yanbing Zhang et al., 2024)</a></li><li><a href=#7282--155318-diffusionface-towards-a-comprehensive-dataset-for-diffusion-based-face-forgery-analysis-zhongxi-chen-et-al-2024>(72/82 | 155/318) DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face Forgery Analysis (Zhongxi Chen et al., 2024)</a></li><li><a href=#7382--156318-aic-unet-anatomy-informed-cascaded-unet-for-robust-multi-organ-segmentation-young-seok-jeon-et-al-2024>(73/82 | 156/318) AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ Segmentation (Young Seok Jeon et al., 2024)</a></li><li><a href=#7482--157318-a-channel-ensemble-approach-unbiased-and-low-variance-pseudo-labels-is-critical-for-semi-supervised-classification-jiaqi-wu-et-al-2024>(74/82 | 157/318) A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is Critical for Semi-supervised Classification (Jiaqi Wu et al., 2024)</a></li><li><a href=#7582--158318-uncertainty-aware-sar-atr-defending-against-adversarial-attacks-via-bayesian-neural-networks-tian-ye-et-al-2024>(75/82 | 158/318) Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via Bayesian Neural Networks (Tian Ye et al., 2024)</a></li><li><a href=#7682--159318-enhancing-generative-class-incremental-learning-performance-with-model-forgetting-approach-taro-togo-et-al-2024>(76/82 | 159/318) Enhancing Generative Class Incremental Learning Performance with Model Forgetting Approach (Taro Togo et al., 2024)</a></li><li><a href=#7782--160318-taformer-a-unified-target-aware-transformer-for-video-and-motion-joint-prediction-in-aerial-scenes-liangyu-xu-et-al-2024>(77/82 | 160/318) TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint Prediction in Aerial Scenes (Liangyu Xu et al., 2024)</a></li><li><a href=#7882--161318-towards-image-ambient-lighting-normalization-florin-alexandru-vasluianu-et-al-2024>(78/82 | 161/318) Towards Image Ambient Lighting Normalization (Florin-Alexandru Vasluianu et al., 2024)</a></li><li><a href=#7982--162318-an-evolutionary-network-architecture-search-framework-with-adaptive-multimodal-fusion-for-hand-gesture-recognition-yizhang-xia-et-al-2024>(79/82 | 162/318) An Evolutionary Network Architecture Search Framework with Adaptive Multimodal Fusion for Hand Gesture Recognition (Yizhang Xia et al., 2024)</a></li><li><a href=#8082--163318-splatface-gaussian-splat-face-reconstruction-leveraging-an-optimizable-surface-jiahao-luo-et-al-2024>(80/82 | 163/318) SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface (Jiahao Luo et al., 2024)</a></li><li><a href=#8182--164318-parco-part-coordinating-text-to-motion-synthesis-qiran-zou-et-al-2024>(81/82 | 164/318) ParCo: Part-Coordinating Text-to-Motion Synthesis (Qiran Zou et al., 2024)</a></li><li><a href=#8282--165318-dvlo-deep-visual-lidar-odometry-with-local-to-global-feature-fusion-and-bi-directional-structure-alignment-jiuming-liu-et-al-2024>(82/82 | 165/318) DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and Bi-Directional Structure Alignment (Jiuming Liu et al., 2024)</a></li></ul></li><li><a href=#cslg-48>cs.LG (48)</a><ul><li><a href=#148--166318-physics-informed-graph-neural-networks-for-water-distribution-systems-inaam-ashraf-et-al-2024>(1/48 | 166/318) Physics-Informed Graph Neural Networks for Water Distribution Systems (Inaam Ashraf et al., 2024)</a></li><li><a href=#248--167318-energy-guided-data-sampling-for-traffic-prediction-with-mini-training-datasets-zhaohui-yang-et-al-2024>(2/48 | 167/318) Energy-Guided Data Sampling for Traffic Prediction with Mini Training Datasets (Zhaohui Yang et al., 2024)</a></li><li><a href=#348--168318-self-expansion-of-pre-trained-models-with-mixture-of-adapters-for-continual-learning-huiyi-wang-et-al-2024>(3/48 | 168/318) Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning (Huiyi Wang et al., 2024)</a></li><li><a href=#448--169318-the-topos-of-transformer-networks-mattia-jacopo-villani-et-al-2024>(4/48 | 169/318) The Topos of Transformer Networks (Mattia Jacopo Villani et al., 2024)</a></li><li><a href=#548--170318-pdnnet-pdn-aware-gnn-cnn-heterogeneous-network-for-dynamic-ir-drop-prediction-yuxiang-zhao-et-al-2024>(5/48 | 170/318) PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction (Yuxiang Zhao et al., 2024)</a></li><li><a href=#648--171318-multi-modal-contrastive-learning-for-online-clinical-time-series-applications-fabian-baldenweg-et-al-2024>(6/48 | 171/318) Multi-Modal Contrastive Learning for Online Clinical Time-Series Applications (Fabian Baldenweg et al., 2024)</a></li><li><a href=#748--172318-branch-tuning-balancing-stability-and-plasticity-for-continual-self-supervised-learning-wenzhuo-liu-et-al-2024>(7/48 | 172/318) Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning (Wenzhuo Liu et al., 2024)</a></li><li><a href=#848--173318-computationally-and-memory-efficient-robust-predictive-analytics-using-big-data-daniel-menges-et-al-2024>(8/48 | 173/318) Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data (Daniel Menges et al., 2024)</a></li><li><a href=#948--174318-contrastive-learning-with-orthonormal-anchors-cloa-huanran-li-et-al-2024>(9/48 | 174/318) Contrastive Learning with Orthonormal Anchors (CLOA) (Huanran Li et al., 2024)</a></li><li><a href=#1048--175318-safe-and-robust-reinforcement-learning-principles-and-practice-taku-yamagata-et-al-2024>(10/48 | 175/318) Safe and Robust Reinforcement Learning: Principles and Practice (Taku Yamagata et al., 2024)</a></li><li><a href=#1148--176318-robustness-and-visual-explanation-for-black-box-image-video-and-ecg-signal-classification-with-reinforcement-learning-soumyendu-sarkar-et-al-2024>(11/48 | 176/318) Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning (Soumyendu Sarkar et al., 2024)</a></li><li><a href=#1248--177318-selective-mixup-fine-tuning-for-optimizing-non-decomposable-objectives-shrinivas-ramasubramanian-et-al-2024>(12/48 | 177/318) Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives (Shrinivas Ramasubramanian et al., 2024)</a></li><li><a href=#1348--178318-genet-a-graph-neural-network-based-anti-noise-task-oriented-semantic-communication-paradigm-chunhang-zheng-et-al-2024>(13/48 | 178/318) GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm (Chunhang Zheng et al., 2024)</a></li><li><a href=#1448--179318-corast-towards-foundation-model-powered-correlated-data-analysis-in-resource-constrained-cps-and-iot-yi-hu-et-al-2024>(14/48 | 179/318) CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT (Yi Hu et al., 2024)</a></li><li><a href=#1548--180318-transfusion-contrastive-learning-with-transformers-huanran-li-et-al-2024>(15/48 | 180/318) TransFusion: Contrastive Learning with Transformers (Huanran Li et al., 2024)</a></li><li><a href=#1648--181318-fusion-approaches-for-emotion-recognition-from-speech-using-acoustic-and-text-based-features-leonardo-pepino-et-al-2024>(16/48 | 181/318) Fusion approaches for emotion recognition from speech using acoustic and text-based features (Leonardo Pepino et al., 2024)</a></li><li><a href=#1748--182318-scalable-lipschitz-estimation-for-cnns-yusuf-sulehman-et-al-2024>(17/48 | 182/318) Scalable Lipschitz Estimation for CNNs (Yusuf Sulehman et al., 2024)</a></li><li><a href=#1848--183318-improving-line-search-methods-for-large-scale-neural-network-training-philip-kenneweg-et-al-2024>(18/48 | 183/318) Improving Line Search Methods for Large Scale Neural Network Training (Philip Kenneweg et al., 2024)</a></li><li><a href=#1948--184318-faster-convergence-for-transformer-fine-tuning-with-line-search-methods-philip-kenneweg-et-al-2024>(19/48 | 184/318) Faster Convergence for Transformer Fine-tuning with Line Search Methods (Philip Kenneweg et al., 2024)</a></li><li><a href=#2048--185318-collaborative-active-learning-in-conditional-trust-environment-zan-kai-chong-et-al-2024>(20/48 | 185/318) Collaborative Active Learning in Conditional Trust Environment (Zan-Kai Chong et al., 2024)</a></li><li><a href=#2148--186318-on-spectrogram-analysis-in-a-multiple-classifier-fusion-framework-for-power-grid-classification-using-electric-network-frequency-georgios-tzolopoulos-et-al-2024>(21/48 | 186/318) On Spectrogram Analysis in a Multiple Classifier Fusion Framework for Power Grid Classification Using Electric Network Frequency (Georgios Tzolopoulos et al., 2024)</a></li><li><a href=#2248--187318-a-geometric-explanation-of-the-likelihood-ood-detection-paradox-hamidreza-kamkari-et-al-2024>(22/48 | 187/318) A Geometric Explanation of the Likelihood OOD Detection Paradox (Hamidreza Kamkari et al., 2024)</a></li><li><a href=#2348--188318-looking-beyond-what-you-see-an-empirical-analysis-on-subgroup-intersectional-fairness-for-multi-label-chest-x-ray-classification-using-social-determinants-of-racial-health-inequities-dana-moukheiber-et-al-2024>(23/48 | 188/318) Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities (Dana Moukheiber et al., 2024)</a></li><li><a href=#2448--189318-detecting-generative-parroting-through-overfitting-masked-autoencoders-saeid-asgari-taghanaki-et-al-2024>(24/48 | 189/318) Detecting Generative Parroting through Overfitting Masked Autoencoders (Saeid Asgari Taghanaki et al., 2024)</a></li><li><a href=#2548--190318-understanding-the-learning-dynamics-of-alignment-with-human-feedback-shawn-im-et-al-2024>(25/48 | 190/318) Understanding the Learning Dynamics of Alignment with Human Feedback (Shawn Im et al., 2024)</a></li><li><a href=#2648--191318-semi-supervised-learning-for-deep-causal-generative-models-yasin-ibrahim-et-al-2024>(26/48 | 191/318) Semi-Supervised Learning for Deep Causal Generative Models (Yasin Ibrahim et al., 2024)</a></li><li><a href=#2748--192318-fresco-federated-reinforcement-energy-system-for-cooperative-optimization-nicolas-mauricio-cuadrado-et-al-2024>(27/48 | 192/318) FRESCO: Federated Reinforcement Energy System for Cooperative Optimization (Nicolas Mauricio Cuadrado et al., 2024)</a></li><li><a href=#2848--193318-the-artificial-neural-twin----process-optimization-and-continual-learning-in-distributed-process-chains-johannes-emmert-et-al-2024>(28/48 | 193/318) The Artificial Neural Twin &ndash; Process Optimization and Continual Learning in Distributed Process Chains (Johannes Emmert et al., 2024)</a></li><li><a href=#2948--194318-macroscale-fracture-surface-segmentation-via-semi-supervised-learning-considering-the-structural-similarity-johannes-rosenberger-et-al-2024>(29/48 | 194/318) Macroscale fracture surface segmentation via semi-supervised learning considering the structural similarity (Johannes Rosenberger et al., 2024)</a></li><li><a href=#3048--195318-a-thermodynamically-consistent-physics-informed-deep-learning-material-model-for-short-fiberpolymer-nanocomposites-betim-bahtiri-et-al-2024>(30/48 | 195/318) A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites (Betim Bahtiri et al., 2024)</a></li><li><a href=#3148--196318-dsf-gan-downstream-feedback-generative-adversarial-network-oriel-perets-et-al-2024>(31/48 | 196/318) DSF-GAN: DownStream Feedback Generative Adversarial Network (Oriel Perets et al., 2024)</a></li><li><a href=#3248--197318-from-two-dimensional-to-three-dimensional-environment-with-q-learning-modeling-autonomous-navigation-with-reinforcement-learning-and-no-libraries-ergon-cugler-de-moraes-silva-2024>(32/48 | 197/318) From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries (Ergon Cugler de Moraes Silva, 2024)</a></li><li><a href=#3348--198318-compression-of-the-koopman-matrix-for-nonlinear-physical-models-via-hierarchical-clustering-tomoya-nishikata-et-al-2024>(33/48 | 198/318) Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering (Tomoya Nishikata et al., 2024)</a></li><li><a href=#3448--199318-equity-in-healthcare-analyzing-disparities-in-machine-learning-predictions-of-diabetic-patient-readmissions-zainab-al-zanbouri-et-al-2024>(34/48 | 199/318) Equity in Healthcare: Analyzing Disparities in Machine Learning Predictions of Diabetic Patient Readmissions (Zainab Al-Zanbouri et al., 2024)</a></li><li><a href=#3548--200318-exploiting-symmetry-in-dynamics-for-model-based-reinforcement-learning-with-asymmetric-rewards-yasin-sonmez-et-al-2024>(35/48 | 200/318) Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards (Yasin Sonmez et al., 2024)</a></li><li><a href=#3648--201318-thelxinoë-recognizing-human-emotions-using-pupillometry-and-machine-learning-darlene-barker-et-al-2024>(36/48 | 201/318) Thelxinoë: Recognizing Human Emotions Using Pupillometry and Machine Learning (Darlene Barker et al., 2024)</a></li><li><a href=#3748--202318-conditional-wasserstein-distances-with-applications-in-bayesian-ot-flow-matching-jannis-chemseddine-et-al-2024>(37/48 | 202/318) Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching (Jannis Chemseddine et al., 2024)</a></li><li><a href=#3848--203318-learning-in-pinns-phase-transition-total-diffusion-and-generalization-sokratis-j-anagnostopoulos-et-al-2024>(38/48 | 203/318) Learning in PINNs: Phase transition, total diffusion, and generalization (Sokratis J. Anagnostopoulos et al., 2024)</a></li><li><a href=#3948--204318-synthesizing-eeg-signals-from-event-related-potential-paradigms-with-conditional-diffusion-models-guido-klein-et-al-2024>(39/48 | 204/318) Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models (Guido Klein et al., 2024)</a></li><li><a href=#4048--205318-generalized-policy-learning-for-smart-grids-fl-trpo-approach-yunxiang-li-et-al-2024>(40/48 | 205/318) Generalized Policy Learning for Smart Grids: FL TRPO Approach (Yunxiang Li et al., 2024)</a></li><li><a href=#4148--206318-global-vegetation-modeling-with-pre-trained-weather-transformers-pascal-janetzky-et-al-2024>(41/48 | 206/318) Global Vegetation Modeling with Pre-Trained Weather Transformers (Pascal Janetzky et al., 2024)</a></li><li><a href=#4248--207318-iip-mixerintra-inter-patch-mixing-architecture-for-battery-remaining-useful-life-prediction-guangzai-ye-et-al-2024>(42/48 | 207/318) IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction (Guangzai Ye et al., 2024)</a></li><li><a href=#4348--208318-stragglers-aware-low-latency-synchronous-federated-learning-via-layer-wise-model-updates-natalie-lang-et-al-2024>(43/48 | 208/318) Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates (Natalie Lang et al., 2024)</a></li><li><a href=#4448--209318-long-and-short-term-constraints-driven-safe-reinforcement-learning-for-autonomous-driving-xuemin-hu-et-al-2024>(44/48 | 209/318) Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving (Xuemin Hu et al., 2024)</a></li><li><a href=#4548--210318-superior-parallel-big-data-clustering-through-competitive-stochastic-sample-size-optimization-in-big-means-rustam-mussabayev-et-al-2024>(45/48 | 210/318) Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means (Rustam Mussabayev et al., 2024)</a></li><li><a href=#4648--211318-tensor-based-graph-learning-with-consistency-and-specificity-for-multi-view-clustering-long-shi-et-al-2024>(46/48 | 211/318) Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering (Long Shi et al., 2024)</a></li><li><a href=#4748--212318-nonlinear-model-reduction-for-operator-learning-hamidreza-eivazi-et-al-2024>(47/48 | 212/318) Nonlinear model reduction for operator learning (Hamidreza Eivazi et al., 2024)</a></li><li><a href=#4848--213318-multi-label-adaptive-batch-selection-by-highlighting-hard-and-imbalanced-samples-ao-zhou-et-al-2024>(48/48 | 213/318) Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples (Ao Zhou et al., 2024)</a></li></ul></li><li><a href=#eessiv-9>eess.IV (9)</a><ul><li><a href=#19--214318-generative-medical-segmentation-jiayu-huo-et-al-2024>(1/9 | 214/318) Generative Medical Segmentation (Jiayu Huo et al., 2024)</a></li><li><a href=#29--215318-hemit-he-to-multiplex-immunohistochemistry-image-translation-with-dual-branch-pix2pix-generator-chang-bian-et-al-2024>(2/9 | 215/318) HEMIT: H&amp;E to Multiplex-immunohistochemistry Image Translation with Dual-Branch Pix2pix Generator (Chang Bian et al., 2024)</a></li><li><a href=#39--216318-benchmarking-image-transformers-for-prostate-cancer-detection-from-ultrasound-data-mohamed-harmanani-et-al-2024>(3/9 | 216/318) Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data (Mohamed Harmanani et al., 2024)</a></li><li><a href=#49--217318-ct-3dflow--leveraging-3d-normalizing-flows-for-unsupervised-detection-of-pathological-pulmonary-ct-scans-aissam-djahnine-et-al-2024>(4/9 | 217/318) CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection of Pathological Pulmonary CT scans (Aissam Djahnine et al., 2024)</a></li><li><a href=#59--218318-a-vascular-synthetic-model-for-improved-aneurysm-segmentation-and-detection-via-deep-neural-networks-rafic-nader-et-al-2024>(5/9 | 218/318) A vascular synthetic model for improved aneurysm segmentation and detection via Deep Neural Networks (Rafic Nader et al., 2024)</a></li><li><a href=#69--219318-theoretical-bound-guided-hierarchical-vae-for-neural-image-codecs-yichi-zhang-et-al-2024>(6/9 | 219/318) Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs (Yichi Zhang et al., 2024)</a></li><li><a href=#79--220318-deep-learning-segmentation-and-classification-of-red-blood-cells-using-a-large-multi-scanner-dataset-mohamed-elmanna-et-al-2024>(7/9 | 220/318) Deep Learning Segmentation and Classification of Red Blood Cells Using a Large Multi-Scanner Dataset (Mohamed Elmanna et al., 2024)</a></li><li><a href=#89--221318-transformers-based-architectures-for-stroke-segmentation-a-review-yalda-zafari-ghadim-et-al-2024>(8/9 | 221/318) Transformers-based architectures for stroke segmentation: A review (Yalda Zafari-Ghadim et al., 2024)</a></li><li><a href=#99--222318-h2aseg-hierarchical-adaptive-interaction-and-weighting-network-for-tumor-segmentation-in-petct-images-jinpeng-lu-et-al-2024>(9/9 | 222/318) H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for Tumor Segmentation in PET/CT Images (Jinpeng Lu et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--223318-noise-robust-keyword-spotting-through-self-supervised-pretraining-jacob-mørk-et-al-2024>(1/3 | 223/318) Noise-Robust Keyword Spotting through Self-supervised Pretraining (Jacob Mørk et al., 2024)</a></li><li><a href=#23--224318-a-diffusion-based-generative-equalizer-for-music-restoration-eloi-moliner-et-al-2024>(2/3 | 224/318) A Diffusion-Based Generative Equalizer for Music Restoration (Eloi Moliner et al., 2024)</a></li><li><a href=#33--225318-dual-path-mamba-short-and-long-term-bidirectional-selective-structured-state-space-models-for-speech-separation-xilin-jiang-et-al-2024>(3/3 | 225/318) Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation (Xilin Jiang et al., 2024)</a></li></ul></li><li><a href=#csai-14>cs.AI (14)</a><ul><li><a href=#114--226318-lc-llm-explainable-lane-change-intention-and-trajectory-predictions-with-large-language-models-mingxing-peng-et-al-2024>(1/14 | 226/318) LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models (Mingxing Peng et al., 2024)</a></li><li><a href=#214--227318-boosting-conversational-question-answering-with-fine-grained-retrieval-augmentation-and-self-check-linhao-ye-et-al-2024>(2/14 | 227/318) Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check (Linhao Ye et al., 2024)</a></li><li><a href=#314--228318-large-language-models-need-consultants-for-reasoning-becoming-an-expert-in-a-complex-human-system-through-behavior-simulation-chuwen-wang-et-al-2024>(3/14 | 228/318) Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation (Chuwen Wang et al., 2024)</a></li><li><a href=#414--229318-leveraging-large-language-models-for-relevance-judgments-in-legal-case-retrieval-shengjie-ma-et-al-2024>(4/14 | 229/318) Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval (Shengjie Ma et al., 2024)</a></li><li><a href=#514--230318-malbert-is-a-compact-multilingual-bert-model-still-worth-it-christophe-servan-et-al-2024>(5/14 | 230/318) mALBERT: Is a Compact Multilingual BERT Model Still Worth It? (Christophe Servan et al., 2024)</a></li><li><a href=#614--231318-probabilistic-model-checking-of-stochastic-reinforcement-learning-policies-dennis-gross-et-al-2024>(6/14 | 231/318) Probabilistic Model Checking of Stochastic Reinforcement Learning Policies (Dennis Gross et al., 2024)</a></li><li><a href=#714--232318-neural-architecture-search-for-sentence-classification-with-bert-philip-kenneweg-et-al-2024>(7/14 | 232/318) Neural Architecture Search for Sentence Classification with BERT (Philip Kenneweg et al., 2024)</a></li><li><a href=#814--233318-identification-and-uses-of-deep-learning-backbones-via-pattern-mining-michael-livanos-et-al-2024>(8/14 | 233/318) Identification and Uses of Deep Learning Backbones via Pattern Mining (Michael Livanos et al., 2024)</a></li><li><a href=#914--234318-a-path-towards-legal-autonomy-an-interoperable-and-explainable-approach-to-extracting-transforming-loading-and-computing-legal-information-using-large-language-models-expert-systems-and-bayesian-networks-axel-constant-et-al-2024>(9/14 | 234/318) A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks (Axel Constant et al., 2024)</a></li><li><a href=#1014--235318-leveraging-large-language-models-for-fuzzy-string-matching-in-political-science-yu-wang-2024>(10/14 | 235/318) Leveraging Large Language Models for Fuzzy String Matching in Political Science (Yu Wang, 2024)</a></li><li><a href=#1114--236318-exploring-the-privacy-protection-capabilities-of-chinese-large-language-models-yuqi-yang-et-al-2024>(11/14 | 236/318) Exploring the Privacy Protection Capabilities of Chinese Large Language Models (Yuqi Yang et al., 2024)</a></li><li><a href=#1214--237318-enhancing-manufacturing-quality-prediction-models-through-the-integration-of-explainability-methods-dennis-gross-et-al-2024>(12/14 | 237/318) Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods (Dennis Gross et al., 2024)</a></li><li><a href=#1314--238318-ftbc-forward-temporal-bias-correction-for-optimizing-ann-snn-conversion-xiaofeng-wu-et-al-2024>(13/14 | 238/318) FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion (Xiaofeng Wu et al., 2024)</a></li><li><a href=#1414--239318-endtoendml-an-open-source-end-to-end-pipeline-for-machine-learning-applications-nisha-pillai-et-al-2024>(14/14 | 239/318) EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications (Nisha Pillai et al., 2024)</a></li></ul></li><li><a href=#cscr-11>cs.CR (11)</a><ul><li><a href=#111--240318-cpr-retrieval-augmented-generation-for-copyright-protection-aditya-golatkar-et-al-2024>(1/11 | 240/318) CPR: Retrieval Augmented Generation for Copyright Protection (Aditya Golatkar et al., 2024)</a></li><li><a href=#211--241318-foc-figure-out-the-cryptographic-functions-in-stripped-binaries-with-llms-guoqiang-chen-et-al-2024>(2/11 | 241/318) FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs (Guoqiang Chen et al., 2024)</a></li><li><a href=#311--242318-misguide--defense-against-data-free-deep-learning-model-extraction-mahendra-gurve-et-al-2024>(3/11 | 242/318) MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction (Mahendra Gurve et al., 2024)</a></li><li><a href=#411--243318-bayesian-learned-models-can-detect-adversarial-malware-for-free-bao-gia-doan-et-al-2024>(4/11 | 243/318) Bayesian Learned Models Can Detect Adversarial Malware For Free (Bao Gia Doan et al., 2024)</a></li><li><a href=#511--244318-privacy-preserving-distributed-nonnegative-matrix-factorization-ehsan-lari-et-al-2024>(5/11 | 244/318) Privacy-Preserving Distributed Nonnegative Matrix Factorization (Ehsan Lari et al., 2024)</a></li><li><a href=#611--245318-a-transformer-based-framework-for-payload-malware-detection-and-classification-kyle-stein-et-al-2024>(6/11 | 245/318) A Transformer-Based Framework for Payload Malware Detection and Classification (Kyle Stein et al., 2024)</a></li><li><a href=#711--246318-dealing-with-imbalanced-classes-in-bot-iot-dataset-jesse-atuhurra-et-al-2024>(7/11 | 246/318) Dealing with Imbalanced Classes in Bot-IoT Dataset (Jesse Atuhurra et al., 2024)</a></li><li><a href=#811--247318-spikewhisper-temporal-spike-backdoor-attacks-on-federated-neuromorphic-learning-over-low-power-devices-hanqing-fu-et-al-2024>(8/11 | 247/318) Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices (Hanqing Fu et al., 2024)</a></li><li><a href=#911--248318-the-impact-of-uniform-inputs-on-activation-sparsity-and-energy-latency-attacks-in-computer-vision-andreas-müller-et-al-2024>(9/11 | 248/318) The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency Attacks in Computer Vision (Andreas Müller et al., 2024)</a></li><li><a href=#1011--249318-statistical-testing-of-random-number-generators-and-their-improvement-using-randomness-extraction-cameron-foreman-et-al-2024>(10/11 | 249/318) Statistical testing of random number generators and their improvement using randomness extraction (Cameron Foreman et al., 2024)</a></li><li><a href=#1111--250318-optimizing-cyber-response-time-on-temporal-active-directory-networks-using-decoys-huy-q-ngo-et-al-2024>(11/11 | 250/318) Optimizing Cyber Response Time on Temporal Active Directory Networks Using Decoys (Huy Q. Ngo et al., 2024)</a></li></ul></li><li><a href=#csse-6>cs.SE (6)</a><ul><li><a href=#16--251318-few-shot-cross-system-anomaly-trace-classification-for-microservice-based-systems-yuqing-wang-et-al-2024>(1/6 | 251/318) Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems (Yuqing Wang et al., 2024)</a></li><li><a href=#26--252318-vulnerability-detection-with-code-language-models-how-far-are-we-yangruibo-ding-et-al-2024>(2/6 | 252/318) Vulnerability Detection with Code Language Models: How Far Are We? (Yangruibo Ding et al., 2024)</a></li><li><a href=#36--253318-a-state-of-the-practice-release-readiness-checklist-for-generative-ai-based-software-products-harsh-patel-et-al-2024>(3/6 | 253/318) A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products (Harsh Patel et al., 2024)</a></li><li><a href=#46--254318-an-exploratory-study-on-upper-level-computing-students-use-of-large-language-models-as-tools-in-a-semester-long-project-ben-arie-tanay-et-al-2024>(4/6 | 254/318) An Exploratory Study on Upper-Level Computing Students&rsquo; Use of Large Language Models as Tools in a Semester-Long Project (Ben Arie Tanay et al., 2024)</a></li><li><a href=#56--255318-cycle-learning-to-self-refine-the-code-generation-yangruibo-ding-et-al-2024>(5/6 | 255/318) CYCLE: Learning to Self-Refine the Code Generation (Yangruibo Ding et al., 2024)</a></li><li><a href=#66--256318-algorithmic-details-behind-the-predator-shape-analyser-kamil-dudka-et-al-2024>(6/6 | 256/318) Algorithmic Details behind the Predator Shape Analyser (Kamil Dudka et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--257318-moderating-illicit-online-image-promotion-for-unsafe-user-generated-content-games-using-large-vision-language-models-keyan-guo-et-al-2024>(1/1 | 257/318) Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models (Keyan Guo et al., 2024)</a></li></ul></li><li><a href=#csdc-5>cs.DC (5)</a><ul><li><a href=#15--258318-resource-allocation-in-large-language-model-integrated-6g-vehicular-networks-chang-liu-et-al-2024>(1/5 | 258/318) Resource Allocation in Large Language Model Integrated 6G Vehicular Networks (Chang Liu et al., 2024)</a></li><li><a href=#25--259318-optimizing-communication-for-latency-sensitive-hpc-applications-on-up-to-48-fpgas-using-accl-marius-meyer-et-al-2024>(2/5 | 259/318) Optimizing Communication for Latency Sensitive HPC Applications on up to 48 FPGAs Using ACCL (Marius Meyer et al., 2024)</a></li><li><a href=#35--260318-distributed-maximum-consensus-over-noisy-links-ehsan-lari-et-al-2024>(3/5 | 260/318) Distributed Maximum Consensus over Noisy Links (Ehsan Lari et al., 2024)</a></li><li><a href=#45--261318-optimal-resource-efficiency-with-fairness-in-heterogeneous-gpu-clusters-zizhao-mo-et-al-2024>(4/5 | 261/318) Optimal Resource Efficiency with Fairness in Heterogeneous GPU Clusters (Zizhao Mo et al., 2024)</a></li><li><a href=#55--262318-enhanced-openmp-algorithm-to-compute-all-pairs-shortest-path-on-x86-architectures-sergio-calderón-et-al-2024>(5/5 | 262/318) Enhanced OpenMP Algorithm to Compute All-Pairs Shortest Path on x86 Architectures (Sergio Calderón et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--263318-smof-streaming-modern-cnns-on-fpgas-with-smart-off-chip-eviction-petros-toupas-et-al-2024>(1/2 | 263/318) SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction (Petros Toupas et al., 2024)</a></li><li><a href=#22--264318-merits-of-time-domain-computing-for-vmm----a-quantitative-comparison-florian-freye-et-al-2024>(2/2 | 264/318) Merits of Time-Domain Computing for VMM &ndash; A Quantitative Comparison (Florian Freye et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--265318-real-acoustic-fields-an-audio-visual-room-acoustics-dataset-and-benchmark-ziyang-chen-et-al-2024>(1/2 | 265/318) Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark (Ziyang Chen et al., 2024)</a></li><li><a href=#22--266318-aces-evaluating-automated-audio-captioning-models-on-the-semantics-of-sounds-gijs-wijngaard-et-al-2024>(2/2 | 266/318) ACES: Evaluating Automated Audio Captioning Models on the Semantics of Sounds (Gijs Wijngaard et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#14--267318-intent-aware-drl-based-uplink-dynamic-scheduler-for-5g-nr-salwa-mostafa-et-al-2024>(1/4 | 267/318) Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR (Salwa Mostafa et al., 2024)</a></li><li><a href=#24--268318-mutual-information-optimization-for-sim-based-holographic-mimo-systems-nemanja-stefan-perović-et-al-2024>(2/4 | 268/318) Mutual Information Optimization for SIM-Based Holographic MIMO Systems (Nemanja Stefan Perović et al., 2024)</a></li><li><a href=#34--269318-representatividad-muestral-en-la-incertidumbre-simétrica-multivariada-para-la-selección-de-atributos-gustavo-sosa-cabrera-2024>(3/4 | 269/318) Representatividad Muestral en la Incertidumbre Simétrica Multivariada para la Selección de Atributos (Gustavo Sosa-Cabrera, 2024)</a></li><li><a href=#44--270318-the-dimensions-of-the-hulls-of-conorm-codes-from-algebraic-geometry-codes-junmin-an-et-al-2024>(4/4 | 270/318) The Dimensions of the Hulls of Conorm Codes from Algebraic Geometry Codes (Junmin An et al., 2024)</a></li></ul></li><li><a href=#eesssy-10>eess.SY (10)</a><ul><li><a href=#110--271318-differentially-private-distributed-nonconvex-stochastic-optimization-with-quantized-communications-jialong-chen-et-al-2024>(1/10 | 271/318) Differentially Private Distributed Nonconvex Stochastic Optimization with Quantized Communications (Jialong Chen et al., 2024)</a></li><li><a href=#210--272318-differentially-private-dual-gradient-tracking-for-distributed-resource-allocation-wei-huo-et-al-2024>(2/10 | 272/318) Differentially Private Dual Gradient Tracking for Distributed Resource Allocation (Wei Huo et al., 2024)</a></li><li><a href=#310--273318-nonlinear-model-predictive-control-for-enhanced-navigation-of-autonomous-surface-vessels-daniel-menges-et-al-2024>(3/10 | 273/318) Nonlinear Model Predictive Control for Enhanced Navigation of Autonomous Surface Vessels (Daniel Menges et al., 2024)</a></li><li><a href=#410--274318-genesis-rl-generating-natural-edge-cases-with-systematic-integration-of-safety-considerations-and-reinforcement-learning-hsin-jung-yang-et-al-2024>(4/10 | 274/318) GENESIS-RL: GEnerating Natural Edge-cases with Systematic Integration of Safety considerations and Reinforcement Learning (Hsin-Jung Yang et al., 2024)</a></li><li><a href=#510--275318-fpga-based-neural-thrust-controller-for-uavs-sharif-azem-et-al-2024>(5/10 | 275/318) FPGA-Based Neural Thrust Controller for UAVs (Sharif Azem et al., 2024)</a></li><li><a href=#610--276318-optimal-control-synthesis-of-markov-decision-processes-for-efficiency-with-surveillance-tasks-yu-chen-et-al-2024>(6/10 | 276/318) Optimal Control Synthesis of Markov Decision Processes for Efficiency with Surveillance Tasks (Yu Chen et al., 2024)</a></li><li><a href=#710--277318-feedback-linearizable-discretizations-of-second-order-mechanical-systems-using-retraction-maps-shreyas-n-b-et-al-2024>(7/10 | 277/318) Feedback Linearizable Discretizations of Second Order Mechanical Systems using Retraction Maps (Shreyas N. B. et al., 2024)</a></li><li><a href=#810--278318-linear-hybrid-asymmetrical-load-modulated-balanced-amplifier-with-multi-band-reconfigurability-and-antenna-vswr-resilience-jiachen-guo-et-al-2024>(8/10 | 278/318) Linear Hybrid Asymmetrical Load-Modulated Balanced Amplifier with Multi-Band Reconfigurability and Antenna-VSWR Resilience (Jiachen Guo et al., 2024)</a></li><li><a href=#910--279318-a-dynamic-programming-approach-for-road-traffic-estimation-mattia-laurini-et-al-2024>(9/10 | 279/318) A Dynamic Programming Approach for Road Traffic Estimation (Mattia Laurini et al., 2024)</a></li><li><a href=#1010--280318-incentive-compatible-vertiport-reservation-in-advanced-air-mobility-an-auction-based-approach-pan-yang-su-et-al-2024>(10/10 | 280/318) Incentive-Compatible Vertiport Reservation in Advanced Air Mobility: An Auction-Based Approach (Pan-Yang Su et al., 2024)</a></li></ul></li><li><a href=#mathna-7>math.NA (7)</a><ul><li><a href=#17--281318-higher-order-multi-dimension-reduction-methods-via-einstein-product-alaeddine-zahir-et-al-2024>(1/7 | 281/318) Higher order multi-dimension reduction methods via Einstein-product (Alaeddine Zahir et al., 2024)</a></li><li><a href=#27--282318-fractional-variational-integrators-based-on-convolution-quadrature-khaled-hariz-et-al-2024>(2/7 | 282/318) Fractional variational integrators based on convolution quadrature (Khaled Hariz et al., 2024)</a></li><li><a href=#37--283318-generalized-convergence-of-the-deep-bsde-method-a-step-towards-fully-coupled-fbsdes-and-applications-in-stochastic-control-balint-negyesi-et-al-2024>(3/7 | 283/318) Generalized convergence of the deep BSDE method: a step towards fully-coupled FBSDEs and applications in stochastic control (Balint Negyesi et al., 2024)</a></li><li><a href=#47--284318-concurrent-level-set-and-fiber-orientation-optimization-of-composite-structures-m-mokhtarzadeh-et-al-2024>(4/7 | 284/318) Concurrent level set and fiber orientation optimization of composite structures (M. Mokhtarzadeh et al., 2024)</a></li><li><a href=#57--285318-robust-numerical-algebraic-geometry-emma-r-cobian-et-al-2024>(5/7 | 285/318) Robust Numerical Algebraic Geometry (Emma R. Cobian et al., 2024)</a></li><li><a href=#67--286318-global-convergence-of-iterative-solvers-for-problems-of-nonlinear-magnetostatics-herbert-egger-et-al-2024>(6/7 | 286/318) Global convergence of iterative solvers for problems of nonlinear magnetostatics (Herbert Egger et al., 2024)</a></li><li><a href=#77--287318-stability-and-convergence-of-the-penalty-formulation-for-nonlinear-magnetostatics-herbert-egger-et-al-2024>(7/7 | 287/318) Stability and convergence of the penalty formulation for nonlinear magnetostatics (Herbert Egger et al., 2024)</a></li></ul></li><li><a href=#mathst-1>math.ST (1)</a><ul><li><a href=#11--288318-meta-learning-with-generalized-ridge-regression-high-dimensional-asymptotics-optimality-and-hyper-covariance-estimation-yanhao-jin-et-al-2024>(1/1 | 288/318) Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation (Yanhao Jin et al., 2024)</a></li></ul></li><li><a href=#astro-phsr-1>astro-ph.SR (1)</a><ul><li><a href=#11--289318-super-resolution-of-sohomdi-magnetograms-of-solar-active-regions-using-sdohmi-data-and-an-attention-aided-convolutional-neural-network-chunhui-xu-et-al-2024>(1/1 | 289/318) Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using SDO/HMI Data and an Attention-Aided Convolutional Neural Network (Chunhui Xu et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--290318-a-study-of-three-influencer-archetypes-for-the-control-of-opinion-spread-in-time-varying-social-networks-michael-debuse-et-al-2024>(1/2 | 290/318) A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks (Michael DeBuse et al., 2024)</a></li><li><a href=#22--291318-the-process-of-polarisation-as-a-loss-of-dimensionality-measuring-changes-in-polarisation-using-singular-value-decomposition-of-network-graphs-sage-anastasi-et-al-2024>(2/2 | 291/318) The process of polarisation as a loss of dimensionality: measuring changes in polarisation using Singular Value Decomposition of network graphs (Sage Anastasi et al., 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--292318-tessellation-and-interactive-visualization-of-four-dimensional-spacetime-geometries-philip-claude-caplan-2024>(1/2 | 292/318) Tessellation and interactive visualization of four-dimensional spacetime geometries (Philip Claude Caplan, 2024)</a></li><li><a href=#22--293318-sliced-online-model-checking-for-optimizing-the-beam-scheduling-problem-in-robotic-radiation-therapy-lars-beckers-et-al-2024>(2/2 | 293/318) Sliced Online Model Checking for Optimizing the Beam Scheduling Problem in Robotic Radiation Therapy (Lars Beckers et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--294318-how-to-cache-important-contents-for-multi-modal-service-in-dynamic-networks-a-drl-based-caching-scheme-zhe-zhang-et-al-2024>(1/1 | 294/318) How to Cache Important Contents for Multi-modal Service in Dynamic Networks: A DRL-based Caching Scheme (Zhe Zhang et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-2>q-bio.QM (2)</a><ul><li><a href=#12--295318-sequential-inference-of-hospitalization-electronichealth-records-using-probabilistic-models-alan-d-kaplan-et-al-2024>(1/2 | 295/318) Sequential Inference of Hospitalization ElectronicHealth Records Using Probabilistic Models (Alan D. Kaplan et al., 2024)</a></li><li><a href=#22--296318-a-python-library-for-efficient-computation-of-molecular-fingerprints-michał-szafarczyk-et-al-2024>(2/2 | 296/318) A Python library for efficient computation of molecular fingerprints (Michał Szafarczyk et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--297318-using-quantum-computing-to-infer-dynamic-behaviors-of-biological-and-artificial-neural-networks-gabriel-a-silva-2024>(1/1 | 297/318) Using Quantum Computing to Infer Dynamic Behaviors of Biological and Artificial Neural Networks (Gabriel A. Silva, 2024)</a></li></ul></li><li><a href=#cssy-1>cs.SY (1)</a><ul><li><a href=#11--298318-mpc-cbf-with-adaptive-safety-margins-for-safety-critical-teleoperation-over-imperfect-network-connections-riccardo-periotto-et-al-2024>(1/1 | 298/318) MPC-CBF with Adaptive Safety Margins for Safety-critical Teleoperation over Imperfect Network Connections (Riccardo Periotto et al., 2024)</a></li></ul></li><li><a href=#hep-ph-1>hep-ph (1)</a><ul><li><a href=#11--299318-one-flow-to-correct-them-all-improving-simulations-in-high-energy-physics-with-a-single-normalising-flow-and-a-switch-caio-cesar-daumann-et-al-2024>(1/1 | 299/318) One flow to correct them all: improving simulations in high-energy physics with a single normalising flow and a switch (Caio Cesar Daumann et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--300318-the-metric-distortion-of-randomized-social-choice-functions-c1-maximal-lottery-rules-and-simulations-fabian-frank-et-al-2024>(1/1 | 300/318) The Metric Distortion of Randomized Social Choice Functions: C1 Maximal Lottery Rules and Simulations (Fabian Frank et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--301318-many-objective-evolutionary-influence-maximization-balancing-spread-budget-fairness-and-time-elia-cunegatti-et-al-2024>(1/1 | 301/318) Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time (Elia Cunegatti et al., 2024)</a></li></ul></li><li><a href=#cslo-2>cs.LO (2)</a><ul><li><a href=#12--302318-algebraic-reasoning-meets-automata-in-solving-linear-integer-arithmetic-peter-habermehl-et-al-2024>(1/2 | 302/318) Algebraic Reasoning Meets Automata in Solving Linear Integer Arithmetic (Peter Habermehl et al., 2024)</a></li><li><a href=#22--303318-on-propositional-dynamic-logic-and-concurrency-matteo-acclavio-et-al-2024>(2/2 | 303/318) On Propositional Dynamic Logic and Concurrency (Matteo Acclavio et al., 2024)</a></li></ul></li><li><a href=#csds-5>cs.DS (5)</a><ul><li><a href=#15--304318-on-the-communication-complexity-of-approximate-pattern-matching-tomasz-kociumaka-et-al-2024>(1/5 | 304/318) On the Communication Complexity of Approximate Pattern Matching (Tomasz Kociumaka et al., 2024)</a></li><li><a href=#25--305318-hypergraph-unreliability-in-quasi-polynomial-time-ruoxu-cen-et-al-2024>(2/5 | 305/318) Hypergraph Unreliability in Quasi-Polynomial Time (Ruoxu Cen et al., 2024)</a></li><li><a href=#35--306318-new-graph-and-hypergraph-container-lemmas-with-applications-in-property-testing-eric-blais-et-al-2024>(3/5 | 306/318) New Graph and Hypergraph Container Lemmas with Applications in Property Testing (Eric Blais et al., 2024)</a></li><li><a href=#45--307318-realizing-temporal-transportation-trees-george-b-mertzios-et-al-2024>(4/5 | 307/318) Realizing temporal transportation trees (George B. Mertzios et al., 2024)</a></li><li><a href=#55--308318-minimum-sum-vertex-cover-kernelization-and-parameterized-algorithms-yixin-cao-et-al-2024>(5/5 | 308/318) Minimum sum vertex cover: kernelization and parameterized algorithms (Yixin Cao et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--309318-constraintflow-a-dsl-for-specification-and-verification-of-neural-network-analyses-avaljot-singh-et-al-2024>(1/1 | 309/318) ConstraintFlow: A DSL for Specification and Verification of Neural Network Analyses (Avaljot Singh et al., 2024)</a></li></ul></li><li><a href=#statml-6>stat.ML (6)</a><ul><li><a href=#16--310318-neural-network-based-piecewise-survival-models-olov-holmer-et-al-2024>(1/6 | 310/318) Neural Network-Based Piecewise Survival Models (Olov Holmer et al., 2024)</a></li><li><a href=#26--311318-supervised-multiple-kernel-learning-approaches-for-multi-omics-data-integration-mitja-briscik-et-al-2024>(2/6 | 311/318) Supervised Multiple Kernel Learning approaches for multi-omics data integration (Mitja Briscik et al., 2024)</a></li><li><a href=#36--312318-minimax-optimal-fair-classification-with-bounded-demographic-disparity-xianli-zeng-et-al-2024>(3/6 | 312/318) Minimax Optimal Fair Classification with Bounded Demographic Disparity (Xianli Zeng et al., 2024)</a></li><li><a href=#46--313318-steingen-generating-fidelitous-and-diverse-graph-samples-gesine-reinert-et-al-2024>(4/6 | 313/318) SteinGen: Generating Fidelitous and Diverse Graph Samples (Gesine Reinert et al., 2024)</a></li><li><a href=#56--314318-skscope-fast-sparsity-constrained-optimization-in-python-zezhi-wang-et-al-2024>(5/6 | 314/318) skscope: Fast Sparsity-Constrained Optimization in Python (Zezhi Wang et al., 2024)</a></li><li><a href=#66--315318-clustering-change-sign-detection-by-fusing-mixture-complexity-kento-urano-et-al-2024>(6/6 | 315/318) Clustering Change Sign Detection by Fusing Mixture Complexity (Kento Urano et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--316318-stability-properties-of-the-impulsive-goodwins-oscillator-in-1-cycle-anton-v-proskurnikov-et-al-2024>(1/1 | 316/318) Stability Properties of the Impulsive Goodwin&rsquo;s Oscillator in 1-cycle (Anton V. Proskurnikov et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--317318-a-census-of-graph-drawing-algorithms-based-on-generalized-transversal-structures-olivier-bernardi-et-al-2024>(1/1 | 317/318) A census of graph-drawing algorithms based on generalized transversal structures (Olivier Bernardi et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--318318-instructbrush-learning-attention-based-instruction-optimization-for-image-editing-ruoyu-zhao-et-al-2024>(1/1 | 318/318) InstructBrush: Learning Attention-based Instruction Optimization for Image Editing (Ruoyu Zhao et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>