<!doctype html><html><head><title>arXiv @ 2024.03.14</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.14"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cs.AI (6) cs.AR (1) cs.CC (1) cs.CE (5) cs.CL (48) cs.CR (6) cs.CV (88) cs.CY (2) cs.DB (3) cs.DC (5) cs.DS (3) cs.FL (1) cs.GT (2) cs.HC (5) cs.IR (8) cs.IT (5) cs.LG (55) cs.LO (1) cs.MA (3) cs.NI (3) cs.RO (16) cs.SD (3) cs.SE (7) cs.SI (1) cs.SY (1) eess.AS (2) eess.IV (9) eess.SP (4) eess.SY (11) math.AT (1) math.MG (1) math.NA (3) math.OC (2) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240314000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-14T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-14T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.14"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240314000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Mar 14, 2024</p></div><div class=title><h1>arXiv @ 2024.03.14</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csai-6>cs.AI (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csce-5>cs.CE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cscl-48>cs.CL (48)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cscr-6>cs.CR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cscv-88>cs.CV (88)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csdb-3>cs.DB (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csdc-5>cs.DC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csfl-1>cs.FL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csgt-2>cs.GT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csir-8>cs.IR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csit-5>cs.IT (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cslg-55>cs.LG (55)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csma-3>cs.MA (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csro-16>cs.RO (16)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cssd-3>cs.SD (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#csse-7>cs.SE (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#cssy-1>cs.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#eessiv-9>eess.IV (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#eesssp-4>eess.SP (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#eesssy-11>eess.SY (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#mathat-1>math.AT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#mathmg-1>math.MG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#mathna-3>math.NA (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#statap-1>stat.AP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/#statml-5>stat.ML (5)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th><th>eess.SY</th></tr></thead><tbody><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Alpaca</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>12</td><td>24</td><td>12</td><td>3</td><td>4</td></tr><tr><td>Black Box</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Claude</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Code Generation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td>2</td><td>3</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td>2</td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>ControlNet</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>8</td><td>2</td><td></td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>9</td><td>1</td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>10</td><td>2</td><td></td><td></td></tr><tr><td>Direct Preference Optimization</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Disambiguation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td>2</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Edge Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Event Argument Extraction</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>2</td><td></td><td>1</td></tr><tr><td>Federated Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Few-shot</td><td>1</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>17</td><td>14</td><td>5</td><td></td><td></td></tr><tr><td>Foundation Model</td><td>1</td><td>5</td><td></td><td></td><td></td></tr><tr><td>GLUE</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT</td><td>11</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-2</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>7</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Graph</td><td>5</td><td>3</td><td>9</td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>8</td><td></td><td></td></tr><tr><td>Grounding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Hate Speech Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td>7</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Karush-Kuhn-Tucker</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>1</td><td>6</td><td>4</td><td>1</td><td></td></tr><tr><td>Knowledge Graph</td><td>6</td><td>1</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>5</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Language Generation</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>46</td><td>18</td><td>10</td><td>3</td><td></td></tr><tr><td>Low-Resource</td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Mistral</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>6</td><td>17</td><td>3</td><td>2</td><td></td></tr><tr><td>Multi-party Dialogue</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Natural Language Generation</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>7</td><td></td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Online Reinforcement Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Open-Domain Question Answering</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Opinion Summarization</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Out-of-domain</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Parameter Sharing</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>6</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>10</td><td>12</td><td></td><td>1</td><td>1</td></tr><tr><td>Pruning</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Question Answering</td><td>4</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>6</td><td>5</td><td>1</td><td>2</td><td></td></tr><tr><td>Recommendation</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>2</td><td>1</td><td>4</td><td></td><td>1</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td>1</td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Scaling Law</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>4</td><td>2</td><td></td><td></td></tr><tr><td>Self-Distillation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-adaption</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Self-supervised Learning</td><td></td><td>3</td><td>3</td><td>2</td><td></td></tr><tr><td>Semantic Parsing</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>1</td><td>6</td><td>4</td></tr><tr><td>Simulator</td><td></td><td></td><td>1</td><td>6</td><td>4</td></tr><tr><td>Sora</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Square Loss</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>7</td><td></td><td></td></tr><tr><td>Style Transfer</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>6</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>4</td><td>8</td><td>5</td><td>1</td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Tensor Decomposition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text Augmentation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td>3</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Mining</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Transformer</td><td>5</td><td>18</td><td>7</td><td>1</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>8</td><td></td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>13</td><td>1</td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Word2vec</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>5</td><td>9</td><td>2</td><td></td><td></td></tr><tr><td>Zero-shot Learning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-48>cs.CL (48)</h2><h3 id=148--1323-investigating-the-performance-of-retrieval-augmented-generation-and-fine-tuning-for-the-development-of-ai-driven-knowledge-based-systems-robert-lakatos-et-al-2024>(1/48 | 1/323) Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems (Robert Lakatos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Lakatos, Peter Pollner, Andras Hajdu, Tamas Joo. (2024)<br><strong>Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems</strong><br><button class=copy-to-clipboard title="Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 120<br>Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, ChatGPT, GPT, Gemini, LLaMA, BLEU, Domain Adaptation, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09727v1.pdf filename=2403.09727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of generative <b>large</b> <b>language</b> <b>models</b> (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to <b>ChatGPT,</b> Bing, or <b>Gemini.</b> <b>Fine-tuning</b> (FN) and <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> are the techniques that can be used to implement <b>domain</b> <b>adaptation</b> for the development of G-LLM-based knowledge systems. In our study, using <b>ROUGE,</b> <b>BLEU,</b> METEOR scores, and cosine similarity, we compare and examine the performance of <b>RAG</b> and FN for the <b>GPT-J-6B,</b> OPT-6.7B, <b>LlaMA,</b> <b>LlaMA-2</b> language models. Based on measurements shown on different datasets, we demonstrate that <b>RAG-based</b> constructions are more efficient than models produced with FN. We point out that connecting <b>RAG</b> and FN is not trivial, because connecting FN models with <b>RAG</b> can cause a decrease in performance. Furthermore, we outline a simple <b>RAG-based</b> architecture which, on average, outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case of the <b>BLEU</b> score, and 53% based on the cosine similarity. This shows the significant advantage of <b>RAG</b> over FN in terms of hallucination, which is not offset by the fact that the average 8% better METEOR score of FN models indicates greater creativity compared to <b>RAG.</b></p></p class="citation"></blockquote><h3 id=248--2323-exploring-safety-generalization-challenges-of-large-language-models-via-code-qibing-ren-et-al-2024>(2/48 | 2/323) Exploring Safety Generalization Challenges of Large Language Models via Code (Qibing Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Yu Qiao, Wai Lam, Lizhuang Ma. (2024)<br><strong>Exploring Safety Generalization Challenges of Large Language Models via Code</strong><br><button class=copy-to-clipboard title="Exploring Safety Generalization Challenges of Large Language Models via Code" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs-SE, cs.CL<br>Keyword Score: 100<br>Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Supervised Learning, Claude, GPT, GPT-4, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07865v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07865v2.pdf filename=2403.07865v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like <b>supervised</b> <b>fine-tuning</b> and <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of <b>LLMs.</b> Our comprehensive studies on state-of-the-art <b>LLMs</b> including <b>GPT-4,</b> <b>Claude-2,</b> and <b>Llama-2</b> series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=348--3323-rethinking-aste-a-minimalist-tagging-scheme-alongside-contrastive-learning-qiao-sun-et-al-2024>(3/48 | 3/323) Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning (Qiao Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiao Sun, Liujia Yang, Minghao Ma, Nanyang Ye, Qinying Gu. (2024)<br><strong>Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning</strong><br><button class=copy-to-clipboard title="Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Contrastive Learning, Few-shot, Few-shot Learning, GPT, GPT-3, GPT-3.5, GPT-4, Sentiment Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07342v1.pdf filename=2403.07342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aspect <b>Sentiment</b> <b>Triplet</b> Extraction (ASTE) is a burgeoning subtask of fine-grained <b>sentiment</b> <b>analysis,</b> aiming to extract structured <b>sentiment</b> <b>triplets</b> from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a <b>contrastive</b> <b>learning</b> approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> our method exhibits superior efficacy compared to <b>GPT</b> 3.5 and <b>GPT</b> 4 in a <b>few-shot</b> <b>learning</b> scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=448--4323-rethinking-generative-large-language-model-evaluation-for-semantic-comprehension-fangyun-wei-et-al-2024>(4/48 | 4/323) Rethinking Generative Large Language Model Evaluation for Semantic Comprehension (Fangyun Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangyun Wei, Xi Chen, Lin Luo. (2024)<br><strong>Rethinking Generative Large Language Model Evaluation for Semantic Comprehension</strong><br><button class=copy-to-clipboard title="Rethinking Generative Large Language Model Evaluation for Semantic Comprehension" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Neural Machine Translation, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07872v1.pdf filename=2403.07872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite their sophisticated capabilities, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice <b>question</b> <b>answering</b> (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 <b>benchmarks,</b> we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 <b>LLMs</b> such as <b>GPT-4,</b> <b>GPT-3.5,</b> Google-Gemini-Pro and <b>LLaMA-1/-2,</b> in a two-player competitive format, with <b>GPT-4</b> serving as the judge. Each <b>LLM</b> receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new <b>benchmark</b> called ``Real-world <b>questions&rsquo;&rsquo;</b> <b>(RWQ),</b> comprising 20,772 authentic user inquiries. Additionally, we thoroughly analyze the characteristics of our system and compare it with prior leaderboards like AlpacaEval and <b>MT-Bench.</b> Our analysis reveals the stability of our RWQ-Elo system, the feasibility of registering new models, and its potential to reshape <b>LLM</b> leaderboards.</p></p class="citation"></blockquote><h3 id=548--5323-knowledge-graph-large-language-model-kg-llm-for-link-prediction-dong-shu-et-al-2024>(5/48 | 5/323) Knowledge Graph Large Language Model (KG-LLM) for Link Prediction (Dong Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Mengnan Du, Yongfeng Zhang. (2024)<br><strong>Knowledge Graph Large Language Model (KG-LLM) for Link Prediction</strong><br><button class=copy-to-clipboard title="Knowledge Graph Large Language Model (KG-LLM) for Link Prediction" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 93<br>Keywords: Graph, Fine-tuning, Knowledge Graph, Knowledge Graph, Zero-shot, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07311v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07311v3.pdf filename=2403.07311v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of predicting multiple links within <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> stands as a challenge in the field of <b>knowledge</b> <b>graph</b> analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and <b>KG</b> embedding techniques. This paper introduces a novel methodology, the <b>Knowledge</b> <b>Graph</b> <b>Large</b> <b>Language</b> <b>Model</b> Framework <b>(KG-LLM),</b> which leverages pivotal NLP paradigms, including chain-of-thought (CoT) <b>prompting</b> and <b>in-context</b> <b>learning</b> <b>(ICL),</b> to enhance multi-hop link prediction in <b>KGs.</b> By converting the <b>KG</b> to a CoT <b>prompt,</b> our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the <b>KG-LLM</b> Framework, we <b>fine-tune</b> three leading <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> within this framework, employing both non-ICL and <b>ICL</b> tasks for a comprehensive evaluation. Further, we explore the framework&rsquo;s potential to provide <b>LLMs</b> with <b>zero-shot</b> capabilities for handling previously unseen <b>prompts.</b> Our experimental findings discover that integrating <b>ICL</b> and CoT not only augments the performance of our approach but also significantly boosts the models&rsquo; generalization capacity, thereby ensuring more precise predictions in unfamiliar scenarios.</p></p class="citation"></blockquote><h3 id=648--6323-fine-tuning-large-language-models-with-sequential-instructions-hanxu-hu-et-al-2024>(6/48 | 6/323) Fine-tuning Large Language Models with Sequential Instructions (Hanxu Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxu Hu, Pinzhen Chen, Edoardo M. Ponti. (2024)<br><strong>Fine-tuning Large Language Models with Sequential Instructions</strong><br><button class=copy-to-clipboard title="Fine-tuning Large Language Models with Sequential Instructions" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 86<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Alpaca, LLaMA, Reasoning, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07794v1.pdf filename=2403.07794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> struggle to follow a sequence of <b>instructions</b> <b>in</b> a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and <b>multimodal</b> (caption then answer) tasks. We empirically verify this with open-source <b>LLMs</b> as <b>large</b> <b>as</b> <b>LLaMA-2</b> 70B and Mixtral-8x7B. Targeting the scarcity of sequential <b>instructions</b> <b>in</b> present-day data, we propose sequential <b>instruction</b> <b>tuning,</b> a simple yet effective strategy to automatically augment <b>instruction</b> <b>tuning</b> data and equip <b>LLMs</b> with the ability to execute multiple sequential <b>instructions.</b> <b>After</b> exploring interleaving <b>instructions</b> <b>in</b> existing datasets, such as <b>Alpaca,</b> with a wide range of intermediate tasks, we find that sequential <b>instruction-tuned</b> <b>models</b> consistently outperform the conventional <b>instruction-tuned</b> <b>baselines</b> in downstream tasks involving <b>reasoning,</b> multilingual, and <b>multimodal</b> abilities. To shed further light on our technique, we analyse how adversarial intermediate texts, unseen tasks, <b>prompt</b> verbalization, number of tasks, and <b>prompt</b> length affect SIT. We hope that this method will open new research avenues on <b>instruction</b> <b>tuning</b> for complex tasks.</p></p class="citation"></blockquote><h3 id=748--7323-rad-phi2-instruction-tuning-phi-2-for-radiology-mercy-ranjit-et-al-2024>(7/48 | 7/323) RAD-PHI2: Instruction Tuning PHI-2 for Radiology (Mercy Ranjit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mercy Ranjit, Gopinath Ganapathy, Shaury Srivastav, Tanuja Ganu, Srujana Oruganti. (2024)<br><strong>RAD-PHI2: Instruction Tuning PHI-2 for Radiology</strong><br><button class=copy-to-clipboard title="RAD-PHI2: Instruction Tuning PHI-2 for Radiology" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: J-3, cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-4, Mistral, Question Answering, Reasoning, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09725v1.pdf filename=2403.09725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Small Language Models (SLMs) have shown remarkable performance in general domain language understanding, <b>reasoning</b> and coding tasks, but their capabilities in the medical domain, particularly concerning radiology text, is less explored. In this study, we investigate the application of SLMs for general radiology knowledge specifically <b>question</b> <b>answering</b> related to understanding of symptoms, radiological appearances of findings, differential diagnosis, assessing prognosis, and suggesting treatments w.r.t diseases pertaining to different organ systems. Additionally, we explore the utility of SLMs in handling text-related tasks with respect to radiology reports within AI-driven radiology workflows. We <b>fine-tune</b> Phi-2, a SLM with 2.7 billion parameters using high-quality educational content from Radiopaedia, a collaborative online radiology resource. The resulting language model, RadPhi-2-Base, exhibits the ability to address general radiology queries across various systems (e.g., chest, cardiac). Furthermore, we investigate Phi-2 for <b>instruction</b> <b>tuning,</b> enabling it to perform specific tasks. By <b>fine-tuning</b> Phi-2 on both general domain tasks and radiology-specific tasks related to chest X-ray reports, we create Rad-Phi2. Our empirical results reveal that Rad-Phi2 Base and Rad-Phi2 perform comparably or even outperform larger models such as <b>Mistral-7B-Instruct-v0.2</b> and <b>GPT-4</b> providing concise and precise answers. In summary, our work demonstrates the feasibility and effectiveness of utilizing SLMs in radiology workflows both for knowledge related queries as well as for performing specific tasks related to radiology reports thereby opening up new avenues for enhancing the quality and efficiency of radiology practice.</p></p class="citation"></blockquote><h3 id=848--8323-sifid-reassess-summary-factual-inconsistency-detection-with-llm-jiuding-yang-et-al-2024>(8/48 | 8/323) SIFiD: Reassess Summary Factual Inconsistency Detection with LLM (Jiuding Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiuding Yang, Hui Liu, Weidong Guo, Zhuwei Rao, Yu Xu, Di Niu. (2024)<br><strong>SIFiD: Reassess Summary Factual Inconsistency Detection with LLM</strong><br><button class=copy-to-clipboard title="SIFiD: Reassess Summary Factual Inconsistency Detection with LLM" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Natural Language Inference, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07557v1.pdf filename=2403.07557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring factual consistency between the summary and the original document is paramount in <b>summarization</b> tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that <b>LLMs</b> underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with <b>LLMs,</b> comparing the performances of <b>GPT-3.5</b> and <b>GPT-4.</b> To advance research in <b>LLM-based</b> inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing <b>natural</b> <b>language</b> <b>inference</b> or measuring semantic similarity between summaries and documents.</p></p class="citation"></blockquote><h3 id=948--9323-training-small-multimodal-models-to-bridge-biomedical-competency-gap-a-case-study-in-radiology-imaging-juan-manuel-zambrano-chaves-et-al-2024>(9/48 | 9/323) Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging (Juan Manuel Zambrano Chaves et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, Hany Awadalla, Julia Gong, Houdong Hu, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Yu Gu, Cliff Wong, Mu Wei, Tristan Naumann, Muhao Chen, Matthew P. Lungren, Serena Yeung-Levy, Curtis P. Langlotz, Sheng Wang, Hoifung Poon. (2024)<br><strong>Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging</strong><br><button class=copy-to-clipboard title="Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 79<br>Keywords: Benchmarking, Foundation Model, Multi-modal, Multi-modal, GPT, GPT-4, Grounding, Image2text, Scaling Law, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08002v1.pdf filename=2403.08002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>scaling</b> <b>laws</b> and extraordinary performance of large <b>foundation</b> <b>models</b> motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical <b>benchmarks,</b> there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as <b>GPT-4V</b> still have major competency gaps in <b>multimodal</b> capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small <b>multimodal</b> models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and <b>text</b> <b>modalities,</b> and focusing on training a lightweight adapter to ground each modality to the <b>text</b> <b>embedding</b> space. We conduct a comprehensive study of this approach on radiology imaging. For training, we assemble a large dataset with over 1 million <b>image-text</b> pairs. For evaluation, we propose a clinically driven novel approach using <b>GPT-4</b> and demonstrate its parity with expert evaluation. We also study <b>grounding</b> qualitatively using attention. For best practice, we conduct a systematic ablation study on various choices in data engineering and <b>multimodal</b> training. The resulting LLaVA-Rad (7B) model attains state-of-the-art results on radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as <b>GPT-4V</b> and Med-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications.</p></p class="citation"></blockquote><h3 id=1048--10323-complex-reasoning-over-logical-queries-on-commonsense-knowledge-graphs-tianqing-fang-et-al-2024>(10/48 | 10/323) Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs (Tianqing Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut. (2024)<br><strong>Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs</strong><br><button class=copy-to-clipboard title="Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 78<br>Keywords: Graph, Knowledge Graph, Out-of-domain, Zero-shot, Common-sense Reasoning, Question Answering, Reasoning, Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07398v1.pdf filename=2403.07398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event <b>commonsense</b> <b>reasoning</b> requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate <b>commonsense</b> <b>inferences</b> for contexts and <b>questions</b> <b>involving</b> interactions between complex events. To address this demand, we present COM2 (COMplex <b>COMmonsense),</b> <b>a</b> new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing <b>commonsense</b> <b>knowledge</b> <b>graph</b> (CSKG), and verbalizing them using handcrafted rules and <b>large</b> <b>language</b> <b>models</b> into multiple-choice and <b>text</b> <b>generation</b> <b>questions.</b> <b>Our</b> experiments show that language models trained on COM2 exhibit significant improvements in complex <b>reasoning</b> ability, resulting in enhanced <b>zero-shot</b> performance in both in-domain and <b>out-of-domain</b> tasks for <b>question</b> <b>answering</b> and generative <b>commonsense</b> <b>reasoning,</b> without expensive human annotations.</p></p class="citation"></blockquote><h3 id=1148--11323-llmvssmall-model-large-language-model-based-text-augmentation-enhanced-personality-detection-model-linmei-hu-et-al-2024>(11/48 | 11/323) LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model (Linmei Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linmei Hu, Hongyu He, Duokang Wang, Ziwang Zhao, Yingxia Shao, Liqiang Nie. (2024)<br><strong>LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model</strong><br><button class=copy-to-clipboard title="LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Contrastive Learning, Fine-tuning, Knowledge Distillation, Text Augmentation, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07581v1.pdf filename=2403.07581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personality detection aims to detect one&rsquo;s personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by <b>fine-tuning</b> the <b>pre-trained</b> <b>language</b> <b>models</b> under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> based <b>text</b> <b>augmentation</b> enhanced personality detection model, which <b>distills</b> the <b>LLM&rsquo;s</b> knowledge to enhance the small model for personality detection, even when the <b>LLM</b> fails in this task. Specifically, we enable <b>LLM</b> to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection. By using <b>contrastive</b> <b>learning</b> to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection. Furthermore, we utilize the <b>LLM</b> to enrich the information of personality labels for enhancing the detection performance. Experimental results on the <b>benchmark</b> datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection.</p></p class="citation"></blockquote><h3 id=1248--12323-matrix-transformation-based-low-rank-adaptation-mtlora-a-brain-inspired-method-for-parameter-efficient-fine-tuning-yao-liang-et-al-2024>(12/48 | 12/323) Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning (Yao Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Liang, Yuwei Wang, Yang Li, Yi Zeng. (2024)<br><strong>Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning</strong><br><button class=copy-to-clipboard title="Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Language Generation, Natural Language Generation, Natural Language Generation, Natural Language Understanding, GLUE, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07440v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07440v2.pdf filename=2403.07440v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> techniques based on Large <b>Pretrained</b> <b>Language</b> <b>Models</b> (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for <b>fine-tuning</b> a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization <b>fine-tuning</b> methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameterization method for efficient <b>fine-tuning,</b> named Matrix-Transformation based Low-Rank Adaptation (MTLoRA). MTLoRA aims to dynamically alter its spatial geometric structure by applying a transformation-matrix T to perform linear transformations, such as rotation, scaling, and translation, on the task-specific parameter matrix, generating new matrix feature patterns (eigenvectors) to mimic the fundamental influence of complex geometric structure feature patterns in the brain on functions, thereby enhancing the model&rsquo;s performance in downstream tasks. In <b>Natural</b> <b>Language</b> <b>Understanding</b> (NLU) tasks, it is evaluated using the <b>GLUE</b> <b>benchmark</b> test, and the results reveal that MTLoRA achieves an overall performance increase of about 1.0% across eight tasks; in <b>Natural</b> <b>Language</b> <b>Generation</b> <b>(NLG)</b> tasks, MTLoRA improves performance by an average of 0.95% and 0.56% in the DART and WebNLG tasks, respectively.</p></p class="citation"></blockquote><h3 id=1348--13323-harnessing-artificial-intelligence-to-combat-online-hate-exploring-the-challenges-and-opportunities-of-large-language-models-in-hate-speech-detection-tharindu-kumarage-et-al-2024>(13/48 | 13/323) Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection (Tharindu Kumarage et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tharindu Kumarage, Amrita Bhattacharjee, Joshua Garland. (2024)<br><strong>Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection</strong><br><button class=copy-to-clipboard title="Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Hate Speech Detection, Language Generation, Sentiment Analysis, Text Classification, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08035v1.pdf filename=2403.08035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> excel in many diverse applications beyond <b>language</b> <b>generation,</b> e.g., translation, <b>summarization,</b> and <b>sentiment</b> <b>analysis.</b> One intriguing application is in <b>text</b> <b>classification.</b> This becomes pertinent in the realm of identifying hateful or toxic speech &ndash; a domain fraught with challenges and ethical dilemmas. In our study, we have two objectives: firstly, to offer a literature review revolving around <b>LLMs</b> as classifiers, emphasizing their role in detecting and classifying hateful or toxic content. Subsequently, we explore the efficacy of several <b>LLMs</b> in classifying <b>hate</b> <b>speech:</b> <b>identifying</b> which <b>LLMs</b> excel in this task as well as their underlying attributes and training. Providing insight into the factors that contribute to an <b>LLM</b> proficiency (or lack thereof) in discerning hateful content. By combining a comprehensive literature review with an empirical analysis, our paper strives to shed light on the capabilities and constraints of <b>LLMs</b> in the crucial domain of <b>hate</b> <b>speech</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=1448--14323-semeval-2024-shared-task-6-shroom-a-shared-task-on-hallucinations-and-related-observable-overgeneration-mistakes-timothee-mickus-et-al-2024>(14/48 | 14/323) SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes (Timothee Mickus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timothee Mickus, Elaine Zosa, Raúl Vázquez, Teemu Vahtola, Jörg Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki. (2024)<br><strong>SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes</strong><br><button class=copy-to-clipboard title="SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Zero-shot, Language Generation, Natural Language Generation, Natural Language Generation, Neural Machine Translation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07726v1.pdf filename=2403.07726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from <b>natural</b> <b>language</b> <b>generation</b> <b>(NLG)</b> systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many <b>NLG</b> applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: <b>machine</b> <b>translation,</b> paraphrase generation and definition modeling. The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task. We observe a number of key trends in how this approach was tackled &ndash; many participants rely on a handful of model, and often rely either on synthetic data for <b>fine-tuning</b> or <b>zero-shot</b> <b>prompting</b> strategies. While a majority of the teams did outperform our proposed baseline system, the performances of top-scoring systems are still consistent with a random handling of the more challenging items.</p></p class="citation"></blockquote><h3 id=1548--15323-improving-reinforcement-learning-from-human-feedback-using-contrastive-rewards-wei-shen-et-al-2024>(15/48 | 15/323) Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards (Wei Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang Liu. (2024)<br><strong>Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards</strong><br><button class=copy-to-clipboard title="Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, GPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07708v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07708v2.pdf filename=2403.07708v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> is the mainstream paradigm used to align <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human preferences. Yet existing <b>RLHF</b> heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to <b>prompts</b> that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the <b>LLM</b> to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in PPO. We show empirically contrastive rewards can improve <b>RLHF</b> substantially, evaluated by both <b>GPTs</b> and humans, and our method consistently outperforms strong baselines.</p></p class="citation"></blockquote><h3 id=1648--16323-smalltolarge-s2l-scalable-data-selection-for-fine-tuning-large-language-models-by-summarizing-training-trajectories-of-small-models-yu-yang-et-al-2024>(16/48 | 16/323) SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models (Yu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, Baharan Mirzasoleiman. (2024)<br><strong>SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models</strong><br><button class=copy-to-clipboard title="SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Supervised Learning, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07384v1.pdf filename=2403.07384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the effectiveness of data selection for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> during pretraining and instruction <b>fine-tuning</b> phases, improving data efficiency in <b>supervised</b> <b>fine-tuning</b> (SFT) for specialized domains poses significant challenges due to the complexity of <b>fine-tuning</b> data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most challenging MATH (Hendrycks et al., 2021) <b>benchmark,</b> improving Phi-2 (Li et al., 2023b) by 16.6%. In clinical <b>text</b> <b>summarization</b> on the MIMIC-III dataset (Johnson et al., 2016), S2L again outperforms training on the full dataset using only 50% of the data. Notably, S2L can perform data selection using a reference model 40x smaller than the target model, proportionally reducing the cost of data selection.</p></p class="citation"></blockquote><h3 id=1748--17323-fine-tuning-vs-prompting-can-language-models-understand-human-values-pingwei-sun-2024>(17/48 | 17/323) Fine-tuning vs Prompting, Can Language Models Understand Human Values? (Pingwei Sun, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pingwei Sun. (2024)<br><strong>Fine-tuning vs Prompting, Can Language Models Understand Human Values?</strong><br><button class=copy-to-clipboard title="Fine-tuning vs Prompting, Can Language Models Understand Human Values?" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Reinforcement Learning from Human Feedback, Natural Language Understanding, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09720v1.pdf filename=2403.09720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately handling the underlying support values in sentences is crucial for understanding the speaker&rsquo;s tendencies, yet it poses a challenging task in <b>natural</b> <b>language</b> <b>understanding</b> (NLU). In this article, we explore the potential of <b>fine-tuning</b> and <b>prompt</b> tuning in this downstream task, using the Human Value Detection 2023. Additionally, we attempt to validate whether models can effectively solve the problem based on the knowledge acquired during the pre-training stage. Simultaneously, our interest lies in the capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> aligned with <b>RLHF</b> in this task, and some preliminary attempts are presented.</p></p class="citation"></blockquote><h3 id=1848--18323-ckerc--joint-large-language-models-with-commonsense-knowledge-for-emotion-recognition-in-conversation-yumeng-fu-2024>(18/48 | 18/323) CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation (Yumeng Fu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yumeng Fu. (2024)<br><strong>CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation</strong><br><button class=copy-to-clipboard title="CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Emotion Recognition, Multi-party Dialogue, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07260v1.pdf filename=2403.07260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Emotion</b> <b>recognition</b> in conversation (ERC) is a task which predicts the <b>emotion</b> <b>of</b> an utterance in the context of a conversation. It tightly depends on dialogue context, speaker identity information, <b>multiparty</b> <b>dialogue</b> scenario and so on. However, the state-of-the-art method (instructERC) solely identifying speaker, and ignores commonsense knowledge(i.e., reaction of the listeners and intention of the speaker, etc.) behind speakers during a conversation, which can deeply mine speaker information. To this end, we propose a novel joint <b>large</b> <b>language</b> <b>models</b> with commonsense knowledge framework for <b>emotion</b> <b>recognition</b> in conversation, namely CKERC.We design <b>prompts</b> to generate interlocutors&rsquo; commonsense based on historical utterances with <b>large</b> <b>language</b> <b>model.</b> And we use the interlocutor commonsense identification task for <b>LLM</b> pre-training to <b>fine-tune</b> speaker implicit clues information.By solving above challenge, our method achieve state-of-the-art.We extensive experiment on three widely-used datasets, i.e., IEMOCAP, MELD, EmoryNLP, demonstrate our method superiority. Also, we conduct in-depth analysis and further demonstrate the effectiveness of commonsense knowledge in ERC task in <b>large</b> <b>language</b> <b>model.</b></p></p class="citation"></blockquote><h3 id=1948--19323-stabletoolbench-towards-stable-large-scale-benchmarking-on-tool-learning-of-large-language-models-zhicheng-guo-et-al-2024>(19/48 | 19/323) StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models (Zhicheng Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, Yang Liu. (2024)<br><strong>StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models</strong><br><button class=copy-to-clipboard title="StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Benchmarking, Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07714v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07714v2.pdf filename=2403.07714v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have witnessed remarkable advancements in recent years, <b>prompting</b> the exploration of tool learning, which integrates <b>LLMs</b> with external tools to address diverse real-world challenges. Assessing the capability of <b>LLMs</b> to utilise tools necessitates <b>large-scale</b> <b>and</b> <b>stable</b> <b>benchmarks.</b> However, previous works relied on either hand-crafted online tools with limited scale, or <b>large-scale</b> <b>real</b> <b>online</b> APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a <b>benchmark</b> evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using <b>GPT-4</b> as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.</p></p class="citation"></blockquote><h3 id=2048--20323-a-semantic-mention-graph-augmented-model-for-document-level-event-argument-extraction-jian-zhang-et-al-2024>(20/48 | 20/323) A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction (Jian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Zhang, Changlin Yang, Haiping Zhu, Qika Lin, Fangzhi Xu, Jun Liu. (2024)<br><strong>A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction</strong><br><button class=copy-to-clipboard title="A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Graph, Transformer, Event Argument Extraction, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09721v1.pdf filename=2403.09721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Document-level <b>Event</b> <b>Argument</b> <b>Extraction</b> (DEAE) aims to identify arguments and their specific roles from an unstructured document. The advanced approaches on DEAE utilize <b>prompt-based</b> methods to guide <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> in extracting arguments from input documents. They mainly concentrate on establishing relations between triggers and entity mentions within documents, leaving two unresolved problems: a) independent modeling of entity mentions; b) document-prompt isolation. To this end, we propose a semantic mention <b>Graph</b> Augmented Model (GAM) to address these two problems in this paper. Firstly, GAM constructs a semantic mention <b>graph</b> that captures relations within and between documents and <b>prompts,</b> encompassing co-existence, co-reference and co-type relations. Furthermore, we introduce an ensembled <b>graph</b> <b>transformer</b> module to address mentions and their three semantic relations effectively. Later, the <b>graph-augmented</b> encoder-decoder module incorporates the relation-specific <b>graph</b> into the input embedding of <b>PLMs</b> and optimizes the encoder section with topology information, enhancing the relations comprehensively. Extensive experiments on the RAMS and WikiEvents datasets demonstrate the effectiveness of our approach, surpassing baseline methods and achieving a new state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=2148--21323-orpo-monolithic-preference-optimization-without-reference-model-jiwoo-hong-et-al-2024>(21/48 | 21/323) ORPO: Monolithic Preference Optimization without Reference Model (Jiwoo Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiwoo Hong, Noah Lee, James Thorne. (2024)<br><strong>ORPO: Monolithic Preference Optimization without Reference Model</strong><br><button class=copy-to-clipboard title="ORPO: Monolithic Preference Optimization without Reference Model" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Supervised Learning, LLaMA, Mistral, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07691v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07691v2.pdf filename=2403.07691v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent preference alignment algorithms for language models have demonstrated promising results, <b>supervised</b> <b>fine-tuning</b> (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, <b>fine-tuning</b> Phi-2 (2.7B), <b>Llama-2</b> (7B), and <b>Mistral</b> (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on $\text{AlpacaEval}_{2.0}$ (Figure 1), 66.19% on IFEval (instruction-level loose, Table 6), and 7.32 in <b>MT-Bench</b> (Figure 12). We release code and model checkpoints for <b>Mistral-ORPO-$\alpha$</b> (7B) and <b>Mistral-ORPO-$\beta$</b> (7B).</p></p class="citation"></blockquote><h3 id=2248--22323-moralbert-detecting-moral-values-in-social-discourse-vjosa-preniqi-et-al-2024>(22/48 | 22/323) MoralBERT: Detecting Moral Values in Social Discourse (Vjosa Preniqi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vjosa Preniqi, Iacopo Ghinassi, Kyriaki Kalimeri, Charalampos Saitis. (2024)<br><strong>MoralBERT: Detecting Moral Values in Social Discourse</strong><br><button class=copy-to-clipboard title="MoralBERT: Detecting Moral Values in Social Discourse" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Out-of-domain, BERT, Word2vec, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07678v1.pdf filename=2403.07678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements. Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content. Here, we design a range of language representation models <b>fine-tuned</b> to capture exactly the moral nuances in text, called MoralBERT. We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics. This approach broadens linguistic diversity and potentially enhances the models&rsquo; ability to comprehend morality in various contexts. We also explore a <b>domain</b> <b>adaptation</b> technique and compare it to the standard <b>fine-tuned</b> <b>BERT</b> model, using two different frameworks for moral prediction: single-label and multi-label. We compare in-domain approaches with conventional models relying on lexicon-based techniques, as well as a Machine Learning classifier with <b>Word2Vec</b> representation. Our results showed that in-domain prediction models significantly outperformed traditional models. While the single-label setting reaches a higher accuracy than previously achieved for the task when using <b>BERT</b> pretrained models. Experiments in an <b>out-of-domain</b> setting, instead, suggest that further work is needed for existing <b>domain</b> <b>adaptation</b> techniques to generalise between different social media platforms, especially for the multi-label task. The investigations and outcomes from this study pave the way for further exploration, enabling a more profound comprehension of moral narratives about controversial social issues.</p></p class="citation"></blockquote><h3 id=2348--23323-triples-to-isixhosa-t2x-addressing-the-challenges-of-low-resource-agglutinative-data-to-text-generation-francois-meyer-et-al-2024>(23/48 | 23/323) Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation (Francois Meyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francois Meyer, Jan Buys. (2024)<br><strong>Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation</strong><br><button class=copy-to-clipboard title="Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Low-Resource, Neural Machine Translation, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07567v1.pdf filename=2403.07567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for <b>low-resource</b> languages are largely unexplored. In this paper we tackle data-to-text for isiXhosa, which is <b>low-resource</b> and agglutinative. We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques. We also develop an evaluation framework for T2X that measures how accurately generated text describes the data. This enables future users of T2X to go beyond surface-level metrics in evaluation. On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs).</b> We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy entities, and outperforms existing dedicated models for 2 agglutinative languages (isiXhosa and Finnish). We investigate <b>pretrained</b> <b>solutions</b> <b>for</b> T2X, which reveals that standard <b>PLMs</b> come up short. <b>Fine-tuning</b> <b>machine</b> <b>translation</b> models emerges as the best method overall. These findings underscore the distinct challenge presented by T2X: neither well-established data-to-text architectures nor customary <b>pretrained</b> <b>methodologies</b> <b>prove</b> optimal. We conclude with a qualitative analysis of generation errors and an ablation study.</p></p class="citation"></blockquote><h3 id=2448--24323-curry-dpo-enhancing-alignment-using-curriculum-learning--ranked-preferences-pulkit-pattnaik-et-al-2024>(24/48 | 24/323) Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences (Pulkit Pattnaik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, Sathwik Tejaswi Madhusudhan. (2024)<br><strong>Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences</strong><br><button class=copy-to-clipboard title="Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Curriculum Learning, Direct Preference Optimization, Neural Machine Translation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07230v1.pdf filename=2403.07230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user <b>prompt)</b> to align <b>LLMs</b> to human preferences. In practice, multiple responses can exist for a given <b>prompt</b> with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given <b>prompt.</b> Our work focuses on systematically using the constructed multiple preference pair in DPO training via <b>curriculum</b> <b>learning</b> methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating <b>curriculum</b> <b>training)</b> according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set, highlighting its effectiveness. More specifically, Curry-DPO achieves a score of 7.43 on <b>MT-bench</b> with Zephy-7B model outperforming majority of existing <b>LLMs</b> with similar parameter size. Curry-DPO also achieves the highest adjusted win rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and 87.9% respectively) in our experiments, with notable gains of upto 7.5% when compared to standard DPO technique.</p></p class="citation"></blockquote><h3 id=2548--25323-finemath-a-fine-grained-mathematical-evaluation-benchmark-for-chinese-large-language-models-yan-liu-et-al-2024>(25/48 | 25/323) FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models (Yan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Liu, Renren Jin, Lin Shi, Zheng Yao, Deyi Xiong. (2024)<br><strong>FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models</strong><br><button class=copy-to-clipboard title="FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07747v1.pdf filename=2403.07747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To thoroughly assess the <b>mathematical</b> <b>reasoning</b> abilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> we need to carefully curate evaluation datasets covering diverse <b>mathematical</b> <b>concepts</b> and <b>mathematical</b> <b>problems</b> at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained <b>mathematical</b> <b>evaluation</b> <b>benchmark</b> dataset for assessing Chinese <b>LLMs.</b> FineMath is created to cover the major key <b>mathematical</b> <b>concepts</b> taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of <b>mathematical</b> <b>reasoning</b> abilities of <b>LLMs.</b> All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of <b>reasoning</b> steps required to solve these problems. We conduct extensive experiments on a wide range of <b>LLMs</b> on FineMath and find that there is still considerable room for improvements in terms of <b>mathematical</b> <b>reasoning</b> capability of Chinese <b>LLMs.</b> We also carry out an in-depth analysis on the evaluation process and methods that have been overlooked previously. These two factors significantly influence the model results and our understanding of their <b>mathematical</b> <b>reasoning</b> capabilities. The dataset will be publicly available soon.</p></p class="citation"></blockquote><h3 id=2648--26323-contextual-clarity-generating-sentences-with-transformer-models-using-context-reverso-data-ruslan-musaev-2024>(26/48 | 26/323) Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data (Ruslan Musaev, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruslan Musaev. (2024)<br><strong>Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data</strong><br><button class=copy-to-clipboard title="Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: T5, Transformer, In-context Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08103v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08103v2.pdf filename=2403.08103v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the age of information abundance, the ability to provide users with contextually relevant and concise information is crucial. Keyword in Context (KIC) generation is a task that plays a vital role in and generation applications, such as search engines, personal assistants, and content <b>summarization.</b> In this paper, we present a novel approach to generating unambiguous and brief sentence-contexts for given keywords using the <b>T5</b> <b>transformer</b> model, leveraging data obtained from the Context-Reverso API. The code is available at <a href=https://github.com/Rusamus/word2context/tree/main>https://github.com/Rusamus/word2context/tree/main</a> .</p></p class="citation"></blockquote><h3 id=2748--27323-gujarati-english-code-switching-speech-recognition-using-ensemble-prediction-of-spoken-language-yash-sharma-et-al-2024>(27/48 | 27/323) Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language (Yash Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yash Sharma, Basil Abraham, Preethi Jyothi. (2024)<br><strong>Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language</strong><br><button class=copy-to-clipboard title="Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Supervised Learning, Transformer, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08011v1.pdf filename=2403.08011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An important and difficult task in code-switched <b>speech</b> <b>recognition</b> is to recognize the language, as lots of words in two languages can sound similar, especially in some accents. We focus on improving performance of end-to-end <b>Automatic</b> <b>Speech</b> <b>Recognition</b> models by conditioning <b>transformer</b> layers on language ID of words and character in the output in an per layer <b>supervised</b> manner. To this end, we propose two methods of introducing language specific parameters and explainability in the multi-head attention mechanism, and implement a Temporal Loss that helps maintain continuity in input alignment. Despite being unable to reduce WER significantly, our method shows promise in predicting the correct language from just spoken data. We introduce regularization in the language prediction by dropping LID in the sequence, which helps align long repeated output sequences.</p></p class="citation"></blockquote><h3 id=2848--28323-branch-train-mix-mixing-expert-llms-into-a-mixture-of-experts-llm-sainbayar-sukhbaatar-et-al-2024>(28/48 | 28/323) Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM (Sainbayar Sukhbaatar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston, Xian Li. (2024)<br><strong>Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM</strong><br><button class=copy-to-clipboard title="Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07816v1.pdf filename=2403.07816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate efficient methods for training <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to possess capabilities in multiple specialized domains, such as coding, math <b>reasoning</b> and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE <b>finetuning</b> stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.</p></p class="citation"></blockquote><h3 id=2948--29323-beyond-memorization-the-challenge-of-random-memory-access-in-language-models-tongyao-zhu-et-al-2024>(29/48 | 29/323) Beyond Memorization: The Challenge of Random Memory Access in Language Models (Tongyao Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min Lin. (2024)<br><strong>Beyond Memorization: The Challenge of Random Memory Access in Language Models</strong><br><button class=copy-to-clipboard title="Beyond Memorization: The Challenge of Random Memory Access in Language Models" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-2, Open-Domain Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07805v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07805v2.pdf filename=2403.07805v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., <b>GPT-2)</b> is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded <b>question</b> <b>answering,</b> we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of <b>open-domain</b> <b>question</b> <b>answering,</b> we validate that enhancing random access by recitation leads to notable improvements in <b>question</b> <b>answering.</b> The code to reproduce our experiments can be found at <a href=https://github.com/sail-sg/lm-random-memory-access>https://github.com/sail-sg/lm-random-memory-access</a>.</p></p class="citation"></blockquote><h3 id=3048--30323-large-small-or-both-a-novel-data-augmentation-framework-based-on-language-models-for-debiasing-opinion-summarization-yanyue-zhang-et-al-2024>(30/48 | 30/323) Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization (Yanyue Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanyue Zhang, Pengfei Li, Yilong Lai, Deyu Zhou. (2024)<br><strong>Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization</strong><br><button class=copy-to-clipboard title="Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Data Augmentation, Opinion Summarization, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07693v1.pdf filename=2403.07693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As more than 70$%$ of reviews in the existing <b>opinion</b> <b>summary</b> <b>data</b> <b>set</b> are positive, current <b>opinion</b> <b>summarization</b> approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional <b>data</b> <b>based</b> on <b>large</b> <b>language</b> <b>models</b> to balance the emotional distribution of the dataset. However, <b>data</b> <b>augmentation</b> based on <b>large</b> <b>language</b> <b>models</b> faces two disadvantages: 1) the potential issues or toxicity in the augmented <b>data;</b> <b>2)</b> the expensive costs. Therefore, in this paper, we propose a novel <b>data</b> <b>augmentation</b> framework based on both <b>large</b> <b>and</b> <b>small</b> language models for debiasing <b>opinion</b> <b>summarization.</b> In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a <b>large</b> <b>language</b> <b>model.</b> Then, a disentangle reconstruction model is trained based on the generated <b>data.</b> <b>After</b> training, a <b>large</b> <b>amount</b> <b>of</b> synthetic <b>data</b> <b>can</b> be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on confusion degree and sentiment classification. Experiments have proved that our framework can effectively alleviate emotional bias same as using only <b>large</b> <b>models,</b> <b>but</b> more economically.</p></p class="citation"></blockquote><h3 id=3148--31323-truth-aware-context-selection-mitigating-the-hallucinations-of-large-language-models-being-misled-by-untruthful-contexts-tian-yu-et-al-2024>(31/48 | 31/323) Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts (Tian Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Yu, Shaolei Zhang, Yang Feng. (2024)<br><strong>Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts</strong><br><button class=copy-to-clipboard title="Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Text Generation, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07556v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07556v2.pdf filename=2403.07556v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated impressive <b>text</b> <b>generation</b> capabilities, they are easily misled by the untruthful context provided by users or knowledge augmentation tools, thereby producing hallucinations. To alleviate the <b>LLMs</b> from being misled by untruthful information and take advantage of knowledge augmentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the <b>LLM.</b> Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the <b>LLMs&rsquo;</b> ability to accept truthful information and resist untruthful information. Experimental results show that TACS can effectively filter information in context and significantly improve the overall quality of <b>LLMs&rsquo;</b> responses when presented with misleading information.</p></p class="citation"></blockquote><h3 id=3248--32323-comprehensive-implementation-of-textcnn-for-enhanced-collaboration-between-natural-language-processing-and-system-recommendation-xiaonan-xu-et-al-2024>(32/48 | 32/323) Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation (Xiaonan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaonan Xu, Zheng Xu, Zhipeng Ling, Zhengyu Jin, ShuQian Du. (2024)<br><strong>Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation</strong><br><button class=copy-to-clipboard title="Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Recommendation, Semantic Parsing, Text Classification, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09718v1.pdf filename=2403.09718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural Language Processing (NLP) is an important branch of artificial intelligence that studies how to enable computers to understand, process, and generate human language. <b>Text</b> <b>classification</b> is a fundamental task in NLP, which aims to classify <b>text</b> <b>into</b> different predefined categories. <b>Text</b> <b>classification</b> is the most basic and classic task in natural language processing, and most of the tasks in natural language processing can be regarded as classification tasks. In recent years, deep learning has achieved great success in many research fields, and today, it has also become a standard technology in the field of NLP, which is widely integrated into <b>text</b> <b>classification</b> tasks. Unlike numbers and images, <b>text</b> <b>processing</b> emphasizes fine-grained processing ability. Traditional <b>text</b> <b>classification</b> methods generally require preprocessing the input model&rsquo;s <b>text</b> <b>data.</b> Additionally, they also need to obtain good sample features through manual annotation and then use classical machine learning algorithms for classification. Therefore, this paper analyzes the application status of deep learning in the three core tasks of NLP (including <b>text</b> <b>representation,</b> word order modeling, and knowledge representation). This content explores the improvement and synergy achieved through natural language processing in the context of <b>text</b> <b>classification,</b> while also taking into account the challenges posed by adversarial techniques in <b>text</b> <b>generation,</b> <b>text</b> <b>classification,</b> and <b>semantic</b> <b>parsing.</b> An empirical study on <b>text</b> <b>classification</b> tasks demonstrates the effectiveness of interactive integration training, particularly in conjunction with TextCNN, highlighting the significance of these advancements in <b>text</b> <b>classification</b> augmentation and enhancement.</p></p class="citation"></blockquote><h3 id=3348--33323-big-city-bias-evaluating-the-impact-of-metropolitan-size-on-computational-job-market-abilities-of-language-models-charlie-campanella-et-al-2024>(33/48 | 33/323) Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models (Charlie Campanella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charlie Campanella, Rob van der Goot. (2024)<br><strong>Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models</strong><br><button class=copy-to-clipboard title="Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08046v1.pdf filename=2403.08046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have emerged as a useful technology for job matching, for both candidates and employers. Job matching is often based on a particular geographic location, such as a city or region. However, <b>LLMs</b> have known biases, commonly derived from their training data. In this work, we aim to quantify the metropolitan size bias encoded within <b>large</b> <b>language</b> <b>models,</b> evaluating <b>zero-shot</b> salary, employer presence, and commute duration predictions in 384 of the United States&rsquo; metropolitan regions. Across all <b>benchmarks,</b> we observe negative correlations between the metropolitan size and the performance of the <b>LLMS,</b> indicating that smaller regions are indeed underrepresented. More concretely, the smallest 10 metropolitan regions show upwards of 300% worse <b>benchmark</b> performance than the largest 10.</p></p class="citation"></blockquote><h3 id=3448--34323-gpt-generated-text-detection-benchmark-dataset-and-tensor-based-detection-method-zubair-qazi-et-al-2024>(34/48 | 34/323) GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method (Zubair Qazi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zubair Qazi, William Shiao, Evangelos E. Papalexakis. (2024)<br><strong>GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method</strong><br><button class=copy-to-clipboard title="GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, ChatGPT, GPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07321v1.pdf filename=2403.07321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As natural language models like <b>ChatGPT</b> become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance. In this paper, we present <b>GPT</b> Reddit Dataset (GRiD), a novel Generative Pretrained <b>Transformer</b> <b>(GPT)-generated</b> text detection dataset designed to assess the performance of detection models in identifying generated responses from <b>ChatGPT.</b> The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and <b>ChatGPT-generated</b> responses. We provide an analysis of the dataset&rsquo;s characteristics, including linguistic diversity, context complexity, and response quality. To showcase the dataset&rsquo;s utility, we <b>benchmark</b> several detection methods on it, demonstrating their efficacy in distinguishing between human and <b>ChatGPT-generated</b> responses. This dataset serves as a resource for evaluating and advancing detection techniques in the context of <b>ChatGPT</b> and contributes to the ongoing efforts to ensure responsible and trustworthy AI-driven communication on the internet. Finally, we propose GpTen, a novel tensor-based <b>GPT</b> text detection method that is semi-supervised in nature since it only has access to human-generated text and performs on par with fully-supervised baselines.</p></p class="citation"></blockquote><h3 id=3548--35323-bagel-bootstrapping-agents-by-guiding-exploration-with-language-shikhar-murty-et-al-2024>(35/48 | 35/323) BAGEL: Bootstrapping Agents by Guiding Exploration with Language (Shikhar Murty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, Kenton Lee. (2024)<br><strong>BAGEL: Bootstrapping Agents by Guiding Exploration with Language</strong><br><button class=copy-to-clipboard title="BAGEL: Bootstrapping Agents by Guiding Exploration with Language" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Zero-shot, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08140v1.pdf filename=2403.08140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a <b>zero-shot</b> LM agent which maps the synthetic instruction into a refined trajectory. By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We use BAGEL demonstrations to adapt a zero shot LM agent at test time via <b>in-context</b> <b>learning</b> over retrieved demonstrations, and find improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x reduction in execution failures.</p></p class="citation"></blockquote><h3 id=3648--36323-simulating-weighted-automata-over-sequences-and-trees-with-transformers-michael-rizvi-et-al-2024>(36/48 | 36/323) Simulating Weighted Automata over Sequences and Trees with Transformers (Michael Rizvi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Rizvi, Maude Lizaire, Clara Lacroce, Guillaume Rabusseau. (2024)<br><strong>Simulating Weighted Automata over Sequences and Trees with Transformers</strong><br><button class=copy-to-clipboard title="Simulating Weighted Automata over Sequences and Trees with Transformers" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CC, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Recurrent Neural Network, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09728v1.pdf filename=2403.09728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> are ubiquitous models in the natural language processing (NLP) community and have shown impressive empirical successes in the past few years. However, little is understood about how they reason and the limits of their computational capabilities. These models do not process data sequentially, and yet outperform sequential neural models such as <b>RNNs.</b> Recent work has shown that these models can compactly simulate the sequential <b>reasoning</b> abilities of deterministic finite automata (DFAs). This leads to the following question: can <b>transformers</b> simulate the <b>reasoning</b> of more complex finite state machines? In this work, we show that <b>transformers</b> can simulate weighted finite automata (WFAs), a class of models which subsumes DFAs, as well as weighted tree automata (WTA), a generalization of weighted automata to tree structured inputs. We prove these claims formally and provide upper bounds on the sizes of the <b>transformer</b> models needed as a function of the number of states the target automata. Empirically, we perform synthetic experiments showing that <b>transformers</b> are able to learn these compact solutions via standard gradient-based training.</p></p class="citation"></blockquote><h3 id=3748--37323-authorship-style-transfer-with-policy-optimization-shuai-liu-et-al-2024>(37/48 | 37/323) Authorship Style Transfer with Policy Optimization (Shuai Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Liu, Shantanu Agarwal, Jonathan May. (2024)<br><strong>Authorship Style Transfer with Policy Optimization</strong><br><button class=copy-to-clipboard title="Authorship Style Transfer with Policy Optimization" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, Transfer Learning, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08043v1.pdf filename=2403.08043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Authorship <b>style</b> <b>transfer</b> <b>aims</b> to rewrite a given text into a specified target while preserving the original meaning in the source. Existing approaches rely on the availability of a large number of target <b>style</b> <b>exemplars</b> for model training. However, these overlook cases where a limited number of target <b>style</b> <b>examples</b> are available. The development of parameter-efficient <b>transfer</b> <b>learning</b> techniques and policy optimization (PO) approaches suggest lightweight PO is a feasible approach to <b>low-resource</b> <b>style</b> <b>transfer.</b> <b>In</b> this work, we propose a simple two step tune-and-optimize technique for <b>low-resource</b> textual <b>style</b> <b>transfer.</b> <b>We</b> apply our technique to authorship <b>transfer</b> <b>as</b> well as a larger-data native language <b>style</b> <b>task</b> and in both cases find it outperforms state-of-the-art baseline models.</p></p class="citation"></blockquote><h3 id=3848--38323-the-missing-piece-in-model-editing-a-deep-dive-into-the-hidden-damage-brought-by-model-editing-jianchen-wang-et-al-2024>(38/48 | 38/323) The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing (Jianchen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianchen Wang, Zhouhong Gu, Zhuozhi Xiong, Hongwei Feng, Yanghua Xiao. (2024)<br><strong>The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing</strong><br><button class=copy-to-clipboard title="The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Sora, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07825v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07825v1.pdf filename=2403.07825v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space. This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect. Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods. However, our proposed methods, GORA and <b>SORA,</b> effectively identify and alleviate this issue, respectively, contributing to the advancement of <b>LLM</b> editing techniques.</p></p class="citation"></blockquote><h3 id=3948--39323-svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression-xin-wang-et-al-2024>(39/48 | 39/323) SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression (Xin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang. (2024)<br><strong>SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression</strong><br><button class=copy-to-clipboard title="SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Model Compression, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07378v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07378v2.pdf filename=2403.07378v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been hindered by their substantial sizes, which necessitate <b>LLM</b> compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for <b>LLM</b> compression. However, state-of-the-art SVD-based <b>LLM</b> compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining <b>model</b> <b>parameters</b> after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based <b>LLM</b> compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form <b>model</b> <b>parameter</b> update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven <b>models</b> <b>from</b> three different <b>LLM</b> families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high <b>model</b> <b>compression</b> ratios. The source code is available at <a href=https://github.com/AIoT-MLSys-Lab/SVD-LLM>https://github.com/AIoT-MLSys-Lab/SVD-LLM</a>.</p></p class="citation"></blockquote><h3 id=4048--40323-kebench-a-benchmark-on-knowledge-editing-for-large-vision-language-models-han-huang-et-al-2024>(40/48 | 40/323) KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models (Han Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Huang, Haitian Zhong, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan. (2024)<br><strong>KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 27<br>Keywords: Graph, Benchmarking, Knowledge Graph, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07350v1.pdf filename=2403.07350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, little research has been done on <b>knowledge</b> <b>editing</b> for Large <b>Vision-Language</b> Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing <b>benchmark</b> has three metrics (Reliability, Locality and Generality) to measure <b>knowledge</b> <b>editing</b> for LVLMs. However, the <b>benchmark</b> falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited <b>knowledge</b> <b>in</b> relation to the associated content. We adopt different data collection methods to construct a new <b>benchmark,</b> $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a <b>multimodal</b> <b>knowledge</b> <b>graph,</b> our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related <b>knowledge</b> <b>and</b> form editing data. We conducted experiments of different editing methods on five LVLMs, and thoroughly analyze how these methods impact the models. The results reveal strengths and deficiencies of these methods and, hopefully, provide insights into potential avenues for future research.</p></p class="citation"></blockquote><h3 id=4148--41323-debatrix-multi-dimensinal-debate-judge-with-iterative-chronological-analysis-based-on-llm-jingcong-liang-et-al-2024>(41/48 | 41/323) Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM (Jingcong Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, Zhongyu Wei. (2024)<br><strong>Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM</strong><br><button class=copy-to-clipboard title="Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08010v1.pdf filename=2403.08010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments. At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate. In this paper, by leveraging <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration. To align with real-world debate scenarios, we introduced the PanelBench <b>benchmark,</b> comparing our system&rsquo;s performance to actual debate outcomes. The findings indicate a notable enhancement over directly using <b>LLMs</b> for debate evaluation. Source code and <b>benchmark</b> data are available online at <a href=https://github.com/ljcleo/Debatrix>https://github.com/ljcleo/Debatrix</a> .</p></p class="citation"></blockquote><h3 id=4248--42323-claimver-explainable-claim-level-verification-and-evidence-attribution-of-text-through-knowledge-graphs-preetam-prabhu-srikar-dammu-et-al-2024>(42/48 | 42/323) ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs (Preetam Prabhu Srikar Dammu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Preetam Prabhu Srikar Dammu, Himanshu Naidu, Mouly Dewan, YoungMin Kim, Tanya Roosta, Aman Chadha, Chirag Shah. (2024)<br><strong>ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs</strong><br><button class=copy-to-clipboard title="ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09724v1.pdf filename=2403.09724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many <b>fact-checking</b> <b>approaches</b> and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people&rsquo;s belief in automated systems. It is also paramount to localize and bring users&rsquo; attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users&rsquo; informational and verification needs by generating rich annotations and thereby reducing cognitive load. Designed to deliver comprehensive evaluations of texts, it highlights each claim, verifies it against a trusted <b>knowledge</b> <b>graph</b> <b>(KG),</b> presents the evidence, and provides succinct, clear explanations for each claim prediction. Finally, our framework introduces an attribution score, enhancing applicability across a wide range of downstream tasks.</p></p class="citation"></blockquote><h3 id=4348--43323-generating-clarification-questions-for-disambiguating-contracts-anmol-singhal-et-al-2024>(43/48 | 43/323) Generating Clarification Questions for Disambiguating Contracts (Anmol Singhal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anmol Singhal, Chirag Jain, Preethu Rose Anish, Arkajyoti Chakraborty, Smita Ghaisas. (2024)<br><strong>Generating Clarification Questions for Disambiguating Contracts</strong><br><button class=copy-to-clipboard title="Generating Clarification Questions for Disambiguating Contracts" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: ChatGPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08053v1.pdf filename=2403.08053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enterprises frequently enter into commercial contracts that can serve as vital sources of project-specific requirements. Contractual clauses are obligatory, and the requirements derived from contracts can detail the downstream implementation activities that non-legal stakeholders, including requirement analysts, engineers, and delivery personnel, need to conduct. However, comprehending contracts is cognitively demanding and error-prone for such stakeholders due to the extensive use of Legalese and the inherent complexity of contract language. Furthermore, contracts often contain ambiguously worded clauses to ensure comprehensive coverage. In contrast, non-legal stakeholders require a detailed and unambiguous comprehension of contractual clauses to craft actionable requirements. In this work, we introduce a novel legal NLP task that involves generating clarification questions for contracts. These questions aim to identify contract ambiguities on a document level, thereby assisting non-legal stakeholders in obtaining the necessary details for eliciting requirements. This task is challenged by three core issues: (1) data availability, (2) the length and unstructured nature of contracts, and (3) the complexity of legal text. To address these issues, we propose ConRAP, a retrieval-augmented <b>prompting</b> framework for generating clarification questions to disambiguate contractual text. Experiments conducted on contracts sourced from the publicly available CUAD dataset show that ConRAP with <b>ChatGPT</b> can detect ambiguities with an F2 score of 0.87. 70% of the generated clarification questions are deemed useful by human evaluators.</p></p class="citation"></blockquote><h3 id=4448--44323-pix2pix-onthefly-leveraging-llms-for-instruction-guided-image-editing-rodrigo-santos-et-al-2024>(44/48 | 44/323) Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing (Rodrigo Santos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rodrigo Santos, João Silva, António Branco. (2024)<br><strong>Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing</strong><br><button class=copy-to-clipboard title="Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08004v1.pdf filename=2403.08004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The combination of language processing and image processing keeps attracting increased interest given recent impressive advances that leverage the combined strengths of both domains of research. Among these advances, the task of editing an image on the basis solely of a natural language instruction stands out as a most challenging endeavour. While recent approaches for this task resort, in one way or other, to some form of preliminary preparation, training or <b>fine-tuning,</b> this paper explores a novel approach: We propose a preparation-free method that permits instruction-guided image editing on the fly. This approach is organized along three steps properly orchestrated that resort to image captioning and DDIM inversion, followed by obtaining the edit direction embedding, followed by image editing proper. While dispensing with preliminary preparation, our approach demonstrates to be effective and competitive, outperforming recent, state of the art models for this task when evaluated on the MAGICBRUSH dataset.</p></p class="citation"></blockquote><h3 id=4548--45323-mammoth-massively-multilingual-modular-open-translation--helsinki-timothee-mickus-et-al-2024>(45/48 | 45/323) MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki (Timothee Mickus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timothee Mickus, Stig-Arne Grönroos, Joseph Attieh, Michele Boggia, Ona De Gibert, Shaoxiong Ji, Niki Andreas Lopi, Alessandro Raganato, Raúl Vázquez, Jörg Tiedemann. (2024)<br><strong>MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki</strong><br><button class=copy-to-clipboard title="MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Neural Machine Translation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07544v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07544v1.pdf filename=2403.07544v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>NLP in the age of monolithic <b>large</b> <b>language</b> <b>models</b> is approaching its limits in terms of size and information that can be handled. The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality. In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular <b>machine</b> <b>translation</b> systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters. We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information. The toolkit is publicly available online.</p></p class="citation"></blockquote><h3 id=4648--46323-prediction-of-readmission-of-patients-by-extracting-biomedical-concepts-from-clinical-texts-rasoul-samani-et-al-2024>(46/48 | 46/323) Prediction of readmission of patients by extracting biomedical concepts from clinical texts (Rasoul Samani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rasoul Samani, Fahime Shahrokh, Mohammad Dehghani. (2024)<br><strong>Prediction of readmission of patients by extracting biomedical concepts from clinical texts</strong><br><button class=copy-to-clipboard title="Prediction of readmission of patients by extracting biomedical concepts from clinical texts" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Text Mining<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09722v1.pdf filename=2403.09722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Today, the existence of a vast amount of electronic health data has created potential capacities for conducting studies aiming to improve the medical services provided to patients and reduce the costs of the healthcare system. One of the topics that has been receiving attention in the field of medicine in recent years is the identification of patients who are likely to be re-hospitalized shortly after being discharged from the hospital. This identification can help doctors choose appropriate treatment methods, thereby reducing the rate of patient re-hospitalization and resulting in effective treatment cost reduction. In this study, the prediction of patient re-hospitalization using <b>text</b> <b>mining</b> approaches and the processing of discharge report <b>texts</b> <b>in</b> the patient&rsquo;s electronic file has been discussed. To this end, the performance of various machine learning models has been evaluated using two approaches: bag of word and bag of concept, in the process of predicting patient readmission. Comparing the efficiency of these approaches has shown the superiority of the random forest model and the bag of concept approach over other machine learning models and approaches. This research has achieved the highest score in predicting the probability of patient re-hospitalization, with a recall score of 68.9%, compared to similar works that have utilized machine learning models in this field.</p></p class="citation"></blockquote><h3 id=4748--47323-mevaker-conclusion-extraction-and-allocation-resources-for-the-hebrew-language-vitaly-shalumov-et-al-2024>(47/48 | 47/323) Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language (Vitaly Shalumov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vitaly Shalumov, Harel Haskey, Yuval Solaz. (2024)<br><strong>Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language</strong><br><button class=copy-to-clipboard title="Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09719v1.pdf filename=2403.09719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce <b>summarization</b> MevakerSumm and conclusion extraction MevakerConc datasets for the Hebrew language based on the State Comptroller and Ombudsman of Israel reports, along with two auxiliary datasets. We accompany these datasets with models for conclusion extraction (HeConE, HeConEspc) and conclusion allocation (HeCross). All of the code, datasets, and model checkpoints used in this work are publicly available.</p></p class="citation"></blockquote><h3 id=4848--48323-a-survey-of-explainable-knowledge-tracing-yanhong-bai-et-al-2024>(48/48 | 48/323) A Survey of Explainable Knowledge Tracing (Yanhong Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanhong Bai, Jiabao Zhao, Tingjiang Wei, Qing Cai, Liang He. (2024)<br><strong>A Survey of Explainable Knowledge Tracing</strong><br><button class=copy-to-clipboard title="A Survey of Explainable Knowledge Tracing" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07279v1.pdf filename=2403.07279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the long term accumulation of high quality educational data, artificial intelligence has shown excellent performance in knowledge tracing. However, due to the lack of interpretability and transparency of some algorithms, this approach will result in reduced stakeholder trust and a decreased acceptance of intelligent decisions. Therefore, algorithms need to achieve high accuracy, and users need to understand the internal operating mechanism and provide reliable explanations for decisions. This paper thoroughly analyzes the interpretability of KT algorithms. First, the concepts and common methods of explainable artificial intelligence and knowledge tracing are introduced. Next, explainable knowledge tracing models are classified into two categories: transparent models and <b>black</b> <b>box</b> models. Then, the interpretable methods used are reviewed from three stages: ante hoc interpretable methods, post hoc interpretable methods, and other dimensions. It is worth noting that current evaluation methods for explainable knowledge tracing are lacking. Hence, contrast and deletion experiments are conducted to explain the prediction results of the deep knowledge tracing model on the ASSISTment2009 by using three XAI methods. Moreover, this paper offers some insights into evaluation methods from the perspective of educational stakeholders. This paper provides a detailed and comprehensive review of the research on explainable knowledge tracing, aiming to offer some basis and inspiration for researchers interested in the interpretability of knowledge tracing.</p></p class="citation"></blockquote><h2 id=cscv-88>cs.CV (88)</h2><h3 id=188--49323-in-context-learning-enables-multimodal-large-language-models-to-classify-cancer-pathology-images-dyke-ferber-et-al-2024>(1/88 | 49/323) In-context learning enables multimodal large language models to classify cancer pathology images (Dyke Ferber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dyke Ferber, Georg Wölflein, Isabella C. Wiest, Marta Ligero, Srividhya Sainath, Narmin Ghaffari Laleh, Omar S. M. El Nahhas, Gustav Müller-Franzes, Dirk Jäger, Daniel Truhn, Jakob Nikolas Kather. (2024)<br><strong>In-context learning enables multimodal large language models to classify cancer pathology images</strong><br><button class=copy-to-clipboard title="In-context learning enables multimodal large language models to classify cancer pathology images" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 96<br>Keywords: Fine-tuning, Foundation Model, Multi-modal, Multi-modal, GPT, Transformer, In-context Learning, In-context Learning, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07407v1.pdf filename=2403.07407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image classification requires labeled, task-specific datasets which are used to train deep learning networks de novo, or to <b>fine-tune</b> <b>foundation</b> <b>models.</b> However, this process is computationally and technically demanding. In language processing, <b>in-context</b> <b>learning</b> provides an alternative, where models learn from within <b>prompts,</b> bypassing the need for parameter updates. Yet, <b>in-context</b> <b>learning</b> remains underexplored in medical image analysis. Here, we systematically evaluate the model Generative Pretrained <b>Transformer</b> 4 with Vision capabilities <b>(GPT-4V)</b> on cancer image processing with <b>in-context</b> <b>learning</b> on three cancer histopathology tasks of high importance: Classification of tissue subtypes in colorectal cancer, colon polyp subtyping and breast tumor detection in lymph node sections. Our results show that <b>in-context</b> <b>learning</b> is sufficient to match or even outperform specialized neural networks trained for particular tasks, while only requiring a minimal number of samples. In summary, this study demonstrates that <b>large</b> <b>vision</b> <b>language</b> models trained on non-domain specific data can be applied out-of-the box to solve medical image-processing tasks in histopathology. This democratizes access of generalist AI models to medical experts without technical background especially for areas where annotated data is scarce.</p></p class="citation"></blockquote><h3 id=288--50323-a-survey-of-vision-transformers-in-autonomous-driving-current-trends-and-future-directions-quoc-vinh-lai-dang-2024>(2/88 | 50/323) A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions (Quoc-Vinh Lai-Dang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quoc-Vinh Lai-Dang. (2024)<br><strong>A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions</strong><br><button class=copy-to-clipboard title="A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 80<br>Keywords: Vision Transformer, Object Detection, Convolution, Convolutional Neural Network, Recurrent Neural Network, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07542v1.pdf filename=2403.07542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This survey explores the adaptation of visual <b>transformer</b> models in Autonomous Driving, a transition inspired by their success in Natural Language Processing. Surpassing traditional <b>Recurrent</b> <b>Neural</b> <b>Networks</b> in tasks like sequential image processing and outperforming <b>Convolutional</b> <b>Neural</b> <b>Networks</b> in global context capture, as evidenced in complex scene recognition, <b>Transformers</b> are gaining traction in computer <b>vision.</b> <b>These</b> capabilities are crucial in Autonomous Driving for real-time, dynamic visual scene processing. Our survey provides a comprehensive overview of <b>Vision</b> <b>Transformer</b> applications in Autonomous Driving, focusing on foundational concepts such as <b>self-attention,</b> multi-head attention, and encoder-decoder architecture. We cover applications in <b>object</b> <b>detection,</b> segmentation, pedestrian detection, lane detection, and more, comparing their architectural merits and limitations. The survey concludes with future research directions, highlighting the growing role of <b>Vision</b> <b>Transformers</b> in Autonomous Driving.</p></p class="citation"></blockquote><h3 id=388--51323-moai-mixture-of-all-intelligence-for-large-language-and-vision-models-byung-kwan-lee-et-al-2024>(3/88 | 51/323) MoAI: Mixture of All Intelligence for Large Language and Vision Models (Byung-Kwan Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro. (2024)<br><strong>MoAI: Mixture of All Intelligence for Large Language and Vision Models</strong><br><button class=copy-to-clipboard title="MoAI: Mixture of All Intelligence for Large Language and Vision Models" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Optical Character Recognition, Optical Character Recognition, Graph, Zero-shot, Instruction Tuning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07508v1.pdf filename=2403.07508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>instruction</b> <b>tuning</b> has led to the current trend of <b>instruction-tuned</b> <b>large</b> <b>language</b> <b>and</b> vision models (LLVMs). This trend involves either meticulously curating numerous <b>instruction</b> <b>tuning</b> datasets tailored to specific objectives or enlarging LLVMs to manage vast amounts of vision language (VL) data. However, current LLVMs have disregarded the detailed and comprehensive real-world scene understanding available from specialized computer vision (CV) models in visual perception tasks such as segmentation, detection, scene <b>graph</b> generation (SGG), and <b>optical</b> <b>character</b> <b>recognition</b> <b>(OCR).</b> Instead, the existing LLVMs rely mainly on the <b>large</b> <b>capacity</b> <b>and</b> emergent capabilities of their <b>LLM</b> backbones. Therefore, we present a new LLVM, Mixture of All Intelligence (MoAI), which leverages auxiliary visual information obtained from the outputs of external segmentation, detection, SGG, and <b>OCR</b> models. MoAI operates through two newly introduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the outputs of the external CV models, the MoAI-Compressor aligns and condenses them to efficiently use relevant auxiliary visual information for VL tasks. MoAI-Mixer then blends three types of intelligence (1) visual features, (2) auxiliary features from the external CV models, and (3) language features by utilizing the concept of Mixture of Experts. Through this integration, MoAI significantly outperforms both open-source and closed-source LLVMs in numerous <b>zero-shot</b> VL tasks, particularly those related to real-world scene understanding such as object existence, positions, relations, and <b>OCR</b> without enlarging the model size or curating extra visual <b>instruction</b> <b>tuning</b> datasets.</p></p class="citation"></blockquote><h3 id=488--52323-navcot-boosting-llm-based-vision-and-language-navigation-via-learning-disentangled-reasoning-bingqian-lin-et-al-2024>(4/88 | 52/323) NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning (Bingqian Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang. (2024)<br><strong>NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning</strong><br><button class=copy-to-clipboard title="NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-RO, cs.CV<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07376v1.pdf filename=2403.07376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-Language</b> Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in VLN by improving navigational <b>reasoning</b> accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the <b>LLM</b> training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the <b>LLM</b> is <b>prompted</b> to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the <b>reasoning</b> from the prior steps. Through constructing formalized labels for training, the <b>LLM</b> can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN <b>benchmarks</b> (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient <b>finetuning,</b> our NavCoT outperforms a recent <b>GPT4-based</b> approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable <b>LLM-based</b> embodied agents, which are helpful for developing real-world robotics applications. Code is available at <a href=https://github.com/expectorlin/NavCoT>https://github.com/expectorlin/NavCoT</a>.</p></p class="citation"></blockquote><h3 id=588--53323-pelk-parameter-efficient-large-kernel-convnets-with-peripheral-convolution-honghao-chen-et-al-2024>(5/88 | 53/323) PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution (Honghao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Honghao Chen, Xiangxiang Chu, Yongjian Ren, Xin Zhao, Kaiqi Huang. (2024)<br><strong>PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution</strong><br><button class=copy-to-clipboard title="PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Object Detection, Convolution, Convolutional Neural Network, Parameter Sharing, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07589v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07589v2.pdf filename=2403.07589v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, some large kernel convnets strike back with appealing performance and efficiency. However, given the square complexity of <b>convolution,</b> scaling up kernels can bring about an enormous amount of <b>parameters</b> <b>and</b> the proliferated <b>parameters</b> <b>can</b> induce severe optimization problem. Due to these issues, current <b>CNNs</b> compromise to scale up to 51x51 in the form of stripe <b>convolution</b> (i.e., 51x5 + 5x51) and start to saturate as the kernel size continues growing. In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains. Inspired by human <b>vision,</b> <b>we</b> propose a human-like peripheral <b>convolution</b> that efficiently reduces over 90% <b>parameter</b> <b>count</b> of dense grid <b>convolution</b> through <b>parameter</b> <b>sharing,</b> and manage to scale up kernel size to extremely large. Our peripheral <b>convolution</b> behaves highly similar to human, reducing the complexity of <b>convolution</b> from O(K^2) to O(logK) without backfiring performance. Built on this, we propose <b>Parameter-efficient</b> <b>Large</b> Kernel Network (PeLK). Our PeLK outperforms modern <b>vision</b> <b>Transformers</b> and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various <b>vision</b> <b>tasks</b> including ImageNet classification, semantic segmentation on ADE20K and <b>object</b> <b>detection</b> on MS COCO. For the first time, we successfully scale up the kernel size of <b>CNNs</b> to an unprecedented 101x101 and demonstrate consistent improvements.</p></p class="citation"></blockquote><h3 id=688--54323-multi-modal-auto-regressive-modeling-via-visual-words-tianshuo-peng-et-al-2024>(6/88 | 54/323) Multi-modal Auto-regressive Modeling via Visual Words (Tianshuo Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianshuo Peng, Zuchao Li, Lefei Zhang, Hai Zhao, Ping Wang, Bo Du. (2024)<br><strong>Multi-modal Auto-regressive Modeling via Visual Words</strong><br><button class=copy-to-clipboard title="Multi-modal Auto-regressive Modeling via Visual Words" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Benchmarking, Multi-modal, Supervised Learning, Reasoning, Visual Question Answering, Large Language Model, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07720v1.pdf filename=2403.07720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> benefiting from the auto-regressive modelling approach performed on massive unannotated <b>texts</b> <b>corpora,</b> demonstrates powerful perceptual and <b>reasoning</b> capabilities. However, as for extending auto-regressive modelling to <b>multi-modal</b> scenarios to build <b>Large</b> <b>Multi-modal</b> <b>Models</b> (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete <b>supervised</b> labels for classification. In this paper, we successfully perform <b>multi-modal</b> auto-regressive modeling with a unified objective for the first time. Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over <b>LLM&rsquo;s</b> vocabulary, providing supervision information for visual modelling. We further explore the distribution of visual features in the semantic space within LMM and the possibility of using <b>text</b> <b>embeddings</b> to represent visual information. Experimental results and ablation studies on 5 <b>VQA</b> tasks and 4 <b>benchmark</b> toolkits validate the powerful performance of our proposed approach.</p></p class="citation"></blockquote><h3 id=788--55323-distilling-the-knowledge-in-data-pruning-emanuel-ben-baruch-et-al-2024>(7/88 | 55/323) Distilling the Knowledge in Data Pruning (Emanuel Ben-Baruch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emanuel Ben-Baruch, Adam Botach, Igor Kviatkovsky, Manoj Aggarwal, Gérard Medioni. (2024)<br><strong>Distilling the Knowledge in Data Pruning</strong><br><button class=copy-to-clipboard title="Distilling the Knowledge in Data Pruning" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Pruning, Self-Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07854v1.pdf filename=2403.07854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing size of datasets used for training neural networks, data <b>pruning</b> becomes an attractive field of research. However, most current data <b>pruning</b> algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high <b>pruning</b> regimes. In this paper we explore the application of data <b>pruning</b> while incorporating <b>knowledge</b> <b>distillation</b> <b>(KD)</b> when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating <b>KD</b> into training, we demonstrate significant improvement across datasets, <b>pruning</b> methods, and on all <b>pruning</b> fractions. We first establish a theoretical motivation for employing <b>self-distillation</b> to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using <b>KD,</b> simple random <b>pruning</b> is comparable or superior to sophisticated <b>pruning</b> methods across all <b>pruning</b> regimes. On ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50% of the data. Additionally, we demonstrate a crucial connection between the <b>pruning</b> factor and the optimal <b>knowledge</b> <b>distillation</b> weight. This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical <b>pruning</b> algorithms. Finally, we make an intriguing observation: when using lower <b>pruning</b> fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student&rsquo;s may improve results. Our code will be made available.</p></p class="citation"></blockquote><h3 id=888--56323-towards-zero-shot-human-object-interaction-detection-via-vision-language-integration-weiying-xue-et-al-2024>(8/88 | 56/323) Towards Zero-shot Human-Object Interaction Detection via Vision-Language Integration (Weiying Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiying Xue, Qi Liu, Qiwei Xiong, Yuxiao Wang, Zhenao Wei, Xiaofen Xing, Xiangmin Xu. (2024)<br><strong>Towards Zero-shot Human-Object Interaction Detection via Vision-Language Integration</strong><br><button class=copy-to-clipboard title="Towards Zero-shot Human-Object Interaction Detection via Vision-Language Integration" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Supervised Learning, Supervised Learning, Zero-shot, Self-Attention, Vision-and-Language, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07246v1.pdf filename=2403.07246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-object interaction (HOI) detection aims to locate human-object pairs and identify their interaction categories in images. Most existing methods primarily focus on <b>supervised</b> <b>learning,</b> which relies on extensive manual HOI annotations. In this paper, we propose a novel framework, termed Knowledge Integration to HOI (KI2HOI), that effectively integrates the knowledge of visual-language model to improve <b>zero-shot</b> <b>HOI</b> detection. Specifically, the verb feature learning module is designed based on visual semantics, by employing the verb extraction decoder to convert corresponding verb queries into interaction-specific category representations. We develop an effective additive <b>self-attention</b> mechanism to generate more comprehensive visual representations. Moreover, the innovative interaction representation decoder effectively extracts informative regions by integrating spatial and visual feature information through a cross-attention mechanism. To deal with <b>zero-shot</b> <b>learning</b> in low-data, we leverage a priori knowledge from the CLIP text encoder to initialize the linear classifier for enhanced interaction understanding. Extensive experiments conducted on the mainstream HICO-DET and V-COCO datasets demonstrate that our model outperforms the previous methods in various <b>zero-shot</b> <b>and</b> full-supervised settings.</p></p class="citation"></blockquote><h3 id=988--57323-beyond-text-frozen-large-language-models-in-visual-signal-comprehension-lei-zhu-et-al-2024>(9/88 | 57/323) Beyond Text: Frozen Large Language Models in Visual Signal Comprehension (Lei Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Zhu, Fangyun Wei, Yanye Lu. (2024)<br><strong>Beyond Text: Frozen Large Language Models in Visual Signal Comprehension</strong><br><button class=copy-to-clipboard title="Beyond Text: Frozen Large Language Models in Visual Signal Comprehension" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Fine-tuning, Multi-modal, Question Answering, Visual Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07874v1.pdf filename=2403.07874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we investigate the potential of a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to directly comprehend <b>visual</b> <b>signals</b> <b>without</b> the necessity of <b>fine-tuning</b> on <b>multi-modal</b> datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the <b>LLM&rsquo;s</b> vocabulary. To achieve this, we present the Vision-to-Language Tokenizer, abbreviated as V2T Tokenizer, which transforms an image into a ``foreign language&rsquo;&rsquo; with the combined aid of an encoder-decoder, the <b>LLM</b> vocabulary, and a CLIP model. With this innovative image encoding, the <b>LLM</b> gains the ability not only for <b>visual</b> <b>comprehension</b> <b>but</b> also for image denoising and restoration in an auto-regressive fashion-crucially, without any <b>fine-tuning.</b> We undertake rigorous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and <b>visual</b> <b>question</b> <b>answering,</b> as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at <a href=https://github.com/zh460045050/V2L-Tokenizer>https://github.com/zh460045050/V2L-Tokenizer</a>.</p></p class="citation"></blockquote><h3 id=1088--58323-genuine-knowledge-from-practice-diffusion-test-time-adaptation-for-video-adverse-weather-removal-yijun-yang-et-al-2024>(10/88 | 58/323) Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal (Yijun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijun Yang, Hongtao Wu, Angelica I. Aviles-Rivero, Yulun Zhang, Jing Qin, Lei Zhu. (2024)<br><strong>Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal</strong><br><button class=copy-to-clipboard title="Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Vision Transformer, Benchmarking, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07684v1.pdf filename=2403.07684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world <b>vision</b> <b>tasks</b> frequently suffer from the appearance of unexpected adverse weather conditions, including rain, haze, snow, and raindrops. In the last decade, <b>convolutional</b> <b>neural</b> <b>networks</b> and <b>vision</b> <b>transformers</b> have yielded outstanding results in single-weather video removal. However, due to the absence of appropriate adaptation, most of them fail to generalize to other weather conditions. Although ViWS-Net is proposed to remove adverse weather conditions in videos with a single set of pre-trained weights, it is seriously blinded by seen weather at train-time and degenerates when coming to unseen weather during test-time. In this work, we introduce test-time adaptation into adverse weather removal in videos, and propose the first framework that integrates test-time adaptation into the iterative diffusion reverse process. Specifically, we devise a diffusion-based network with a novel temporal noise model to efficiently explore frame-correlated information in degraded video clips at training stage. During inference stage, we introduce a proxy task named Diffusion Tubelet Self-Calibration to learn the primer distribution of test video stream and optimize the model by approximating the temporal noise model for online adaptation. Experimental results, on <b>benchmark</b> datasets, demonstrate that our Test-Time Adaptation method with Diffusion-based network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring videos degraded by seen weather conditions. Its generalizable capability is also validated with unseen weather conditions in both synthesized and real-world videos.</p></p class="citation"></blockquote><h3 id=1188--59323-gabor-guided-transformer-for-single-image-deraining-sijin-he-et-al-2024>(11/88 | 59/323) Gabor-guided transformer for single image deraining (Sijin He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sijin He, Guangfeng Lin. (2024)<br><strong>Gabor-guided transformer for single image deraining</strong><br><button class=copy-to-clipboard title="Gabor-guided transformer for single image deraining" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07380v1.pdf filename=2403.07380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image deraining have have gained a great deal of attention in order to address the challenges posed by the effects of harsh weather conditions on visual tasks. While <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> are popular, their limitations in capturing global information may result in ineffective rain removal. <b>Transformer-based</b> methods with <b>self-attention</b> mechanisms have improved, but they tend to distort high-frequency details that are crucial for image fidelity. To solve this problem, we propose the Gabor-guided tranformer (Gabformer) for single image deraining. The focus on local texture features is enhanced by incorporating the information processed by the Gabor filter into the query vector, which also improves the robustness of the model to noise due to the properties of the filter. Extensive experiments on the <b>benchmarks</b> demonstrate that our method outperforms state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=1288--60323-optimizing-negative-prompts-for-enhanced-aesthetics-and-fidelity-in-text-to-image-generation-michael-ogezi-et-al-2024>(12/88 | 60/323) Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation (Michael Ogezi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Ogezi, Ning Shi. (2024)<br><strong>Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation</strong><br><button class=copy-to-clipboard title="Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reinforcement Learning, Supervised Learning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07605v1.pdf filename=2403.07605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>text-to-image</b> generation, using negative <b>prompts,</b> which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative <b>prompts</b> is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative <b>prompt</b> generation toward enhanced image generation, using <b>supervised</b> <b>fine-tuning</b> and <b>reinforcement</b> <b>learning.</b> Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative <b>prompts</b> from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative <b>Prompts</b> DB, a dataset of negative <b>prompts.</b></p></p class="citation"></blockquote><h3 id=1388--61323-vit-comer-vision-transformer-with-convolutional-multi-scale-feature-interaction-for-dense-predictions-chunlong-xia-et-al-2024>(13/88 | 61/323) ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions (Chunlong Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, Yifeng Shi. (2024)<br><strong>ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions</strong><br><button class=copy-to-clipboard title="ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07392v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07392v2.pdf filename=2403.07392v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>Vision</b> <b>Transformer</b> (ViT) has achieved significant success in computer <b>vision,</b> <b>it</b> does not perform well in dense prediction tasks due to the lack of inner-patch information interaction and the limited diversity of feature scale. Most existing studies are devoted to designing <b>vision-specific</b> <b>transformers</b> to solve the above problems, which introduce additional pre-training costs. Therefore, we present a plain, pre-training-free, and feature-enhanced ViT backbone with <b>Convolutional</b> Multi-scale feature interaction, named ViT-CoMer, which facilitates bidirectional interaction between <b>CNN</b> and <b>transformer.</b> Compared to the state-of-the-art, ViT-CoMer has the following advantages: (1) We inject spatial pyramid multi-receptive field <b>convolutional</b> features into the ViT architecture, which effectively alleviates the problems of limited local information interaction and single-feature representation in ViT. (2) We propose a simple and efficient <b>CNN-Transformer</b> bidirectional fusion interaction module that performs multi-scale fusion across hierarchical features, which is beneficial for handling dense prediction tasks. (3) We evaluate the performance of ViT-CoMer across various dense prediction tasks, different frameworks, and multiple advanced pre-training. Notably, our ViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and 62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art methods. We hope ViT-CoMer can serve as a new backbone for dense prediction tasks to facilitate future research. The code will be released at <a href=https://github.com/Traffic-X/ViT-CoMer>https://github.com/Traffic-X/ViT-CoMer</a>.</p></p class="citation"></blockquote><h3 id=1488--62323-large-window-based-mamba-unet-for-medical-image-segmentation-beyond-convolution-and-self-attention-jinhong-wang-et-al-2024>(14/88 | 62/323) Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention (Jinhong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhong Wang, Jintai Chen, Danny Chen, Jian Wu. (2024)<br><strong>Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention</strong><br><button class=copy-to-clipboard title="Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07332v1.pdf filename=2403.07332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In clinical practice, medical image segmentation provides useful information on the contours and dimensions of target organs or tissues, facilitating improved diagnosis, analysis, and treatment. In the past few years, <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and <b>Transformers</b> have dominated this area, but they still suffer from either limited receptive fields or costly long-range modeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a promising paradigm for long-range dependency modeling with linear complexity. In this paper, we introduce a Large Window-based Mamba U}-shape Network, or LMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of our LMa-UNet is its utilization of large windows, excelling in locally spatial modeling compared to small kernel-based <b>CNNs</b> and small window-based <b>Transformers,</b> while maintaining superior efficiency in global modeling compared to <b>self-attention</b> with quadratic complexity. Additionally, we design a novel hierarchical and bidirectional Mamba block to further enhance the global and neighborhood spatial modeling capability of Mamba. Comprehensive experiments demonstrate the effectiveness and efficiency of our method and the feasibility of using large window size to achieve large receptive fields. Codes are available at <a href=https://github.com/wjh892521292/LMa-UNet>https://github.com/wjh892521292/LMa-UNet</a>.</p></p class="citation"></blockquote><h3 id=1588--63323-mentor-multilingual-text-detection-toward-learning-by-analogy-hsin-ju-lin-et-al-2024>(15/88 | 63/323) MENTOR: Multilingual tExt detectioN TOward leaRning by analogy (Hsin-Ju Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hsin-Ju Lin, Tsu-Chun Chung, Ching-Chun Hsiao, Pin-Yu Chen, Wei-Chen Chiu, Ching-Chun Huang. (2024)<br><strong>MENTOR: Multilingual tExt detectioN TOward leaRning by analogy</strong><br><button class=copy-to-clipboard title="MENTOR: Multilingual tExt detectioN TOward leaRning by analogy" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Few-shot, Few-shot Learning, Supervised Learning, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07286v1.pdf filename=2403.07286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text detection is frequently used in vision-based mobile robots when they need to interpret texts in their surroundings to perform a given task. For instance, delivery robots in multilingual cities need to be capable of doing multilingual text detection so that the robots can read traffic signs and road markings. Moreover, the target languages change from region to region, implying the need of efficiently re-training the models to recognize the novel/new languages. However, collecting and labeling training data for novel languages are cumbersome, and the efforts to re-train an existing/trained text detector are considerable. Even worse, such a routine would repeat whenever a novel language appears. This motivates us to propose a new problem setting for tackling the aforementioned challenges in a more efficient way: &ldquo;We ask for a generalizable multilingual text detection framework to detect and identify both seen and unseen language regions inside scene images without the requirement of collecting <b>supervised</b> training data for unseen languages as well as model re-training&rdquo;. To this end, we propose &ldquo;MENTOR&rdquo;, the first work to realize a learning strategy between <b>zero-shot</b> <b>learning</b> and <b>few-shot</b> <b>learning</b> for multilingual scene text detection.</p></p class="citation"></blockquote><h3 id=1688--64323-cuvler-enhanced-unsupervised-object-discoveries-through-exhaustive-self-supervised-transformers-shahaf-arica-et-al-2024>(16/88 | 64/323) CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers (Shahaf Arica et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahaf Arica, Or Rubin, Sapir Gershov, Shlomi Laufer. (2024)<br><strong>CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers</strong><br><button class=copy-to-clipboard title="CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Graph, Clustering, Self-supervised Learning, Unsupervised Learning, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07700v1.pdf filename=2403.07700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce VoteCut, an innovative method for <b>unsupervised</b> object discovery that leverages feature representations from multiple <b>self-supervised</b> models. VoteCut employs normalized-cut based <b>graph</b> partitioning, <b>clustering</b> and a pixel voting approach. Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a <b>zero-shot</b> model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy. Through rigorous evaluations across multiple datasets and several <b>unsupervised</b> setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models. Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation.</p></p class="citation"></blockquote><h3 id=1788--65323-aesopagent-agent-driven-evolutionary-system-on-story-to-video-production-jiuniu-wang-et-al-2024>(17/88 | 65/323) AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production (Jiuniu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiuniu Wang, Zehua Du, Yuyuan Zhao, Bo Yuan, Kexiang Wang, Jian Liang, Yaxi Zhao, Yihen Lu, Gengliang Li, Junlong Gao, Xin Tu, Zhenyu Guo. (2024)<br><strong>AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production</strong><br><button class=copy-to-clipboard title="AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 46<br>Keywords: Sora, Multi-modal, Multi-modal, Retrieval-Augmented Generation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07952v1.pdf filename=2403.07952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Agent and AIGC (Artificial Intelligence Generated Content) technologies have recently made significant progress. We propose AesopAgent, an Agent-driven Evolutionary System on Story-to-Video Production. AesopAgent is a practical application of agent technology for <b>multimodal</b> content generation. The system integrates multiple generative capabilities within a unified framework, so that individual users can leverage these modules easily. This innovative system would convert user story proposals into scripts, images, and audio, and then integrate these <b>multimodal</b> contents into videos. Additionally, the animating units (e.g., Gen-2 and <b>Sora)</b> could make the videos more infectious. The AesopAgent system could orchestrate task workflow for video generation, ensuring that the generated video is both rich in content and coherent. This system mainly contains two layers, i.e., the Horizontal Layer and the Utility Layer. In the Horizontal Layer, we introduce a novel <b>RAG-based</b> evolutionary system that optimizes the whole video generation workflow and the steps within the workflow. It continuously evolves and iteratively optimizes workflow by accumulating expert experience and professional knowledge, including optimizing the <b>LLM</b> <b>prompts</b> and utilities usage. The Utility Layer provides multiple utilities, leading to consistent image generation that is visually coherent in terms of composition, characters, and style. Meanwhile, it provides audio and special effects, integrating them into expressive and logically arranged videos. Overall, our AesopAgent achieves state-of-the-art performance compared with many previous works in visual storytelling. Our AesopAgent is designed for convenient service for individual users, which is available on the following page: <a href=https://aesopai.github.io/>https://aesopai.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1888--66323-text-to-image-diffusion-models-are-great-sketch-photo-matchmakers-subhadeep-koley-et-al-2024>(18/88 | 66/323) Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers (Subhadeep Koley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song. (2024)<br><strong>Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers</strong><br><button class=copy-to-clipboard title="Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Diffusion Model, Benchmarking, Zero-shot, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07214v1.pdf filename=2403.07214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper, for the first time, explores <b>text-to-image</b> <b>diffusion</b> <b>models</b> for <b>Zero-Shot</b> Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of <b>text-to-image</b> <b>diffusion</b> <b>models</b> to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained <b>diffusion</b> <b>models</b> effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual <b>prompts.</b> For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual <b>prompts</b> to guide the model&rsquo;s feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several <b>benchmark</b> datasets validate significant performance improvements.</p></p class="citation"></blockquote><h3 id=1988--67323-lg-traj-llm-guided-pedestrian-trajectory-prediction-pranav-singh-chib-et-al-2024>(19/88 | 67/323) LG-Traj: LLM Guided Pedestrian Trajectory Prediction (Pranav Singh Chib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranav Singh Chib, Pravendra Singh. (2024)<br><strong>LG-Traj: LLM Guided Pedestrian Trajectory Prediction</strong><br><button class=copy-to-clipboard title="LG-Traj: LLM Guided Pedestrian Trajectory Prediction" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 41<br>Keywords: Benchmarking, Clustering, Representation Learning, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08032v1.pdf filename=2403.08032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate pedestrian trajectory prediction is crucial for various applications, and it requires a deep understanding of pedestrian motion patterns in dynamic environments. However, existing pedestrian trajectory prediction methods still need more exploration to fully leverage these motion patterns. This paper investigates the possibilities of using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to improve pedestrian trajectory prediction tasks by inducing motion cues. We introduce LG-Traj, a novel approach incorporating <b>LLMs</b> to generate motion cues present in pedestrian past/observed trajectories. Our approach also incorporates motion cues present in pedestrian future trajectories by <b>clustering</b> future trajectories of training data using a mixture of Gaussians. These motion cues, along with pedestrian coordinates, facilitate a better understanding of the underlying <b>representation.</b> <b>Furthermore,</b> we utilize singular value decomposition to augment the observed trajectories, incorporating them into the model learning process to further enhance <b>representation</b> <b>learning.</b> Our method employs a <b>transformer-based</b> architecture comprising a motion encoder to model motion patterns and a social decoder to capture social interactions among pedestrians. We demonstrate the effectiveness of our approach on popular pedestrian trajectory prediction <b>benchmarks,</b> namely ETH-UCY and SDD, and present various ablation experiments to validate our approach.</p></p class="citation"></blockquote><h3 id=2088--68323-taskclip-extend-large-vision-language-model-for-task-oriented-object-detection-hanning-chen-et-al-2024>(20/88 | 68/323) TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection (Hanning Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanning Chen, Wenjun Huang, Yang Ni, Sanggeon Yun, Fei Wen, Hugo Latapie, Mohsen Imani. (2024)<br><strong>TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection</strong><br><button class=copy-to-clipboard title="TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Transformer, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08108v1.pdf filename=2403.08108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Task-oriented <b>object</b> <b>detection</b> aims to find <b>objects</b> <b>suitable</b> for accomplishing specific tasks. As a challenging task, it requires simultaneous visual data processing and <b>reasoning</b> under ambiguous semantics. Recent solutions are mainly all-in-one models. However, the <b>object</b> <b>detection</b> backbones are pre-trained without text supervision. Thus, to incorporate task requirements, their intricate models undergo extensive learning on a highly imbalanced and scarce dataset, resulting in capped performance, laborious training, and poor generalizability. In contrast, we propose TaskCLIP, a more natural two-stage design composed of general <b>object</b> <b>detection</b> and task-guided <b>object</b> <b>selection.</b> Particularly for the latter, we resort to the recently successful large <b>Vision-Language</b> Models (VLMs) as our backbone, which provides rich semantic knowledge and a uniform embedding space for images and texts. Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of <b>object</b> <b>images</b> and their visual attributes, which are mainly adjective phrases. To this end, we design a <b>transformer-based</b> aligner after the pre-trained VLMs to re-calibrate both embeddings. Finally, we employ a trainable score function to post-process the VLM matching results for <b>object</b> <b>selection.</b> Experimental results demonstrate that our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by 3.5% and only requires a single NVIDIA RTX 4090 for both training and inference.</p></p class="citation"></blockquote><h3 id=2188--69323-real-time-surgical-instrument-segmentation-in-video-using-point-tracking-and-segment-anything-zijian-wu-et-al-2024>(21/88 | 69/323) Real-time Surgical Instrument Segmentation in Video Using Point Tracking and Segment Anything (Zijian Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Wu, Adam Schmidt, Peter Kazanzides, Septimiu E. Salcudean. (2024)<br><strong>Real-time Surgical Instrument Segmentation in Video Using Point Tracking and Segment Anything</strong><br><button class=copy-to-clipboard title="Real-time Surgical Instrument Segmentation in Video Using Point Tracking and Segment Anything" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08003v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08003v1.pdf filename=2403.08003v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Segment Anything Model (SAM) is a powerful vision <b>foundation</b> <b>model</b> that is revolutionizing the traditional paradigm of segmentation. Despite this, a reliance on <b>prompting</b> each frame and large computational cost limit its usage in robotically assisted surgery. Applications, such as augmented reality guidance, require little user intervention along with efficient inference to be usable clinically. In this study, we address these limitations by adopting lightweight SAM variants to meet the speed requirement and employing <b>fine-tuning</b> techniques to enhance their generalization in surgical scenes. Recent advancements in Tracking Any Point (TAP) have shown promising results in both accuracy and efficiency, particularly when points are occluded or leave the field of view. Inspired by this progress, we present a novel framework that combines an online point tracker with a lightweight SAM model that is <b>fine-tuned</b> for surgical instrument segmentation. Sparse points within the region of interest are tracked and used to <b>prompt</b> SAM throughout the video sequence, providing temporal consistency. The quantitative results surpass the state-of-the-art semi-supervised video object segmentation method on the EndoVis 2015 dataset, with an over 25 FPS inference speed running on a single GeForce RTX 4060 GPU.</p></p class="citation"></blockquote><h3 id=2288--70323-bridging-different-language-models-and-generative-vision-models-for-text-to-image-generation-shihao-zhao-et-al-2024>(22/88 | 70/323) Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation (Shihao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong. (2024)<br><strong>Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Text2image, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07860v1.pdf filename=2403.07860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> generation has made significant advancements with the introduction of <b>text-to-image</b> <b>diffusion</b> <b>models.</b> These models typically consist of a language model that interprets user <b>prompts</b> and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in <b>text-to-image</b> <b>diffusion</b> <b>models</b> with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for <b>text-to-image</b> generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse <b>pre-trained</b> <b>language</b> <b>models</b> and generative vision models for <b>text-to-image</b> generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at <a href=https://github.com/ShihaoZhaoZSH/LaVi-Bridge>https://github.com/ShihaoZhaoZSH/LaVi-Bridge</a>.</p></p class="citation"></blockquote><h3 id=2388--71323-mope-clip-structured-pruning-for-efficient-vision-language-models-with-module-wise-pruning-error-metric-haokun-lin-et-al-2024>(23/88 | 71/323) MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric (Haokun Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, Zhenan Sun. (2024)<br><strong>MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric</strong><br><button class=copy-to-clipboard title="MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Pruning, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07839v1.pdf filename=2403.07839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> pre-trained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based <b>pruning</b> on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper, we first propose the Module-wise <b>Pruning</b> Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric, we introduce a unified <b>pruning</b> framework applicable to both pre-training and task-specific <b>fine-tuning</b> compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly reducing pre-training costs while maintaining strong <b>zero-shot</b> capabilities. For <b>fine-tuning,</b> consecutive <b>pruning</b> from width to depth yields highly competitive task-specific models. Extensive experiments in two stages demonstrate the effectiveness of the MoPE metric, and MoPE-CLIP outperforms previous state-of-the-art VLP compression methods.</p></p class="citation"></blockquote><h3 id=2488--72323-synth2-boosting-visual-language-models-with-synthetic-captions-and-image-embeddings-sahand-sharifzadeh-et-al-2024>(24/88 | 72/323) Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings (Sahand Sharifzadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, Dharshan Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, Andrea Banino. (2024)<br><strong>Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings</strong><br><button class=copy-to-clipboard title="Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Image2text, Text2image, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07750v1.pdf filename=2403.07750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and image generation models to create synthetic <b>image-text</b> pairs for efficient and effective VLM training. Our method employs pretraining a <b>text-to-image</b> model to synthesize image embeddings starting from captions generated by an <b>LLM.</b> These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel space. This research introduces a promising technique for generating <b>large-scale,</b> <b>customizable</b> <b>image</b> datasets, leading to enhanced VLM performance and wider applicability across various domains, all with improved data efficiency and resource utilization.</p></p class="citation"></blockquote><h3 id=2588--73323-decomposing-disease-descriptions-for-enhanced-pathology-detection-a-multi-aspect-vision-language-matching-framework-minh-hieu-phan-et-al-2024>(25/88 | 73/323) Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Matching Framework (Minh Hieu Phan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu, Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, Johan W. Verjans. (2024)<br><strong>Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Matching Framework</strong><br><button class=copy-to-clipboard title="Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Matching Framework" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Zero-shot, Transformer, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07636v1.pdf filename=2403.07636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical vision language pre-training (VLP) has emerged as a frontier of research, enabling <b>zero-shot</b> pathological recognition by comparing the query image with the textual descriptions for each disease. Due to the complex semantics of biomedical texts, current methods struggle to align medical images with key pathological findings in unstructured reports. This leads to the misalignment with the target disease&rsquo;s textual representation. In this paper, we introduce a novel VLP framework designed to dissect disease descriptions into their fundamental aspects, leveraging prior knowledge about the visual manifestations of pathologies. This is achieved by consulting a <b>large</b> <b>language</b> <b>model</b> and medical experts. Integrating a <b>Transformer</b> module, our approach aligns an input image with the diverse elements of a disease, generating aspect-centric image representations. By consolidating the matches from each aspect, we improve the compatibility between an image and its associated disease. Additionally, capitalizing on the aspect-oriented representations, we present a dual-head <b>Transformer</b> tailored to process known and unknown diseases, optimizing the comprehensive detection efficacy. Conducting experiments on seven downstream datasets, ours outperforms recent methods by up to 8.07% and 11.23% in AUC scores for seen and novel categories, respectively. Our code is released at \href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}.</p></p class="citation"></blockquote><h3 id=2688--74323-block-wise-lora-revisiting-fine-grained-lora-for-effective-personalization-and-stylization-in-text-to-image-generation-likun-li-et-al-2024>(26/88 | 74/323) Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation (Likun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Likun Li, Haoqi Zeng, Changpeng Yang, Haozhe Jia, Di Xu. (2024)<br><strong>Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07500v1.pdf filename=2403.07500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The objective of personalization and stylization in <b>text-to-image</b> is to instruct a pre-trained <b>diffusion</b> <b>model</b> to analyze new concepts introduced by users and incorporate them into expected styles. Recently, parameter-efficient <b>fine-tuning</b> (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field. Despite their popularity, existing efficient <b>fine-tuning</b> methods still struggle to achieve effective personalization and stylization in T2I generation. To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained <b>fine-tuning</b> for different blocks of SD, which can generate images faithful to input <b>prompts</b> and target identity and also with desired style. Extensive experiments demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=2788--75323-calibrating-multi-modal-representations-a-pursuit-of-group-robustness-without-annotations-chenyu-you-et-al-2024>(27/88 | 75/323) Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations (Chenyu You et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyu You, Yifei Min, Weicheng Dai, Jasjeet S. Sekhon, Lawrence Staib, James S. Duncan. (2024)<br><strong>Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations</strong><br><button class=copy-to-clipboard title="Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Contrastive Learning, Fine-tuning, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07241v1.pdf filename=2403.07241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> pre-trained <b>vision-language</b> models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained <b>vision-language</b> classifiers may overly depend on spurious features &ndash; patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any group annotation. To this end, we systematically study the existence of spurious correlation on CLIP and CILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR), verify that last-layer retraining can greatly improve group robustness on pretrained CLIP. In view of them, we advocate a lightweight representation calibration method for <b>fine-tuning</b> CLIP, by first generating a calibration set using the pretrained CLIP, and then calibrating representations of samples within this set through <b>contrastive</b> <b>learning,</b> all without the need for group labels. Extensive experiments and in-depth visualizations on several <b>benchmarks</b> validate the effectiveness of our proposals, largely reducing reliance and significantly boosting the model generalization.</p></p class="citation"></blockquote><h3 id=2888--76323-premonition-using-generative-models-to-preempt-future-data-changes-in-continual-learning-mark-d-mcdonnell-et-al-2024>(28/88 | 76/323) Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning (Mark D. McDonnell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark D. McDonnell, Dong Gong, Ehsan Abbasnejad, Anton van den Hengel. (2024)<br><strong>Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning</strong><br><button class=copy-to-clipboard title="Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Continual Learning, Supervised Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07356v1.pdf filename=2403.07356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed. It is rare, however, that the data and task changes are completely unpredictable. Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it. We show here that the combination of a <b>large</b> <b>language</b> <b>model</b> and an image generation model can similarly provide useful premonitions as to how a <b>continual</b> <b>learning</b> challenge might develop over time. We use the <b>large</b> <b>language</b> <b>model</b> to generate text descriptions of semantically related classes that might potentially appear in the data stream in future. These descriptions are then rendered using Stable Diffusion to generate new labelled image samples. The resulting synthetic dataset is employed for <b>supervised</b> pre-training, but is discarded prior to commencing <b>continual</b> <b>learning,</b> along with the pre-training classification head. We find that the backbone of our pre-trained networks can learn representations useful for the downstream <b>continual</b> <b>learning</b> problem, thus becoming a valuable input to any existing <b>continual</b> <b>learning</b> method. Although there are complexities arising from the domain gap between real and synthetic images, we show that pre-training models in this manner improves multiple Class Incremenal Learning (CIL) methods on fine-grained image classification <b>benchmarks.</b> Supporting code can be found at <a href=https://github.com/cl-premonition/premonition>https://github.com/cl-premonition/premonition</a>.</p></p class="citation"></blockquote><h3 id=2988--77323-mitigating-the-impact-of-attribute-editing-on-face-recognition-sudipta-banerjee-et-al-2024>(29/88 | 77/323) Mitigating the Impact of Attribute Editing on Face Recognition (Sudipta Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sudipta Banerjee, Sai Pranaswi Mullangi, Shruti Wagle, Chinmay Hegde, Nasir Memon. (2024)<br><strong>Mitigating the Impact of Attribute Editing on Face Recognition</strong><br><button class=copy-to-clipboard title="Mitigating the Impact of Attribute Editing on Face Recognition" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: ControlNet, Face Recognition, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08092v1.pdf filename=2403.08092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial attribute editing using generative models can impair automated <b>face</b> <b>recognition.</b> This degradation persists even with recent identity-preserving models such as InstantID. To mitigate this issue, we propose two techniques that perform local and global attribute editing. Local editing operates on the finer details via a regularization-free method based on <b>ControlNet</b> conditioned on depth maps and auxiliary semantic segmentation masks. Global editing operates on coarser details via a regularization-based method guided by custom loss and regularization set. In this work, we empirically ablate twenty-six facial semantic, demographic and expression-based attributes altered using state-of-the-art generative models and evaluate them using ArcFace and AdaFace matchers on CelebA, CelebAMaskHQ and LFW datasets. Finally, we use LLaVA, a <b>vision-language</b> framework for attribute prediction to validate our editing techniques. Our methods outperform SoTA (BLIP, InstantID) at facial editing while retaining identity.</p></p class="citation"></blockquote><h3 id=3088--78323-fluorosam-a-language-aligned-foundation-model-for-x-ray-image-segmentation-benjamin-d-killeen-et-al-2024>(30/88 | 78/323) FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation (Benjamin D. Killeen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin D. Killeen, Liam J. Wang, Han Zhang, Mehran Armand, Russell H. Taylor, Greg Osgood, Mathias Unberath. (2024)<br><strong>FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation</strong><br><button class=copy-to-clipboard title="FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Foundation Model, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08059v1.pdf filename=2403.08059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, <b>foundation</b> <b>models</b> (FMs) &ndash; machine learning models trained on large amounts of highly variable data thus enabling broad applicability &ndash; have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D structures are projected in transmission onto the imaging plane, resulting in overlapping features of varying opacity and shape. To pave the way toward an FM for comprehensive and automated analysis of arbitrary medical X-ray images, we develop FluoroSAM, a language-aligned variant of the Segment-Anything Model, trained from scratch on 1.6M synthetic X-ray images. FluoroSAM is trained on data including masks for 128 organ types and 464 non-anatomical objects, such as tools and implants. In real X-ray images of cadaveric specimens, FluoroSAM is able to segment bony anatomical structures based on text-only <b>prompting</b> with 0.51 and 0.79 DICE with point-based refinement, outperforming competing SAM variants for all structures. FluoroSAM is also capable of <b>zero-shot</b> generalization to segmenting classes beyond the training set thanks to its language alignment, which we demonstrate for full lung segmentation on real chest X-rays.</p></p class="citation"></blockquote><h3 id=3188--79323-unified-source-free-domain-adaptation-song-tang-et-al-2024>(31/88 | 79/323) Unified Source-Free Domain Adaptation (Song Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Song Tang, Wenxin Su, Mao Ye, Jianwei Zhang, Xiatian Zhu. (2024)<br><strong>Unified Source-Free Domain Adaptation</strong><br><button class=copy-to-clipboard title="Unified Source-Free Domain Adaptation" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Domain Adaptation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07601v1.pdf filename=2403.07601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the pursuit of transferring a source model to a target <b>domain</b> <b>without</b> access to the source training data, Source-Free <b>Domain</b> <b>Adaptation</b> (SFDA) has been extensively explored across various scenarios, including closed-set, open-set, partial-set, and generalized settings. Existing methods, focusing on specific scenarios, not only address only a subset of challenges but also necessitate prior knowledge of the target <b>domain,</b> <b>significantly</b> limiting their practical utility and deployability. In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner. To tackle this unified SFDA problem, we propose a novel approach called Latent Causal Factors Discovery (LCFD). In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate LCFD from a causality perspective. The objective is to uncover the causal relationships between latent variables and model decisions, enhancing the reliability and robustness of the learned model against <b>domain</b> <b>shifts.</b> To integrate extensive world knowledge, we leverage a pre-trained <b>vision-language</b> model such as CLIP. This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees. Extensive experiments demonstrate that LCFD can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free <b>out-of-distribution</b> generalization.Our code and data are available at <a href=https://github.com/tntek/source-free-domain-adaptation>https://github.com/tntek/source-free-domain-adaptation</a>.</p></p class="citation"></blockquote><h3 id=3288--80323-fpt-fine-grained-prompt-tuning-for-parameter-and-memory-efficient-fine-tuning-in-high-resolution-medical-image-classification-yijin-huang-et-al-2024>(32/88 | 80/323) FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine Tuning in High-resolution Medical Image Classification (Yijin Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang. (2024)<br><strong>FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine Tuning in High-resolution Medical Image Classification</strong><br><button class=copy-to-clipboard title="FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine Tuning in High-resolution Medical Image Classification" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07576v1.pdf filename=2403.07576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> (PEFT) is proposed as a cost-effective way to transfer pre-trained models to downstream tasks, avoiding the high cost of updating entire large-scale pre-trained models (LPMs). In this work, we present Fine-grained <b>Prompt</b> Tuning (FPT), a novel PEFT method for medical image classification. FPT significantly reduces memory consumption compared to other PEFT methods, especially in high-resolution contexts. To achieve this, we first freeze the weights of the LPM and construct a learnable lightweight side network. The frozen LPM takes high-resolution images as input to extract fine-grained features, while the side network is fed low-resolution images to reduce memory usage. To allow the side network to access pre-trained knowledge, we introduce fine-grained <b>prompts</b> that <b>summarize</b> information from the LPM through a fusion module. Important tokens selection and preloading techniques are employed to further reduce training cost and memory requirements. We evaluate FPT on four medical datasets with varying sizes, modalities, and complexities. Experimental results demonstrate that FPT achieves comparable performance to <b>fine-tuning</b> the entire LPM while using only 1.8% of the learnable parameters and 13% of the memory costs of an encoder ViT-B model with a 512 x 512 input resolution.</p></p class="citation"></blockquote><h3 id=3388--81323-rsbuilding-towards-general-remote-sensing-image-building-extraction-and-change-detection-with-foundation-model-mingze-wang-et-al-2024>(33/88 | 81/323) RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model (Mingze Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingze Wang, Keyan Chen, Lili Su, Cilin Yan, Sheng Xu, Haotian Zhang, Pengcheng Yuan, Xiaolong Jiang, Baochang Zhang. (2024)<br><strong>RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model</strong><br><button class=copy-to-clipboard title="RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Foundation Model, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07564v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07564v1.pdf filename=2403.07564v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The intelligent interpretation of buildings plays a significant role in urban planning and management, macroeconomic analysis, population dynamics, etc. Remote sensing image building interpretation primarily encompasses building extraction and change detection. However, current methodologies often treat these two tasks as separate entities, thereby failing to leverage shared knowledge. Moreover, the complexity and diversity of remote sensing image scenes pose additional challenges, as most algorithms are designed to model individual small datasets, thus lacking cross-scene generalization. In this paper, we propose a comprehensive remote sensing image building understanding model, termed RSBuilding, developed from the perspective of the <b>foundation</b> <b>model.</b> RSBuilding is designed to enhance cross-scene generalization and task universality. Specifically, we extract image features based on the prior knowledge of the <b>foundation</b> <b>model</b> and devise a multi-level feature sampler to augment scale information. To unify task representation and integrate image spatiotemporal clues, we introduce a cross-attention decoder with task <b>prompts.</b> Addressing the current shortage of datasets that incorporate annotations for both tasks, we have developed a federated training strategy to facilitate smooth model convergence even when supervision for some tasks is missing, thereby bolstering the complementarity of different tasks. Our model was trained on a dataset comprising up to 245,000 images and validated on multiple building extraction and change detection datasets. The experimental results substantiate that RSBuilding can concurrently handle two structurally distinct tasks and exhibits robust <b>zero-shot</b> generalization capabilities.</p></p class="citation"></blockquote><h3 id=3488--82323-continual-all-in-one-adverse-weather-removal-with-knowledge-replay-on-a-unified-network-structure-de-cheng-et-al-2024>(34/88 | 82/323) Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure (De Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>De Cheng, Yanling Ji, Dong Gong, Yan Li, Nannan Wang, Junwei Han, Dingwen Zhang. (2024)<br><strong>Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure</strong><br><button class=copy-to-clipboard title="Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Continual Learning, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07292v1.pdf filename=2403.07292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world applications, image degeneration caused by adverse weather is always complex and changes with different weather conditions from days and seasons. Systems in real-world environments constantly encounter adverse weather conditions that are not previously observed. Therefore, it practically requires adverse weather removal models to continually learn from incrementally collected data reflecting various degeneration types. Existing adverse weather removal approaches, for either single or multiple adverse weathers, are mainly designed for a static learning paradigm, which assumes that the data of all types of degenerations to handle can be finely collected at one time before a single-phase learning process. They thus cannot directly handle the incremental learning requirements. To address this issue, we made the earliest effort to investigate the <b>continual</b> <b>all-in-one</b> adverse weather removal task, in a setting closer to real-world applications. Specifically, we develop a novel <b>continual</b> <b>learning</b> framework with effective <b>knowledge</b> <b>replay</b> (KR) on a unified network structure. Equipped with a principal component projection and an effective <b>knowledge</b> <b>distillation</b> mechanism, the proposed KR techniques are tailored for the all-in-one weather removal task. It considers the characteristics of the image restoration task with multiple degenerations in <b>continual</b> <b>learning,</b> and the <b>knowledge</b> <b>for</b> different degenerations can be shared and accumulated in the unified network structure. Extensive experimental results demonstrate the effectiveness of the proposed method to deal with this challenging task, which performs competitively to existing dedicated or joint training image restoration methods. Our code is available at <a href=https://github.com/xiaojihh/CL_all-in-one>https://github.com/xiaojihh/CL_all-in-one</a>.</p></p class="citation"></blockquote><h3 id=3588--83323-its-all-about-your-sketch-democratising-sketch-control-in-diffusion-models-subhadeep-koley-et-al-2024>(35/88 | 83/323) It&rsquo;s All About Your Sketch: Democratising Sketch Control in Diffusion Models (Subhadeep Koley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song. (2024)<br><strong>It&rsquo;s All About Your Sketch: Democratising Sketch Control in Diffusion Models</strong><br><button class=copy-to-clipboard title="It's All About Your Sketch: Democratising Sketch Control in Diffusion Models" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Generative AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07234v1.pdf filename=2403.07234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper unravels the potential of sketches for <b>diffusion</b> <b>models,</b> addressing the deceptive promise of direct sketch control in <b>generative</b> <b>AI.</b> We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of &ldquo;what you sketch is what you get&rdquo;. A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning. To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association. Our approach operates seamlessly during inference without the need for textual <b>prompts;</b> a simple, rough sketch akin to what you and I can create suffices! We welcome everyone to examine results presented in the paper and its supplementary. Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments.</p></p class="citation"></blockquote><h3 id=3688--84323-dynamic-graph-representation-with-knowledge-aware-attention-for-histopathology-whole-slide-image-analysis-jiawen-li-et-al-2024>(36/88 | 84/323) Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis (Jiawen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawen Li, Yuxuan Chen, Hongbo Chu, Qiehe Sun, Tian Guan, Anjia Han, Yonghong He. (2024)<br><strong>Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis</strong><br><button class=copy-to-clipboard title="Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Edge Embedding, Graph, Benchmarking, Knowledge Graph, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07719v1.pdf filename=2403.07719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Histopathological whole slide images (WSIs) classification has become a foundation task in medical microscopic imaging processing. Prevailing approaches involve learning WSIs as instance-bag <b>representations,</b> <b>emphasizing</b> significant instances but struggling to capture the interactions between instances. Additionally, conventional <b>graph</b> <b>representation</b> <b>methods</b> utilize explicit spatial positions to construct topological structures but restrict the flexible interaction capabilities between instances at arbitrary locations, particularly when spatially distant. In response, we propose a novel dynamic <b>graph</b> <b>representation</b> <b>algorithm</b> that conceptualizes WSIs as a form of the <b>knowledge</b> <b>graph</b> structure. Specifically, we dynamically construct neighbors and directed <b>edge</b> <b>embeddings</b> based on the head and tail relationships between instances. Then, we devise a <b>knowledge-aware</b> <b>attention</b> mechanism that can update the head node features by learning the joint attention score of each neighbor and <b>edge.</b> <b>Finally,</b> we obtain a <b>graph-level</b> embedding through the global pooling process of the updated head, serving as an implicit <b>representation</b> <b>for</b> the WSI classification. Our end-to-end <b>graph</b> <b>representation</b> <b>learning</b> approach has outperformed the state-of-the-art WSI analysis methods on three TCGA <b>benchmark</b> datasets and in-house test sets. Our code is available at <a href=https://github.com/WonderLandxD/WiKG>https://github.com/WonderLandxD/WiKG</a>.</p></p class="citation"></blockquote><h3 id=3788--85323-let-storytelling-tell-vivid-stories-an-expressive-and-fluent-multimodal-storyteller-chuanqi-zang-et-al-2024>(37/88 | 85/323) Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller (Chuanqi Zang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuanqi Zang, Jiji Tang, Rongsheng Zhang, Zeng Zhao, Tangjie Lv, Mingtao Pei, Wei Liang. (2024)<br><strong>Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller</strong><br><button class=copy-to-clipboard title="Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07301v1.pdf filename=2403.07301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Storytelling aims to generate reasonable and vivid narratives based on an ordered image stream. The fidelity to the image story theme and the divergence of story plots attract readers to keep reading. Previous works iteratively improved the alignment of multiple modalities but ultimately resulted in the generation of simplistic storylines for image streams. In this work, we propose a new pipeline, termed LLaMS, to generate <b>multimodal</b> human-level stories that are embodied in expressiveness and consistency. Specifically, by fully exploiting the commonsense knowledge within the <b>LLM,</b> we first employ a sequence data auto-enhancement strategy to enhance factual content expression and leverage a textual <b>reasoning</b> architecture for expressive story generation and prediction. Secondly, we propose SQ-Adatpter module for story illustration generation which can maintain sequence consistency. Numerical results are conducted through human evaluation to verify the superiority of proposed LLaMS. Evaluations show that LLaMS achieves state-of-the-art storytelling performance and 86% correlation and 100% consistency win rate as compared with previous SOTA methods. Furthermore, ablation experiments are conducted to verify the effectiveness of proposed sequence data enhancement and SQ-Adapter.</p></p class="citation"></blockquote><h3 id=3888--86323-a-fourier-transform-framework-for-domain-adaptation-le-luo-et-al-2024>(38/88 | 86/323) A Fourier Transform Framework for Domain Adaptation (Le Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Le Luo, Bingrong Xu, Qingyong Zhang, Cheng Lian, Jie Luo. (2024)<br><strong>A Fourier Transform Framework for Domain Adaptation</strong><br><button class=copy-to-clipboard title="A Fourier Transform Framework for Domain Adaptation" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07798v1.pdf filename=2403.07798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By using <b>unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA), knowledge can be transferred from a label-rich source <b>domain</b> <b>to</b> a target <b>domain</b> <b>that</b> contains relevant information but lacks labels. Many existing UDA algorithms suffer from directly using raw images as input, resulting in models that overly focus on redundant information and exhibit poor generalization capability. To address this issue, we attempt to improve the performance of <b>unsupervised</b> <b>domain</b> <b>adaptation</b> by employing the Fourier method (FTF).Specifically, FTF is inspired by the amplitude of Fourier spectra, which primarily preserves low-level statistical information. In FTF, we effectively incorporate low-level information from the target <b>domain</b> <b>into</b> the source <b>domain</b> <b>by</b> fusing the amplitudes of both <b>domains</b> <b>in</b> the Fourier <b>domain.</b> <b>Additionally,</b> we observe that extracting features from batches of images can eliminate redundant information while retaining class-specific features relevant to the task. Building upon this observation, we apply the Fourier Transform at the data stream level for the first time. To further align multiple sources of data, we introduce the concept of correlation alignment. To evaluate the effectiveness of our FTF method, we conducted evaluations on four <b>benchmark</b> datasets for <b>domain</b> <b>adaptation,</b> including Office-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results demonstrate superior performance.</p></p class="citation"></blockquote><h3 id=3988--87323-eliminating-cross-modal-conflicts-in-bev-space-for-lidar-camera-3d-object-detection-jiahui-fu-et-al-2024>(39/88 | 87/323) Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection (Jiahui Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahui Fu, Chen Gao, Zitian Wang, Lirong Yang, Xiaofei Wang, Beipeng Mu, Si Liu. (2024)<br><strong>Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection</strong><br><button class=copy-to-clipboard title="Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Multi-modal, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07372v1.pdf filename=2403.07372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent 3D <b>object</b> <b>detectors</b> typically utilize multi-sensor data and unify <b>multi-modal</b> features in the shared bird&rsquo;s-eye view (BEV) representation space. However, our empirical findings indicate that previous methods have limitations in generating fusion BEV features free from cross-modal conflicts. These conflicts encompass extrinsic conflicts caused by BEV feature construction and inherent conflicts <b>stemming</b> from heterogeneous sensor signals. Therefore, we propose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly eliminate the extrinsic/inherent conflicts in BEV space and produce improved <b>multi-modal</b> BEV features. Specifically, we devise a Semantic-guided Flow-based Alignment (SFA) module to resolve extrinsic conflicts via unifying spatial distribution in BEV space before fusion. Moreover, we design a Dissolved Query Recovering (DQR) mechanism to remedy inherent conflicts by preserving objectness clues that are lost in the fusion BEV feature. In general, our method maximizes the effective information utilization of each modality and leverages inter-modal complementarity. Our method achieves state-of-the-art performance in the highly competitive nuScenes 3D <b>object</b> <b>detection</b> dataset. The code is released at <a href=https://github.com/fjhzhixi/ECFusion>https://github.com/fjhzhixi/ECFusion</a>.</p></p class="citation"></blockquote><h3 id=4088--88323-a-bayesian-approach-to-ood-robustness-in-image-classification-prakhar-kaushik-et-al-2024>(40/88 | 88/323) A Bayesian Approach to OOD Robustness in Image Classification (Prakhar Kaushik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prakhar Kaushik, Adam Kortylewski, Alan Yuille. (2024)<br><strong>A Bayesian Approach to OOD Robustness in Image Classification</strong><br><button class=copy-to-clipboard title="A Bayesian Approach to OOD Robustness in Image Classification" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Out-of-domain, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07277v1.pdf filename=2403.07277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV <b>benchmark</b> where we encounter real world <b>Out-of-Domain</b> (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transitional dictionary of vMF kernels that are intermediate between the source and target domains and train the generative model on this dictionary using the annotations on the source domain, followed by iterative refinement. This approach, termed <b>Unsupervised</b> Generative Transition (UGT), performs very well in OOD scenarios even when occlusion is present. UGT is evaluated on different OOD <b>benchmarks</b> including the OOD-CV dataset, several popular datasets (e.g., ImageNet-C [9]), artificial image corruptions (including adding occluders), and synthetic-to-real domain transfer, and does well in all scenarios outperforming SOTA alternatives (e.g. up to 10% top-1 accuracy on Occluded OOD-CV dataset).</p></p class="citation"></blockquote><h3 id=4188--89323-stylegaussian-instant-3d-style-transfer-with-gaussian-splatting-kunhao-liu-et-al-2024>(41/88 | 89/323) StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting (Kunhao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu. (2024)<br><strong>StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting</strong><br><button class=copy-to-clipboard title="StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07807v1.pdf filename=2403.07807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce StyleGaussian, a novel 3D <b>style</b> <b>transfer</b> technique that allows instant transfer of any image&rsquo;s <b>style</b> <b>to</b> a 3D scene at 10 frames per second (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves <b>style</b> <b>transfer</b> without compromising its real-time rendering ability and multi-view consistency. It achieves instant <b>style</b> <b>transfer</b> with three steps: embedding, transfer, and decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D Gaussians. Next, the embedded features are transformed according to a reference <b>style</b> <b>image.</b> Finally, the transformed features are decoded into the stylized RGB. StyleGaussian has two novel designs. The first is an efficient feature rendering strategy that first renders low-dimensional features and then maps them into high-dimensional features while embedding VGG features. It cuts the memory consumption significantly and enables 3DGS to render the high-dimensional memory-intensive features. The second is a K-nearest-neighbor-based 3D <b>CNN.</b> Working as the decoder for the stylized features, it eliminates the 2D <b>CNN</b> operations that compromise strict multi-view consistency. Extensive experiments show that StyleGaussian achieves instant 3D stylization with superior stylization quality while preserving real-time rendering and strict multi-view consistency. Project page: <a href=https://kunhao-liu.github.io/StyleGaussian/>https://kunhao-liu.github.io/StyleGaussian/</a></p></p class="citation"></blockquote><h3 id=4288--90323-stable-makeup-when-real-world-makeup-transfer-meets-diffusion-model-yuxuan-zhang-et-al-2024>(42/88 | 90/323) Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model (Yuxuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, Haibo Zhao. (2024)<br><strong>Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model</strong><br><button class=copy-to-clipboard title="Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07764v1.pdf filename=2403.07764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel <b>diffusion-based</b> <b>makeup</b> transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained <b>diffusion</b> <b>model</b> and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided <b>text-to-image</b> generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields.</p></p class="citation"></blockquote><h3 id=4388--91323-fast-and-simple-explainability-for-point-cloud-networks-meir-yossef-levi-et-al-2024>(43/88 | 91/323) Fast and Simple Explainability for Point Cloud Networks (Meir Yossef Levi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meir Yossef Levi, Guy Gilboa. (2024)<br><strong>Fast and Simple Explainability for Point Cloud Networks</strong><br><button class=copy-to-clipboard title="Fast and Simple Explainability for Point Cloud Networks" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Explainable AI, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07706v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07706v2.pdf filename=2403.07706v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a fast and simple <b>explainable</b> <b>AI</b> (XAI) method for point cloud data. It computes pointwise importance with respect to a trained network downstream task. This allows better understanding of the network properties, which is imperative for safety-critical applications. In addition to debugging and visualization, our low computational complexity facilitates online feedback to the network at inference. This can be used to reduce uncertainty and to increase robustness. In this work, we introduce \emph{Feature Based Interpretability} (FBI), where we compute the features&rsquo; norm, per point, before the bottleneck. We analyze the use of gradients and post- and pre-bottleneck strategies, showing pre-bottleneck is preferred, in terms of smoothness and ranking. We obtain at least three orders of magnitude speedup, compared to current XAI methods, thus, scalable for big point clouds or large-scale architectures. Our approach achieves SOTA results, in terms of classification explainability. We demonstrate how the proposed measure is helpful in analyzing and characterizing various aspects of 3D learning, such as rotation invariance, robustness to <b>out-of-distribution</b> (OOD) outliers or domain shift and dataset bias.</p></p class="citation"></blockquote><h3 id=4488--92323-annotations-on-a-budget-leveraging-geo-data-similarity-to-balance-model-performance-and-annotation-cost-oana-ignat-et-al-2024>(44/88 | 92/323) Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost (Oana Ignat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oana Ignat, Longju Bai, Joan Nwatu, Rada Mihalcea. (2024)<br><strong>Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost</strong><br><button class=copy-to-clipboard title="Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07687v1.pdf filename=2403.07687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>foundation</b> <b>models</b> have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large <b>vision-language</b> <b>foundation</b> <b>models.</b> Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at <a href=https://github.com/MichiganNLP/visual_diversity_budget>https://github.com/MichiganNLP/visual_diversity_budget</a>.</p></p class="citation"></blockquote><h3 id=4588--93323-hunting-attributes-context-prototype-aware-learning-for-weakly-supervised-semantic-segmentation-feilong-tang-et-al-2024>(45/88 | 93/323) Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation (Feilong Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feilong Tang, Zhongxing Xu, Zhaojun Qu, Wei Feng, Xingjian Jiang, Zongyuan Ge. (2024)<br><strong>Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07630v1.pdf filename=2403.07630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent weakly <b>supervised</b> semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM). In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics. Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances. The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias. Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions. With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension. The core of this method is to accurately capture intra-class variations in object features through context-aware prototypes, facilitating the adaptation to the semantic attributes of various instances. We design feature distribution alignment to optimize prototype awareness, aligning instance feature distributions with dense features. In addition, a unified training framework is proposed to combine label-guided classification supervision and prototypes-guided self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show that CPAL significantly improves off-the-shelf methods and achieves state-of-the-art performance. The project is available at <a href=https://github.com/Barrett-python/CPAL>https://github.com/Barrett-python/CPAL</a>.</p></p class="citation"></blockquote><h3 id=4688--94323-minkunext-point-cloud-based-large-scale-place-recognition-using-3d-sparse-convolutions-j-j-cabrera-et-al-2024>(46/88 | 94/323) MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D Sparse Convolutions (J. J. Cabrera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. J. Cabrera, A. Santo, A. Gil, C. Viegas, L. Payá. (2024)<br><strong>MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D Sparse Convolutions</strong><br><button class=copy-to-clipboard title="MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D Sparse Convolutions" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07593v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07593v2.pdf filename=2403.07593v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents MinkUNeXt, an effective and efficient architecture for place-recognition from point clouds entirely based on the new 3D MinkNeXt Block, a residual block composed of 3D sparse <b>convolutions</b> that follows the philosophy established by recent <b>Transformers</b> but purely using simple 3D <b>convolutions.</b> Feature extraction is performed at different scales by a U-Net encoder-decoder network and the feature aggregation of those features into a single descriptor is carried out by a Generalized Mean Pooling (GeM). The proposed architecture demonstrates that it is possible to surpass the current state-of-the-art by only relying on conventional 3D sparse <b>convolutions</b> without making use of more complex and sophisticated proposals such as <b>Transformers,</b> Attention-Layers or Deformable <b>Convolutions.</b> A thorough assessment of the proposal has been carried out using the Oxford RobotCar and the In-house datasets. As a result, MinkUNeXt proves to outperform other methods in the state-of-the-art.</p></p class="citation"></blockquote><h3 id=4788--95323-aacp-aesthetics-assessment-of-childrens-paintings-based-on-self-supervised-learning-shiqi-jiang-et-al-2024>(47/88 | 95/323) AACP: Aesthetics assessment of children&rsquo;s paintings based on self-supervised learning (Shiqi Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiqi Jiang, Ning Li, Chen Shi, Liping Guo, Changbo Wang, Chenhui Li. (2024)<br><strong>AACP: Aesthetics assessment of children&rsquo;s paintings based on self-supervised learning</strong><br><button class=copy-to-clipboard title="AACP: Aesthetics assessment of children's paintings based on self-supervised learning" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07578v1.pdf filename=2403.07578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Aesthetics Assessment of Children&rsquo;s Paintings (AACP) is an important branch of the image aesthetics assessment (IAA), playing a significant role in children&rsquo;s education. This task presents unique challenges, such as limited available data and the requirement for evaluation metrics from multiple perspectives. However, previous approaches have relied on training large datasets and subsequently providing an aesthetics score to the image, which is not applicable to AACP. To solve this problem, we construct an aesthetics assessment dataset of children&rsquo;s paintings and a model based on <b>self-supervised</b> <b>learning.</b> 1) We build a novel dataset composed of two parts: the first part contains more than 20k unlabeled images of children&rsquo;s paintings; the second part contains 1.2k images of children&rsquo;s paintings, and each image contains eight attributes labeled by multiple design experts. 2) We design a pipeline that includes a feature extraction module, perception modules and a disentangled evaluation module. 3) We conduct both qualitative and quantitative experiments to compare our model&rsquo;s performance with five other methods using the AACP dataset. Our experiments reveal that our method can accurately capture aesthetic features and achieve state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=4888--96323-d4d-an-rgbd-diffusion-model-to-boost-monocular-depth-estimation-l-papa-et-al-2024>(48/88 | 96/323) D4D: An RGBD diffusion model to boost monocular depth estimation (L. Papa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>L. Papa, P. Russo, I. Amerini. (2024)<br><strong>D4D: An RGBD diffusion model to boost monocular depth estimation</strong><br><button class=copy-to-clipboard title="D4D: An RGBD diffusion model to boost monocular depth estimation" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07516v1.pdf filename=2403.07516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ground-truth RGBD data are fundamental for a wide range of computer vision applications; however, those labeled samples are difficult to collect and time-consuming to produce. A common solution to overcome this lack of data is to employ graphic engines to produce synthetic proxies; however, those data do not often reflect real-world images, resulting in poor performance of the trained models at the inference step. In this paper we propose a novel training pipeline that incorporates Diffusion4D (D4D), a customized 4-channels <b>diffusion</b> <b>model</b> able to generate realistic RGBD samples. We show the effectiveness of the developed solution in improving the performances of deep learning models on the monocular depth estimation task, where the correspondence between RGB and depth map is crucial to achieving accurate measurements. Our <b>supervised</b> training pipeline, enriched by the generated samples, outperforms synthetic and original data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%) respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset.</p></p class="citation"></blockquote><h3 id=4988--97323-a-comprehensive-survey-of-3d-dense-captioning-localizing-and-describing-objects-in-3d-scenes-ting-yu-et-al-2024>(49/88 | 97/323) A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing Objects in 3D Scenes (Ting Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting Yu, Xiaojun Lin, Shuhui Wang, Weiguo Sheng, Qingming Huang, Jun Yu. (2024)<br><strong>A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing Objects in 3D Scenes</strong><br><button class=copy-to-clipboard title="A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing Objects in 3D Scenes" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Summarization, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07469v1.pdf filename=2403.07469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Three-Dimensional (3D) dense captioning is an emerging <b>vision-language</b> bridging task that aims to generate multiple detailed and accurate descriptions for 3D scenes. It presents significant potential and challenges due to its closer representation of the real world compared to 2D visual captioning, as well as complexities in data collection and processing of 3D point cloud sources. Despite the popularity and success of existing methods, there is a lack of comprehensive surveys summarizing the advancements in this field, which hinders its progress. In this paper, we provide a comprehensive review of 3D dense captioning, covering task definition, architecture classification, dataset analysis, evaluation metrics, and in-depth prosperity discussions. Based on a synthesis of previous literature, we refine a standard pipeline that serves as a common paradigm for existing methods. We also introduce a clear taxonomy of existing models, <b>summarize</b> technologies involved in different modules, and conduct detailed experiment analysis. Instead of a chronological order introduction, we categorize the methods into different classes to facilitate exploration and analysis of the differences and connections among existing techniques. We also provide a reading guideline to assist readers with different backgrounds and purposes in reading efficiently. Furthermore, we propose a series of promising future directions for 3D dense captioning by identifying challenges and aligning them with the development of related tasks, offering valuable insights and inspiring future research in this field. Our aim is to provide a comprehensive understanding of 3D dense captioning, foster further investigations, and contribute to the development of novel applications in multimedia and related domains.</p></p class="citation"></blockquote><h3 id=5088--98323-jstr-joint-spatio-temporal-reasoning-for-event-based-moving-object-detection-hanyu-zhou-et-al-2024>(50/88 | 98/323) JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection (Hanyu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanyu Zhou, Zhiwei Shi, Hao Dong, Shihan Peng, Yi Chang, Luxin Yan. (2024)<br><strong>JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection</strong><br><button class=copy-to-clipboard title="JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07436v1.pdf filename=2403.07436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event-based moving <b>object</b> <b>detection</b> is a challenging task, where static background and moving <b>object</b> <b>are</b> mixed together. Typically, existing methods mainly align the background events to the same spatial coordinate system via motion compensation to distinguish the moving <b>object.</b> <b>However,</b> they neglect the potential spatial tailing effect of moving <b>object</b> <b>events</b> caused by excessive motion, which may affect the structure integrity of the extracted moving <b>object.</b> <b>We</b> discover that the moving <b>object</b> <b>has</b> a complete columnar structure in the point cloud composed of motion-compensated events along the timestamp. Motivated by this, we propose a novel joint spatio-temporal <b>reasoning</b> method for event-based moving <b>object</b> <b>detection.</b> Specifically, we first compensate the motion of background events using inertial measurement unit. In spatial <b>reasoning</b> stage, we project the compensated events into the same image coordinate, discretize the timestamp of events to obtain a time image that can reflect the motion confidence, and further segment the moving <b>object</b> <b>through</b> adaptive threshold on the time image. In temporal <b>reasoning</b> stage, we construct the events into a point cloud along timestamp, and use RANSAC algorithm to extract the columnar shape in the cloud for peeling off the background. Finally, we fuse the results from the two <b>reasoning</b> stages to extract the final moving <b>object</b> <b>region.</b> This joint spatio-temporal <b>reasoning</b> framework can effectively detect the moving <b>object</b> <b>from</b> motion confidence and geometric structure. Moreover, we conduct extensive experiments on various datasets to verify that the proposed method can improve the moving <b>object</b> <b>detection</b> accuracy by 13%.</p></p class="citation"></blockquote><h3 id=5188--99323-auxiliary-cyclegan-guidance-for-task-aware-domain-translation-from-duplex-to-monoplex-ihc-images-nicolas-brieu-et-al-2024>(51/88 | 99/323) Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images (Nicolas Brieu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Brieu, Nicolas Triltsch, Philipp Wortmann, Dominik Winter, Shashank Saran, Marlon Rebelatto, Günter Schmidt. (2024)<br><strong>Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images</strong><br><button class=copy-to-clipboard title="Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-10, J-3, I-4-6, cs-AI, cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07389v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07389v1.pdf filename=2403.07389v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>models</b> <b>enable</b> the translation from a source image domain where readily trained models are available to a target domain unseen during training. While Cycle <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains. This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays. Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain. Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches.</p></p class="citation"></blockquote><h3 id=5288--100323-textual-knowledge-matters-cross-modality-co-teaching-for-generalized-visual-class-discovery-haiyang-zheng-et-al-2024>(52/88 | 100/323) Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery (Haiyang Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiyang Zheng, Nan Pu, Wenjing Li, Nicu Sebe, Zhun Zhong. (2024)<br><strong>Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery</strong><br><button class=copy-to-clipboard title="Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07369v1.pdf filename=2403.07369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the problem of Generalized Category Discovery (GCD), which aims to cluster unlabeled data from both known and unknown categories using the knowledge of labeled data from known categories. Current GCD methods rely on only visual cues, which however neglect the multi-modality perceptive nature of human cognitive processes in discovering novel visual categories. To address this, we propose a two-phase TextGCD framework to accomplish multi-modality GCD by exploiting powerful Visual-Language Models. TextGCD mainly includes a retrieval-based <b>text</b> <b>generation</b> (RTG) phase and a cross-modality co-teaching (CCT) phase. First, RTG constructs a visual lexicon using category tags from diverse datasets and attributes from <b>Large</b> <b>Language</b> <b>Models,</b> generating descriptive <b>texts</b> <b>for</b> images in a retrieval manner. Second, CCT leverages disparities between textual and visual modalities to foster mutual learning, thereby enhancing visual GCD. In addition, we design an adaptive class aligning strategy to ensure the alignment of category perceptions between modalities as well as a soft-voting mechanism to integrate multi-modality cues. Experiments on eight datasets show the <b>large</b> <b>superiority</b> <b>of</b> our approach over state-of-the-art methods. Notably, our approach outperforms the best competitor, by 7.7% and 10.8% in All accuracy on ImageNet-1k and CUB, respectively.</p></p class="citation"></blockquote><h3 id=5388--101323-bid-boundary-interior-decoding-for-unsupervised-temporal-action-localization-pre-trainin-qihang-fang-et-al-2024>(53/88 | 101/323) BID: Boundary-Interior Decoding for Unsupervised Temporal Action Localization Pre-Trainin (Qihang Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qihang Fang, Chengcheng Tang, Shugao Ma, Yanchao Yang. (2024)<br><strong>BID: Boundary-Interior Decoding for Unsupervised Temporal Action Localization Pre-Trainin</strong><br><button class=copy-to-clipboard title="BID: Boundary-Interior Decoding for Unsupervised Temporal Action Localization Pre-Trainin" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T45, I-4-8, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07354v1.pdf filename=2403.07354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skeleton-based motion representations are robust for action localization and understanding for their invariance to perspective, lighting, and occlusion, compared with images. Yet, they are often ambiguous and incomplete when taken out of context, even for human annotators. As infants discern gestures before associating them with words, actions can be conceptualized before being grounded with labels. Therefore, we propose the first <b>unsupervised</b> pre-training framework, Boundary-Interior Decoding (BID), that partitions a skeleton-based motion sequence into discovered semantically meaningful pre-action segments. By <b>fine-tuning</b> our pre-training network with a small number of annotated data, we show results out-performing SOTA methods by a large margin.</p></p class="citation"></blockquote><h3 id=5488--102323-frequency-aware-deepfake-detection-improving-generalizability-through-frequency-space-learning-chuangchuang-tan-et-al-2024>(54/88 | 102/323) Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Learning (Chuangchuang Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, Yunchao Wei. (2024)<br><strong>Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Learning</strong><br><button class=copy-to-clipboard title="Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Learning" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07240v1.pdf filename=2403.07240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research addresses the challenge of developing a universal deepfake detector that can effectively identify unseen deepfake images despite limited training data. Existing frequency-based paradigms have relied on frequency-level artifacts introduced during the up-sampling in <b>GAN</b> pipelines to detect forgeries. However, the rapid advancements in synthesis technology have led to specific artifacts for each generation model. Consequently, these detectors have exhibited a lack of proficiency in learning the frequency domain and tend to overfit to the artifacts present in the training data, leading to suboptimal performance on unseen sources. To address this issue, we introduce a novel frequency-aware approach called FreqNet, centered around frequency domain learning, specifically designed to enhance the generalizability of deepfake detectors. Our method forces the detector to continuously focus on high-frequency information, exploiting high-frequency representation of features across spatial and channel dimensions. Additionally, we incorporate a straightforward frequency domain learning module to learn source-agnostic features. It involves <b>convolutional</b> layers applied to both the phase spectrum and amplitude spectrum between the Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (iFFT). Extensive experimentation involving 17 <b>GANs</b> demonstrates the effectiveness of our proposed method, showcasing state-of-the-art performance (+9.8%) while requiring fewer parameters. The code is available at {\cred \url{https://github.com/chuangchuangtan/FreqNet-DeepfakeDetection}}.</p></p class="citation"></blockquote><h3 id=5588--103323-learn-and-search-an-elegant-technique-for-object-lookup-using-contrastive-learning-chandan-kumar-et-al-2024>(55/88 | 103/323) Learn and Search: An Elegant Technique for Object Lookup using Contrastive Learning (Chandan Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chandan Kumar, Jansel Herrera-Gerena, John Just, Matthew Darr, Ali Jannesari. (2024)<br><strong>Learn and Search: An Elegant Technique for Object Lookup using Contrastive Learning</strong><br><button class=copy-to-clipboard title="Learn and Search: An Elegant Technique for Object Lookup using Contrastive Learning" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07231v1.pdf filename=2403.07231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid proliferation of digital content and the ever-growing need for precise object recognition and segmentation have driven the advancement of cutting-edge techniques in the field of object classification and segmentation. This paper introduces &ldquo;Learn and Search&rdquo;, a novel approach for object lookup that leverages the power of <b>contrastive</b> <b>learning</b> to enhance the efficiency and effectiveness of retrieval systems. In this study, we present an elegant and innovative methodology that integrates deep learning principles and <b>contrastive</b> <b>learning</b> to tackle the challenges of object search. Our extensive experimentation reveals compelling results, with &ldquo;Learn and Search&rdquo; achieving superior Similarity Grid Accuracy, showcasing its efficacy in discerning regions of utmost similarity within an image relative to a cropped image. The seamless fusion of deep learning and <b>contrastive</b> <b>learning</b> to address the intricacies of object identification not only promises transformative applications in image recognition, <b>recommendation</b> systems, and content tagging but also revolutionizes content-based search and retrieval. The amalgamation of these techniques, as exemplified by &ldquo;Learn and Search,&rdquo; represents a significant stride in the ongoing evolution of methodologies in the dynamic realm of object classification and segmentation.</p></p class="citation"></blockquote><h3 id=5688--104323-lumen-unleashing-versatile-vision-centric-capabilities-of-large-multimodal-models-yang-jiao-et-al-2024>(56/88 | 104/323) Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models (Yang Jiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, Yu-Gang Jiang. (2024)<br><strong>Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models</strong><br><button class=copy-to-clipboard title="Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07304v1.pdf filename=2403.07304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>Multimodal</b> Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields. A recent trend is to further extend and enhance the perception capabilities of LMMs. The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM. This adaptation leads to convenient development of such LMMs with minimal modifications, however, it overlooks the intrinsic characteristics of diverse visual tasks and hinders the learning of perception capabilities. To address this issue, we propose a novel LMM architecture named Lumen, a Large <b>multimodal</b> model with versatile vision-centric capability enhancement. We decouple the LMM&rsquo;s learning of perception capabilities into task-agnostic and task-specific stages. Lumen first promotes fine-grained <b>vision-language</b> concept alignment, which is the fundamental capability for various visual tasks. Thus the output of the task-agnostic stage is a shared representation for all the tasks we address in this paper. Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts. Benefiting from such a decoupled design, our Lumen surpasses existing LMM-based approaches on the COCO detection <b>benchmark</b> with a clear margin and exhibits seamless scalability to additional visual tasks. Furthermore, we also conduct comprehensive ablation studies and generalization evaluations for deeper insights. The code will be released at <a href=https://github.com/SxJyJay/Lumen>https://github.com/SxJyJay/Lumen</a>.</p></p class="citation"></blockquote><h3 id=5788--105323-q-slam-quadric-representations-for-monocular-slam-chensheng-peng-et-al-2024>(57/88 | 105/323) Q-SLAM: Quadric Representations for Monocular SLAM (Chensheng Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chensheng Peng, Chenfeng Xu, Yue Wang, Mingyu Ding, Heng Yang, Masayoshi Tomizuka, Kurt Keutzer, Marco Pavone, Wei Zhan. (2024)<br><strong>Q-SLAM: Quadric Representations for Monocular SLAM</strong><br><button class=copy-to-clipboard title="Q-SLAM: Quadric Representations for Monocular SLAM" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08125v1.pdf filename=2403.08125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular SLAM has long grappled with the challenge of accurately modeling 3D geometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular SLAM have shown promise, yet these methods typically focus on novel view synthesis rather than precise 3D <b>geometry</b> modeling. This focus results in a significant disconnect between NeRF applications, i.e., novel-view synthesis and the requirements of SLAM. We identify that the gap results from the volumetric representations used in NeRF, which are often dense and noisy. In this study, we propose a novel approach that reimagines volumetric representations through the lens of quadric forms. We posit that most scene components can be effectively represented as quadric planes. Leveraging this assumption, we reshape the volumetric representations with million of cubes by several quadric planes, which leads to more accurate and efficient modeling of 3D scenes in SLAM contexts. Our method involves two key steps: First, we use the quadric assumption to enhance coarse depth estimations obtained from tracking modules, e.g., Droid-SLAM. This step alone significantly improves depth estimation accuracy. Second, in the subsequent mapping phase, we diverge from previous NeRF-based SLAM methods that distribute sampling points across the entire volume space. Instead, we concentrate sampling points around quadric planes and aggregate them using a novel quadric-decomposed <b>Transformer.</b> Additionally, we introduce an end-to-end joint optimization strategy that synchronizes pose estimation with 3D reconstruction.</p></p class="citation"></blockquote><h3 id=5888--106323-spatiotemporal-representation-learning-for-short-and-long-medical-image-time-series-chengzhi-shen-et-al-2024>(58/88 | 106/323) Spatiotemporal Representation Learning for Short and Long Medical Image Time Series (Chengzhi Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengzhi Shen, Martin J. Menten, Hrvoje Bogunović, Ursula Schmidt-Erfurth, Hendrik Scholl, Sobha Sivaprasad, Andrew Lotery, Daniel Rueckert, Paul Hager, Robbie Holland. (2024)<br><strong>Spatiotemporal Representation Learning for Short and Long Medical Image Time Series</strong><br><button class=copy-to-clipboard title="Spatiotemporal Representation Learning for Short and Long Medical Image Time Series" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Contrastive Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07513v1.pdf filename=2403.07513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Analyzing temporal developments is crucial for the accurate prognosis of many medical conditions. Temporal changes that occur over short time scales are key to assessing the health of physiological functions, such as the cardiac cycle. Moreover, tracking longer term developments that occur over months or years in evolving processes, such as age-related macular degeneration (AMD), is essential for accurate prognosis. Despite the importance of both short and long term analysis to clinical decision making, they remain understudied in medical deep learning. State of the art methods for spatiotemporal <b>representation</b> <b>learning,</b> developed for short natural videos, prioritize the detection of temporal constants rather than temporal developments. Moreover, they do not account for varying time intervals between acquisitions, which are essential for contextualizing observed changes. To address these issues, we propose two approaches. First, we combine clip-level <b>contrastive</b> <b>learning</b> with a novel temporal embedding to adapt to irregular time series. Second, we propose masking and predicting latent frame <b>representations</b> <b>of</b> the temporal sequence. Our two approaches outperform all prior methods on temporally-dependent tasks including cardiac output estimation and three prognostic AMD tasks. Overall, this enables the automated analysis of temporal patterns which are typically overlooked in applications of deep learning to medicine.</p></p class="citation"></blockquote><h3 id=5988--107323-unleashing-hydra-hybrid-fusion-depth-consistency-and-radar-for-unified-3d-perception-philipp-wolters-et-al-2024>(59/88 | 107/323) Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception (Philipp Wolters et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Wolters, Johannes Gilg, Torben Teepe, Fabian Herzog, Anouar Laouichi, Martin Hofmann, Gerhard Rigoll. (2024)<br><strong>Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception</strong><br><button class=copy-to-clipboard title="Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07746v1.pdf filename=2403.07746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-cost, vision-centric 3D perception systems for autonomous driving have made significant progress in recent years, narrowing the gap to expensive LiDAR-based methods. The primary challenge in becoming a fully reliable alternative lies in robust depth prediction capabilities, as camera-based systems struggle with long detection ranges and adverse lighting and weather conditions. In this work, we introduce HyDRa, a novel camera-radar fusion architecture for diverse 3D perception tasks. Building upon the principles of dense BEV (Bird&rsquo;s Eye View)-based architectures, HyDRa introduces a hybrid fusion approach to combine the strengths of complementary camera and radar features in two distinct representation spaces. Our Height Association <b>Transformer</b> module leverages radar features already in the perspective view to produce more robust and accurate depth predictions. In the BEV, we refine the initial sparse representation by a Radar-weighted Depth Consistency. HyDRa achieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and 58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new semantically rich and spatially accurate BEV features can be directly converted into a powerful occupancy representation, beating all previous camera-based methods on the Occ3D <b>benchmark</b> by an impressive 3.7 mIoU.</p></p class="citation"></blockquote><h3 id=6088--108323-ssm-meets-video-diffusion-models-efficient-video-generation-with-structured-state-spaces-yuta-oshima-et-al-2024>(60/88 | 108/323) SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces (Yuta Oshima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo. (2024)<br><strong>SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces</strong><br><button class=copy-to-clipboard title="SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07711v1.pdf filename=2403.07711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the remarkable achievements in image generation through <b>diffusion</b> <b>models,</b> the research community has shown increasing interest in extending these models to video generation. Recent <b>diffusion</b> <b>models</b> for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using <b>diffusion</b> <b>models.</b> To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard <b>benchmark</b> of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at <a href=https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models>https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models</a>.</p></p class="citation"></blockquote><h3 id=6188--109323-unleashing-network-potentials-for-semantic-scene-completion-fengyun-wang-et-al-2024>(61/88 | 109/323) Unleashing Network Potentials for Semantic Scene Completion (Fengyun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengyun Wang, Qianru Sun, Dong Zhang, Jinhui Tang. (2024)<br><strong>Unleashing Network Potentials for Semantic Scene Completion</strong><br><button class=copy-to-clipboard title="Unleashing Network Potentials for Semantic Scene Completion" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Adversarial Learning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07560v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07560v2.pdf filename=2403.07560v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy and semantics from a single-view RGB-D image, and recent SSC methods commonly adopt <b>multi-modal</b> inputs. However, our investigation reveals two limitations: ineffective feature learning from single modalities and overfitting to limited datasets. To address these issues, this paper proposes a novel SSC framework - <b>Adversarial</b> <b>Modality</b> Modulation Network (AMMNet) - with a fresh perspective of optimizing gradient updates. The proposed AMMNet introduces two core modules: a cross-modal modulation enabling the interdependence of gradient flows between modalities, and a customized <b>adversarial</b> <b>training</b> scheme leveraging dynamic gradient competition. Specifically, the cross-modal modulation adaptively re-calibrates the features to better excite representation potentials from each single modality. The <b>adversarial</b> <b>training</b> employs a minimax game of evolving gradients, with customized guidance to strengthen the generator&rsquo;s perception of visual fidelity from both geometric completeness and semantic correctness. Extensive experimental results demonstrate that AMMNet outperforms state-of-the-art SSC methods by a large margin, providing a promising direction for improving the effectiveness and generalization of SSC methods.</p></p class="citation"></blockquote><h3 id=6288--110323-sparselif-high-performance-sparse-lidar-camera-fusion-for-3d-object-detection-hongcheng-zhang-et-al-2024>(62/88 | 110/323) SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection (Hongcheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongcheng Zhang, Liu Liang, Pengxin Zeng, Xiao Song, Zhe Wang. (2024)<br><strong>SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection</strong><br><button class=copy-to-clipboard title="SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07284v1.pdf filename=2403.07284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse 3D detectors have received significant attention since the query-based paradigm embraces low latency without explicit dense BEV feature construction. However, these detectors achieve worse performance than their dense counterparts. In this paper, we find the key to bridging the performance gap is to enhance the awareness of rich representations in two modalities. Here, we present a high-performance fully sparse detector for end-to-end multi-modality 3D <b>object</b> <b>detection.</b> The detector, termed SparseLIF, contains three key designs, which are (1) Perspective-Aware Query Generation (PAQG) to generate high-quality 3D queries with perspective priors, (2) RoI-Aware Sampling (RIAS) to further refine prior queries by sampling RoI features from each modality, (3) Uncertainty-Aware Fusion (UAF) to precisely quantify the uncertainty of each sensor modality and adaptively conduct final multi-modality fusion, thus achieving great robustness against sensor noises. By the time of submission (2024/03/08), SparseLIF achieves state-of-the-art performance on the nuScenes dataset, ranking 1st on both validation set and test <b>benchmark,</b> outperforming all state-of-the-art 3D <b>object</b> <b>detectors</b> by a notable margin. The source code will be released upon acceptance.</p></p class="citation"></blockquote><h3 id=6388--111323-learning-data-association-for-multi-object-tracking-using-only-coordinates-mehdi-miah-et-al-2024>(63/88 | 111/323) Learning Data Association for Multi-Object Tracking using Only Coordinates (Mehdi Miah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehdi Miah, Guillaume-Alexandre Bilodeau, Nicolas Saunier. (2024)<br><strong>Learning Data Association for Multi-Object Tracking using Only Coordinates</strong><br><button class=copy-to-clipboard title="Learning Data Association for Multi-Object Tracking using Only Coordinates" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08018v1.pdf filename=2403.08018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel <b>Transformer-based</b> module to address the data association problem for multi-object tracking. From detections obtained by a pretrained detector, this module uses only coordinates from bounding boxes to estimate an affinity score between pairs of tracks extracted from two distinct temporal windows. This module, named TWiX, is trained on sets of tracks with the objective of discriminating pairs of tracks coming from the same object from those which are not. Our module does not use the intersection over union measure, nor does it requires any motion priors or any camera motion compensation technique. By inserting TWiX within an online cascade matching pipeline, our tracker C-TWiX achieves state-of-the-art performance on the DanceTrack and KITTIMOT datasets, and gets competitive results on the MOT17 dataset. The code will be made available upon publication.</p></p class="citation"></blockquote><h3 id=6488--112323-red-teaming-models-for-hyperspectral-image-analysis-using-explainable-ai-vladimir-zaigrajew-et-al-2024>(64/88 | 112/323) Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI (Vladimir Zaigrajew et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladimir Zaigrajew, Hubert Baniecki, Lukasz Tulczyjew, Agata M. Wijata, Jakub Nalepa, Nicolas Longépé, Przemyslaw Biecek. (2024)<br><strong>Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI</strong><br><button class=copy-to-clipboard title="Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08017v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08017v2.pdf filename=2403.08017v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote sensing (RS) applications in the space domain demand machine learning (ML) models that are reliable, robust, and quality-assured, making red teaming a vital approach for identifying and exposing potential flaws and biases. Since both fields advance independently, there is a notable gap in integrating red teaming strategies into RS. This paper introduces a methodology for examining ML models operating on hyperspectral images within the HYPERVIEW challenge, focusing on soil parameters&rsquo; estimation. We use post-hoc explanation methods from the <b>Explainable</b> <b>AI</b> (XAI) domain to critically assess the best performing model that won the HYPERVIEW challenge and served as an inspiration for the model deployed on board the INTUITION-1 hyperspectral mission. Our approach effectively red teams the model by pinpointing and validating key shortcomings, constructing a model that achieves comparable performance using just 1% of the input features and a mere up to 5% performance loss. Additionally, we propose a novel way of visualizing explanations that integrate domain-specific information about hyperspectral bands (wavelengths) and data transformations to better suit interpreting models for hyperspectral image analysis.</p></p class="citation"></blockquote><h3 id=6588--113323-semcity-semantic-scene-generation-with-triplane-diffusion-jumin-lee-et-al-2024>(65/88 | 113/323) SemCity: Semantic Scene Generation with Triplane Diffusion (Jumin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, Sung-Eui Yoon. (2024)<br><strong>SemCity: Semantic Scene Generation with Triplane Diffusion</strong><br><button class=copy-to-clipboard title="SemCity: Semantic Scene Generation with Triplane Diffusion" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07773v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07773v3.pdf filename=2403.07773v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present &ldquo;SemCity,&rdquo; a 3D <b>diffusion</b> <b>model</b> for semantic scene generation in real-world outdoor environments. Most 3D <b>diffusion</b> <b>models</b> focus on generating a single object, synthetic indoor scenes, or synthetic outdoor scenes, while the generation of real-world outdoor scenes is rarely addressed. In this paper, we concentrate on generating a real-outdoor scene through learning a <b>diffusion</b> <b>model</b> on a real-world outdoor dataset. In contrast to synthetic data, real-outdoor datasets often contain more empty spaces due to sensor limitations, causing challenges in learning real-outdoor distributions. To address this issue, we exploit a triplane representation as a proxy form of scene distributions to be learned by our <b>diffusion</b> <b>model.</b> Furthermore, we propose a triplane manipulation that integrates seamlessly with our triplane <b>diffusion</b> <b>model.</b> The manipulation improves our <b>diffusion</b> <b>model&rsquo;s</b> applicability in a variety of downstream tasks related to outdoor scene generation such as scene inpainting, scene outpainting, and semantic scene completion refinements. In experimental results, we demonstrate that our triplane <b>diffusion</b> <b>model</b> shows meaningful generation results compared with existing work in a real-outdoor dataset, SemanticKITTI. We also show our triplane manipulation facilitates seamlessly adding, removing, or modifying objects within a scene. Further, it also enables the expansion of scenes toward a city-level scale. Finally, we evaluate our method on semantic scene completion refinements where our <b>diffusion</b> <b>model</b> enhances predictions of semantic scene completion networks by learning scene distribution. Our code is available at <a href=https://github.com/zoomin-lee/SemCity>https://github.com/zoomin-lee/SemCity</a>.</p></p class="citation"></blockquote><h3 id=6688--114323-robust-synthetic-to-real-transfer-for-stereo-matching-jiawei-zhang-et-al-2024>(66/88 | 114/323) Robust Synthetic-to-Real Transfer for Stereo Matching (Jiawei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Zhang, Jiahe Li, Lei Huang, Xiaohan Yu, Lin Gu, Jin Zheng, Xiao Bai. (2024)<br><strong>Robust Synthetic-to-Real Transfer for Stereo Matching</strong><br><button class=copy-to-clipboard title="Robust Synthetic-to-Real Transfer for Stereo Matching" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07705v1.pdf filename=2403.07705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With advancements in domain generalized stereo matching networks, models pre-trained on synthetic data demonstrate strong robustness to unseen domains. However, few studies have investigated the robustness after <b>fine-tuning</b> them in real-world scenarios, during which the domain generalization ability can be seriously degraded. In this paper, we explore <b>fine-tuning</b> stereo matching networks without compromising their robustness to unseen domains. Our motivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for <b>fine-tuning:</b> GT degrades, but PL preserves the domain generalization ability. Empirically, we find the difference between GT and PL implies valuable information that can regularize networks during <b>fine-tuning.</b> We also propose a framework to utilize this difference for <b>fine-tuning,</b> consisting of a frozen Teacher, an exponential moving average (EMA) Teacher, and a Student network. The core idea is to utilize the EMA Teacher to measure what the Student has learned and dynamically improve GT and PL for <b>fine-tuning.</b> We integrate our framework with state-of-the-art networks and evaluate its effectiveness on several real-world datasets. Extensive experiments show that our method effectively preserves the domain generalization ability during <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=6788--115323-masked-autodecoder-is-effective-multi-task-vision-generalist-han-qiu-et-al-2024>(67/88 | 115/323) Masked AutoDecoder is Effective Multi-Task Vision Generalist (Han Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Qiu, Jiaxing Huang, Peng Gao, Lewei Lu, Xiaoqin Zhang, Shijian Lu. (2024)<br><strong>Masked AutoDecoder is Effective Multi-Task Vision Generalist</strong><br><button class=copy-to-clipboard title="Masked AutoDecoder is Effective Multi-Task Vision Generalist" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07692v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07692v2.pdf filename=2403.07692v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by the success of general-purpose models in NLP, recent studies attempt to unify different vision tasks in the same sequence format and employ autoregressive <b>Transformers</b> for sequence prediction. They apply uni-directional attention to capture sequential dependencies and generate task sequences recursively. However, such autoregressive <b>Transformers</b> may not fit vision tasks well, as vision task sequences usually lack the sequential dependencies typically observed in natural languages. In this work, we design Masked AutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of two core designs. First, we develop a parallel decoding framework that introduces bi-directional attention to capture contextual dependencies comprehensively and decode vision task sequences in parallel. Second, we design a masked sequence modeling approach that learns rich task contexts by masking and reconstructing task sequences. In this way, MAD handles all the tasks by a single network branch and a simple cross-entropy loss with minimal task-specific designs. Extensive experiments demonstrate the great potential of MAD as a new paradigm for unifying various vision tasks. MAD achieves superior performance and inference efficiency compared to autoregressive counterparts while obtaining competitive accuracy with task-specific models. Code will be released.</p></p class="citation"></blockquote><h3 id=6888--116323-smartphone-region-wise-image-indoor-localization-using-deep-learning-for-indoor-tourist-attraction-gabriel-toshio-hirokawa-higa-et-al-2024>(68/88 | 116/323) Smartphone region-wise image indoor localization using deep learning for indoor tourist attraction (Gabriel Toshio Hirokawa Higa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel Toshio Hirokawa Higa, Rodrigo Stuqui Monzani, Jorge Fernando da Silva Cecatto, Maria Fernanda Balestieri Mariano de Souza, Vanessa Aparecida de Moraes Weber, Hemerson Pistori, Edson Takashi Matsubara. (2024)<br><strong>Smartphone region-wise image indoor localization using deep learning for indoor tourist attraction</strong><br><button class=copy-to-clipboard title="Smartphone region-wise image indoor localization using deep learning for indoor tourist attraction" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07621v1.pdf filename=2403.07621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart indoor tourist attractions, such as smart museums and aquariums, usually require a significant investment in indoor localization devices. The smartphone Global Positional Systems use is unsuitable for scenarios where dense materials such as concrete and metal block weaken the GPS signals, which is the most common scenario in an indoor tourist attraction. Deep learning makes it possible to perform region-wise indoor localization using smartphone images. This approach does not require any investment in infrastructure, reducing the cost and time to turn museums and aquariums into smart museums or smart aquariums. This paper proposes using deep learning algorithms to classify locations using smartphone camera images for indoor tourism attractions. We evaluate our proposal in a real-world scenario in Brazil. We extensively collect images from ten different smartphones to classify biome-themed fish tanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We tested seven state-of-the-art neural networks, three being <b>transformer-based,</b> achieving precision around 90% on average and recall and f-score around 89% on average. The results indicate good feasibility of the proposal in a most indoor tourist attractions.</p></p class="citation"></blockquote><h3 id=6988--117323-mondrian-on-device-high-performance-video-analytics-with-compressive-packed-inference-changmin-jeon-et-al-2024>(69/88 | 117/323) Mondrian: On-Device High-Performance Video Analytics with Compressive Packed Inference (Changmin Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changmin Jeon, Seonjun Kim, Juheon Yi, Youngki Lee. (2024)<br><strong>Mondrian: On-Device High-Performance Video Analytics with Compressive Packed Inference</strong><br><button class=copy-to-clipboard title="Mondrian: On-Device High-Performance Video Analytics with Compressive Packed Inference" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07598v1.pdf filename=2403.07598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present Mondrian, an edge system that enables high-performance <b>object</b> <b>detection</b> on high-resolution video streams. Many lightweight models and system optimization techniques have been proposed for resource-constrained devices, but they do not fully utilize the potential of the accelerators over dynamic, high-resolution videos. To enable such capability, we devise a novel Compressive Packed Inference to minimize per-pixel processing costs by selectively determining the necessary pixels to process and combining them to maximize processing parallelism. In particular, our system quickly extracts ROIs and dynamically shrinks them, reflecting the effect of the fast-changing characteristics of <b>objects</b> <b>and</b> scenes. It then intelligently combines such scaled ROIs into large canvases to maximize the utilization of inference accelerators such as GPU. Evaluation across various datasets, models, and devices shows Mondrian outperforms state-of-the-art baselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by 15.0-19.7% higher accuracy, leading to $\times$6.65 higher throughput than frame-wise inference for processing various 1080p video streams. We will release the code after the paper review.</p></p class="citation"></blockquote><h3 id=7088--118323-lab-gatr-geometric-algebra-transformers-for-large-biomedical-surface-and-volume-meshes-julian-suk-et-al-2024>(70/88 | 118/323) LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes (Julian Suk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Suk, Baris Imre, Jelmer M. Wolterink. (2024)<br><strong>LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes</strong><br><button class=copy-to-clipboard title="LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07536v1.pdf filename=2403.07536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra <b>transformer</b> (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three tasks in cardiovascular hemodynamics modelling and neurodevelopmental phenotype prediction, featuring meshes of up to 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful architecture for learning with high-fidelity meshes which has the potential to enable interesting downstream applications. Our implementation is publicly available.</p></p class="citation"></blockquote><h3 id=7188--119323-uncertainty-guided-contrastive-learning-for-single-source-domain-generalisation-anastasios-arsenos-et-al-2024>(71/88 | 119/323) Uncertainty-guided Contrastive Learning for Single Source Domain Generalisation (Anastasios Arsenos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anastasios Arsenos, Dimitrios Kollias, Evangelos Petrongonas, Christos Skliros, Stefanos Kollias. (2024)<br><strong>Uncertainty-guided Contrastive Learning for Single Source Domain Generalisation</strong><br><button class=copy-to-clipboard title="Uncertainty-guided Contrastive Learning for Single Source Domain Generalisation" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07514v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07514v2.pdf filename=2403.07514v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of single domain generalisation, the objective is for models that have been exclusively trained on data from a single domain to demonstrate strong performance when confronted with various unfamiliar domains. In this paper, we introduce a novel model referred to as <b>Contrastive</b> <b>Uncertainty</b> Domain Generalisation Network (CUDGNet). The key idea is to augment the source capacity in both input and label spaces through the fictitious domain generator and jointly learn the domain invariant representation of each class through <b>contrastive</b> <b>learning.</b> Extensive experiments on two Single Source Domain Generalisation (SSDG) datasets demonstrate the effectiveness of our approach, which surpasses the state-of-the-art single-DG methods by up to $7.08%$. Our method also provides efficient uncertainty estimation at inference time from a single forward pass through the generator subnetwork.</p></p class="citation"></blockquote><h3 id=7288--120323-nighthaze-nighttime-image-dehazing-via-self-prior-learning-beibei-lin-et-al-2024>(72/88 | 120/323) NightHaze: Nighttime Image Dehazing via Self-Prior Learning (Beibei Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beibei Lin, Yeying Jin, Wending Yan, Wei Ye, Yuan Yuan, Robby T. Tan. (2024)<br><strong>NightHaze: Nighttime Image Dehazing via Self-Prior Learning</strong><br><button class=copy-to-clipboard title="NightHaze: Nighttime Image Dehazing via Self-Prior Learning" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07408v1.pdf filename=2403.07408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Masked <b>autoencoder</b> (MAE) shows that severe augmentation during training produces robust representations for high-level tasks. This paper brings the MAE-like framework to nighttime image enhancement, demonstrating that severe augmentation during training produces strong network priors that are resilient to real-world night haze degradations. We propose a novel nighttime image dehazing method with self-prior learning. Our main novelty lies in the design of severe augmentation, which allows our model to learn robust priors. Unlike MAE that uses masking, we leverage two key challenging factors of nighttime images as augmentation: light effects and noise. During training, we intentionally degrade clear images by blending them with light effects as well as by adding noise, and subsequently restore the clear images. This enables our model to learn clear background priors. By increasing the noise values to approach as high as the pixel intensity values of the glow and light effect blended images, our augmentation becomes severe, resulting in stronger priors. While our self-prior learning is considerably effective in suppressing glow and revealing details of background scenes, in some cases, there are still some undesired artifacts that remain, particularly in the forms of over-suppression. To address these artifacts, we propose a self-refinement module based on the semi-supervised teacher-student framework. Our NightHaze, especially our MAE-like self-prior learning, shows that models trained with severe augmentation effectively improve the visibility of input haze images, approaching the clarity of clear nighttime images. Extensive experiments demonstrate that our NightHaze achieves state-of-the-art performance, outperforming existing nighttime image dehazing methods by a substantial margin of 15.5% for MUSIQ and 23.5% for ClipIQA.</p></p class="citation"></blockquote><h3 id=7388--121323-fetril-feature-translation-for-exemplar-free-class-incremental-learning-with-hill-climbing-eduard-hogea-et-al-2024>(73/88 | 121/323) FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental Learning with Hill-Climbing (Eduard Hogea et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eduard Hogea, Adrian Popescu, Darian Onchis, Grégoire Petit. (2024)<br><strong>FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental Learning with Hill-Climbing</strong><br><button class=copy-to-clipboard title="FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental Learning with Hill-Climbing" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07406v1.pdf filename=2403.07406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exemplar-free class-incremental learning (EFCIL) poses significant challenges, primarily due to catastrophic forgetting, necessitating a delicate balance between stability and plasticity to accurately recognize both new and previous classes. Traditional EFCIL approaches typically skew towards either model plasticity through successive <b>fine-tuning</b> or stability by employing a fixed feature extractor beyond the initial incremental state. Building upon the foundational FeTrIL framework, our research extends into novel experimental domains to examine the efficacy of various oversampling techniques and dynamic optimization strategies across multiple challenging datasets and incremental settings. We specifically explore how oversampling impacts accuracy relative to feature availability and how different optimization methodologies, including dynamic recalibration and feature pool diversification, influence incremental learning outcomes. The results from these comprehensive experiments, conducted on CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior performance of FeTrIL in balancing accuracy for both new and past classes against ten contemporary methods. Notably, our extensions reveal the nuanced impacts of oversampling and optimization on EFCIL, contributing to a more refined understanding of feature-space manipulation for class incremental learning. FeTrIL and its extended analysis in this paper FeTrIL++ pave the way for more adaptable and efficient EFCIL methodologies, promising significant improvements in handling catastrophic forgetting without the need for exemplars.</p></p class="citation"></blockquote><h3 id=7488--122323-time-efficient-and-identity-consistent-virtual-try-on-using-a-variant-of-altered-diffusion-models-phuong-dam-et-al-2024>(74/88 | 122/323) Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of Altered Diffusion Models (Phuong Dam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phuong Dam, Jihoon Jeong, Anh Tran, Daeyoung Kim. (2024)<br><strong>Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of Altered Diffusion Models</strong><br><button class=copy-to-clipboard title="Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of Altered Diffusion Models" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07371v1.pdf filename=2403.07371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study discusses the critical issues of Virtual Try-On in contemporary e-commerce and the prospective metaverse, emphasizing the challenges of preserving intricate texture details and distinctive features of the target person and the clothes in various scenarios, such as clothing texture and identity characteristics like tattoos or accessories. In addition to the fidelity of the synthesized images, the efficiency of the synthesis process presents a significant hurdle. Various existing approaches are explored, highlighting the limitations and unresolved aspects, e.g., identity information omission, uncontrollable artifacts, and low synthesis speed. It then proposes a novel <b>diffusion-based</b> <b>solution</b> that addresses garment texture preservation and user identity retention during virtual try-on. The proposed network comprises two primary modules - a warping module aligning clothing with individual features and a try-on module refining the attire and generating missing parts integrated with a mask-aware post-processing technique ensuring the integrity of the individual&rsquo;s identity. It demonstrates impressive results, surpassing the state-of-the-art in speed by nearly 20 times during inference, with superior fidelity in qualitative assessments. Quantitative evaluations confirm comparable performance with the recent SOTA method on the VITON-HD and Dresscode datasets.</p></p class="citation"></blockquote><h3 id=7588--123323-entropy-is-not-enough-for-test-time-adaptation-from-the-perspective-of-disentangled-factors-jonghyun-lee-et-al-2024>(75/88 | 123/323) Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors (Jonghyun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonghyun Lee, Dahuin Jung, Saehyung Lee, Junsung Park, Juhyeon Shin, Uiwon Hwang, Sungroh Yoon. (2024)<br><strong>Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors</strong><br><button class=copy-to-clipboard title="Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07366v1.pdf filename=2403.07366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-time adaptation (TTA) <b>fine-tunes</b> pre-trained deep neural networks for unseen test data. The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation. To mitigate it, TTA methods have utilized the model output&rsquo;s entropy as a confidence metric that aims to determine which samples have a lower likelihood of causing error. Through experimental studies, however, we observed the unreliability of entropy as a confidence metric for TTA under biased scenarios and theoretically revealed that it stems from the neglect of the influence of latent disentangled factors of data on predictions. Building upon these findings, we introduce a novel TTA method named Destroy Your Object (DeYO), which leverages a newly proposed confidence metric named Pseudo-Label Probability Difference (PLPD). PLPD quantifies the influence of the shape of an object on prediction by measuring the difference between predictions before and after applying an object-destructive transformation. DeYO consists of sample selection and sample weighting, which employ entropy and PLPD concurrently. For robust adaptation, DeYO prioritizes samples that dominantly incorporate shape information when making predictions. Our extensive experiments demonstrate the consistent superiority of DeYO over baseline methods across various scenarios, including biased and wild. Project page is publicly available at <a href=https://whitesnowdrop.github.io/DeYO/>https://whitesnowdrop.github.io/DeYO/</a>.</p></p class="citation"></blockquote><h3 id=7688--124323-efficient-diffusion-model-for-image-restoration-by-residual-shifting-zongsheng-yue-et-al-2024>(76/88 | 124/323) Efficient Diffusion Model for Image Restoration by Residual Shifting (Zongsheng Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zongsheng Yue, Jianyi Wang, Chen Change Loy. (2024)<br><strong>Efficient Diffusion Model for Image Restoration by Residual Shifting</strong><br><button class=copy-to-clipboard title="Efficient Diffusion Model for Image Restoration by Residual Shifting" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-4, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07319v1.pdf filename=2403.07319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>diffusion-based</b> <b>image</b> restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient <b>diffusion</b> <b>model</b> for IR that significantly reduces the required number of <b>diffusion</b> <b>steps.</b> Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the <b>diffusion</b> <b>process.</b> Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \textit{\textbf{even only with four sampling steps}}. Our code and model are publicly available at \url{https://github.com/zsyOAOA/ResShift}.</p></p class="citation"></blockquote><h3 id=7788--125323-rediscovering-bce-loss-for-uniform-classification-qiufu-li-et-al-2024>(77/88 | 125/323) Rediscovering BCE Loss for Uniform Classification (Qiufu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiufu Li, Xi Jia, Jiancan Zhou, Linlin Shen, Jinming Duan. (2024)<br><strong>Rediscovering BCE Loss for Uniform Classification</strong><br><button class=copy-to-clipboard title="Rediscovering BCE Loss for Uniform Classification" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07289v1.pdf filename=2403.07289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the concept of uniform classification, which employs a unified threshold to classify all samples rather than adaptive threshold classifying each individual sample. We also propose the uniform classification accuracy as a metric to measure the model&rsquo;s performance in uniform classification. Furthermore, begin with a naive loss, we mathematically derive a loss function suitable for the uniform classification, which is the BCE function integrated with a unified bias. We demonstrate the unified threshold could be learned via the bias. The extensive experiments on six classification datasets and three feature extraction models show that, compared to the SoftMax loss, the models trained with the BCE loss not only exhibit higher uniform classification accuracy but also higher sample-wise classification accuracy. In addition, the learned bias from BCE loss is very close to the unified threshold used in the uniform classification. The features extracted by the models trained with BCE loss not only possess uniformity but also demonstrate better intra-class compactness and inter-class distinctiveness, yielding superior performance on open-set tasks such as <b>face</b> <b>recognition.</b></p></p class="citation"></blockquote><h3 id=7888--126323-a-multimodal-intermediate-fusion-network-with-manifold-learning-for-stress-detection-morteza-bodaghi-et-al-2024>(78/88 | 126/323) A Multimodal Intermediate Fusion Network with Manifold Learning for Stress Detection (Morteza Bodaghi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Morteza Bodaghi, Majid Hosseini, Raju Gottumukkala. (2024)<br><strong>A Multimodal Intermediate Fusion Network with Manifold Learning for Stress Detection</strong><br><button class=copy-to-clipboard title="A Multimodal Intermediate Fusion Network with Manifold Learning for Stress Detection" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08077v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08077v1.pdf filename=2403.08077v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> deep learning methods capture synergistic features from multiple modalities and have the potential to improve accuracy for stress detection compared to unimodal methods. However, this accuracy gain typically comes from high computational cost due to the high-dimensional feature spaces, especially for intermediate fusion. Dimensionality reduction is one way to optimize <b>multimodal</b> learning by simplifying data and making the features more amenable to processing and analysis, thereby reducing computational complexity. This paper introduces an intermediate <b>multimodal</b> fusion network with manifold learning-based dimensionality reduction. The <b>multimodal</b> network generates independent representations from biometric signals and facial landmarks through 1D-CNN and 2D-CNN. Finally, these features are fused and fed to another 1D-CNN layer, followed by a fully connected dense layer. We compared various dimensionality reduction techniques for different variations of unimodal and <b>multimodal</b> networks. We observe that the intermediate-level fusion with the Multi-Dimensional Scaling (MDS) manifold method showed promising results with an accuracy of 96.00% in a Leave-One-Subject-Out Cross-Validation (LOSO-CV) paradigm over other dimensional reduction methods. MDS had the highest computational cost among manifold learning methods. However, while outperforming other networks, it managed to reduce the computational cost of the proposed networks by 25% when compared to six well-known conventional feature selection methods used in the preprocessing step.</p></p class="citation"></blockquote><h3 id=7988--127323-indicstr12-a-dataset-for-indic-scene-text-recognition-harsh-lunia-et-al-2024>(79/88 | 127/323) IndicSTR12: A Dataset for Indic Scene Text Recognition (Harsh Lunia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Lunia, Ajoy Mondal, C V Jawahar. (2024)<br><strong>IndicSTR12: A Dataset for Indic Scene Text Recognition</strong><br><button class=copy-to-clipboard title="IndicSTR12: A Dataset for Indic Scene Text Recognition" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08007v1.pdf filename=2403.08007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The importance of Scene Text Recognition (STR) in today&rsquo;s increasingly digital world cannot be overstated. Given the significance of STR, data intensive deep learning approaches that auto-learn feature mappings have primarily driven the development of STR solutions. Several <b>benchmark</b> datasets and substantial work on deep learning models are available for Latin languages to meet this need. On more complex, syntactically and semantically, Indian languages spoken and read by 1.3 billion people, there is less work and datasets available. This paper aims to address the Indian space&rsquo;s lack of a comprehensive dataset by proposing the largest and most comprehensive real dataset - IndicSTR12 - and <b>benchmarking</b> STR performance on 12 major Indian languages. A few works have addressed the same issue, but to the best of our knowledge, they focused on a small number of Indian languages. The size and complexity of the proposed dataset are comparable to those of existing Latin contemporaries, while its multilingualism will catalyse the development of robust text detection and recognition models. It was created specifically for a group of related languages with different scripts. The dataset contains over 27000 word-images gathered from various natural scenes, with over 1000 word-images for each language. Unlike previous datasets, the images cover a broader range of realistic conditions, including blur, illumination changes, occlusion, non-iconic texts, low resolution, perspective text etc. Along with the new dataset, we provide a high-performing baseline on three models - PARSeq, CRNN, and STARNet.</p></p class="citation"></blockquote><h3 id=8088--128323-bring-event-into-rgb-and-lidar-hierarchical-visual-motion-fusion-for-scene-flow-hanyu-zhou-et-al-2024>(80/88 | 128/323) Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow (Hanyu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanyu Zhou, Yi Chang, Zhiwei Shi, Luxin Yan. (2024)<br><strong>Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow</strong><br><button class=copy-to-clipboard title="Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07432v1.pdf filename=2403.07432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single RGB or LiDAR is the mainstream sensor for the challenging scene flow, which relies heavily on visual features to match motion features. Compared with single modality, existing methods adopt a fusion strategy to directly fuse the cross-modal complementary knowledge in motion space. However, these direct fusion methods may suffer the modality gap due to the visual intrinsic heterogeneous nature between RGB and LiDAR, thus deteriorating motion features. We discover that event has the homogeneous nature with RGB and LiDAR in both visual and motion spaces. In this work, we bring the event as a bridge between RGB and LiDAR, and propose a novel hierarchical visual-motion fusion framework for scene flow, which explores a homogeneous space to fuse the cross-modal complementary knowledge for physical interpretation. In visual fusion, we discover that event has a complementarity (relative v.s. absolute) in luminance space with RGB for high dynamic imaging, and has a complementarity (local boundary v.s. global shape) in scene structure space with LiDAR for structure integrity. In motion fusion, we figure out that RGB, event and LiDAR are complementary (spatial-dense, temporal-dense v.s. spatiotemporal-sparse) to each other in correlation space, which motivates us to fuse their motion correlations for motion continuity. The proposed hierarchical fusion can explicitly fuse the <b>multimodal</b> knowledge to progressively improve scene flow from visual space to motion space. Extensive experiments have been performed to verify the superiority of the proposed method.</p></p class="citation"></blockquote><h3 id=8188--129323-frequency-decoupling-for-motion-magnification-via-multi-level-isomorphic-architecture-fei-wang-et-al-2024>(81/88 | 129/323) Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture (Fei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Wang, Dan Guo, Kun Li, Zhun Zhong, Meng Wang. (2024)<br><strong>Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture</strong><br><button class=copy-to-clipboard title="Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07347v1.pdf filename=2403.07347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video Motion Magnification (VMM) aims to reveal subtle and imperceptible motion information of objects in the macroscopic world. Prior methods directly model the motion field from the Eulerian perspective by <b>Representation</b> <b>Learning</b> that separates shape and texture or Multi-domain Learning from phase fluctuations. Inspired by the frequency spectrum, we observe that the low-frequency components with stable energy always possess spatial structure and less noise, making them suitable for modeling the subtle motion field. To this end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion Magnification with a Multi-level Isomorphic Architecture to capture multi-level high-frequency details and a stable low-frequency structure (motion field) in video space. Since high-frequency details and subtle motions are susceptible to information degradation due to their inherent subtlety and unavoidable external interference from noise, we carefully design Sparse High/Low-pass Filters to enhance the integrity of details and motion structures, and a Sparse Frequency Mixer to promote seamless recoupling. Besides, we innovatively design a contrastive regularization for this task to strengthen the model&rsquo;s ability to discriminate irrelevant features, reducing undesired motion magnification. Extensive experiments on both Real-world and Synthetic Datasets show that our FD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\times$ and boosts inference speed by 1.68$\times$ than the latest method. Our code is available at <a href=https://github.com/Jiafei127/FD4MM>https://github.com/Jiafei127/FD4MM</a>.</p></p class="citation"></blockquote><h3 id=8288--130323-mrc-net-6-dof-pose-estimation-with-multiscale-residual-correlation-yuelong-li-et-al-2024>(82/88 | 130/323) MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation (Yuelong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuelong Li, Yafei Mao, Raja Bala, Sunil Hadap. (2024)<br><strong>MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation</strong><br><button class=copy-to-clipboard title="MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08019v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08019v2.pdf filename=2403.08019v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a single-shot approach to determining 6-DoF pose of an object with available 3D computer-aided design (CAD) model from a single RGB image. Our method, dubbed MRC-Net, comprises two stages. The first performs pose classification and renders the 3D object in the classified pose. The second stage performs regression to predict fine-grained residual pose within class. Connecting the two stages is a novel multi-scale residual correlation (MRC) layer that captures high-and-low level correspondences between the input image and rendering from first stage. MRC-Net employs a Siamese network with shared weights between both stages to learn embeddings for input and rendered images. To mitigate ambiguity when predicting discrete pose class labels on symmetric objects, we use soft probabilistic labels to define pose class in the first stage. We demonstrate state-of-the-art accuracy, outperforming all competing RGB-based methods on four challenging BOP <b>benchmark</b> datasets: T-LESS, LM-O, YCB-V, and ITODD. Our method is non-iterative and requires no complex post-processing.</p></p class="citation"></blockquote><h3 id=8388--131323-dseg-lime---improving-image-explanation-by-hierarchical-data-driven-segmentation-patrick-knab-et-al-2024>(83/88 | 131/323) DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven Segmentation (Patrick Knab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Knab, Sascha Marton, Christian Bartelt. (2024)<br><strong>DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven Segmentation</strong><br><button class=copy-to-clipboard title="DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven Segmentation" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07733v1.pdf filename=2403.07733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We <b>benchmark</b> DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented by a qualitative assessment through a user study. Our findings demonstrate that DSEG outperforms in most of the XAI metrics and enhances the alignment of explanations with human-recognized concepts, significantly improving interpretability. The code is available under: https://github. com/patrick-knab/DSEG-LIME</p></p class="citation"></blockquote><h3 id=8488--132323-accurate-spatial-gene-expression-prediction-by-integrating-multi-resolution-features-youngmin-chung-et-al-2024>(84/88 | 132/323) Accurate Spatial Gene Expression Prediction by integrating Multi-resolution features (Youngmin Chung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youngmin Chung, Ji Hun Ha, Kyeong Chan Im, Joo Sang Lee. (2024)<br><strong>Accurate Spatial Gene Expression Prediction by integrating Multi-resolution features</strong><br><button class=copy-to-clipboard title="Accurate Spatial Gene Expression Prediction by integrating Multi-resolution features" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07592v1.pdf filename=2403.07592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in Spatial Transcriptomics (ST) technology have facilitated detailed gene expression analysis within tissue contexts. However, the high costs and methodological limitations of ST necessitate a more robust predictive model. In response, this paper introduces TRIPLEX, a novel deep learning framework designed to predict spatial gene expression from Whole Slide Images (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing cellular morphology at individual spots, the local context around these spots, and the global tissue organization. By integrating these features through an effective fusion strategy, TRIPLEX achieves accurate gene expression prediction. Our comprehensive <b>benchmark</b> study, conducted on three public ST datasets and supplemented with Visium data from 10X Genomics, demonstrates that TRIPLEX outperforms current state-of-the-art models in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC). The model&rsquo;s predictions align closely with ground truth gene expression profiles and tumor annotations, underscoring TRIPLEX&rsquo;s potential in advancing cancer diagnosis and treatment.</p></p class="citation"></blockquote><h3 id=8588--133323-smurf-continuous-dynamics-for-motion-deblurring-radiance-fields-jungho-lee-et-al-2024>(85/88 | 133/323) SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields (Jungho Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee. (2024)<br><strong>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</strong><br><button class=copy-to-clipboard title="SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07547v1.pdf filename=2403.07547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity. However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes. While recent studies have addressed this issue, they do not consider the continuous dynamics of camera movements during image acquisition, leading to inaccurate scene reconstruction. Additionally, these methods are plagued by slow training and rendering speed. To effectively handle these issues, we propose sequential motion understanding radiance fields (SMURF), a novel approach that employs neural ordinary differential equation (Neural-ODE) to model continuous camera motion and leverages the explicit volumetric representation method for faster training and robustness to motion-blurred input images. The core idea of the SMURF is continuous motion blurring kernel (CMBK), a unique module designed to model a continuous camera movements for processing blurry inputs. Our model, rigorously evaluated against <b>benchmark</b> datasets, demonstrates state-of-the-art performance both quantitatively and qualitatively.</p></p class="citation"></blockquote><h3 id=8688--134323-adaptive-fusion-of-single-view-and-multi-view-depth-for-autonomous-driving-junda-cheng-et-al-2024>(86/88 | 134/323) Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving (JunDa Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>JunDa Cheng, Wei Yin, Kaixuan Wang, Xiaozhi Chen, Shijie Wang, Xin Yang. (2024)<br><strong>Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving</strong><br><button class=copy-to-clipboard title="Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07535v1.pdf filename=2403.07535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-view depth estimation has achieved impressive performance over various <b>benchmarks.</b> However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving. In this work, we propose a new robustness <b>benchmark</b> to evaluate the depth estimation system under various noisy pose settings. Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings. To address this challenge, we propose a single-view and multi-view fused depth estimation system, which adaptively integrates high-confident multi-view and single-view results for both robust and accurate depth estimations. The adaptive fusion module performs fusion by dynamically selecting high-confidence regions between two branches based on a wrapping confidence map. Thus, the system tends to choose the more reliable branch when facing textureless scenes, inaccurate calibration, dynamic objects, and other degradation or challenging conditions. Our method outperforms state-of-the-art multi-view and fusion methods under robustness testing. Furthermore, we achieve state-of-the-art performance on challenging <b>benchmarks</b> (KITTI and DDAD) when given accurate pose estimations. Project website: <a href=https://github.com/Junda24/AFNet/>https://github.com/Junda24/AFNet/</a>.</p></p class="citation"></blockquote><h3 id=8788--135323-from-canteen-food-to-daily-meals-generalizing-food-recognition-to-more-practical-scenarios-guoshan-liu-et-al-2024>(87/88 | 135/323) From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios (Guoshan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoshan Liu, Yang Jiao, Jingjing Chen, Bin Zhu, Yu-Gang Jiang. (2024)<br><strong>From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios</strong><br><button class=copy-to-clipboard title="From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07403v1.pdf filename=2403.07403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The precise recognition of food categories plays a pivotal role for intelligent health management, attracting significant research attention in recent years. Prominent <b>benchmarks,</b> such as Food-101 and VIREO Food-172, provide abundant food image resources that catalyze the prosperity of research in this field. Nevertheless, these datasets are well-curated from canteen scenarios and thus deviate from food appearances in daily life. This discrepancy poses great challenges in effectively transferring classifiers trained on these canteen datasets to broader daily-life scenarios encountered by humans. Toward this end, we present two new <b>benchmarks,</b> namely DailyFood-172 and DailyFood-16, specifically designed to curate food images from everyday meals. These two datasets are used to evaluate the transferability of approaches from the well-curated food image domain to the everyday-life food image domain. In addition, we also propose a simple yet effective baseline method named Multi-Cluster Reference Learning (MCRL) to tackle the aforementioned domain gap. MCRL is motivated by the observation that food images in daily-life scenarios exhibit greater intra-class appearance variance compared with those in well-curated <b>benchmarks.</b> Notably, MCRL can be seamlessly coupled with existing approaches, yielding non-trivial performance enhancements. We hope our new <b>benchmarks</b> can inspire the community to explore the transferability of food recognition models trained on well-curated datasets toward practical real-life applications.</p></p class="citation"></blockquote><h3 id=8888--136323-learning-hierarchical-color-guidance-for-depth-map-super-resolution-runmin-cong-et-al-2024>(88/88 | 136/323) Learning Hierarchical Color Guidance for Depth Map Super-Resolution (Runmin Cong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runmin Cong, Ronghui Sheng, Hao Wu, Yulan Guo, Yunchao Wei, Wangmeng Zuo, Yao Zhao, Sam Kwong. (2024)<br><strong>Learning Hierarchical Color Guidance for Depth Map Super-Resolution</strong><br><button class=copy-to-clipboard title="Learning Hierarchical Color Guidance for Depth Map Super-Resolution" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07290v1.pdf filename=2403.07290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Color information is the most commonly used prior knowledge for depth map super-resolution (DSR), which can provide high-frequency boundary guidance for detail restoration. However, its role and functionality in DSR have not been fully developed. In this paper, we rethink the utilization of color information and propose a hierarchical color guidance network to achieve DSR. On the one hand, the low-level detail embedding module is designed to supplement high-frequency color information of depth features in a residual mask manner at the low-level stages. On the other hand, the high-level abstract guidance module is proposed to maintain semantic consistency in the reconstruction process by using a semantic mask that encodes the global guidance information. The color information of these two dimensions plays a role in the front and back ends of the attention-based feature projection (AFP) module in a more comprehensive form. Simultaneously, the AFP module integrates the multi-scale content enhancement block and adaptive attention projection block to make full use of multi-scale information and adaptively project critical restoration information in an attention manner for DSR. Compared with the state-of-the-art methods on four <b>benchmark</b> datasets, our method achieves more competitive performance both qualitatively and quantitatively.</p></p class="citation"></blockquote><h2 id=cslg-55>cs.LG (55)</h2><h3 id=155--137323-knowcoder-coding-structured-knowledge-into-llms-for-universal-information-extraction-zixuan-li-et-al-2024>(1/55 | 137/323) KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction (Zixuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixuan Li, Yutao Zeng, Yuxin Zuo, Weicheng Ren, Wenxuan Liu, Miao Su, Yucan Guo, Yantao Liu, Xiang Li, Zhilei Hu, Long Bai, Wei Li, Yidan Liu, Pan Yang, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng. (2024)<br><strong>KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction</strong><br><button class=copy-to-clipboard title="KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 90<br>Keywords: Few-shot, Low-Resource, Supervised Learning, Zero-shot, Code Generation, Information Retrieval, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07969v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07969v2.pdf filename=2403.07969v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose KnowCoder, a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to conduct Universal <b>Information</b> <b>Extraction</b> (UIE) via <b>code</b> <b>generation.</b> KnowCoder aims to develop a kind of unified schema representation that <b>LLMs</b> can easily understand and an effective learning framework that encourages <b>LLMs</b> to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a <b>code-style</b> <b>schema</b> representation method to uniformly transform different schemas into Python classes, with which complex schema <b>information,</b> <b>such</b> as constraints among tasks in UIE, can be captured in an <b>LLM-friendly</b> manner. We further construct a <b>code-style</b> <b>schema</b> library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of <b>LLMs,</b> KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via <b>code</b> <b>pretraining</b> and its schema following ability via <b>instruction</b> <b>tuning.</b> After <b>code</b> <b>pretraining</b> on around $1.5$B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by $\textbf{49.8%}$ F1, compared to LLaMA2, under the <b>few-shot</b> setting. After <b>instruction</b> <b>tuning,</b> KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to $\textbf{12.5%}$ and $\textbf{21.9%}$, compared to sota baselines, under the <b>zero-shot</b> setting and the low resource setting, respectively. Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to $\textbf{7.5%}$ under the <b>supervised</b> setting.</p></p class="citation"></blockquote><h3 id=255--138323-propml-probability-partial-multi-label-learning-łukasz-struski-et-al-2024>(2/55 | 138/323) ProPML: Probability Partial Multi-label Learning (Łukasz Struski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Łukasz Struski, Adam Pardyl, Jacek Tabor, Bartosz Zieliński. (2024)<br><strong>ProPML: Probability Partial Multi-label Learning</strong><br><button class=copy-to-clipboard title="ProPML: Probability Partial Multi-label Learning" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Disambiguation, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07603v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07603v1.pdf filename=2403.07603v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Partial Multi-label Learning (PML) is a type of <b>weakly</b> <b>supervised</b> <b>learning</b> where each training instance corresponds to a set of candidate labels, among which only some are true. In this paper, we introduce \our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the PML setup. In contrast to existing methods, it does not require suboptimal <b>disambiguation</b> and, as such, can be applied to any deep architecture. Furthermore, experiments conducted on artificial and real-world datasets indicate that \our{} outperforms existing approaches, especially for high noise in a candidate set.</p></p class="citation"></blockquote><h3 id=355--139323-chai-clustered-head-attention-for-efficient-llm-inference-saurabh-agarwal-et-al-2024>(3/55 | 139/323) CHAI: Clustered Head Attention for Efficient LLM Inference (Saurabh Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saurabh Agarwal, Bilge Acun, Basil Homer, Mostafa Elhoushi, Yejin Lee, Shivaram Venkataraman, Dimitris Papailiopoulos, Carole-Jean Wu. (2024)<br><strong>CHAI: Clustered Head Attention for Efficient LLM Inference</strong><br><button class=copy-to-clipboard title="CHAI: Clustered Head Attention for Efficient LLM Inference" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, LLaMA, Large Language Model, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08058v1.pdf filename=2403.08058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of <b>LLMs,</b> which can account for over 50% of <b>LLMs</b> memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for <b>self-attention</b> at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any <b>fine-tuning</b> required. CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, <b>LLAMA-7B,</b> <b>LLAMA-33B)</b> and 5 different evaluation datasets.</p></p class="citation"></blockquote><h3 id=455--140323-chronos-learning-the-language-of-time-series-abdul-fatir-ansari-et-al-2024>(4/55 | 140/323) Chronos: Learning the Language of Time Series (Abdul Fatir Ansari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, Yuyang Wang. (2024)<br><strong>Chronos: Learning the Language of Time Series</strong><br><button class=copy-to-clipboard title="Chronos: Learning the Language of Time Series" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Quantization, Zero-shot, T5, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07815v1.pdf filename=2403.07815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and <b>quantization</b> into a fixed vocabulary and trains existing <b>transformer-based</b> language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the <b>T5</b> family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive <b>benchmark</b> consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior <b>zero-shot</b> performance on new datasets, relative to methods that were trained specifically on them. Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve <b>zero-shot</b> accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines.</p></p class="citation"></blockquote><h3 id=555--141323-efficient-language-model-architectures-for-differentially-private-federated-learning-jae-hun-ro-et-al-2024>(5/55 | 141/323) Efficient Language Model Architectures for Differentially Private Federated Learning (Jae Hun Ro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jae Hun Ro, Srinadh Bhojanapalli, Zheng Xu, Yanxiang Zhang, Ananda Theertha Suresh. (2024)<br><strong>Efficient Language Model Architectures for Differentially Private Federated Learning</strong><br><button class=copy-to-clipboard title="Efficient Language Model Architectures for Differentially Private Federated Learning" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Federated Learning, Stochastic Gradient Descent, Transformer, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08100v1.pdf filename=2403.08100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-device <b>federated</b> <b>learning</b> (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices. <b>SGD</b> is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency. However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance. In light of this, we ask if language models can be modified such that they can be efficiently trained with <b>SGD</b> client optimizers and answer this affirmatively. We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent network by modifying the sigmoid and tanh activations in the recurrent cell and show that this new model converges faster and achieves better utility than the standard CIFG recurrent model in cross-device FL in large scale experiments. We further show that the proposed scale invariant modification also helps in <b>federated</b> <b>learning</b> of larger <b>transformer</b> models. Finally, we demonstrate the scale invariant modification is also compatible with other non-adaptive algorithms. Particularly, our results suggest an improved privacy utility trade-off in <b>federated</b> <b>learning</b> with <b>differential</b> <b>privacy.</b></p></p class="citation"></blockquote><h3 id=655--142323-fairness-feedback-loops-training-on-synthetic-data-amplifies-bias-sierra-wyllie-et-al-2024>(6/55 | 142/323) Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias (Sierra Wyllie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sierra Wyllie, Ilia Shumailov, Nicolas Papernot. (2024)<br><strong>Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias</strong><br><button class=copy-to-clipboard title="Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Fairness, Stochastic Gradient Descent, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07857v1.pdf filename=2403.07857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model-induced <b>distribution</b> <b>shifts</b> (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for <b>supervised</b> models. When a model induces a <b>distribution</b> <b>shift,</b> it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, <b>fairness,</b> and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative training batches for <b>stochastic</b> <b>gradient</b> <b>descent</b> to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS. Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective.</p></p class="citation"></blockquote><h3 id=755--143323-quantifying-and-mitigating-privacy-risks-for-tabular-generative-models-chaoyi-zhu-et-al-2024>(7/55 | 143/323) Quantifying and Mitigating Privacy Risks for Tabular Generative Models (Chaoyi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. Pérez, Marten van Dijk, Lydia Y. Chen. (2024)<br><strong>Quantifying and Mitigating Privacy Risks for Tabular Generative Models</strong><br><button class=copy-to-clipboard title="Quantifying and Mitigating Privacy Risks for Tabular Generative Models" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Diffusion Model, Autoencoder, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07842v1.pdf filename=2403.07842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synthetic data from <b>generative</b> <b>models</b> <b>emerges</b> as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image <b>generative</b> <b>models,</b> <b>ranging</b> from <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> to recent <b>diffusion</b> <b>models.</b> Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular <b>diffusion,</b> <b>we</b> propose DP-TLDM, Differentially Private Tabular Latent <b>Diffusion</b> <b>Model,</b> which is composed of an <b>autoencoder</b> network to encode the tabular data and a latent <b>diffusion</b> <b>model</b> to synthesize the latent tables. Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms. Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data. Specifically, compared to other DP-protected tabular <b>generative</b> <b>models,</b> <b>DP-TLDM</b> improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk.</p></p class="citation"></blockquote><h3 id=855--144323-taming-pre-trained-llms-for-generalised-time-series-forecasting-via-cross-modal-knowledge-distillation-peiyuan-liu-et-al-2024>(8/55 | 144/323) Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation (Peiyuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, Shu-Tao Xia. (2024)<br><strong>Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07300v1.pdf filename=2403.07300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> several works have attempted to introduce <b>LLMs</b> into time series forecasting. Despite promising results, these methods directly take time series as the input to <b>LLMs,</b> ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel <b>Large</b> <b>Language</b> <b>Models</b> and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of <b>LLMs</b> in the time series forecasting challenge. Based on cross-modal <b>knowledge</b> <b>distillation,</b> the proposed method exploits both input-agnostic static <b>knowledge</b> <b>and</b> input-dependent dynamic <b>knowledge</b> <b>in</b> pre-trained <b>LLMs.</b> In this way, it empowers the forecasting model with favorable performance as well as strong generalization abilities. Extensive experiments demonstrate the proposed method establishes a new state of the art for both long- and short-term forecasting. Code is available at \url{https://github.com/Hank0626/LLaTA}.</p></p class="citation"></blockquote><h3 id=955--145323-disentangling-policy-from-offline-task-representation-learning-via-adversarial-data-augmentation-chengxing-jia-et-al-2024>(9/55 | 145/323) Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation (Chengxing Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengxing Jia, Fuxiang Zhang, Yi-Chen Li, Chen-Xiao Gao, Xu-Hui Liu, Lei Yuan, Zongzhang Zhang, Yang Yu. (2024)<br><strong>Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation</strong><br><button class=copy-to-clipboard title="Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 35<br>Keywords: Contrastive Learning, Data Augmentation, Out-of-distribution, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07261v1.pdf filename=2403.07261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task <b>representations</b> <b>that</b> be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task <b>representations</b> <b>is</b> to adopt <b>contrastive</b> <b>learning</b> using multi-task offline <b>data.</b> <b>The</b> dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing <b>data</b> <b>from</b> a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task <b>data</b> <b>collection</b> occurs with a limited number of policies. We observed that learned task <b>representations</b> <b>from</b> previous OMRL methods tend to correlate spuriously with the behavior policy instead of reflecting the essential characteristics of the task, resulting in unfavorable <b>out-of-distribution</b> generalization. To alleviate this issue, we introduce a novel algorithm to disentangle the impact of behavior policy from task <b>representation</b> <b>learning</b> through a process called adversarial <b>data</b> <b>augmentation.</b> Specifically, the objective of adversarial <b>data</b> <b>augmentation</b> is not merely to generate <b>data</b> <b>analogous</b> to offline <b>data</b> <b>distribution;</b> instead, it aims to create adversarial examples designed to confound learned task <b>representations</b> <b>and</b> lead to incorrect task identification. Our experiments show that learning from such adversarial samples significantly enhances the robustness and effectiveness of the task identification process and realizes satisfactory <b>out-of-distribution</b> generalization.</p></p class="citation"></blockquote><h3 id=1055--146323-iterative-graph-neural-network-enhancement-via-frequent-subgraph-mining-of-explanations-harish-g-naik-et-al-2024>(10/55 | 146/323) Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations (Harish G. Naik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harish G. Naik, Jan Polster, Raj Shekhar, Tamás Horváth, György Turán. (2024)<br><strong>Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations</strong><br><button class=copy-to-clipboard title="Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07849v1.pdf filename=2403.07849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We formulate an XAI-based model improvement approach for <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> for <b>node</b> <b>classification,</b> called Explanation Enhanced <b>Graph</b> <b>Learning</b> <b>(EEGL).</b> The goal is to improve predictive performance of <b>GNN</b> using explanations. EEGL is an iterative self-improving algorithm, which starts with a learned &ldquo;vanilla&rdquo; <b>GNN,</b> and repeatedly uses frequent subgraph mining to find relevant patterns in explanation subgraphs. These patterns are then filtered further to obtain application-dependent features corresponding to the presence of certain subgraphs in the <b>node</b> <b>neighborhoods.</b> Giving an application-dependent algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL) algorithm has previously been posed as an open problem. We present experimental evidence, with synthetic and real-world data, which show that EEGL outperforms related approaches in predictive performance and that it has a <b>node-distinguishing</b> <b>power</b> beyond that of vanilla <b>GNNs.</b> We also analyze EEGL&rsquo;s training dynamics.</p></p class="citation"></blockquote><h3 id=1155--147323-conditional-computation-in-neural-networks-principles-and-research-trends-simone-scardapane-et-al-2024>(11/55 | 147/323) Conditional computation in neural networks: principles and research trends (Simone Scardapane et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simone Scardapane, Alessandro Baiocchi, Alessio Devoto, Valerio Marsocci, Pasquale Minervini, Jary Pomponi. (2024)<br><strong>Conditional computation in neural networks: principles and research trends</strong><br><button class=copy-to-clipboard title="Conditional computation in neural networks: principles and research trends" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Convolution, Transfer Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07965v1.pdf filename=2403.07965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article <b>summarizes</b> principles and ideas from the emerging area of applying \textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational <b>graph</b> conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a <b>convolutional</b> filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and <b>transfer</b> <b>learning,</b> with a focus on emerging applicative areas ranging from automated scientific discovery to semantic communication.</p></p class="citation"></blockquote><h3 id=1255--148323-advantage-aware-policy-optimization-for-offline-reinforcement-learning-yunpeng-qing-et-al-2024>(12/55 | 148/323) Advantage-Aware Policy Optimization for Offline Reinforcement Learning (Yunpeng Qing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunpeng Qing, Shunyu liu, Jingyuan Cong, Kaixuan Chen, Yihe Zhou, Mingli Song. (2024)<br><strong>Advantage-Aware Policy Optimization for Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Advantage-Aware Policy Optimization for Offline Reinforcement Learning" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Offline Reinforcement Learning, Out-of-distribution, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07262v1.pdf filename=2403.07262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>Reinforcement</b> <b>Learning</b> (RL) endeavors to leverage <b>offline</b> <b>datasets</b> <b>to</b> craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the <b>Out-Of-Distribution</b> (OOD) problem. However, existing works often suffer from the constraint conflict issue when <b>offline</b> <b>datasets</b> <b>are</b> collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for <b>offline</b> <b>learning</b> <b>under</b> mixed-quality datasets. Specifically, A2PO employs a Conditional Variational Auto-Encoder (CVAE) to disentangle the action distributions of intertwined behavior policies by modeling the advantage values of all training data as conditional variables. Then the agent can follow such disentangled action distribution constraints to optimize the advantage-aware policy towards high advantage values. Extensive experiments conducted on both the single-quality and mixed-quality datasets of the D4RL <b>benchmark</b> demonstrate that A2PO yields results superior to state-of-the-art counterparts. Our code will be made publicly available.</p></p class="citation"></blockquote><h3 id=1355--149323-microt-low-energy-and-adaptive-models-for-mcus-yushan-huang-et-al-2024>(13/55 | 149/323) MicroT: Low-Energy and Adaptive Models for MCUs (Yushan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushan Huang, Ranya Aloufi, Xavier Cadet, Yuchen Zhao, Payam Barnaghi, Hamed Haddadi. (2024)<br><strong>MicroT: Low-Energy and Adaptive Models for MCUs</strong><br><button class=copy-to-clipboard title="MicroT: Low-Energy and Adaptive Models for MCUs" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08040v1.pdf filename=2403.08040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through <b>self-supervised</b> <b>knowledge</b> <b>distillation</b> and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%. On MCUs, compared to the standard full model inference, MicroT can save up to about 29.13% in energy consumption. MicroT also allows users to adaptively adjust the stage-decision ratio as needed, better balancing model performance and energy consumption. Under the standard stage-decision ratio configuration, MicroT can increase accuracy by 5.91% and save about 14.47% of energy consumption.</p></p class="citation"></blockquote><h3 id=1455--150323-maxwells-demon-at-work-efficient-pruning-by-leveraging-saturation-of-neurons-simon-dufort-labbé-et-al-2024>(14/55 | 150/323) Maxwell&rsquo;s Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons (Simon Dufort-Labbé et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Dufort-Labbé, Pierluca D&rsquo;Oro, Evgenii Nikishin, Razvan Pascanu, Pierre-Luc Bacon, Aristide Baratin. (2024)<br><strong>Maxwell&rsquo;s Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons</strong><br><button class=copy-to-clipboard title="Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Continual Learning, Model Compression, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07688v1.pdf filename=2403.07688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in <b>continual</b> <b>learning</b> scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and <b>pruning.</b> By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured <b>pruning</b> algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 and ImageNet datasets demonstrate that DemP surpasses existing structured <b>pruning</b> techniques, showcasing superior accuracy-sparsity tradeoffs and training speedups. These findings suggest a novel perspective on dying neurons as a valuable resource for efficient <b>model</b> <b>compression</b> and optimization.</p></p class="citation"></blockquote><h3 id=1555--151323-efficient-knowledge-deletion-from-trained-models-through-layer-wise-partial-machine-unlearning-vinay-chakravarthi-gogineni-et-al-2024>(15/55 | 151/323) Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning (Vinay Chakravarthi Gogineni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vinay Chakravarthi Gogineni, Esmaeil S. Nadimi. (2024)<br><strong>Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning</strong><br><button class=copy-to-clipboard title="Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Machine Unlearning, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07611v1.pdf filename=2403.07611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>unlearning</b> has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained <b>machine</b> <b>learning</b> model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief <b>fine-tuning</b> post unlearning, and requiring significant storage. In response, this paper introduces a novel class of <b>machine</b> <b>unlearning</b> algorithms. First method is partial amnesiac unlearning, integration of layer-wise <b>pruning</b> with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of data deletion on model efficacy. Through a detailed experimental evaluation, we showcase the effectiveness of proposed unlearning methods. Experimental results highlight that the partial amnesiac unlearning not only preserves model efficacy but also eliminates the necessity for brief post <b>fine-tuning,</b> unlike conventional amnesiac unlearning. Moreover, employing layer-wise partial updates in label-flipping and optimization-based unlearning techniques demonstrates superiority in preserving model efficacy compared to their naive counterparts.</p></p class="citation"></blockquote><h3 id=1655--152323-hallmarks-of-optimization-trajectories-in-neural-networks-and-llms-the-lengths-bends-and-dead-ends-sidak-pal-singh-et-al-2024>(16/55 | 152/323) Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends (Sidak Pal Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sidak Pal Singh, Bobby He, Thomas Hofmann, Bernhard Schölkopf. (2024)<br><strong>Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends</strong><br><button class=copy-to-clipboard title="Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07379v1.pdf filename=2403.07379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich structure of parameters contained within their optimization trajectories. Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which reveal the inherent nuance and interplay involved between various optimization choices, such as momentum, weight decay, and batch size. We use them to provide key hallmarks about the nature of optimization in deep neural networks: when it goes right, and when it finds itself in a dead end. Further, thanks to our trajectory perspective, we uncover an intertwined behaviour of momentum and weight decay that promotes directional exploration, as well as a directional regularization behaviour of some others. We perform experiments over <b>large-scale</b> <b>vision</b> <b>and</b> language settings, including <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with up to 12 billion parameters, to demonstrate the value of our approach.</p></p class="citation"></blockquote><h3 id=1755--153323-abstracting-sparse-dnn-acceleration-via-structured-sparse-tensor-decomposition-geonhwa-jeong-et-al-2024>(17/55 | 153/323) Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition (Geonhwa Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geonhwa Jeong, Po-An Tsai, Abhimanyu R. Bambhaniya, Stephen W. Keckler, Tushar Krishna. (2024)<br><strong>Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition</strong><br><button class=copy-to-clipboard title="Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-AR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07953v1.pdf filename=2403.07953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model <b>fine-tuning.</b> Moreover, any sparse model <b>fine-tuned</b> for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes <b>tensor</b> <b>approximation</b> via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse <b>tensor</b> <b>into</b> a series of structured sparse <b>tensors.</b> <b>Next,</b> we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and activation <b>tensors</b> <b>so</b> that they can be accelerated by any systems with structured sparse hardware support. Evaluation results show that, by exploiting prior structured sparse hardware baselines, our method can accelerate off-the-shelf dense and sparse DNNs without <b>fine-tuning</b> and improves energy-delay-product by up to 83% and 74% on average.</p></p class="citation"></blockquote><h3 id=1855--154323-verification-aided-learning-of-neural-network-barrier-functions-with-termination-guarantees-shaoru-chen-et-al-2024>(18/55 | 154/323) Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees (Shaoru Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoru Chen, Lekan Molu, Mahyar Fazlyab. (2024)<br><strong>Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees</strong><br><button class=copy-to-clipboard title="Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 30<br>Keywords: Fine-tuning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07308v1.pdf filename=2403.07308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Barrier functions are a general framework for establishing a safety guarantee for a system. However, there is no general method for finding these functions. To address this shortcoming, recent approaches use <b>self-supervised</b> <b>learning</b> techniques to learn these functions using training data that are periodically generated by a verification procedure, leading to a verification-aided learning framework. Despite its immense potential in automating barrier function synthesis, the verification-aided learning framework does not have termination guarantees and may suffer from a low success rate of finding a valid barrier function in practice. In this paper, we propose a holistic approach to address these drawbacks. With a convex formulation of the barrier function synthesis, we propose to first learn an empirically well-behaved NN basis function and then apply a <b>fine-tuning</b> algorithm that exploits the convexity and counterexamples from the verification failure to find a valid barrier function with finite-step termination guarantees: if there exist valid barrier functions, the <b>fine-tuning</b> algorithm is guaranteed to find one in a finite number of iterations. We demonstrate that our <b>fine-tuning</b> method can significantly boost the performance of the verification-aided learning framework on examples of different scales and using various neural network verifiers.</p></p class="citation"></blockquote><h3 id=1955--155323-workarena-how-capable-are-web-agents-at-solving-common-knowledge-work-tasks-alexandre-drouin-et-al-2024>(19/55 | 155/323) WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks? (Alexandre Drouin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, Alexandre Lacoste. (2024)<br><strong>WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?</strong><br><button class=copy-to-clipboard title="WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07718v1.pdf filename=2403.07718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the use of <b>large</b> <b>language</b> <b>model-based</b> agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents&rsquo; ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted <b>benchmark</b> of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as <b>multimodal</b> observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source <b>LLMs,</b> highlighting a critical area for future exploration and development in the field.</p></p class="citation"></blockquote><h3 id=2055--156323-early-directional-convergence-in-deep-homogeneous-neural-networks-for-small-initializations-akshay-kumar-et-al-2024>(20/55 | 156/323) Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations (Akshay Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshay Kumar, Jarvis Haupt. (2024)<br><strong>Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations</strong><br><button class=copy-to-clipboard title="Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 25<br>Keywords: Karush-Kuhn-Tucker, Karush-Kuhn-Tucker, Square Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08121v1.pdf filename=2403.08121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations. The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the <b>Karush-Kuhn-Tucker</b> <b>(KKT)</b> points of the neural correlation function introduced in [1]. Additionally, for <b>square</b> <b>loss</b> and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function.</p></p class="citation"></blockquote><h3 id=2155--157323-mechanics-of-next-token-prediction-with-self-attention-yingcong-li-et-al-2024>(21/55 | 157/323) Mechanics of Next Token Prediction with Self-Attention (Yingcong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingcong Li, Yixiao Huang, M. Emrullah Ildiz, Ankit Singh Rawat, Samet Oymak. (2024)<br><strong>Mechanics of Next Token Prediction with Self-Attention</strong><br><button class=copy-to-clipboard title="Mechanics of Next Token Prediction with Self-Attention" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, math-OC<br>Keyword Score: 23<br>Keywords: Graph, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08081v1.pdf filename=2403.08081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the <b>self-attention</b> mechanism. In this work, we ask: $\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$ $\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$ $\textit{next-token}$ $\textit{prediction?}$ We show that training <b>self-attention</b> with gradient descent learns an automaton which generates the next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$ $\textbf{retrieval:}$ Given input sequence, <b>self-attention</b> precisely selects the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It then creates a convex combination of the high-priority tokens from which the next token can be sampled. Under suitable conditions, we rigorously characterize these mechanics through a directed <b>graph</b> over tokens extracted from the training data. We prove that gradient descent implicitly discovers the strongly-connected components (SCC) of this <b>graph</b> and <b>self-attention</b> learns to retrieve the tokens that belong to the highest-priority SCC available in the context window. Our theory relies on decomposing the model weights into a directional component and a finite component that correspond to hard retrieval and soft composition steps respectively. This also formalizes a related implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that these findings shed light on how <b>self-attention</b> processes sequential data and pave the path toward demystifying more complex architectures.</p></p class="citation"></blockquote><h3 id=2255--158323-drivaernet-a-parametric-car-dataset-for-data-driven-aerodynamic-design-and-graph-based-drag-prediction-mohamed-elrefaie-et-al-2024>(22/55 | 158/323) DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction (Mohamed Elrefaie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Elrefaie, Angela Dai, Faez Ahmed. (2024)<br><strong>DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction</strong><br><button class=copy-to-clipboard title="DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 23<br>Keywords: Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08055v1.pdf filename=2403.08055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of 3D industry-standard car shapes, and RegDGCNN, a dynamic <b>graph</b> <b>convolutional</b> <b>neural</b> <b>network</b> model, both aimed at aerodynamic car design through machine learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million surface mesh faces and comprehensive aerodynamic performance data comprising of full 3D pressure, velocity fields, and wall-shear stresses, addresses the critical need for extensive datasets to train deep learning models in engineering applications. It is 60% larger than the previously available largest public dataset of cars, and is the only open-source dataset that also models wheels and underbody. RegDGCNN leverages this large-scale dataset to provide high-precision drag estimates directly from 3D meshes, bypassing traditional limitations such as the need for 2D image rendering or Signed Distance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN facilitates rapid aerodynamic assessments, offering a substantial leap towards integrating data-driven methods in automotive design. Together, DrivAerNet and RegDGCNN promise to accelerate the car design process and contribute to the development of more efficient vehicles. To lay the groundwork for future innovations in the field, the dataset and code used in our study are publicly accessible at \url{https://github.com/Mohamedelrefaie/DrivAerNet}</p></p class="citation"></blockquote><h3 id=2355--159323-symmetric-q-learning-reducing-skewness-of-bellman-error-in-online-reinforcement-learning-motoki-omura-et-al-2024>(23/55 | 159/323) Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning (Motoki Omura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Motoki Omura, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada. (2024)<br><strong>Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Online Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07704v1.pdf filename=2403.07704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In deep <b>reinforcement</b> <b>learning,</b> estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control <b>benchmark</b> tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art <b>reinforcement</b> <b>learning</b> method by reducing the skewness of the error distribution.</p></p class="citation"></blockquote><h3 id=2455--160323-harder-tasks-need-more-experts-dynamic-routing-in-moe-models-quzhe-huang-et-al-2024>(24/55 | 160/323) Harder Tasks Need More Experts: Dynamic Routing in MoE Models (Quzhe Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Songfang Huang, Yansong Feng. (2024)<br><strong>Harder Tasks Need More Experts: Dynamic Routing in MoE Models</strong><br><button class=copy-to-clipboard title="Harder Tasks Need More Experts: Dynamic Routing in MoE Models" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07652v1.pdf filename=2403.07652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input&rsquo;s complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced <b>reasoning</b> and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various <b>benchmarks,</b> achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model dispatches more experts to tasks requiring complex <b>reasoning</b> skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input&rsquo;s complexity. Our findings also highlight a variation in the number of experts needed across different layers of the <b>transformer</b> model, offering insights into the potential for designing heterogeneous MoE frameworks. The code and models are available at <a href=https://github.com/ZhenweiAn/Dynamic_MoE>https://github.com/ZhenweiAn/Dynamic_MoE</a>.</p></p class="citation"></blockquote><h3 id=2555--161323-federated-learning-of-socially-appropriate-agent-behaviours-in-simulated-home-environments-saksham-checker-et-al-2024>(25/55 | 161/323) Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments (Saksham Checker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saksham Checker, Nikhil Churamani, Hatice Gunes. (2024)<br><strong>Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments</strong><br><button class=copy-to-clipboard title="Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs-RO, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Continual Learning, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07586v1.pdf filename=2403.07586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial. For their widespread open-world application, it is important to explore <b>Federated</b> <b>Learning</b> (FL) settings where individual robots can learn about their unique environments while also learning from each others&rsquo; experiences. In this paper, we present a novel FL <b>benchmark</b> that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others. Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel <b>Federated</b> <b>Continual</b> <b>Learning</b> (FCL) <b>benchmark</b> that adapts FL-based methods to use state-of-the-art <b>Continual</b> <b>Learning</b> (CL) methods to continually learn socially appropriate agent behaviours under different contextual settings. <b>Federated</b> <b>Averaging</b> (FedAvg) of weights emerges as a robust FL strategy while rehearsal-based FCL enables incrementally learning the social appropriateness of robot actions, across contextual splits.</p></p class="citation"></blockquote><h3 id=2655--162323-optimizing-polynomial-graph-filters-a-novel-adaptive-krylov-subspace-approach-keke-huang-et-al-2024>(26/55 | 162/323) Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach (Keke Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keke Huang, Wencai Cao, Hoang Ta, Xiaokui Xiao, Pietro Liò. (2024)<br><strong>Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach</strong><br><button class=copy-to-clipboard title="Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07954v1.pdf filename=2403.07954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> known as spectral <b>graph</b> <b>filters,</b> <b>find</b> a wide range of applications in web networks. To bypass eigendecomposition, polynomial <b>graph</b> <b>filters</b> <b>are</b> proposed to approximate <b>graph</b> <b>filters</b> <b>by</b> leveraging various polynomial bases for filter training. However, no existing studies have explored the diverse polynomial <b>graph</b> <b>filters</b> <b>from</b> a unified perspective for optimization. In this paper, we first unify polynomial <b>graph</b> <b>filters,</b> <b>as</b> well as the optimal filters of identical degrees into the Krylov subspace of the same order, thus providing equivalent expressive power theoretically. Next, we investigate the asymptotic convergence property of polynomials from the unified Krylov subspace perspective, revealing their limited adaptability in <b>graphs</b> <b>with</b> <b>varying</b> heterophily degrees. Inspired by those facts, we design a novel adaptive Krylov subspace approach to optimize polynomial bases with provable controllability over the <b>graph</b> <b>spectrum</b> <b>so</b> as to adapt various heterophily <b>graphs.</b> <b>Subsequently,</b> <b>we</b> propose AdaptKry, an optimized polynomial <b>graph</b> <b>filter</b> <b>utilizing</b> bases from the adaptive Krylov subspaces. Meanwhile, in light of the diverse spectral properties of complex <b>graphs,</b> <b>we</b> <b>extend</b> AdaptKry by leveraging multiple adaptive Krylov bases without incurring extra training costs. As a consequence, extended AdaptKry is able to capture the intricate characteristics of <b>graphs</b> <b>and</b> <b>provide</b> insights into their inherent complexity. We conduct extensive experiments across a series of real-world datasets. The experimental results demonstrate the superior filtering capability of AdaptKry, as well as the optimized efficacy of the adaptive Krylov basis.</p></p class="citation"></blockquote><h3 id=2755--163323-graph-unlearning-with-efficient-partial-retraining-jiahao-zhang-et-al-2024>(27/55 | 163/323) Graph Unlearning with Efficient Partial Retraining (Jiahao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Zhang, Lin Wang, Shijie Wang, Wenqi Fan. (2024)<br><strong>Graph Unlearning with Efficient Partial Retraining</strong><br><button class=copy-to-clipboard title="Graph Unlearning with Efficient Partial Retraining" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07353v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07353v2.pdf filename=2403.07353v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have achieved remarkable success in various real-world applications. However, <b>GNNs</b> may be trained on undesirable <b>graph</b> <b>data,</b> <b>which</b> can degrade their performance and reliability. To enable trained <b>GNNs</b> to efficiently unlearn unwanted data, a desirable solution is retraining-based <b>graph</b> <b>unlearning,</b> <b>which</b> partitions the training <b>graph</b> <b>into</b> <b>subgraphs</b> and trains sub-models on them, allowing fast unlearning through partial retraining. However, the <b>graph</b> <b>partition</b> <b>process</b> causes information loss in the training <b>graph,</b> <b>resulting</b> <b>in</b> the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel <b>graph</b> <b>unlearning</b> <b>framework</b> that better maintains the model utility of unlearnable <b>GNNs.</b> Specifically, we preserve the <b>graph</b> <b>property</b> <b>with</b> <b>graph</b> <b>property-aware</b> <b>sharding</b> and effectively aggregate the sub-GNN models for prediction with <b>graph</b> <b>contrastive</b> <b>sub-model</b> aggregation. We conduct extensive experiments to demonstrate the superiority of our proposed approach.</p></p class="citation"></blockquote><h3 id=2855--164323-graph-data-condensation-via-self-expressive-graph-structure-reconstruction-zhanyu-liu-et-al-2024>(28/55 | 164/323) Graph Data Condensation via Self-expressive Graph Structure Reconstruction (Zhanyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanyu Liu, Chaolv Zeng, Guanjie Zheng. (2024)<br><strong>Graph Data Condensation via Self-expressive Graph Structure Reconstruction</strong><br><button class=copy-to-clipboard title="Graph Data Condensation via Self-expressive Graph Structure Reconstruction" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07294v1.pdf filename=2403.07294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing demands of training <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> on large-scale <b>graphs,</b> <b>graph</b> <b>data</b> <b>condensation</b> has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale <b>graph</b> <b>to</b> <b>a</b> much smaller synthetic <b>graph</b> <b>while</b> <b>preserving</b> the essential information necessary for efficiently training a downstream <b>GNN.</b> However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the <b>graph</b> <b>structure</b> <b>generator.</b> They could not explicitly leverage the information of the original <b>graph</b> <b>structure</b> <b>and</b> failed to construct an interpretable <b>graph</b> <b>structure</b> <b>for</b> the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive <b>Graph</b> <b>Structure</b> <b>\textbf{R}econstruction</b> (\textbf{GCSR}). Our method stands out by (1) explicitly incorporating the original <b>graph</b> <b>structure</b> <b>into</b> the condensing process and (2) capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive <b>graph</b> <b>structure.</b> <b>Extensive</b> experiments and comprehensive analysis validate the efficacy of the proposed method across diverse <b>GNN</b> models and datasets. Our code is available at <a href="https://www.dropbox.com/scl/fi/2aonyp5ln5gisdqtjimu8/GCSR.zip?rlkey=11cuwfpsf54wxiiktu0klud0x&dl=0">https://www.dropbox.com/scl/fi/2aonyp5ln5gisdqtjimu8/GCSR.zip?rlkey=11cuwfpsf54wxiiktu0klud0x&dl=0</a></p></p class="citation"></blockquote><h3 id=2955--165323-supervised-time-series-classification-for-anomaly-detection-in-subsea-engineering-ergys-çokaj-et-al-2024>(29/55 | 165/323) Supervised Time Series Classification for Anomaly Detection in Subsea Engineering (Ergys Çokaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ergys Çokaj, Halvor Snersrud Gustad, Andrea Leone, Per Thomas Moe, Lasse Moldestad. (2024)<br><strong>Supervised Time Series Classification for Anomaly Detection in Subsea Engineering</strong><br><button class=copy-to-clipboard title="Supervised Time Series Classification for Anomaly Detection in Subsea Engineering" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: Primary: 62M10, Secondary: 62P30, 68T07, cs-LG, cs.LG, math-DS<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08013v1.pdf filename=2403.08013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series classification is of significant importance in monitoring structural systems. In this work, we investigate the use of <b>supervised</b> machine learning classification algorithms on simulated data based on a physical system with two states: Intact and Broken. We provide a comprehensive discussion of the preprocessing of temporal data, using measures of statistical dispersion and dimension reduction techniques. We present an intuitive baseline method and discuss its efficiency. We conclude with a comparison of the various methods based on different performance metrics, showing the advantage of using machine learning techniques as a tool in decision making.</p></p class="citation"></blockquote><h3 id=3055--166323-on-the-last-iterate-convergence-of-shuffling-gradient-methods-zijian-liu-et-al-2024>(30/55 | 166/323) On the Last-Iterate Convergence of Shuffling Gradient Methods (Zijian Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Liu, Zhengyuan Zhou. (2024)<br><strong>On the Last-Iterate Convergence of Shuffling Gradient Methods</strong><br><button class=copy-to-clipboard title="On the Last-Iterate Convergence of Shuffling Gradient Methods" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07723v1.pdf filename=2403.07723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shuffling gradient methods, which are also known as <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objective value even without strong convexity. Our new results either (nearly) match the existing last-iterate lower bounds or are as fast as the previous best upper bounds for the average iterate.</p></p class="citation"></blockquote><h3 id=3155--167323-do-deep-neural-network-solutions-form-a-star-domain-ankit-sonthalia-et-al-2024>(31/55 | 167/323) Do Deep Neural Network Solutions Form a Star Domain? (Ankit Sonthalia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankit Sonthalia, Alexander Rubinstein, Ehsan Abbasnejad, Seong Joon Oh. (2024)<br><strong>Do Deep Neural Network Solutions Form a Star Domain?</strong><br><button class=copy-to-clipboard title="Do Deep Neural Network Solutions Form a Star Domain?" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07968v1.pdf filename=2403.07968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entezari et al. (2022) conjectured that neural network solution sets reachable via <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> are convex, considering permutation invariances. This means that two independent solutions can be connected by a linear path with low loss, given one of them is appropriately permuted. However, current methods to test this theory often fail to eliminate loss barriers between two independent solutions (Ainsworth et al., 2022; Benzing et al., 2022). In this work, we conjecture that a more relaxed claim holds: the <b>SGD</b> solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations. We propose the Starlight algorithm that finds a star model of a given learning task. We validate our claim by showing that this star model is linearly connected with other independently found solutions. As an additional benefit of our study, we demonstrate better uncertainty estimates on Bayesian Model Averaging over the obtained star domain. Code is available at <a href=https://github.com/aktsonthalia/starlight>https://github.com/aktsonthalia/starlight</a>.</p></p class="citation"></blockquote><h3 id=3255--168323-visual-privacy-auditing-with-diffusion-models-kristian-schwethelm-et-al-2024>(32/55 | 168/323) Visual Privacy Auditing with Diffusion Models (Kristian Schwethelm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kristian Schwethelm, Johannes Kaiser, Moritz Knolle, Daniel Rueckert, Georgios Kaissis, Alexander Ziller. (2024)<br><strong>Visual Privacy Auditing with Diffusion Models</strong><br><button class=copy-to-clipboard title="Visual Privacy Auditing with Diffusion Models" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07588v1.pdf filename=2403.07588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image reconstruction attacks on machine learning models pose a significant risk to privacy by potentially leaking sensitive information. Although defending against such attacks using <b>differential</b> <b>privacy</b> (DP) has proven effective, determining appropriate DP parameters remains challenging. Current formal guarantees on data reconstruction success suffer from overly theoretical assumptions regarding adversary knowledge about the target data, particularly in the image domain. In this work, we empirically investigate this discrepancy and find that the practicality of these assumptions strongly depends on the domain shift between the data prior and the reconstruction target. We propose a reconstruction attack based on <b>diffusion</b> <b>models</b> (DMs) that assumes adversary access to real-world image priors and assess its implications on privacy leakage under DP-SGD. We show that (1) real-world data priors significantly influence reconstruction success, (2) current reconstruction bounds do not model the risk posed by data priors well, and (3) DMs can serve as effective auditing tools for visualizing privacy leakage.</p></p class="citation"></blockquote><h3 id=3355--169323-experimental-comparison-of-ensemble-methods-and-time-to-event-analysis-models-through-integrated-brier-score-and-concordance-index-camila-fernandez-et-al-2024>(33/55 | 169/323) Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index (Camila Fernandez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Camila Fernandez, Chung Shue Chen, Chen Pierre Gaillard, Alonso Silva. (2024)<br><strong>Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index</strong><br><button class=copy-to-clipboard title="Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07460v1.pdf filename=2403.07460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time-to-event analysis is a branch of statistics that has increased in popularity during the last decades due to its many application fields, such as predictive maintenance, customer churn prediction and population lifetime estimation. In this paper, we review and compare the performance of several prediction models for time-to-event analysis. These consist of semi-parametric and parametric statistical models, in addition to machine learning approaches. Our study is carried out on three datasets and evaluated in two different scores (the integrated Brier score and concordance index). Moreover, we show how ensemble methods, which surprisingly have not yet been much studied in time-to-event analysis, can improve the prediction accuracy and enhance the robustness of the prediction performance. We conclude the analysis with a <b>simulation</b> experiment in which we evaluate the factors influencing the performance ranking of the methods using both scores.</p></p class="citation"></blockquote><h3 id=3455--170323-proxy-methods-for-domain-adaptation-katherine-tsai-et-al-2024>(34/55 | 170/323) Proxy Methods for Domain Adaptation (Katherine Tsai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katherine Tsai, Stephen R. Pfohl, Olawale Salaudeen, Nicole Chiou, Matt J. Kusner, Alexander D&rsquo;Amour, Sanmi Koyejo, Arthur Gretton. (2024)<br><strong>Proxy Methods for Domain Adaptation</strong><br><button class=copy-to-clipboard title="Proxy Methods for Domain Adaptation" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07442v1.pdf filename=2403.07442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of <b>domain</b> <b>adaptation</b> under <b>distribution</b> <b>shift,</b> where the shift is due to a change in the <b>distribution</b> <b>of</b> an unobserved, latent variable that confounds both the covariates and the labels. In this setting, neither the covariate shift nor the label shift assumptions apply. Our approach to adaptation employs proximal causal learning, a technique for estimating causal effects in settings where proxies of unobserved confounders are available. We demonstrate that proxy variables allow for adaptation to <b>distribution</b> <b>shift</b> without explicitly recovering or modeling latent variables. We consider two settings, (i) Concept Bottleneck: an additional &lsquo;&lsquo;concept&rsquo;&rsquo; variable is observed that mediates the relationship between the covariates and labels; (ii) Multi-domain: training data from multiple source <b>domains</b> <b>is</b> available, where each source <b>domain</b> <b>exhibits</b> a different <b>distribution</b> <b>over</b> the latent confounder. We develop a two-stage kernel estimation approach to adapt to complex <b>distribution</b> <b>shifts</b> in both settings. In our experiments, we show that our approach outperforms other methods, notably those which explicitly recover the latent confounder.</p></p class="citation"></blockquote><h3 id=3555--171323-enhancing-transfer-learning-with-flexible-nonparametric-posterior-sampling-hyungi-lee-et-al-2024>(35/55 | 171/323) Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling (Hyungi Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyungi Lee, Giung Nam, Edwin Fong, Juho Lee. (2024)<br><strong>Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling</strong><br><button class=copy-to-clipboard title="Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07282v1.pdf filename=2403.07282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transfer</b> <b>learning</b> has recently shown significant performance across various tasks involving deep neural networks. In these <b>transfer</b> <b>learning</b> scenarios, the prior <b>distribution</b> <b>for</b> downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with <b>distribution</b> <b>shifts</b> between upstream and downstream data. This paper introduces nonparametric <b>transfer</b> <b>learning</b> (NPTL), a flexible posterior sampling method to address the <b>distribution</b> <b>shift</b> issue within the context of nonparametric learning. The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for <b>transfer</b> <b>learning</b> scenarios that may involve the <b>distribution</b> <b>shift</b> between upstream and downstream tasks. Through extensive empirical validations, we demonstrate that our approach surpasses other baselines in BMA performance.</p></p class="citation"></blockquote><h3 id=3655--172323-lookupffn-making-transformers-compute-lite-for-cpu-inference-zhanpeng-zeng-et-al-2024>(36/55 | 172/323) LookupFFN: Making Transformers Compute-lite for CPU inference (Zhanpeng Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanpeng Zeng, Michael Davies, Pranav Pulijala, Karthikeyan Sankaralingam, Vikas Singh. (2024)<br><strong>LookupFFN: Making Transformers Compute-lite for CPU inference</strong><br><button class=copy-to-clipboard title="LookupFFN: Making Transformers Compute-lite for CPU inference" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: RoBERTa, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07221v1.pdf filename=2403.07221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While GPU clusters are the de facto choice for training large deep neural network (DNN) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether CPUs may be viable for inference in routine use in many sectors of the industry. But the imbalance between the compute capabilities of GPUs and CPUs is huge. Motivated by these considerations, we study a module which is a workhorse within modern DNN architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent to which it can be made compute- (or FLOP-) lite. Specifically, we propose an alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by the recent studies of using Locality Sensitive Hashing (LSH) to approximate FFNs. Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since CPUs offer it in abundance). For <b>RoBERTa</b> language model pretraining, our formulation achieves similar performance compared to GEMM based FFNs, while dramatically reducing the required FLOP. Our development is complemented with a detailed hardware profiling of strategies that will maximize efficiency &ndash; not just on contemporary hardware but on products that will be offered in the near/medium term future. Code is avaiable at \url{https://github.com/mlpen/LookupFFN}.</p></p class="citation"></blockquote><h3 id=3755--173323-a-tutorial-on-multi-view-autoencoders-using-the-multi-view-ae-library-ana-lawry-aguila-et-al-2024>(37/55 | 173/323) A tutorial on multi-view autoencoders using the multi-view-AE library (Ana Lawry Aguila et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ana Lawry Aguila, Andre Altmann. (2024)<br><strong>A tutorial on multi-view autoencoders using the multi-view-AE library</strong><br><button class=copy-to-clipboard title="A tutorial on multi-view autoencoders using the multi-view-AE library" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 19<br>Keywords: Autoencoder, Benchmarking, Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07456v1.pdf filename=2403.07456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data. Multi-view <b>autoencoders</b> have gained significant traction for their adaptability and versatility in modelling <b>multi-modal</b> data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand. However, most multi-view <b>autoencoders</b> have inconsistent notation and are often implemented using different coding frameworks. To address this, we present a unified mathematical framework for multi-view <b>autoencoders,</b> consolidating their formulations. Moreover, we offer insights into the motivation and theoretical advantages of each model. To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \texttt{multi-view-AE} library. This library offers Python implementations of numerous multi-view <b>autoencoder</b> models, presented within a user-friendly framework. Through <b>benchmarking</b> experiments, we evaluate our implementations against previous ones, demonstrating comparable or superior performance. This work aims to establish a cohesive foundation for <b>multi-modal</b> modelling, serving as a valuable educational resource in the field.</p></p class="citation"></blockquote><h3 id=3855--174323-12-mj-per-class-on-device-online-few-shot-class-incremental-learning-yoga-esa-wibowo-et-al-2024>(38/55 | 174/323) 12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning (Yoga Esa Wibowo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoga Esa Wibowo, Cristian Cioflan, Thorir Mar Ingolfsson, Michael Hersche, Leo Zhao, Abbas Rahimi, Luca Benini. (2024)<br><strong>12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07851v1.pdf filename=2403.07851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-Shot</b> Class-Incremental Learning (FSCIL) enables machine learning systems to expand their inference capabilities to new classes using only a few labeled examples, without forgetting the previously learned classes. Classical backpropagation-based learning and its variants are often unsuitable for battery-powered, memory-constrained systems at the extreme edge. In this work, we introduce Online <b>Few-Shot</b> Class-Incremental Learning (O-FSCIL), based on a lightweight model consisting of a pretrained and metalearned feature extractor and an expandable explicit memory storing the class prototypes. The architecture is pretrained with a novel feature orthogonality regularization and metalearned with a multi-margin loss. For learning a new class, our approach extends the explicit memory with novel class prototypes, while the remaining architecture is kept frozen. This allows learning previously unseen classes based on only a few examples with one single pass (hence online). O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 <b>benchmark,</b> achieving state-of-the-art results. Tailored for ultra-low-power platforms, we implement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online learning capabilities within just 12 mJ per new class.</p></p class="citation"></blockquote><h3 id=3955--175323-accelerated-inference-and-reduced-forgetting-the-dual-benefits-of-early-exit-networks-in-continual-learning-filip-szatkowski-et-al-2024>(39/55 | 175/323) Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning (Filip Szatkowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filip Szatkowski, Fei Yang, Bartłomiej Twardowski, Tomasz Trzciński, Joost van de Weijer. (2024)<br><strong>Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning</strong><br><button class=copy-to-clipboard title="Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07404v1.pdf filename=2403.07404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the <b>continual</b> <b>learning</b> of the early-exit networks. We adapt existing <b>continual</b> <b>learning</b> methods to fit with early-exit architectures and investigate their behavior in the <b>continual</b> <b>setting.</b> We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task-wise Logits Correction (TLC), a simple method that equalizes this bias and improves the network performance for every given compute budget in the class-incremental setting. We assess the accuracy and computational cost of various <b>continual</b> <b>learning</b> techniques enhanced with early-exits and TLC across standard class-incremental learning <b>benchmarks</b> such as 10 split CIFAR100 and ImageNetSubset and show that TLC can achieve the accuracy of the standard methods using less than 70% of their computations. Moreover, at full computational budget, our method outperforms the accuracy of the standard counterparts by up to 15 percentage points. Our research underscores the inherent synergy between early-exit networks and <b>continual</b> <b>learning,</b> emphasizing their practical utility in resource-constrained environments.</p></p class="citation"></blockquote><h3 id=4055--176323-dataset-condensation-for-time-series-classification-via-dual-domain-matching-zhanyu-liu-et-al-2024>(40/55 | 176/323) Dataset Condensation for Time Series Classification via Dual Domain Matching (Zhanyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanyu Liu, Ke Hao, Guanjie Zheng, Yanwei Yu. (2024)<br><strong>Dataset Condensation for Time Series Classification via Dual Domain Matching</strong><br><button class=copy-to-clipboard title="Dataset Condensation for Time Series Classification via Dual Domain Matching" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07245v1.pdf filename=2403.07245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series <b>data</b> <b>has</b> been demonstrated to be crucial in various research fields. The management of large quantities of time series <b>data</b> <b>presents</b> challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named \textit{Dataset Condensation} has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and <b>graph</b> datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series <b>data,</b> <b>particularly</b> in the frequency domain. In this paper, we propose a novel framework named Dataset \textit{\textbf{Cond}}ensation for \textit{\textbf{T}}ime \textit{\textbf{S}}eries \textit{\textbf{C}}lassification via Dual Domain Matching (\textbf{CondTSC}) which focuses on the time series classification dataset condensation task. Different from previous methods, our proposed framework aims to generate a condensed dataset that matches the surrogate objectives in both the time and frequency domains. Specifically, CondTSC incorporates multi-view <b>data</b> <b>augmentation,</b> dual domain training, and dual surrogate objectives to enhance the dataset condensation process in the time and frequency domains. Through extensive experiments, we demonstrate the effectiveness of our proposed framework, which outperforms other baselines and learns a condensed synthetic dataset that exhibits desirable characteristics such as conforming to the distribution of the original data.</p></p class="citation"></blockquote><h3 id=4155--177323-towards-independence-criterion-in-machine-unlearning-of-features-and-labels-ling-han-et-al-2024>(41/55 | 177/323) Towards Independence Criterion in Machine Unlearning of Features and Labels (Ling Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ling Han, Nanqing Luo, Hao Huang, Jing Chen, Mary-Anne Hartley. (2024)<br><strong>Towards Independence Criterion in Machine Unlearning of Features and Labels</strong><br><button class=copy-to-clipboard title="Towards Independence Criterion in Machine Unlearning of Features and Labels" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6, cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08124v1.pdf filename=2403.08124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work delves into the complexities of <b>machine</b> <b>unlearning</b> in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal. With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, <b>machine</b> <b>learning</b> models face the daunting task of unlearning sensitive information without compromising their integrity or performance. Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges. By proposing a comprehensive framework for <b>machine</b> <b>unlearning,</b> we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions. Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities. Through extensive experimentation, we demonstrate the efficacy of our approach in scenarios characterized by significant distributional shifts, making substantial contributions to the field of <b>machine</b> <b>unlearning.</b> This research paves the way for developing more resilient and adaptable unlearning techniques, ensuring models remain robust and accurate in the dynamic landscape of data privacy and <b>machine</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=4255--178323-do-agents-dream-of-electric-sheep-improving-generalization-in-reinforcement-learning-through-generative-learning-giorgio-franceschelli-et-al-2024>(42/55 | 178/323) Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning (Giorgio Franceschelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giorgio Franceschelli, Mirco Musolesi. (2024)<br><strong>Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning</strong><br><button class=copy-to-clipboard title="Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07979v1.pdf filename=2403.07979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Overfitted Brain hypothesis suggests dreams happen to allow generalization in the human brain. Here, we ask if the same is true for <b>reinforcement</b> <b>learning</b> agents as well. Given limited experience in a real environment, we use imagination-based <b>reinforcement</b> <b>learning</b> to train a policy on dream-like episodes, where non-imaginative, predicted trajectories are modified through generative augmentations. Experiments on four ProcGen environments show that, compared to classic imagination and offline training on collected experience, our method can reach a higher level of generalization when dealing with sparsely rewarded environments.</p></p class="citation"></blockquote><h3 id=4355--179323-balancing-fairness-and-accuracy-in-data-restricted-binary-classification-zachary-mcbride-lazri-et-al-2024>(43/55 | 179/323) Balancing Fairness and Accuracy in Data-Restricted Binary Classification (Zachary McBride Lazri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zachary McBride Lazri, Danial Dervovic, Antigoni Polychroniadou, Ivan Brugere, Dana Dachman-Soled, Min Wu. (2024)<br><strong>Balancing Fairness and Accuracy in Data-Restricted Binary Classification</strong><br><button class=copy-to-clipboard title="Balancing Fairness and Accuracy in Data-Restricted Binary Classification" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07724v1.pdf filename=2403.07724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier. For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions. This paper proposes a framework that models the trade-off between accuracy and <b>fairness</b> under four practical scenarios that dictate the type of data available for analysis. Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset. In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself. This approach enables us to formulate multiple convex optimization problems, which allow us to answer the question: How is the accuracy of a Bayesian classifier affected in different data restricting scenarios when constrained to be fair? Analysis is performed on a set of <b>fairness</b> definitions that include group and individual <b>fairness.</b> Experiments on three datasets demonstrate the utility of the proposed framework as a tool for quantifying the trade-offs among different <b>fairness</b> notions and their distributional dependencies.</p></p class="citation"></blockquote><h3 id=4455--180323-scalable-spatiotemporal-prediction-with-bayesian-neural-fields-feras-saad-et-al-2024>(44/55 | 180/323) Scalable Spatiotemporal Prediction with Bayesian Neural Fields (Feras Saad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs Köster, Rif A. Saurous, Matthew Hoffman. (2024)<br><strong>Scalable Spatiotemporal Prediction with Bayesian Neural Fields</strong><br><button class=copy-to-clipboard title="Scalable Spatiotemporal Prediction with Bayesian Neural Fields" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-AP, stat-ME<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07657v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07657v1.pdf filename=2403.07657v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via <b>stochastic</b> <b>gradient</b> <b>descent.</b> We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements. The paper is accompanied with an open-source software package (<a href=https://github.com/google/bayesnf>https://github.com/google/bayesnf</a>) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform.</p></p class="citation"></blockquote><h3 id=4555--181323-constrained-optimal-fuel-consumption-of-hev-a-constrained-reinforcement-learning-approach-shuchang-yan-2024>(45/55 | 181/323) Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach (Shuchang Yan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuchang Yan. (2024)<br><strong>Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach</strong><br><button class=copy-to-clipboard title="Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07503v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07503v1.pdf filename=2403.07503v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hybrid electric vehicles (HEVs) are becoming increasingly popular because they can better combine the working characteristics of internal combustion engines and electric motors. However, the minimum fuel consumption of an HEV for a battery electrical balance case under a specific assembly condition and a specific speed curve still needs to be clarified in academia and industry. Regarding this problem, this work provides the mathematical expression of constrained optimal fuel consumption (COFC) from the perspective of constrained <b>reinforcement</b> <b>learning</b> (CRL) for the first time globally. Also, two mainstream approaches of CRL, constrained variational policy optimization (CVPO) and Lagrangian-based approaches, are utilized for the first time to obtain the vehicle&rsquo;s minimum fuel consumption under the battery electrical balance condition. We conduct case studies on the well-known Prius TOYOTA hybrid system (THS) under the NEDC condition; we give vital steps to implement CRL approaches and compare the performance between the CVPO and Lagrangian-based approaches. Our case study found that CVPO and Lagrangian-based approaches can obtain the lowest fuel consumption while maintaining the SOC balance constraint. The CVPO approach converges stable, but the Lagrangian-based approach can obtain the lowest fuel consumption at 3.95 L/100km, though with more significant oscillations. This result verifies the effectiveness of our proposed CRL approaches to the COFC problem.</p></p class="citation"></blockquote><h3 id=4655--182323-xpertai-uncovering-model-strategies-for-sub-manifolds-simon-letzgus-et-al-2024>(46/55 | 182/323) XpertAI: uncovering model strategies for sub-manifolds (Simon Letzgus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Letzgus, Klaus-Robert Müller, Grégoire Montavon. (2024)<br><strong>XpertAI: uncovering model strategies for sub-manifolds</strong><br><button class=copy-to-clipboard title="XpertAI: uncovering model strategies for sub-manifolds" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07486v1.pdf filename=2403.07486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>Explainable</b> <b>AI</b> (XAI) methods have facilitated profound validation and knowledge extraction from ML models. While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models. In regression, explanations need to be precisely formulated to address specific user queries (e.g.\ distinguishing between <code>Why is the output above 0?' and </code>Why is the output above 50?&rsquo;). They should furthermore reflect the model&rsquo;s behavior on the relevant data sub-manifold. In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum&rsquo;) as a linear combination of those sub-strategies. XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation. Qualitative and quantitative results, demonstrate the benefits of our approach.</p></p class="citation"></blockquote><h3 id=4755--183323-deepcdcl-an-cdcl-based-neural-network-verification-framework-zongxin-liu-et-al-2024>(47/55 | 183/323) DeepCDCL: An CDCL-based Neural Network Verification Framework (Zongxin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zongxin Liu, Pengfei Yang, Lijun Zhang, Xiaowei Huang. (2024)<br><strong>DeepCDCL: An CDCL-based Neural Network Verification Framework</strong><br><button class=copy-to-clipboard title="DeepCDCL: An CDCL-based Neural Network Verification Framework" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07956v1.pdf filename=2403.07956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks in safety-critical applications face increasing safety and security concerns due to their susceptibility to little disturbance. In this paper, we propose DeepCDCL, a novel neural network verification framework based on the Conflict-Driven Clause Learning (CDCL) algorithm. We introduce an asynchronous clause learning and management structure, reducing redundant time consumption compared to the direct application of the CDCL framework. Furthermore, we also provide a detailed evaluation of the performance of our approach on the ACAS Xu and <b>MNIST</b> datasets, showing that a significant speed-up is achieved in most cases.</p></p class="citation"></blockquote><h3 id=4855--184323-towards-faithful-explanations-boosting-rationalization-with-shortcuts-discovery-linan-yue-et-al-2024>(48/55 | 184/323) Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery (Linan Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linan Yue, Qi Liu, Yichao Du, Li Wang, Weibo Gao, Yanqing An. (2024)<br><strong>Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery</strong><br><button class=copy-to-clipboard title="Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07955v1.pdf filename=2403.07955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in <b>data</b> <b>to</b> compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two <b>data</b> <b>augmentations</b> methods to close the gap in the number of annotated rationales. Extensive experimental results on real-world datasets clearly validate the effectiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=4955--185323-challenging-forgets-unveiling-the-worst-case-forget-sets-in-machine-unlearning-chongyu-fan-et-al-2024>(49/55 | 185/323) Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning (Chongyu Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chongyu Fan, Jiancheng Liu, Alfred Hero, Sijia Liu. (2024)<br><strong>Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning</strong><br><button class=copy-to-clipboard title="Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07362v1.pdf filename=2403.07362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The trustworthy <b>machine</b> <b>learning</b> (ML) community is increasingly recognizing the crucial need for models capable of selectively &lsquo;unlearning&rsquo; data points after training. This leads to the problem of <b>machine</b> <b>unlearning</b> (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model&rsquo;s utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization level to emulate worst-case scenarios, while simultaneously engaging in standard training and unlearning at the lower level, achieving a balance between data influence erasure and model utility. Our proposal offers a worst-case evaluation of MU&rsquo;s resilience and effectiveness. Through extensive experiments across different datasets (including CIFAR-10, 100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image classifiers and generative models), we expose critical pros and cons in existing (approximate) unlearning strategies. Our results illuminate the complex challenges of MU in practice, guiding the future development of more accurate and robust unlearning algorithms. The code is available at <a href=https://github.com/OPTML-Group/Unlearn-WorstCase>https://github.com/OPTML-Group/Unlearn-WorstCase</a>.</p></p class="citation"></blockquote><h3 id=5055--186323-im-unpack-training-and-inference-with-arbitrarily-low-precision-integers-zhanpeng-zeng-et-al-2024>(50/55 | 186/323) IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers (Zhanpeng Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanpeng Zeng, Karthikeyan Sankaralingam, Vikas Singh. (2024)<br><strong>IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers</strong><br><button class=copy-to-clipboard title="IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07339v1.pdf filename=2403.07339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of <b>Transformer-based</b> models, whether integers are sufficient for all GEMMs need &ndash; for {\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\em low} bit-width integers, the existence of a few heavy hitter entries make it difficult to achieve efficiency gains via the exclusive use of low bit-width GEMMs alone. To address this issue, we develop a simple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\em unpack} a matrix with large integer entries into a larger matrix whose entries all lie within the representable range of arbitrarily low bit-width integers. This allows {\em equivalence} with the original GEMM, i.e., the exact result can be obtained using purely low bit-width integer GEMMs. This comes at the cost of additional operations &ndash; we show that for many popular models, this overhead is quite small.</p></p class="citation"></blockquote><h3 id=5155--187323-reinforced-sequential-decision-making-for-sepsis-treatment-the-posnegdm-framework-with-mortality-classifier-and-transformer-dipesh-tamboli-et-al-2024>(51/55 | 187/323) Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer (Dipesh Tamboli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dipesh Tamboli, Jiayu Chen, Kiran Pranesh Jotheeswaran, Denny Yu, Vaneet Aggarwal. (2024)<br><strong>Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer</strong><br><button class=copy-to-clipboard title="Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07309v1.pdf filename=2403.07309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sepsis, a life-threatening condition triggered by the body&rsquo;s exaggerated response to infection, demands urgent intervention to prevent severe complications. Existing machine learning methods for managing sepsis struggle in offline scenarios, exhibiting suboptimal performance with survival rates below 50%. This paper introduces the POSNEGDM &ndash; ``Reinforcement Learning with Positive and Negative Demonstrations for Sequential Decision-Making" framework utilizing an innovative <b>transformer-based</b> model and a feedback reinforcer to replicate expert actions while considering individual patient characteristics. A mortality classifier with 96.7% accuracy guides treatment decisions towards positive outcomes. The POSNEGDM framework significantly improves patient survival, saving 97.39% of patients, outperforming established machine learning algorithms (Decision <b>Transformer</b> and Behavioral Cloning) with survival rates of 33.4% and 43.5%, respectively. Additionally, ablation studies underscore the critical role of the <b>transformer-based</b> decision maker and the integration of a mortality classifier in enhancing overall survival rates. In summary, our proposed approach presents a promising avenue for enhancing sepsis treatment outcomes, contributing to improved patient care and reduced healthcare costs.</p></p class="citation"></blockquote><h3 id=5255--188323-learning-augmented-algorithms-with-explicit-predictors-marek-elias-et-al-2024>(52/55 | 188/323) Learning-Augmented Algorithms with Explicit Predictors (Marek Elias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marek Elias, Haim Kaplan, Yishay Mansour, Shay Moran. (2024)<br><strong>Learning-Augmented Algorithms with Explicit Predictors</strong><br><button class=copy-to-clipboard title="Learning-Augmented Algorithms with Explicit Predictors" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07413v1.pdf filename=2403.07413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data. These approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail. In this paper we focus on online problems; prior research in this context was focused on a paradigm where the predictor is pre-trained on past data and then used as a <b>black</b> <b>box</b> (to get the predictions it was trained for). In contrast, in this work, we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge. In particular we allow the predictor to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand. Adopting this perspective, we focus on a number of fundamental problems, including caching and scheduling, which have been well-studied in the <b>black-box</b> <b>setting.</b> For each of the problems we consider, we introduce new algorithms that take advantage of explicit learning algorithms which we carefully design towards optimizing the overall performance. We demonstrate the potential of our approach by deriving performance bounds which improve over those established in previous work.</p></p class="citation"></blockquote><h3 id=5355--189323-mccatch-scalable-microcluster-detection-in-dimensional-and-nondimensional-datasets-braulio-v-sánchez-vinces-et-al-2024>(53/55 | 189/323) McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets (Braulio V. Sánchez Vinces et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Braulio V. Sánchez Vinces, Robson L. F. Cordeiro, Christos Faloutsos. (2024)<br><strong>McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets</strong><br><button class=copy-to-clipboard title="McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08027v1.pdf filename=2403.08027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How could we have an outlier detector that works even with nondimensional data, and ranks together both singleton microclusters (&lsquo;one-off&rsquo; outliers) and nonsingleton microclusters by their anomaly scores? How to obtain scores that are principled in one scalable and &lsquo;hands-off&rsquo; manner? Microclusters of outliers indicate coalition or repetition in fraud activities, etc.; their identification is thus highly desirable. This paper presents McCatch: a new algorithm that detects microclusters by leveraging our proposed &lsquo;Oracle&rsquo; plot (1NN Distance versus Group 1NN Distance). We study 31 real and synthetic datasets with up to 1M data elements to show that McCatch is the only method that answers both of the questions above; and, it outperforms 11 other methods, especially when the data has nonsingleton microclusters or is nondimensional. We also showcase McCatch&rsquo;s ability to detect meaningful microclusters in <b>graphs,</b> fingerprints, logs of network connections, text data, and satellite imagery. For example, it found a 30-elements microcluster of confirmed &lsquo;Denial of Service&rsquo; attacks in the network logs, taking only ~3 minutes for 222K data elements on a stock desktop.</p></p class="citation"></blockquote><h3 id=5455--190323-robustifying-and-boosting-training-free-neural-architecture-search-zhenfeng-he-et-al-2024>(54/55 | 190/323) Robustifying and Boosting Training-Free Neural Architecture Search (Zhenfeng He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenfeng He, Yao Shu, Zhongxiang Dai, Bryan Kian Hsiang Low. (2024)<br><strong>Robustifying and Boosting Training-Free Neural Architecture Search</strong><br><button class=copy-to-clipboard title="Robustifying and Boosting Training-Free Neural Architecture Search" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07591v1.pdf filename=2403.07591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further. Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights. Our extensive experiments on various NAS <b>benchmark</b> tasks yield substantial empirical evidence to support our theoretical results.</p></p class="citation"></blockquote><h3 id=5555--191323-unknown-domain-inconsistency-minimization-for-domain-generalization-seungjae-shin-et-al-2024>(55/55 | 191/323) Unknown Domain Inconsistency Minimization for Domain Generalization (Seungjae Shin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungjae Shin, HeeSun Bae, Byeonghu Na, Yoon-Yeong Kim, Il-Chul Moon. (2024)<br><strong>Unknown Domain Inconsistency Minimization for Domain Generalization</strong><br><button class=copy-to-clipboard title="Unknown Domain Inconsistency Minimization for Domain Generalization" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07329v1.pdf filename=2403.07329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain&rsquo;s loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there&rsquo;s still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscape of perturbed domains, we expect to achieve generalization grounded on these flat minima for the unknown domains. Theoretically, we validate that merging SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. In an empirical aspect, UDIM consistently outperforms SAM variants across multiple DG <b>benchmark</b> datasets. Notably, UDIM shows statistically significant improvements in scenarios with more restrictive domain information, underscoring UDIM&rsquo;s generalization capability in unseen domains. Our code is available at \url{https://github.com/SJShin-AI/UDIM}.</p></p class="citation"></blockquote><h2 id=statml-5>stat.ML (5)</h2><h3 id=15--192323-knowledge-transfer-across-multiple-principal-component-analysis-studies-zeyu-li-et-al-2024>(1/5 | 192/323) Knowledge Transfer across Multiple Principal Component Analysis Studies (Zeyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Li, Kangxiang Qin, Yong He, Wang Zhou, Xinsheng Zhang. (2024)<br><strong>Knowledge Transfer across Multiple Principal Component Analysis Studies</strong><br><button class=copy-to-clipboard title="Knowledge Transfer across Multiple Principal Component Analysis Studies" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 80<br>Keywords: Knowledge Transfer, Simulation, Simulator, Supervised Learning, Supervised Learning, Transfer Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07431v1.pdf filename=2403.07431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transfer</b> <b>learning</b> has aroused great interest in the statistical community. In this article, we focus on <b>knowledge</b> <b>transfer</b> <b>for</b> <b>unsupervised</b> <b>learning</b> tasks in contrast to the <b>supervised</b> <b>learning</b> tasks in the literature. Given the transferable source populations, we propose a two-step <b>transfer</b> <b>learning</b> algorithm to extract useful information from multiple source principal component analysis (PCA) studies, thereby enhancing estimation accuracy for the target PCA task. In the first step, we integrate the shared subspace information across multiple studies by a proposed method named as Grassmannian barycenter, instead of directly performing PCA on the pooled dataset. The proposed Grassmannian barycenter method enjoys robustness and computational advantages in more general cases. Then the resulting estimator for the shared subspace from the first step is further utilized to estimate the target private subspace in the second step. Our theoretical analysis credits the gain of <b>knowledge</b> <b>transfer</b> <b>between</b> PCA studies to the enlarged eigenvalue gap, which is different from the existing <b>supervised</b> <b>transfer</b> <b>learning</b> tasks where sparsity plays the central role. In addition, we prove that the bilinear forms of the empirical spectral projectors have asymptotic normality under weaker eigenvalue gap conditions after <b>knowledge</b> <b>transfer.</b> <b>When</b> the set of informativesources is unknown, we endow our algorithm with the capability of useful dataset selection by solving a rectified optimization problem on the Grassmann manifold, which in turn leads to a computationally friendly rectified Grassmannian K-means procedure. In the end, extensive numerical <b>simulation</b> results and a real data case concerning activity recognition are reported to support our theoretical claims and to illustrate the empirical usefulness of the proposed <b>transfer</b> <b>learning</b> methods.</p></p class="citation"></blockquote><h3 id=25--193323-fast-accurate-and-lightweight-sequential-simulation-based-inference-using-gaussian-locally-linear-mappings-henrik-häggström-et-al-2024>(2/5 | 193/323) Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings (Henrik Häggström et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henrik Häggström, Pedro L. C. Rodrigues, Geoffroy Oudoumanessah, Florence Forbes, Umberto Picchini. (2024)<br><strong>Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings</strong><br><button class=copy-to-clipboard title="Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07454v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07454v2.pdf filename=2403.07454v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators. These approaches are collectively known as <b>&ldquo;simulation-based</b> inference&rdquo; (SBI). Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution. However, they do not generally achieve an optimal trade-off between accuracy and computational demand. In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions. Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint. We illustrate our results on several <b>benchmark</b> models from the SBI literature.</p></p class="citation"></blockquote><h3 id=35--194323-fairrr-pre-processing-for-group-fairness-through-randomized-response-xianli-zeng-et-al-2024>(3/5 | 194/323) FairRR: Pre-Processing for Group Fairness through Randomized Response (Xianli Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianli Zeng, Joshua Ward, Guang Cheng. (2024)<br><strong>FairRR: Pre-Processing for Group Fairness through Randomized Response</strong><br><button class=copy-to-clipboard title="FairRR: Pre-Processing for Group Fairness through Randomized Response" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07780v1.pdf filename=2403.07780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing usage of machine learning models in consequential decision-making processes has spurred research into the <b>fairness</b> of these systems. While significant work has been done to study group <b>fairness</b> in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain. This paper proposes that achieving group <b>fairness</b> in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework. We show that measures of group <b>fairness</b> can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and <b>fairness.</b></p></p class="citation"></blockquote><h3 id=45--195323-cas-a-general-algorithm-for-online-selective-conformal-prediction-with-fcr-control-yajie-bao-et-al-2024>(4/5 | 195/323) CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control (Yajie Bao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou. (2024)<br><strong>CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control</strong><br><button class=copy-to-clipboard title="CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07728v1.pdf filename=2403.07728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for popular online selection rules. We proved that CAS can achieve an exact selection-conditional coverage guarantee in the finite-sample and <b>distribution-free</b> <b>regimes.</b> For the decision-driven selection rule, including most online multiple-testing procedures, CAS can exactly control the real-time FCR below the target level without any <b>distributional</b> <b>assumptions.</b> For the online selection with symmetric thresholds, we establish the error bound for the control gap of FCR under mild <b>distributional</b> <b>assumptions.</b> To account for the <b>distribution</b> <b>shift</b> in online data, we also embed CAS into some recent dynamic conformal prediction methods and examine the long-run FCR control. Numerical results on both synthetic and real data corroborate that CAS can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings.</p></p class="citation"></blockquote><h3 id=55--196323-on-the-nonconvexity-of-some-push-forward-constraints-and-its-consequences-in-machine-learning-lucas-de-lara-et-al-2024>(5/5 | 196/323) On the nonconvexity of some push-forward constraints and its consequences in machine learning (Lucas de Lara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas de Lara, Mathis Deronzier, Alberto González-Sanz, Virgile Foy. (2024)<br><strong>On the nonconvexity of some push-forward constraints and its consequences in machine learning</strong><br><button class=copy-to-clipboard title="On the nonconvexity of some push-forward constraints and its consequences in machine learning" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-PR, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07471v1.pdf filename=2403.07471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The push-forward operation enables one to redistribute a probability measure through a deterministic map. It plays a key role in statistics and optimization: many learning problems (notably from optimal transport, generative modeling, and algorithmic <b>fairness)</b> include constraints or penalties framed as push-forward conditions on the model. However, the literature lacks general theoretical insights on the (non)convexity of such constraints and its consequences on the associated learning problems. This paper aims at filling this gap. In a first part, we provide a range of sufficient and necessary conditions for the (non)convexity of two sets of functions: the maps transporting one probability measure to another; the maps inducing equal output distributions across distinct probability measures. This highlights that for most probability measures, these push-forward constraints are not convex. In a second time, we show how this result implies critical limitations on the design of convex optimization problems for learning generative models or group-fair predictors. This work will hopefully help researchers and practitioners have a better understanding of the critical impact of push-forward conditions onto convexity.</p></p class="citation"></blockquote><h2 id=csir-8>cs.IR (8)</h2><h3 id=18--197323-towards-graph-foundation-models-for-personalization-andreas-damianou-et-al-2024>(1/8 | 197/323) Towards Graph Foundation Models for Personalization (Andreas Damianou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Damianou, Francesco Fabbri, Paul Gigioli, Marco De Nadai, Alice Wang, Enrico Palumbo, Mounia Lalmas. (2024)<br><strong>Towards Graph Foundation Models for Personalization</strong><br><button class=copy-to-clipboard title="Towards Graph Foundation Models for Personalization" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 63<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Foundation Model, Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07478v1.pdf filename=2403.07478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> and <b>Foundation</b> <b>Models</b> (FMs). While <b>GNNs</b> emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a <b>graph-based</b> <b>foundation</b> <b>modeling</b> approach tailored to personalization. Central to this approach is a Heterogeneous <b>GNN</b> (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a <b>Foundation</b> <b>Model,</b> we employ a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> text-based featurization of nodes that accommodates all item types, and construct the <b>graph</b> <b>using</b> <b>co-interaction</b> signals, which inherently transcend content specificity. To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type. This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data. Our comprehensive approach has been rigorously tested and proven effective in delivering <b>recommendations</b> across a diverse array of products within a real-world, industrial audio streaming platform.</p></p class="citation"></blockquote><h3 id=28--198323-the-future-of-document-indexing-gpt-and-donut-revolutionize-table-of-content-processing-degaga-wolde-feyisa-et-al-2024>(2/8 | 198/323) The future of document indexing: GPT and Donut revolutionize table of content processing (Degaga Wolde Feyisa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Degaga Wolde Feyisa, Haylemicheal Berihun, Amanuel Zewdu, Mahsa Najimoghadam, Marzieh Zare. (2024)<br><strong>The future of document indexing: GPT and Donut revolutionize table of content processing</strong><br><button class=copy-to-clipboard title="The future of document indexing: GPT and Donut revolutionize table of content processing" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CV, cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Optical Character Recognition, GPT, GPT-3, GPT-3.5, Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07553v1.pdf filename=2403.07553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industrial projects rely heavily on lengthy, complex specification documents, making tedious manual extraction of structured <b>information</b> <b>a</b> major bottleneck. This paper introduces an innovative approach to automate this process, leveraging the capabilities of two cutting-edge AI models: Donut, a model that extracts <b>information</b> <b>directly</b> from scanned documents without <b>OCR,</b> and OpenAI <b>GPT-3.5</b> Turbo, a robust <b>large</b> <b>language</b> <b>model.</b> The proposed methodology is initiated by acquiring the table of contents (ToCs) from construction specification documents and subsequently structuring the ToCs text into JSON data. Remarkable accuracy is achieved, with Donut reaching 85% and <b>GPT-3.5</b> Turbo reaching 89% in effectively organizing the ToCs. This landmark achievement represents a significant leap forward in document indexing, demonstrating the immense potential of AI to automate <b>information</b> <b>extraction</b> tasks across diverse document types, boosting efficiency and liberating critical resources in various industries.</p></p class="citation"></blockquote><h3 id=38--199323-analyzing-adversarial-attacks-on-sequence-to-sequence-relevance-models-andrew-parry-et-al-2024>(3/8 | 199/323) Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models (Andrew Parry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Parry, Maik Fröbe, Sean MacAvaney, Martin Potthast, Matthias Hagen. (2024)<br><strong>Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models</strong><br><button class=copy-to-clipboard title="Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Large Language Model, Prompt, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07654v1.pdf filename=2403.07654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern sequence-to-sequence relevance models like monoT5 can effectively capture complex textual interactions between queries and documents through cross-encoding. However, the use of natural language tokens in <b>prompts,</b> such as Query, Document, and Relevant for monoT5, opens an attack vector for malicious documents to manipulate their relevance score through <b>prompt</b> injection, e.g., by adding target words such as true. Since such possibilities have not yet been considered in retrieval evaluation, we analyze the impact of query-independent <b>prompt</b> injection via manually constructed templates and <b>LLM-based</b> rewriting of documents on several existing relevance models. Our experiments on the TREC Deep Learning track show that <b>adversarial</b> <b>documents</b> can easily manipulate different sequence-to-sequence relevance models, while BM25 (as a typical lexical model) is not affected. Remarkably, the attacks also affect encoder-only relevance models (which do not rely on natural language <b>prompt</b> tokens), albeit to a lesser extent.</p></p class="citation"></blockquote><h3 id=48--200323-self-supervised-contrastive-learning-for-implicit-collaborative-filtering-shipeng-song-et-al-2024>(4/8 | 200/323) Self-supervised Contrastive Learning for Implicit Collaborative Filtering (Shipeng Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shipeng Song, Bin Liu, Fei Teng, Tianrui Li. (2024)<br><strong>Self-supervised Contrastive Learning for Implicit Collaborative Filtering</strong><br><button class=copy-to-clipboard title="Self-supervised Contrastive Learning for Implicit Collaborative Filtering" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Recommendation, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07265v1.pdf filename=2403.07265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>learning-based</b> <b>recommendation</b> algorithms have significantly advanced the field of <b>self-supervised</b> <b>recommendation,</b> particularly with BPR as a representative ranking prediction task that dominates implicit collaborative filtering. However, the presence of false-positive and false-negative examples in <b>recommendation</b> systems hampers accurate preference learning. In this study, we propose a simple <b>self-supervised</b> <b>contrastive</b> <b>learning</b> framework that leverages positive feature augmentation and negative label augmentation to improve the self-supervisory signal. Theoretical analysis demonstrates that our learning method is equivalent to maximizing the likelihood estimation with latent variables representing user interest centers. Additionally, we establish an efficient negative label augmentation technique that samples unlabeled examples with a probability linearly dependent on their relative ranking positions, enabling efficient augmentation in constant time complexity. Through validation on multiple datasets, we illustrate the significant improvements our method achieves over the widely used BPR optimization objective while maintaining comparable runtime.</p></p class="citation"></blockquote><h3 id=58--201323-empowering-sequential-recommendation-from-collaborative-signals-and-semantic-relatedness-mingyue-cheng-et-al-2024>(5/8 | 201/323) Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness (Mingyue Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyue Cheng, Hao Zhang, Qi Liu, Fajie Yuan, Zhi Li, Zhenya Huang, Enhong Chen, Jun Zhou, Longfei Li. (2024)<br><strong>Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness</strong><br><button class=copy-to-clipboard title="Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07623v1.pdf filename=2403.07623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential <b>recommender</b> <b>systems</b> (SRS) could capture dynamic user preferences by modeling historical behaviors ordered in time. Despite effectiveness, focusing only on the \textit{collaborative signals} from behaviors does not fully grasp user interests. It is also significant to model the \textit{semantic relatedness} reflected in content features, e.g., images and text. Towards that end, in this paper, we aim to enhance the SRS tasks by effectively unifying collaborative signals and semantic relatedness together. Notably, we empirically point out that it is nontrivial to achieve such a goal due to semantic gap issues. Thus, we propose an end-to-end two-stream architecture for sequential <b>recommendation,</b> named TSSR, to learn user preferences from ID-based and content-based sequence. Specifically, we first present novel hierarchical contrasting module, including coarse user-grained and fine item-grained terms, to align the representations of inter-modality. Furthermore, we also design a two-stream architecture to learn the dependence of intra-modality sequence and the complex interactions of inter-modality sequence, which can yield more expressive capacity in understanding user interests. We conduct extensive experiments on five public datasets. The experimental results show that the TSSR could yield superior performance than competitive baselines. We also make our experimental codes publicly available at <a href=https://anonymous.4open.science/r/TSSR-2A27/>https://anonymous.4open.science/r/TSSR-2A27/</a>.</p></p class="citation"></blockquote><h3 id=68--202323-proactive-recommendation-with-iterative-preference-guidance-shuxian-bi-et-al-2024>(6/8 | 202/323) Proactive Recommendation with Iterative Preference Guidance (Shuxian Bi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuxian Bi, Wenjie Wang, Hang Pan, Fuli Feng, Xiangnan He. (2024)<br><strong>Proactive Recommendation with Iterative Preference Guidance</strong><br><button class=copy-to-clipboard title="Proactive Recommendation with Iterative Preference Guidance" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07571v1.pdf filename=2403.07571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> mainly tailor personalized <b>recommendations</b> according to user interests learned from user feedback. However, such <b>recommender</b> <b>systems</b> passively cater to user interests and even reinforce existing interests in the feedback loop, leading to problems like filter bubbles and opinion polarization. To counteract this, proactive <b>recommendation</b> actively steers users towards developing new interests in a target item or topic by strategically modulating <b>recommendation</b> sequences. Existing work for proactive <b>recommendation</b> faces significant hurdles: 1) overlooking the user feedback in the guidance process; 2) lacking explicit modeling of the guiding objective; and 3) insufficient flexibility for integration into existing industrial <b>recommender</b> <b>systems.</b> To address these issues, we introduce an Iterative Preference Guidance (IPG) framework. IPG performs proactive <b>recommendation</b> in a flexible post-processing manner by ranking items according to their IPG scores that consider both interaction probability and guiding value. These scores are explicitly estimated with iteratively updated user representation that considers the most recent user interactions. Extensive experiments validate that IPG can effectively guide user interests toward target interests with a reasonable trade-off in <b>recommender</b> <b>accuracy.</b> The code is available at <a href=https://github.com/GabyUSTC/IPG-Rec>https://github.com/GabyUSTC/IPG-Rec</a>.</p></p class="citation"></blockquote><h3 id=78--203323-desere-the-1st-workshop-on-decentralised-search-and-recommendation-mohamed-ragab-et-al-2024>(7/8 | 203/323) DESERE: The 1st Workshop on Decentralised Search and Recommendation (Mohamed Ragab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Ragab, Yury Savateev, Wenjie Wang, Reza Moosaei, Thanassis Tiropanis, Alexandra Poulovassilis, Adriane Chapman, Helen Oliver, George Roussos. (2024)<br><strong>DESERE: The 1st Workshop on Decentralised Search and Recommendation</strong><br><button class=copy-to-clipboard title="DESERE: The 1st Workshop on Decentralised Search and Recommendation" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07732v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07732v1.pdf filename=2403.07732v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The DESERE Workshop, our First Workshop on Decentralised Search and <b>Recommendation,</b> offers a platform for researchers to explore and share innovative ideas on decentralised web services, mainly focusing on three major topics: (i) societal impact of decentralised systems: their effect on privacy, policy, and regulation; (ii) decentralising applications: algorithmic and performance challenges that arise from decentralisation; and (iii) infrastructure to support decentralised systems and services: peer-to-peer networks, routing, and performance evaluation tools</p></p class="citation"></blockquote><h3 id=88--204323-list-learning-to-index-spatio-textual-data-for-embedding-based-spatial-keyword-queries-ziqi-yin-et-al-2024>(8/8 | 204/323) LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries (Ziqi Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqi Yin, Shanshan Feng, Shang Liu, Gao Cong, Yew Soon Ong, Bin Cui. (2024)<br><strong>LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries</strong><br><button class=copy-to-clipboard title="LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-DB, cs-IR, cs.IR<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07331v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07331v2.pdf filename=2403.07331v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the proliferation of spatio-textual data, Top-k KNN spatial keyword queries (TkQs), which return a list of objects based on a ranking function that evaluates both spatial and textual relevance, have found many real-life applications. Existing geo-textual indexes for TkQs use traditional retrieval models like BM25 to compute text relevance and usually exploit a simple linear function to compute spatial relevance, but its effectiveness is limited. To improve effectiveness, several deep learning models have recently been proposed, but they suffer severe efficiency issues. To the best of our knowledge, there are no efficient indexes specifically designed to accelerate the top-k search process for these deep learning models. To tackle these issues, we propose a novel technique, which Learns to Index the Spatio-Textual data for answering embedding based spatial keyword queries (called LIST). LIST is featured with two novel components. Firstly, we propose a lightweight and effective relevance model that is capable of learning both textual and spatial relevance. Secondly, we introduce a novel machine learning based Approximate Nearest Neighbor Search (ANNS) index, which utilizes a new learning-to-cluster technique to group relevant queries and objects together while separating irrelevant queries and objects. Two key challenges in building an effective and efficient index are the absence of high-quality labels and unbalanced <b>clustering</b> results. We develop a novel pseudo-label generation technique to address the two challenges. Experimental results show that LIST significantly outperforms state-of-the-art methods on effectiveness, with improvements up to 19.21% and 12.79% in terms of NDCG@1 and Recall@10, and is three orders of magnitude faster than the most effective baseline.</p></p class="citation"></blockquote><h2 id=eesssp-4>eess.SP (4)</h2><h3 id=14--205323-vector-quantization-for-deep-learning-based-csi-feedback-in-massive-mimo-systems-junyong-shin-et-al-2024>(1/4 | 205/323) Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems (Junyong Shin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyong Shin, Yujin Kang, Yo-Seb Jeon. (2024)<br><strong>Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems</strong><br><button class=copy-to-clipboard title="Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, cs-CV, eess-SP, eess.SP<br>Keyword Score: 60<br>Keywords: Autoencoder, Quantization, Quantization, Simulation, Simulator, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07355v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07355v2.pdf filename=2403.07355v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized <b>variational</b> <b>autoencoder</b> (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector <b>quantization.</b> In this method, the magnitude of the latent vector is <b>quantized</b> using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is <b>quantized</b> using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. <b>Simulation</b> results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.</p></p class="citation"></blockquote><h3 id=24--206323-deep-learning-assisted-parallel-interference-cancellation-for-grant-free-noma-in-machine-type-communication-yongjeong-oh-et-al-2024>(2/4 | 206/323) Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication (Yongjeong Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongjeong Oh, Jaehong Jo, Byonghyo Shim, Yo-Seb Jeon. (2024)<br><strong>Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication</strong><br><button class=copy-to-clipboard title="Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, cs-LG, eess-SP, eess.SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07255v1.pdf filename=2403.07255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel approach for joint activity detection (AD), channel estimation (CE), and data detection (DD) in uplink grant-free non-orthogonal multiple access (NOMA) systems. Our approach employs an iterative and parallel interference removal strategy inspired by parallel interference cancellation (PIC), enhanced with deep learning to jointly tackle the AD, CE, and DD problems. Based on this approach, we develop three PIC frameworks, each of which is designed for either coherent or non-coherence schemes. The first framework performs joint AD and CE using received pilot signals in the coherent scheme. Building upon this framework, the second framework utilizes both the received pilot and data signals for CE, further enhancing the performances of AD, CE, and DD in the coherent scheme. The third framework is designed to accommodate the non-coherent scheme involving a small number of data bits, which simultaneously performs AD and DD. Through joint loss functions and interference cancellation modules, our approach supports end-to-end training, contributing to enhanced performances of AD, CE, and DD for both coherent and non-coherent schemes. <b>Simulation</b> results demonstrate the superiority of our approach over traditional techniques, exhibiting enhanced performances of AD, CE, and DD while maintaining lower computational complexity.</p></p class="citation"></blockquote><h3 id=34--207323-discrete-time-modeling-and-handover-analysis-of-intelligent-reflecting-surface-assisted-networks-hongtao-zhang-et-al-2024>(3/4 | 207/323) Discrete-Time Modeling and Handover Analysis of Intelligent Reflecting Surface-Assisted Networks (Hongtao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongtao Zhang, Haoyan Wei. (2024)<br><strong>Discrete-Time Modeling and Handover Analysis of Intelligent Reflecting Surface-Assisted Networks</strong><br><button class=copy-to-clipboard title="Discrete-Time Modeling and Handover Analysis of Intelligent Reflecting Surface-Assisted Networks" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-NI, eess-SP, eess.SP<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07323v1.pdf filename=2403.07323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Owning to the reflection gain and double path loss featured by intelligent reflecting surface (IRS) channels, handover (HO) locations become irregular and the signal strength fluctuates sharply with variations in IRS connections during HO, the risk of HO failures (HOFs) is exacerbated and thus HO parameters require reconfiguration. However, existing HO models only assume monotonic negative exponential path loss and cannot obtain sound HO parameters. This paper proposes a <b>discrete-time</b> <b>model</b> to explicitly track the HO process with variations in IRS connections, where IRS connections and HO process are discretized as finite states by measurement intervals, and transitions between states are modeled as stochastic processes. Specifically, to capture signal fluctuations during HO, IRS connection state-dependent distributions of the user-IRS distance are modified by the correlation between measurement intervals. In addition, states of the HO process are formed with Time-to-Trigger and HO margin whose transition probabilities are integrated concerning all IRS connection states. Trigger location distributions and probabilities of HO, HOF, and ping-pong (PP) are obtained by tracing user HO states. Results show IRSs mitigate PPs by 48% but exacerbate HOFs by 90% under regular parameters. Optimal parameters are mined ensuring probabilities of HOF and PP are both less than 0.1%.</p></p class="citation"></blockquote><h3 id=44--208323-advancements-in-continuous-glucose-monitoring-integrating-deep-learning-and-ecg-signal-mohammadreza-hosseinzadehketilateh-et-al-2024>(4/4 | 208/323) Advancements in Continuous Glucose Monitoring: Integrating Deep Learning and ECG Signal (MohammadReza Hosseinzadehketilateh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>MohammadReza Hosseinzadehketilateh, Banafsheh Adami, Nima Karimian. (2024)<br><strong>Advancements in Continuous Glucose Monitoring: Integrating Deep Learning and ECG Signal</strong><br><button class=copy-to-clipboard title="Advancements in Continuous Glucose Monitoring: Integrating Deep Learning and ECG Signal" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-CV, cs-HC, eess-SP, eess.SP<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07296v1.pdf filename=2403.07296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach to noninvasive hyperglycemia monitoring utilizing electrocardiograms (ECG) from an extensive database comprising 1119 subjects. Previous research on hyperglycemia or glucose detection using ECG has been constrained by challenges related to generalization and scalability, primarily due to using all subjects&rsquo; ECG in training without considering unseen subjects as a critical factor for developing methods with effective generalization. We designed a deep neural network model capable of identifying significant features across various spatial locations and examining the interdependencies among different features within each <b>convolutional</b> layer. To expedite processing speed, we segment the ECG of each user to isolate one heartbeat or one cycle of the ECG. Our model was trained using data from 727 subjects, while 168 were used for validation. The testing phase involved 224 unseen subjects, with a dataset consisting of 9,000 segments. The result indicates that the proposed algorithm effectively detects hyperglycemia with a 91.60% area under the curve (AUC), 81.05% sensitivity, and 85.54% specificity.</p></p class="citation"></blockquote><h2 id=csro-16>cs.RO (16)</h2><h3 id=116--209323-vanp-learning-where-to-see-for-navigation-with-self-supervised-vision-action-pre-training-mohammad-nazeri-et-al-2024>(1/16 | 209/323) VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training (Mohammad Nazeri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Nazeri, Junzhe Wang, Amirreza Payandeh, Xuesu Xiao. (2024)<br><strong>VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training</strong><br><button class=copy-to-clipboard title="VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Mutual Information, Self-supervised Learning, Self-supervised Learning, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08109v1.pdf filename=2403.08109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans excel at efficiently navigating through crowds without collision by focusing on specific visual regions relevant to navigation. However, most robotic visual navigation methods rely on deep learning models pre-trained on vision tasks, which prioritize salient objects &ndash; not necessarily relevant to navigation and potentially misleading. Alternative approaches train specialized navigation models from scratch, requiring significant computation. On the other hand, <b>self-supervised</b> <b>learning</b> has revolutionized computer vision and natural language processing, but its application to robotic navigation remains underexplored due to the difficulty of defining effective self-supervision signals. Motivated by these observations, in this work, we propose a <b>Self-Supervised</b> <b>Vision-Action</b> Model for Visual Navigation Pre-Training (VANP). Instead of detecting salient objects that are beneficial for tasks such as classification or detection, VANP learns to focus only on specific visual regions that are relevant to the navigation task. To achieve this, VANP uses a history of visual observations, future actions, and a goal image for self-supervision, and embeds them using two small <b>Transformer</b> Encoders. Then, VANP maximizes the information between the embeddings by using a <b>mutual</b> <b>information</b> maximization objective function. We demonstrate that most VANP-extracted features match with human navigation intuition. VANP achieves comparable performance as models learned end-to-end with half the training time and models trained on a large-scale, fully <b>supervised</b> dataset, i.e., ImageNet, with only 0.08% data.</p></p class="citation"></blockquote><h3 id=216--210323-deligrasp-inferring-object-mass-friction-and-compliance-with-llms-for-adaptive-and-minimally-deforming-grasp-policies-william-xie-et-al-2024>(2/16 | 210/323) DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies (William Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Xie, Jensen Lavering, Nikolaus Correll. (2024)<br><strong>DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies</strong><br><button class=copy-to-clipboard title="DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07832v1.pdf filename=2403.07832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can provide rich physical descriptions of most worldly objects, allowing robots to achieve more informed and capable grasping. We leverage <b>LLMs&rsquo;</b> common sense physical <b>reasoning</b> and code-writing abilities to infer an object&rsquo;s physical characteristics&ndash;mass $m$, friction coefficient $\mu$, and spring constant $k$&ndash;from a semantic description, and then translate those characteristics into an executable adaptive grasp policy. Using a current-controllable, two-finger gripper with a built-in depth camera, we demonstrate that <b>LLM-generated,</b> physically-grounded grasp policies outperform traditional grasp policies on a custom <b>benchmark</b> of 12 delicate and deformable items including food, produce, toys, and other everyday items, spanning two orders of magnitude in mass and required pick-up force. We also demonstrate how compliance feedback from DeliGrasp policies can aid in downstream tasks such as measuring produce ripeness. Our code and videos are available at: <a href=https://deligrasp.github.io>https://deligrasp.github.io</a></p></p class="citation"></blockquote><h3 id=316--211323-drplanner-diagnosis-and-repair-of-motion-planners-using-large-language-models-yuanfei-lin-et-al-2024>(3/16 | 211/323) DrPlanner: Diagnosis and Repair of Motion Planners Using Large Language Models (Yuanfei Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanfei Lin, Chenran Li, Mingyu Ding, Masayoshi Tomizuka, Wei Zhan, Matthias Althoff. (2024)<br><strong>DrPlanner: Diagnosis and Repair of Motion Planners Using Large Language Models</strong><br><button class=copy-to-clipboard title="DrPlanner: Diagnosis and Repair of Motion Planners Using Large Language Models" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-PL, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07470v1.pdf filename=2403.07470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motion planners are essential for the safe operation of automated vehicles across various scenarios. However, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive. To tackle the aforementioned issues, we present DrPlanner, the first framework designed to automatically diagnose and repair motion planners using <b>large</b> <b>language</b> <b>models.</b> Initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages. Leveraging the profound capabilities of <b>large</b> <b>language</b> <b>models</b> in addressing <b>reasoning</b> challenges, our framework returns repaired planners with detailed diagnostic descriptions. Furthermore, the framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes. Our approach is validated using search-based motion planners; experimental results highlight the need of demonstrations in the <b>prompt</b> and the ability of our framework in identifying and rectifying elusive issues effectively.</p></p class="citation"></blockquote><h3 id=416--212323-toward-an-analytic-theory-of-intrinsic-robustness-for-dexterous-grasping-albert-h-li-et-al-2024>(4/16 | 212/323) Toward An Analytic Theory of Intrinsic Robustness for Dexterous Grasping (Albert H. Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Albert H. Li, Preston Culbertson, Aaron D. Ames. (2024)<br><strong>Toward An Analytic Theory of Intrinsic Robustness for Dexterous Grasping</strong><br><button class=copy-to-clipboard title="Toward An Analytic Theory of Intrinsic Robustness for Dexterous Grasping" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07249v1.pdf filename=2403.07249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional approaches to grasp planning require perfect knowledge of an object&rsquo;s pose and <b>geometry.</b> Uncertainties in these quantities induce uncertainties in the quality of planned grasps, which can lead to failure. Classically, grasp robustness refers to the ability to resist external disturbances after grasping an object. In contrast, this work studies robustness to intrinsic sources of uncertainty like object pose or <b>geometry</b> affecting grasp planning before execution. To do so, we develop a novel analytic theory of grasping that reasons about this intrinsic robustness by characterizing the effect of friction cone uncertainty on a grasp&rsquo;s force closure status. As a result, we show the Ferrari-Canny metric &ndash; which measures the size of external disturbances a grasp can reject &ndash; bounds the friction cone uncertainty a grasp can tolerate, and thus also measures intrinsic robustness. In tandem, we show that the recently proposed min-weight metric lower bounds the Ferrari-Canny metric, justifying it as a computationally-efficient, uncertainty-aware alternative. We validate this theory on hardware experiments versus a competitive baseline and demonstrate superior performance. Finally, we use our theory to develop an analytic notion of probabilistic force closure, which we show in <b>simulation</b> generates grasps that can incorporate uncertainty distributions over an object&rsquo;s <b>geometry.</b></p></p class="citation"></blockquote><h3 id=516--213323-multi-task-manipulation-policy-modeling-with-visuomotor-latent-diffusion-wenhui-tan-et-al-2024>(5/16 | 213/323) Multi-task Manipulation Policy Modeling with Visuomotor Latent Diffusion (Wenhui Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhui Tan, Bei Liu, Junbo Zhang, Ruihua Song, Jianlong Fu. (2024)<br><strong>Multi-task Manipulation Policy Modeling with Visuomotor Latent Diffusion</strong><br><button class=copy-to-clipboard title="Multi-task Manipulation Policy Modeling with Visuomotor Latent Diffusion" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Autoencoder, Benchmarking, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07312v1.pdf filename=2403.07312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling a generalized visuomotor policy has been a longstanding challenge for both computer vision and robotics communities. Existing approaches often fail to efficiently leverage cross-dataset resources or rely on heavy <b>Vision-Language</b> models, which require substantial computational resources, thereby limiting their multi-task performance and application potential. In this paper, we introduce a novel paradigm that effectively utilizes latent modeling of manipulation skills and an efficient visuomotor latent diffusion policy, which enhances the utilizing of existing cross-embodiment and cross-environment datasets, thereby improving multi-task capabilities. Our methodology consists of two decoupled phases: action modeling and policy modeling. Firstly, we introduce a task-agnostic, embodiment-aware trajectory latent <b>autoencoder</b> for unified action skills modeling. This step condenses action data and observation into a condensed latent space, effectively benefiting from large-scale cross-datasets. Secondly, we propose to use a visuomotor latent diffusion policy that recovers target skill latent from noises for effective task execution. We conducted extensive experiments on two widely used <b>benchmarks,</b> and the results demonstrate the effectiveness of our proposed paradigms on multi-tasking and pre-training. Code is available at <a href=https://github.com/AlbertTan404/RoLD>https://github.com/AlbertTan404/RoLD</a>.</p></p class="citation"></blockquote><h3 id=616--214323-telemoma-a-modular-and-versatile-teleoperation-system-for-mobile-manipulation-shivin-dass-et-al-2024>(6/16 | 214/323) TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation (Shivin Dass et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivin Dass, Wensi Ai, Yuqian Jiang, Samik Singh, Jiaheng Hu, Ruohan Zhang, Peter Stone, Ben Abbatematteo, Roberto Martin-Martin. (2024)<br><strong>TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation</strong><br><button class=copy-to-clipboard title="TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07869v1.pdf filename=2403.07869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in <b>simulation</b> and the real world. We demonstrate the quality of the demonstrations collected with TeleMoMa by training imitation learning policies for mobile manipulation tasks involving synchronized whole-body motion. Finally, we also show that TeleMoMa&rsquo;s teleoperation channel enables teleoperation on site, looking at the robot, or remote, sending commands and observations through a computer network, and perform user studies to evaluate how easy it is for novice users to learn to collect demonstrations with different combinations of human interfaces enabled by our system. We hope TeleMoMa becomes a helpful tool for the community enabling researchers to collect whole-body mobile manipulation demonstrations. For more information and video results, <a href=https://robin-lab.cs.utexas.edu/telemoma-web>https://robin-lab.cs.utexas.edu/telemoma-web</a>.</p></p class="citation"></blockquote><h3 id=716--215323-the-virtues-of-laziness-multi-query-kinodynamic-motion-planning-with-lazy-methods-anuj-pasricha-et-al-2024>(7/16 | 215/323) The Virtues of Laziness: Multi-Query Kinodynamic Motion Planning with Lazy Methods (Anuj Pasricha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anuj Pasricha, Alessandro Roncone. (2024)<br><strong>The Virtues of Laziness: Multi-Query Kinodynamic Motion Planning with Lazy Methods</strong><br><button class=copy-to-clipboard title="The Virtues of Laziness: Multi-Query Kinodynamic Motion Planning with Lazy Methods" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07867v1.pdf filename=2403.07867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce LazyBoE, a multi-query method for kinodynamic motion planning with forward propagation. This algorithm allows for the simultaneous exploration of a robot&rsquo;s state and control spaces, thereby enabling a wider suite of dynamic tasks in real-world applications. Our contributions are three-fold: i) a method for discretizing the state and control spaces to amortize planning times across multiple queries; ii) lazy approaches to collision checking and propagation of control sequences that decrease the cost of physics-based <b>simulation;</b> and iii) LazyBoE, a robust kinodynamic planner that leverages these two contributions to produce dynamically-feasible trajectories. The proposed framework not only reduces planning time but also increases success rate in comparison to previous approaches.</p></p class="citation"></blockquote><h3 id=816--216323-a-framework-for-controlling-multiple-industrial-robots-using-mobile-applications-daniela-alvarado-et-al-2024>(8/16 | 216/323) A Framework for Controlling Multiple Industrial Robots using Mobile Applications (Daniela Alvarado et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniela Alvarado, Dr. Seemal Asif. (2024)<br><strong>A Framework for Controlling Multiple Industrial Robots using Mobile Applications</strong><br><button class=copy-to-clipboard title="A Framework for Controlling Multiple Industrial Robots using Mobile Applications" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07639v1.pdf filename=2403.07639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: Over the last few decades, the development of the hardware and software has enabled the application of advanced systems. In the robotics field, the UI design is an intriguing area to be explored due to the creation of devices with a wide range of functionalities in a reduced size. Moreover, the idea of using the same UI to control several systems arouses a great interest considering that this involves less learning effort and time for the users. Therefore, this paper will present a mobile application to control two industrial robots with four modes of operation. Design/methodology/approach: The smartphone was selected to be the interface due to its wide range of capabilities and the MIT Inventor App was used to create the application, whose environment is supported by Android smartphones. For the validation, ROS was used since it is a fundamental framework utilised in industrial robotics and the Arduino Uno was used to establish the data transmission between the smartphone and the board NVIDIA Jetson TX2. In MIT Inventor App, the graphical interface was created to visualize the options available in the app whereas two scripts in python were programmed to perform the <b>simulations</b> in ROS and carry out the tests. Findings: The results indicated that the use of the sliders to control the robots is more favourable than the Orientation Sensor due to the sensibility of the sensor and human limitations to hold the smartphone perfectly still. Another important finding was the limitations of the autonomous mode, in which the robot grabs an object. In this case, the configuration of the Kinect camera and the controllers has a significant impact on the success of the <b>simulation.</b> Finally, it was observed that the delay was appropriate despite the use of the Arduino UNO to transfer the data between the Smartphone and the Nvidia Jetson TX2.</p></p class="citation"></blockquote><h3 id=916--217323-online-adaptation-of-sampling-based-motion-planning-with-inaccurate-models-marco-faroni-et-al-2024>(9/16 | 217/323) Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models (Marco Faroni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Faroni, Dmitry Berenson. (2024)<br><strong>Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models</strong><br><button class=copy-to-clipboard title="Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07638v1.pdf filename=2403.07638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic manipulation relies on analytical or learned models to simulate the system dynamics. These models are often inaccurate and based on offline information, so that the robot planner is unable to cope with mismatches between the expected and the actual behavior of the system (e.g., the presence of an unexpected obstacle). In these situations, the robot should use information gathered online to correct its planning strategy and adapt to the actual system response. We propose a sampling-based motion planning approach that uses an estimate of the model error and online observations to correct the planning strategy at each new replanning. Our approach adapts the cost function and the sampling bias of a kinodynamic motion planner when the outcome of the executed transitions is different from the expected one (e.g., when the robot unexpectedly collides with an obstacle) so that future trajectories will avoid unreliable motions. To infer the properties of a new transition, we introduce the notion of context-awareness, i.e., we store local environment information for each executed transition and avoid new transitions with context similar to previous unreliable ones. This is helpful for leveraging online information even if the simulated transitions are far (in the state-and-action space) from the executed ones. <b>Simulation</b> and experimental results show that the proposed approach increases the success rate in execution and reduces the number of replannings needed to reach the goal.</p></p class="citation"></blockquote><h3 id=1016--218323-efficient-global-navigational-planning-in-3d-structures-based-on-point-cloud-tomography-bowen-yang-et-al-2024>(10/16 | 218/323) Efficient Global Navigational Planning in 3D Structures based on Point Cloud Tomography (Bowen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Yang, Jie Cheng, Bohuan Xue, Jianhao Jiao, Ming Liu. (2024)<br><strong>Efficient Global Navigational Planning in 3D Structures based on Point Cloud Tomography</strong><br><button class=copy-to-clipboard title="Efficient Global Navigational Planning in 3D Structures based on Point Cloud Tomography" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07631v1.pdf filename=2403.07631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Navigation in complex 3D scenarios requires appropriate environment representation for efficient scene understanding and trajectory generation. We propose a highly efficient and extensible global navigation framework based on a tomographic understanding of the environment to navigate ground robots in multi-layer structures. Our approach generates tomogram slices using the point cloud map to encode the geometric structure as ground and ceiling elevations. Then it evaluates the scene traversability considering the robot&rsquo;s motion capabilities. Both the tomogram construction and the scene evaluation are accelerated through parallel computation. Our approach further alleviates the trajectory generation complexity compared with planning in 3D spaces directly. It generates 3D trajectories by searching through multiple tomogram slices and separately adjusts the robot height to avoid overhangs. We evaluate our framework in various <b>simulation</b> scenarios and further test it in the real world on a quadrupedal robot. Our approach reduces the scene evaluation time by 3 orders of magnitude and improves the path planning speed by 3 times compared with existing approaches, demonstrating highly efficient global navigation in various complex 3D environments. The code is available at: <a href=https://github.com/byangw/PCT_planner>https://github.com/byangw/PCT_planner</a>.</p></p class="citation"></blockquote><h3 id=1116--219323-learning-generalizable-feature-fields-for-mobile-manipulation-ri-zhao-qiu-et-al-2024>(11/16 | 219/323) Learning Generalizable Feature Fields for Mobile Manipulation (Ri-Zhao Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ri-Zhao Qiu, Yafei Hu, Ge Yang, Yuchen Song, Yang Fu, Jianglong Ye, Jiteng Mu, Ruihan Yang, Nikolay Atanasov, Sebastian Scherer, Xiaolong Wang. (2024)<br><strong>Learning Generalizable Feature Fields for Mobile Manipulation</strong><br><button class=copy-to-clipboard title="Learning Generalizable Feature Fields for Mobile Manipulation" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 15<br>Keywords: Geometry, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07563v1.pdf filename=2403.07563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An open problem in mobile manipulation is how to represent objects and scenes in a unified manner, so that robots can use it both for navigating in the environment and manipulating objects. The latter requires capturing intricate <b>geometry</b> while understanding fine-grained semantics, whereas the former involves capturing the complexity inherit to an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature <b>distillation.</b> We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF&rsquo;s ability to generalize to open-set objects as well as running time, when performing open-vocabulary mobile manipulation in dynamic scenes.</p></p class="citation"></blockquote><h3 id=1216--220323-dexcap-scalable-and-portable-mocap-data-collection-system-for-dexterous-manipulation-chen-wang-et-al-2024>(12/16 | 220/323) DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation (Chen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C. Karen Liu. (2024)<br><strong>DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</strong><br><button class=copy-to-clipboard title="DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07788v1.pdf filename=2403.07788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an optional <b>human-in-the-loop</b> correction mechanism to refine and further improve robot performance. Through extensive evaluation across six dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system&rsquo;s capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods for dexterous manipulation. More details can be found at <a href=https://dex-cap.github.io>https://dex-cap.github.io</a></p></p class="citation"></blockquote><h3 id=1316--221323-3d-uncertain-distance-field-mapping-using-gmm-and-gp-qianqian-zou-et-al-2024>(13/16 | 221/323) 3D Uncertain Distance Field Mapping using GMM and GP (Qianqian Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianqian Zou, Monika Sester. (2024)<br><strong>3D Uncertain Distance Field Mapping using GMM and GP</strong><br><button class=copy-to-clipboard title="3D Uncertain Distance Field Mapping using GMM and GP" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07223v1.pdf filename=2403.07223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we address the challenge of constructing continuous three-dimensional (3D) models that accurately represent uncertain surfaces, derived from noisy and incomplete LiDAR scanning data. Building upon our prior work, which utilized the <b>Gaussian</b> <b>Process</b> (GP) and <b>Gaussian</b> <b>Mixture</b> Model (GMM) for structured building models, we introduce a more generalized approach tailored for complex surfaces in urban scenes, where four-dimensional (4D) GMM Regression and GP with derivative observations are applied. A Hierarchical GMM (HGMM) is employed to optimize the number of GMM components and speed up the GMM training. With the prior map obtained from HGMM, GP inference is followed for the refinement of the final map. Our approach models the implicit surface of the geo-object and enables the inference of the regions that are not completely covered by measurements. The integration of GMM and GP yields well-calibrated uncertainty estimates alongside the surface model, enhancing both accuracy and reliability. The proposed method is evaluated on the real data collected by a mobile mapping system. Compared to the performance in mapping accuracy and uncertainty quantification of other methods such as <b>Gaussian</b> <b>Process</b> Implicit Surface map (GPIS) and log-Gaussian Process Implicit Surface map (Log-GPIS), the proposed method achieves lower RMSEs, higher log-likelihood values and fewer computational costs for the evaluated datasets.</p></p class="citation"></blockquote><h3 id=1416--222323-tractable-joint-prediction-and-planning-over-discrete-behavior-modes-for-urban-driving-adam-villaflor-et-al-2024>(14/16 | 222/323) Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving (Adam Villaflor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Villaflor, Brian Yang, Huangyuan Su, Katerina Fragkiadaki, John Dolan, Jeff Schneider. (2024)<br><strong>Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving</strong><br><button class=copy-to-clipboard title="Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07232v1.pdf filename=2403.07232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Significant progress has been made in training <b>multimodal</b> trajectory forecasting models for autonomous driving. However, effectively integrating these models with downstream planners and model-based control approaches is still an open problem. Although these models have conventionally been evaluated for open-loop prediction, we show that they can be used to parameterize autoregressive closed-loop models without retraining. We consider recent trajectory prediction approaches which leverage learned anchor embeddings to predict multiple trajectories, finding that these anchor embeddings can parameterize discrete and distinct modes representing high-level driving behaviors. We propose to perform fully reactive closed-loop planning over these discrete latent modes, allowing us to tractably model the causal interactions between agents at each step. We validate our approach on a suite of more dynamic merging scenarios, finding that our approach avoids the $\textit{frozen robot problem}$ which is pervasive in conventional planners. Our approach also outperforms the previous state-of-the-art in CARLA on challenging dense traffic scenarios when evaluated at realistic speeds.</p></p class="citation"></blockquote><h3 id=1516--223323-task-and-motion-planning-in-hierarchical-3d-scene-graphs-aaron-ray-et-al-2024>(15/16 | 223/323) Task and Motion Planning in Hierarchical 3D Scene Graphs (Aaron Ray et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaron Ray, Christopher Bradley, Luca Carlone, Nicholas Roy. (2024)<br><strong>Task and Motion Planning in Hierarchical 3D Scene Graphs</strong><br><button class=copy-to-clipboard title="Task and Motion Planning in Hierarchical 3D Scene Graphs" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: 68T40, 68T20, I-2-9; I-2-4; I-2-8, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08094v1.pdf filename=2403.08094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work in the construction of 3D scene <b>graphs</b> has enabled mobile robots to build large-scale hybrid metric-semantic hierarchical representations of the world. These detailed models contain information that is useful for planning, however how to derive a planning domain from a 3D scene <b>graph</b> that enables efficient computation of executable plans is an open question. In this work, we present a novel approach for defining and solving Task and Motion Planning problems in large-scale environments using hierarchical 3D scene <b>graphs.</b> We identify a method for building sparse problem domains which enable scaling to large scenes, and propose a technique for incrementally adding objects to that domain during planning time to avoid wasting computation on irrelevant elements of the scene <b>graph.</b> We test our approach in two hand crafted domains as well as two scene <b>graphs</b> built from perception, including one constructed from the KITTI dataset. A video supplement is available at <a href=https://youtu.be/63xuCCaN0I4>https://youtu.be/63xuCCaN0I4</a>.</p></p class="citation"></blockquote><h3 id=1616--224323-mps-a-new-method-for-selecting-the-stable-closed-loop-equilibrium-attitude-error-quaternion-of-a-uav-during-flight-francisco-m-f-r-gonçalves-et-al-2024>(16/16 | 224/323) MPS: A New Method for Selecting the Stable Closed-Loop Equilibrium Attitude-Error Quaternion of a UAV During Flight (Francisco M. F. R. Gonçalves et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francisco M. F. R. Gonçalves, Ryan M. Bena, Konstantin I. Matveev, Néstor O. Pérez-Arancibia. (2024)<br><strong>MPS: A New Method for Selecting the Stable Closed-Loop Equilibrium Attitude-Error Quaternion of a UAV During Flight</strong><br><button class=copy-to-clipboard title="MPS: A New Method for Selecting the Stable Closed-Loop Equilibrium Attitude-Error Quaternion of a UAV During Flight" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07269v1.pdf filename=2403.07269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present model predictive selection (MPS), a new method for selecting the stable closed-loop (CL) equilibrium attitude-error quaternion (AEQ) of an uncrewed aerial vehicle (UAV) during the execution of high-speed yaw maneuvers. In this approach, we minimize the cost of yawing measured with a performance figure of merit (PFM) that takes into account both the aerodynamic-torque control input and attitude-error state of the UAV. Specifically, this method uses a control law with a term whose sign is dynamically switched in real time to select, between two options, the torque associated with the lesser cost of rotation as predicted by a dynamical model of the UAV derived from first principles. This problem is relevant because the selection of the stable CL equilibrium AEQ significantly impacts the performance of a UAV during high-speed rotational flight, from both the power and control-error perspectives. To test and demonstrate the functionality and performance of the proposed method, we present data collected during one hundred real-time high-speed yaw-tracking flight experiments. These results highlight the superior capabilities of the proposed MPS-based scheme when compared to a <b>benchmark</b> controller commonly used in aerial robotics, as the PFM used to quantify the cost of flight is reduced by 60.30 %, on average. To our best knowledge, these are the first flight-test results that thoroughly demonstrate, evaluate, and compare the performance of a real-time controller capable of selecting the stable CL equilibrium AEQ during operation.</p></p class="citation"></blockquote><h2 id=eessiv-9>eess.IV (9)</h2><h3 id=19--225323-equipping-computational-pathology-systems-with-artifact-processing-pipelines-a-showcase-for-computation-and-performance-trade-offs-neel-kanwal-et-al-2024>(1/9 | 225/323) Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs (Neel Kanwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neel Kanwal, Farbod Khoraminia, Umay Kiraz, Andres Mosquera-Zamudio, Carlos Monteagudo, Emiel A. M. Janssen, Tahlita C. M. Zuiverloon, Chunmig Rong, Kjersti Engan. (2024)<br><strong>Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs</strong><br><button class=copy-to-clipboard title="Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07743v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07743v2.pdf filename=2403.07743v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probability distribution to improve the sensitivity of the MoE. We developed DL pipelines using two MoEs and two multiclass models of state-of-the-art deep <b>convolutional</b> <b>neural</b> <b>networks</b> (DCNNs) and <b>vision</b> <b>transformers</b> (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed simpler multiclass models and were tested on datasets from different hospitals and cancer types, where MoE using DCNNs yielded the best results. The proposed MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining less computational cost for inference than MoE using ViTs. This best performance of MoEs comes with relatively higher computational trade-offs than multiclass models. The proposed artifact detection pipeline will not only ensure reliable CPATH predictions but may also provide quality control.</p></p class="citation"></blockquote><h3 id=29--226323-dalsa-domain-adaptation-for-supervised-learning-from-sparsely-annotated-mr-images-michael-götz-et-al-2024>(2/9 | 226/323) DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images (Michael Götz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Götz, Christian Weber, Franciszek Binczyk, Joanna Polanska, Rafal Tarnawski, Barbara Bobek-Billewicz, Ullrich Köthe, Jens Kleesiek, Bram Stieltjes, Klaus H. Maier-Hein. (2024)<br><strong>DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images</strong><br><button class=copy-to-clipboard title="DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 43<br>Keywords: Multi-modal, Supervised Learning, Supervised Learning, Transfer Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07434v1.pdf filename=2403.07434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new method that employs <b>transfer</b> <b>learning</b> techniques to effectively correct sampling selection errors introduced by sparse annotations during <b>supervised</b> <b>learning</b> for automated tumor segmentation. The practicality of current learning-based automated tissue classification approaches is severely impeded by their dependency on manually segmented training databases that need to be recreated for each scenario of application, site, or acquisition setup. The comprehensive annotation of reference datasets can be highly labor-intensive, complex, and error-prone. The proposed method derives high-quality classifiers for the different tissue classes from sparse and unambiguous annotations and employs <b>domain</b> <b>adaptation</b> techniques for effectively correcting sampling selection errors introduced by the sparse sampling. The new approach is validated on labeled, <b>multi-modal</b> MR images of 19 patients with malignant gliomas and by comparative analysis on the BraTS 2013 challenge data sets. Compared to training on fully labeled data, we reduced the time for labeling and training by a factor greater than 70 and 180 respectively without sacrificing accuracy. This dramatically eases the establishment and constant extension of large annotated databases in various scenarios and imaging setups and thus represents an important step towards practical applicability of learning-based approaches in tissue classification.</p></p class="citation"></blockquote><h3 id=39--227323-samda-leveraging-sam-on-few-shot-domain-adaptation-for-electronic-microscopy-segmentation-yiran-wang-et-al-2024>(3/9 | 227/323) SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation (Yiran Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiran Wang, Li Xiao. (2024)<br><strong>SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation</strong><br><button class=copy-to-clipboard title="SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Few-shot, Fine-tuning, Foundation Model, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07951v1.pdf filename=2403.07951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It has been shown that traditional deep learning methods for electronic microscopy segmentation usually suffer from low transferability when samples and annotations are limited, while large-scale vision <b>foundation</b> <b>models</b> are more robust when transferring between different <b>domains</b> <b>but</b> facing sub-optimal improvement under <b>fine-tuning.</b> In this work, we present a new <b>few-shot</b> <b>domain</b> <b>adaptation</b> framework SAMDA, which combines the Segment Anything Model(SAM) with nnUNet in the embedding space to achieve high transferability and accuracy. Specifically, we choose the Unet-based network as the &ldquo;expert&rdquo; component to learn segmentation features efficiently and design a SAM-based adaptation module as the &ldquo;generic&rdquo; component for <b>domain</b> <b>transfer.</b> By amalgamating the &ldquo;generic&rdquo; and &ldquo;expert&rdquo; components, we mitigate the modality imbalance in the complex pre-training knowledge inherent to large-scale Vision <b>Foundation</b> <b>models</b> and the challenge of transferability inherent to traditional neural networks. The effectiveness of our model is evaluated on two electron microscopic image datasets with different modalities for mitochondria segmentation, which improves the dice coefficient on the target <b>domain</b> <b>by</b> 6.7%. Also, the SAM-based adaptor performs significantly better with only a single annotated image than the 10-shot <b>domain</b> <b>adaptation</b> on nnUNet. We further verify our model on four MRI datasets from different sources to prove its generalization ability.</p></p class="citation"></blockquote><h3 id=49--228323-ct-evaluation-of-2d-and-3d-holistic-deep-learning-methods-for-the-volumetric-segmentation-of-airway-lesions-amel-imene-hadj-bouzid-et-al-2024>(4/9 | 228/323) CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions (Amel Imene Hadj Bouzid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amel Imene Hadj Bouzid, Baudouin Denis de Senneville, Fabien Baldacci, Pascal Desbarats, Patrick Berger, Ilyes Benlala, Gaël Dournes. (2024)<br><strong>CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions</strong><br><button class=copy-to-clipboard title="CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08042v1.pdf filename=2403.08042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research embarked on a comparative exploration of the holistic segmentation capabilities of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized data from two CF reference centers, covering five major CF structural changes. Initially, it compared the 2D and 3D models, highlighting the 3D model&rsquo;s superior capability in capturing complex features like mucus plugs and consolidations. To improve the 2D model&rsquo;s performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model&rsquo;s performance. The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings. Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models&rsquo; interpretability and reliability, providing valuable insights for their clinical application.</p></p class="citation"></blockquote><h3 id=59--229323-intra-video-positive-pairs-in-self-supervised-learning-for-ultrasound-blake-vanberlo-et-al-2024>(5/9 | 229/323) Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound (Blake VanBerlo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Blake VanBerlo, Alexander Wong, Jesse Hoey, Robert Arntfield. (2024)<br><strong>Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound</strong><br><button class=copy-to-clipboard title="Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: I-2-10; I-4-9; J-3, cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07715v1.pdf filename=2403.07715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) is one strategy for addressing the paucity of labelled data in medical imaging by learning representations from unlabelled images. <b>Contrastive</b> <b>and</b> non-contrastive SSL methods produce learned representations that are similar for pairs of related images. Such pairs are commonly constructed by randomly distorting the same image twice. The videographic nature of ultrasound offers flexibility for defining the similarity relationship between pairs of images. In this study, we investigated the effect of utilizing proximal, distinct images from the same B-mode ultrasound video as pairs for SSL. Additionally, we introduced a sample weighting scheme that increases the weight of closer image pairs and demonstrated how it can be integrated into SSL objectives. Named Intra-Video Positive Pairs (IVPP), the method surpassed previous ultrasound-specific <b>contrastive</b> <b>learning</b> methods&rsquo; average test accuracy on COVID-19 classification with the POCUS dataset by $\ge 1.3%$. Detailed investigations of IVPP&rsquo;s hyperparameters revealed that some combinations of IVPP hyperparameters can lead to improved or worsened performance, depending on the downstream task. Guidelines for practitioners were synthesized based on the results, such as the merit of IVPP with task-specific hyperparameters, and the improved performance of <b>contrastive</b> <b>methods</b> for ultrasound compared to non-contrastive counterparts.</p></p class="citation"></blockquote><h3 id=69--230323-learning-correction-errors-via-frequency-self-attention-for-blind-image-super-resolution-haochen-sun-et-al-2024>(6/9 | 230/323) Learning Correction Errors via Frequency-Self Attention for Blind Image Super-Resolution (Haochen Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochen Sun, Yan Yuan, Lijuan Su, Haotian Shao. (2024)<br><strong>Learning Correction Errors via Frequency-Self Attention for Blind Image Super-Resolution</strong><br><button class=copy-to-clipboard title="Learning Correction Errors via Frequency-Self Attention for Blind Image Super-Resolution" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07390v1.pdf filename=2403.07390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous approaches for blind image super-resolution (SR) have relied on degradation estimation to restore high-resolution (HR) images from their low-resolution (LR) counterparts. However, accurate degradation estimation poses significant challenges. The SR model&rsquo;s incompatibility with degradation estimation methods, particularly the Correction Filter, may significantly impair performance as a result of correction errors. In this paper, we introduce a novel blind SR approach that focuses on Learning Correction Errors (LCE). Our method employs a lightweight Corrector to obtain a corrected low-resolution (CLR) image. Subsequently, within an SR network, we jointly optimize SR performance by utilizing both the original LR image and the frequency learning of the CLR image. Additionally, we propose a new Frequency-Self Attention block (FSAB) that enhances the global information utilization ability of <b>Transformer.</b> This block integrates both <b>self-attention</b> and frequency spatial attention mechanisms. Extensive ablation and comparison experiments conducted across various settings demonstrate the superiority of our method in terms of visual quality and accuracy. Our approach effectively addresses the challenges associated with degradation estimation and correction errors, paving the way for more accurate blind image SR.</p></p class="citation"></blockquote><h3 id=79--231323-dynamic-u-net-adaptively-calibrate-features-for-abdominal-multi-organ-segmentation-jin-yang-et-al-2024>(7/9 | 231/323) Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ Segmentation (Jin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Yang, Daniel S. Marcus, Aristeidis Sotiras. (2024)<br><strong>Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ Segmentation</strong><br><button class=copy-to-clipboard title="Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ Segmentation" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 13<br>Keywords: Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07303v1.pdf filename=2403.07303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>U-Net has been widely used for segmenting abdominal organs, achieving promising performance. However, when it is used for multi-organ segmentation, first, it may be limited in exploiting global long-range contextual information due to the implementation of standard <b>convolutions.</b> Second, the use of spatial-wise downsampling (e.g., max pooling or strided <b>convolutions)</b> in the encoding path may lead to the loss of deformable or discriminative details. Third, features upsampled from the higher level are concatenated with those that persevered via skip connections. However, repeated downsampling and upsampling operations lead to misalignments between them and their concatenation degrades segmentation performance. To address these limitations, we propose Dynamically Calibrated <b>Convolution</b> (DCC), Dynamically Calibrated Downsampling (DCD), and Dynamically Calibrated Upsampling (DCU) modules, respectively. The DCC module can utilize global inter-dependencies between spatial and channel features to calibrate these features adaptively. The DCD module enables networks to adaptively preserve deformable or discriminative features during downsampling. The DCU module can dynamically align and calibrate upsampled features to eliminate misalignments before concatenations. We integrated the proposed modules into a standard U-Net, resulting in a new architecture, termed Dynamic U-Net. This architectural design enables U-Net to dynamically adjust features for different organs. We evaluated Dynamic U-Net in two abdominal multi-organ segmentation <b>benchmarks.</b> Dynamic U-Net achieved statistically improved segmentation accuracy compared with standard U-Net. Our code is available at <a href=https://github.com/sotiraslab/DynamicUNet>https://github.com/sotiraslab/DynamicUNet</a>.</p></p class="citation"></blockquote><h3 id=89--232323-aedes-aegypti-egg-counting-with-neural-networks-for-object-detection-micheli-nayara-de-oliveira-vicente-et-al-2024>(8/9 | 232/323) Aedes aegypti Egg Counting with Neural Networks for Object Detection (Micheli Nayara de Oliveira Vicente et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Micheli Nayara de Oliveira Vicente, Gabriel Toshio Hirokawa Higa, João Vitor de Andrade Porto, Higor Henrique, Picoli Nucci, Asser Botelho Santana, Karla Rejane de Andrade Porto, Antonia Railda Roel, Hemerson Pistori. (2024)<br><strong>Aedes aegypti Egg Counting with Neural Networks for Object Detection</strong><br><button class=copy-to-clipboard title="Aedes aegypti Egg Counting with Neural Networks for Object Detection" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08016v1.pdf filename=2403.08016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aedes aegypti is still one of the main concerns when it comes to disease vectors. Among the many ways to deal with it, there are important protocols that make use of egg numbers in ovitraps to calculate indices, such as the LIRAa and the Breteau Index, which can provide information on predictable outbursts and epidemics. Also, there are many research lines that require egg numbers, specially when mass production of mosquitoes is needed. Egg counting is a laborious and error-prone task that can be automated via computer vision-based techniques, specially deep learning-based counting with <b>object</b> <b>detection.</b> In this work, we propose a new dataset comprising field and laboratory eggs, along with test results of three neural networks applied to the task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox.</p></p class="citation"></blockquote><h3 id=99--233323-guidegen-a-text-guided-framework-for-joint-ct-volume-and-anatomical-structure-generation-linrui-dai-et-al-2024>(9/9 | 233/323) GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation (Linrui Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linrui Dai, Rongzhao Zhang, Zhongzhen Huang, Xiaofan Zhang. (2024)<br><strong>GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation</strong><br><button class=copy-to-clipboard title="GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07247v1.pdf filename=2403.07247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating. This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field. As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints. This paper explores this possibility and presents \textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text <b>prompt.</b> Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks. Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corresponding mask slice to incorporate both style information and anatomical guidance. This pipeline guarantees high fidelity and variability as well as exact alignment between generated CT volumes and tissue masks. Both qualitative and quantitative experiments on 3D abdominal CTs demonstrate a high performance of our proposed pipeline, thereby proving our method can serve as a dataset generator and provide potential benefits to downstream tasks. It is hoped that our work will offer a promising solution on the multimodality generation of CT and its anatomical mask. Our source code is publicly available at <a href=https://github.com/OvO1111/JointImageGeneration>https://github.com/OvO1111/JointImageGeneration</a>.</p></p class="citation"></blockquote><h2 id=csse-7>cs.SE (7)</h2><h3 id=17--234323-livecodebench-holistic-and-contamination-free-evaluation-of-large-language-models-for-code-naman-jain-et-al-2024>(1/7 | 234/323) LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code (Naman Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica. (2024)<br><strong>LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code</strong><br><button class=copy-to-clipboard title="LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 43<br>Keywords: Benchmarking, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07974v1.pdf filename=2403.07974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> applied to <b>code-related</b> <b>applications</b> have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved <b>LLMs</b> are developed, existing evaluation <b>benchmarks</b> (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of <b>LLMs</b> for <b>code,</b> <b>which</b> continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our <b>benchmark</b> also focuses on a broader range of <b>code</b> <b>related</b> capabilities, such as self-repair, <b>code</b> <b>execution,</b> and test output prediction, beyond just <b>code</b> <b>generation.</b> Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base <b>LLMs</b> and 20 instruction-tuned <b>LLMs</b> on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing <b>benchmarks</b> as well as individual model comparisons. We will release all <b>prompts</b> and model completions for further community analysis, along with a general toolkit for adding new scenarios and model</p></p class="citation"></blockquote><h3 id=27--235323-process-modeling-with-large-language-models-humam-kourani-et-al-2024>(2/7 | 235/323) Process Modeling With Large Language Models (Humam Kourani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Humam Kourani, Alessandro Berti, Daniel Schuster, Wil M. P. van der Aalst. (2024)<br><strong>Process Modeling With Large Language Models</strong><br><button class=copy-to-clipboard title="Process Modeling With Large Language Models" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-DB, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Generative AI, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07541v1.pdf filename=2403.07541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of Business Process Management (BPM), process modeling plays a crucial role in translating complex process dynamics into comprehensible visual representations, facilitating the understanding, analysis, improvement, and automation of organizational processes. Traditional process modeling methods often require extensive expertise and can be time-consuming. This paper explores the integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into process modeling to enhance flexibility, efficiency, and accessibility of process modeling for both expert and non-expert users. We propose a framework that leverages <b>LLMs</b> for the automated generation and iterative refinement of process models starting from textual descriptions. Our framework involves innovative <b>prompting</b> strategies for effective <b>LLM</b> utilization, along with a secure model generation protocol and an error-handling mechanism. Moreover, we instantiate a concrete system extending our framework. This system provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations, such as the Business Process Modeling Notation (BPMN) and Petri nets. Preliminary results demonstrate the framework&rsquo;s ability to streamline process modeling tasks, underscoring the transformative potential of <b>generative</b> <b>AI</b> in the BPM field.</p></p class="citation"></blockquote><h3 id=37--236323-bus-factor-explorer-egor-klimov-et-al-2024>(3/7 | 236/323) Bus Factor Explorer (Egor Klimov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Egor Klimov, Muhammad Umair Ahmed, Nikolai Sviridov, Pouria Derakhshanfar, Eray Tüzün, Vladimir Kovalenko. (2024)<br><strong>Bus Factor Explorer</strong><br><button class=copy-to-clipboard title="Bus Factor Explorer" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08038v1.pdf filename=2403.08038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bus factor (BF) is a metric that tracks knowledge distribution in a project. It is the minimal number of engineers that have to leave for a project to stall. Despite the fact that there are several algorithms for calculating the bus factor, only a few tools allow easy calculation of bus factor and convenient analysis of results for projects hosted on Git-based providers. We introduce Bus Factor Explorer, a web application that provides an interface and an API to compute, export, and explore the Bus Factor metric via treemap visualization, <b>simulation</b> mode, and chart editor. It supports repositories hosted on GitHub and enables functionality to search repositories in the interface and process many repositories at the same time. Our tool allows users to identify the files and subsystems at risk of stalling in the event of developer turnover by analyzing the VCS history. The application and its source code are publicly available on GitHub at <a href=https://github.com/JetBrains-Research/bus-factor-explorer>https://github.com/JetBrains-Research/bus-factor-explorer</a>. The demonstration video can be found on YouTube: <a href=https://youtu.be/uIoV79N14z8>https://youtu.be/uIoV79N14z8</a></p></p class="citation"></blockquote><h3 id=47--237323-robustness-security-privacy-explainability-efficiency-and-usability-of-large-language-models-for-code-zhou-yang-et-al-2024>(4/7 | 237/323) Robustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code (Zhou Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, David Lo. (2024)<br><strong>Robustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code</strong><br><button class=copy-to-clipboard title="Robustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07506v1.pdf filename=2403.07506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> for code (LLM4Code), which demonstrate strong performance (e.g., high accuracy) in processing source code, have significantly transformed software engineering. Many studies separately investigate the non-functional properties of LM4Code, but there is no systematic review of how these properties are evaluated and enhanced. This paper fills this gap by thoroughly examining 146 relevant studies, thereby presenting the first systematic literature review to identify seven important properties beyond accuracy, including robustness, security, privacy, explainability, efficiency, and usability. We discuss the current state-of-the-art methods and trends, identify gaps in existing research, and present promising directions for future study.</p></p class="citation"></blockquote><h3 id=57--238323-fixing-smart-contract-vulnerabilities-a-comparative-analysis-of-literature-and-developers-practices-francesco-salzano-et-al-2024>(5/7 | 238/323) Fixing Smart Contract Vulnerabilities: A Comparative Analysis of Literature and Developer&rsquo;s Practices (Francesco Salzano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Salzano, Simone Scalabrino, Rocco Oliveto, Remo Pareschi. (2024)<br><strong>Fixing Smart Contract Vulnerabilities: A Comparative Analysis of Literature and Developer&rsquo;s Practices</strong><br><button class=copy-to-clipboard title="Fixing Smart Contract Vulnerabilities: A Comparative Analysis of Literature and Developer's Practices" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07458v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07458v1.pdf filename=2403.07458v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart Contracts are programs running logic in the Blockchain network by executing operations through immutable transactions. The Blockchain network validates such transactions, storing them into sequential blocks of which integrity is ensured. Smart Contracts deal with value stakes, if a damaging transaction is validated, it may never be reverted, leading to unrecoverable losses. To prevent this, security aspects have been explored in several fields, with research providing catalogs of security defects, secure code <b>recommendations,</b> and possible solutions to fix vulnerabilities. In our study, we refer to vulnerability fixing in the ways found in the literature as guidelines. However, it is not clear to what extent developers adhere to these guidelines, nor whether there are other viable common solutions and what they are. The goal of our research is to fill knowledge gaps related to developers&rsquo; observance of existing guidelines and to propose new and viable solutions to security vulnerabilities. To reach our goal, we will obtain from Solidity GitHub repositories the commits that fix vulnerabilities included in the DASP TOP 10 and we will conduct a manual analysis of fixing approaches employed by developers. Our analysis aims to determine the extent to which literature-based fixing strategies are followed. Additionally, we will identify and discuss emerging fixing techniques not currently documented in the literature. Through qualitative analysis, we will evaluate the suitability of these new fixing solutions and discriminate between valid approaches and potential mistakes.</p></p class="citation"></blockquote><h3 id=67--239323-bayesflo-bayesian-fault-localization-of-complex-software-systems-yi-ji-et-al-2024>(6/7 | 239/323) BayesFLo: Bayesian fault localization of complex software systems (Yi Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Ji, Simon Mak, Ryan Lekivetz, Joseph Morgan. (2024)<br><strong>BayesFLo: Bayesian fault localization of complex software systems</strong><br><button class=copy-to-clipboard title="BayesFLo: Bayesian fault localization of complex software systems" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE, stat-ME<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08079v1.pdf filename=2403.08079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software testing is essential for the reliable development of complex software systems. A key step in software testing is fault localization, which uses test data to pinpoint failure-inducing combinations for further diagnosis. Existing fault localization methods, however, are largely deterministic, and thus do not provide a principled approach for assessing probabilistic risk of potential root causes, or for integrating domain and/or structural knowledge from test engineers. To address this, we propose a novel Bayesian fault localization framework called BayesFLo, which leverages a flexible Bayesian model on potential root cause combinations. A key feature of BayesFLo is its integration of the principles of combination hierarchy and heredity, which capture the structured nature of failure-inducing combinations. A critical challenge, however, is the sheer number of potential root cause scenarios to consider, which renders the computation of posterior root cause probabilities infeasible even for small software systems. We thus develop new algorithms for efficient computation of such probabilities, leveraging recent tools from integer programming and <b>graph</b> representations. We then demonstrate the effectiveness of BayesFLo over state-of-the-art fault localization methods, in a suite of numerical experiments and in two motivating case studies on the JMP XGBoost interface.</p></p class="citation"></blockquote><h3 id=77--240323-supporting-error-chains-in-static-analysis-for-precise-evaluation-results-and-enhanced-usability-anna-katharina-wickert-et-al-2024>(7/7 | 240/323) Supporting Error Chains in Static Analysis for Precise Evaluation Results and Enhanced Usability (Anna-Katharina Wickert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna-Katharina Wickert, Michael Schlichtig, Marvin Vogel, Lukas Winter, Mira Mezini, Eric Bodden. (2024)<br><strong>Supporting Error Chains in Static Analysis for Precise Evaluation Results and Enhanced Usability</strong><br><button class=copy-to-clipboard title="Supporting Error Chains in Static Analysis for Precise Evaluation Results and Enhanced Usability" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07808v1.pdf filename=2403.07808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: Static analyses are well-established to aid in understanding bugs or vulnerabilities during the development process or in large-scale studies. A low false-positive rate is essential for the adaption in practice and for precise results of empirical studies. Unfortunately, static analyses tend to report where a vulnerability manifests rather than the fix location. This can cause presumed false positives or imprecise results. Method: To address this problem, we designed an adaption of an existing static analysis algorithm that can distinguish between a manifestation and fix location, and reports error chains. An error chain represents at least two interconnected errors that occur successively, thus building the connection between the fix and manifestation location. We used our tool CogniCryptSUBS for a case study on 471 GitHub repositories, a performance <b>benchmark</b> to compare different analysis configurations, and conducted an expert interview. Result: We found that 50 % of the projects with a report had at least one error chain. Our runtime <b>benchmark</b> demonstrated that our improvement caused only a minimal runtime overhead of less than 4 %. The results of our expert interview indicate that with our adapted version participants require fewer executions of the analysis. Conclusion: Our results indicate that error chains occur frequently in real-world projects, and ignoring them can lead to imprecise evaluation results. The runtime <b>benchmark</b> indicates that our tool is a feasible and efficient solution for detecting error chains in real-world projects. Further, our results gave a hint that the usability of static analyses may benefit from supporting error chains.</p></p class="citation"></blockquote><h2 id=cscr-6>cs.CR (6)</h2><h3 id=16--241323-towards-model-extraction-attacks-in-gan-based-image-translation-via-domain-shift-mitigation-di-mi-et-al-2024>(1/6 | 241/323) Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation (Di Mi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Mi, Yanjun Zhang, Leo Yu Zhang, Shengshan Hu, Qi Zhong, Haizhuan Yuan, Shirui Pan. (2024)<br><strong>Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation</strong><br><button class=copy-to-clipboard title="Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Generative Adversarial Network, Image2text, Style Transfer, Model Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07673v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07673v2.pdf filename=2403.07673v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Model</b> <b>extraction</b> attacks (MEAs) enable an attacker to replicate the functionality of a victim deep neural network (DNN) <b>model</b> <b>by</b> only querying its API service remotely, posing a severe threat to the security and integrity of pay-per-query DNN-based services. Although the majority of current research on MEAs has primarily concentrated on neural classifiers, there is a growing prevalence of <b>image-to-image</b> <b>translation</b> (I2IT) tasks in our everyday activities. However, techniques developed for MEA of DNN classifiers cannot be directly transferred to the case of I2IT, rendering the vulnerability of I2IT <b>models</b> <b>to</b> MEA attacks often underestimated. This paper unveils the threat of MEA in I2IT tasks from a new perspective. Diverging from the traditional approach of bridging the distribution gap between attacker queries and victim training samples, we opt to mitigate the effect caused by the different distributions, known as the domain shift. This is achieved by introducing a new regularization term that penalizes high-frequency noise, and seeking a flatter minimum to avoid overfitting to the shifted distribution. Extensive experiments on different <b>image</b> <b>translation</b> tasks, including <b>image</b> <b>super-resolution</b> and <b>style</b> <b>transfer,</b> are performed on different backbone victim <b>models,</b> <b>and</b> the new design consistently outperforms the baseline by a large margin across all metrics. A few real-life I2IT APIs are also verified to be extremely vulnerable to our attack, emphasizing the need for enhanced defenses and potentially revised API publishing policies.</p></p class="citation"></blockquote><h3 id=26--242323-one-for-all-and-all-for-one-gnn-based-control-flow-attestation-for-embedded-devices-marco-chilese-et-al-2024>(2/6 | 242/323) One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices (Marco Chilese et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Chilese, Richard Mitev, Meni Orenbach, Robert Thorburn, Ahmad Atamli, Ahmad-Reza Sadeghi. (2024)<br><strong>One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices</strong><br><button class=copy-to-clipboard title="One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 36<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07465v1.pdf filename=2403.07465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Control-Flow Attestation (CFA) is a security service that allows an entity (verifier) to verify the integrity of code execution on a remote computer system (prover). Existing CFA schemes suffer from impractical assumptions, such as requiring access to the prover&rsquo;s internal state (e.g., memory or code), the complete Control-Flow <b>Graph</b> <b>(CFG)</b> <b>of</b> the prover&rsquo;s software, large sets of measurements, or tailor-made hardware. Moreover, current CFA schemes are inadequate for attesting embedded systems due to their high computational overhead and resource usage. In this paper, we overcome the limitations of existing CFA schemes for embedded devices by introducing RAGE, a novel, lightweight CFA approach with minimal requirements. RAGE can detect Code Reuse Attacks (CRA), including control- and non-control-data attacks. It efficiently extracts features from one execution trace and leverages <b>Unsupervised</b> <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> to identify deviations from benign executions. The core intuition behind RAGE is to exploit the correspondence between execution trace, execution <b>graph,</b> <b>and</b> <b>execution</b> embeddings to eliminate the unrealistic requirement of having access to a complete CFG. We evaluate RAGE on embedded <b>benchmarks</b> and demonstrate that (i) it detects 40 real-world attacks on embedded software; (ii) Further, we stress our scheme with synthetic return-oriented programming (ROP) and data-oriented programming (DOP) attacks on the real-world embedded software <b>benchmark</b> Embench, achieving 98.03% (ROP) and 91.01% (DOP) F1-Score while maintaining a low False Positive Rate of 3.19%; (iii) Additionally, we evaluate RAGE on OpenSSL, used by millions of devices and achieve 97.49% and 84.42% F1-Score for ROP and DOP attack detection, with an FPR of 5.47%.</p></p class="citation"></blockquote><h3 id=36--243323-a-framework-for-cost-effective-and-self-adaptive-llm-shaking-and-recovery-mechanism-zhiyu-chen-et-al-2024>(3/6 | 243/323) A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism (Zhiyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyu Chen, Yu Li, Suochao Zhang, Jingbo Zhou, Jiwen Zhou, Chenfu Bao, Dianhai Yu. (2024)<br><strong>A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism</strong><br><button class=copy-to-clipboard title="A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07283v1.pdf filename=2403.07283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized <b>LLMs</b> through cloud services. Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy. In this study, we introduce a cost-effective and self-adaptive <b>LLM</b> shaking tuning and recovery mechanism, named CypherTalk. With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving <b>LLM</b> schemes using Cryptography-based or <b>Differential</b> <b>Privacy-based</b> methods. Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings. To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in <b>LLM</b> scenarios.</p></p class="citation"></blockquote><h3 id=46--244323-backdoor-attack-with-mode-mixture-latent-modification-hongwei-zhang-et-al-2024>(4/6 | 244/323) Backdoor Attack with Mode Mixture Latent Modification (Hongwei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongwei Zhang, Xiaoyin Xu, Dongsheng An, Xianfeng Gu, Min Zhang. (2024)<br><strong>Backdoor Attack with Mode Mixture Latent Modification</strong><br><button class=copy-to-clipboard title="Backdoor Attack with Mode Mixture Latent Modification" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs.CR<br>Keyword Score: 23<br>Keywords: MNIST, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07463v1.pdf filename=2403.07463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Backdoor attacks become a significant security concern for deep neural networks in recent years. An image classification model can be compromised if malicious backdoors are injected into it. This corruption will cause the model to function normally on clean images but predict a specific target label when triggers are present. Previous research can be categorized into two genres: poisoning a portion of the dataset with triggered images for users to train the model from scratch, or training a backdoored model alongside a triggered image generator. Both approaches require significant amount of attackable parameters for optimization to establish a connection between the trigger and the target label, which may raise suspicions as more people become aware of the existence of backdoor attacks. In this paper, we propose a backdoor attack paradigm that only requires minimal alterations (specifically, the output layer) to a clean model in order to inject the backdoor under the guise of <b>fine-tuning.</b> To achieve this, we leverage mode mixture samples, which are located between different modes in latent space, and introduce a novel method for conducting backdoor attacks. We evaluate the effectiveness of our method on four popular <b>benchmark</b> datasets: <b>MNIST,</b> CIFAR-10, GTSRB, and TinyImageNet.</p></p class="citation"></blockquote><h3 id=56--245323-wannalaugh-a-configurable-ransomware-emulator----learning-to-mimic-malicious-storage-traces-dionysios-diamantopolous-et-al-2024>(5/6 | 245/323) WannaLaugh: A Configurable Ransomware Emulator &ndash; Learning to Mimic Malicious Storage Traces (Dionysios Diamantopolous et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dionysios Diamantopolous, Roman Pletka, Slavisa Sarafijanovic, A. L. Narasimha Reddy, Haris Pozidis. (2024)<br><strong>WannaLaugh: A Configurable Ransomware Emulator &ndash; Learning to Mimic Malicious Storage Traces</strong><br><button class=copy-to-clipboard title="WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic Malicious Storage Traces" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Ransomware<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07540v1.pdf filename=2403.07540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Ransomware,</b> a fearsome and rapidly evolving cybersecurity threat, continues to inflict severe consequences on individuals and organizations worldwide. Traditional detection methods, reliant on static signatures and application behavioral patterns, are challenged by the dynamic nature of these threats. This paper introduces three primary contributions to address this challenge. First, we introduce a <b>ransomware</b> emulator. This tool is designed to safely mimic <b>ransomware</b> attacks without causing actual harm or spreading malware, making it a unique solution for studying <b>ransomware</b> behavior. Second, we demonstrate how we use this emulator to create storage I/O traces. These traces are then utilized to train machine-learning models. Our results show that these models are effective in detecting <b>ransomware,</b> highlighting the practical application of our emulator in developing responsible cybersecurity tools. Third, we show how our emulator can be used to mimic the I/O behavior of existing <b>ransomware</b> thereby enabling safe trace collection. Both the emulator and its application represent significant steps forward in <b>ransomware</b> detection in the era of machine-learning-driven cybersecurity.</p></p class="citation"></blockquote><h3 id=66--246323-an-interpretable-generalization-mechanism-for-accurately-detecting-anomaly-and-identifying-networking-intrusion-techniques-hao-ting-pai-et-al-2024>(6/6 | 246/323) An Interpretable Generalization Mechanism for Accurately Detecting Anomaly and Identifying Networking Intrusion Techniques (Hao-Ting Pai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao-Ting Pai, Yu-Hsuan Kang, Wen-Cheng Chung. (2024)<br><strong>An Interpretable Generalization Mechanism for Accurately Detecting Anomaly and Identifying Networking Intrusion Techniques</strong><br><button class=copy-to-clipboard title="An Interpretable Generalization Mechanism for Accurately Detecting Anomaly and Identifying Networking Intrusion Techniques" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07959v1.pdf filename=2403.07959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in Intrusion Detection Systems (IDS), integrating <b>Explainable</b> <b>AI</b> (XAI) methodologies, have led to notable improvements in system performance via precise feature selection. However, a thorough understanding of cyber-attacks requires inherently <b>explainable</b> <b>decision-making</b> processes within IDS. In this paper, we present the Interpretable Generalization Mechanism (IG), poised to revolutionize IDS capabilities. IG discerns coherent patterns, making it interpretable in distinguishing between normal and anomalous network traffic. Further, the synthesis of coherent patterns sheds light on intricate intrusion pathways, providing essential insights for cybersecurity forensics. By experiments with real-world datasets NSL-KDD, UNSW-NB15, and UKM-IDS20, IG is accurate even at a low ratio of training-to-test. With 10%-to-90%, IG achieves Precision (PRE)=0.93, Recall (REC)=0.94, and Area Under Curve (AUC)=0.94 in NSL-KDD; PRE=0.98, REC=0.99, and AUC=0.99 in UNSW-NB15; and PRE=0.98, REC=0.98, and AUC=0.99 in UKM-IDS20. Notably, in UNSW-NB15, IG achieves REC=1.0 and at least PRE=0.98 since 40%-to-60%; in UKM-IDS20, IG achieves REC=1.0 and at least PRE=0.88 since 20%-to-80%. Importantly, in UKM-IDS20, IG successfully identifies all three anomalous instances without prior exposure, demonstrating its generalization capabilities. These results and inferences are reproducible. In sum, IG showcases superior generalization by consistently performing well across diverse datasets and training-to-test ratios (from 10%-to-90% to 90%-to-10%), and excels in identifying novel anomalies without prior exposure. Its interpretability is enhanced by coherent evidence that accurately distinguishes both normal and anomalous activities, significantly improving detection accuracy and reducing false alarms, thereby strengthening IDS reliability and trustworthiness.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--247323-generaitor-tree-in-the-loop-text-generation-for-language-model-explainability-and-adaptation-thilo-spinner-et-al-2024>(1/5 | 247/323) generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation (Thilo Spinner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thilo Spinner, Rebecca Kehlbeck, Rita Sevastjanova, Tobias Stähle, Daniel A. Keim, Oliver Deussen, Mennatallah El-Assady. (2024)<br><strong>generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation</strong><br><button class=copy-to-clipboard title="generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: I-2-7; H-5-2, cs-HC, cs-LG, cs.HC<br>Keyword Score: 40<br>Keywords: Fine-tuning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07627v1.pdf filename=2403.07627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based <b>text</b> <b>generation.</b> However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as <b>fine-tuning</b> the model based on adapted data. Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods. Additionally, we demonstrate the applicability of our approach in a qualitative user study. Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in <b>text-generation</b> <b>use</b> cases.</p></p class="citation"></blockquote><h3 id=25--248323-enhancing-depression-diagnosis-oriented-chat-with-psychological-state-tracking-yiyang-gu-et-al-2024>(2/5 | 248/323) Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking (Yiyang Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyang Gu, Yougen Zhou, Qin Chen, Ningning Zhou, Jie Zhou, Aimin Zhou, Liang He. (2024)<br><strong>Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking</strong><br><button class=copy-to-clipboard title="Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-CY, cs-HC, cs.HC<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09717v1.pdf filename=2403.09717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We <b>fine-tune</b> an <b>LLM</b> model to generate the dynamic psychological state, which is further used to assist response generation at each turn to simulate the psychiatrist. Experimental results on the existing <b>benchmark</b> show that our proposed method boosts the performance of all subtasks in depression-diagnosis-oriented chat.</p></p class="citation"></blockquote><h3 id=35--249323-from-paper-to-card-transforming-design-implications-with-generative-ai-donghoon-shin-et-al-2024>(3/5 | 249/323) From Paper to Card: Transforming Design Implications with Generative AI (Donghoon Shin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donghoon Shin, Lucy Lu Wang, Gary Hsieh. (2024)<br><strong>From Paper to Card: Transforming Design Implications with Generative AI</strong><br><button class=copy-to-clipboard title="From Paper to Card: Transforming Design Implications with Generative AI" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2; I-2-7, cs-AI, cs-CL, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Generative AI, Text2image, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08137v1.pdf filename=2403.08137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Communicating design implications is common within the HCI community when publishing academic papers, yet these papers are rarely read and used by designers. One solution is to use design cards as a form of translational resource that communicates valuable insights from papers in a more digestible and accessible format to assist in design processes. However, creating design cards can be time-consuming, and authors may lack the resources/know-how to produce cards. Through an iterative design process, we built a system that helps create design cards from academic papers using an <b>LLM</b> and <b>text-to-image</b> model. Our evaluation with designers (N=21) and authors of selected papers (N=12) revealed that designers perceived the design implications from our design cards as more inspiring and <b>generative,</b> <b>compared</b> to reading original paper texts, and the authors viewed our system as an effective way of communicating their design implications. We also propose future enhancements for AI-generated design cards.</p></p class="citation"></blockquote><h3 id=45--250323-visual-decoding-and-reconstruction-via-eeg-embeddings-with-guided-diffusion-dongyang-li-et-al-2024>(4/5 | 250/323) Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion (Dongyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongyang Li, Chen Wei, Shiying Li, Jiachen Zou, Quanying Liu. (2024)<br><strong>Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion</strong><br><button class=copy-to-clipboard title="Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC, eess-SP, q-bio-NC<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07721v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07721v3.pdf filename=2403.07721v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning. Modern <b>contrastive</b> <b>learning</b> and generative models improved the performance of fMRI-based visual decoding and reconstruction. However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), <b>prompting</b> a high need for EEG-based visual reconstruction. In this study, we present an EEG-based visual reconstruction framework. It consists of a plug-and-play EEG encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image embeddings, and a two-stage EEG guidance image generator that first transforms EEG features into image priors and then reconstructs the visual stimuli with a pre-trained image generator. Our approach allows EEG embeddings to achieve superior performance in image classification and retrieval tasks. Our two-stage image generation strategy vividly reconstructs images seen by humans. Furthermore, we analyzed the impact of signals from different time windows and brain regions on decoding and reconstruction. The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality. We report that EEG-based visual decoding achieves SOTA performance, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications. The code of ATM is available at <a href=https://github.com/dongyangli-del/EEG_Image_decode>https://github.com/dongyangli-del/EEG_Image_decode</a>.</p></p class="citation"></blockquote><h3 id=55--251323-tutoai-a-cross-domain-framework-for-ai-assisted-mixed-media-tutorial-creation-on-physical-tasks-yuexi-chen-et-al-2024>(5/5 | 251/323) TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks (Yuexi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuexi Chen, Vlad I. Morariu, Anh Truong, Zhicheng Liu. (2024)<br><strong>TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks</strong><br><button class=copy-to-clipboard title="TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-LG, cs.HC<br>Keyword Score: 13<br>Keywords: Knowledge Distillation, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08049v1.pdf filename=2403.08049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos. However, manually creating such tutorials is tedious, and existing automated solutions are often restricted to a particular domain. While AI models hold promise, it is unclear how to effectively harness their powers, given the <b>multi-modal</b> data involved and the vast landscape of models. We present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks. First, we <b>distill</b> common tutorial components by surveying existing work; then, we present an approach to identify, assemble, and evaluate AI models for component extraction; finally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components. We show that TutoAI has achieved higher or similar quality compared to a baseline model in preliminary user studies.</p></p class="citation"></blockquote><h2 id=csdb-3>cs.DB (3)</h2><h3 id=13--252323-omnimatch-effective-self-supervised-any-join-discovery-in-tabular-data-repositories-christos-koutras-et-al-2024>(1/3 | 252/323) OmniMatch: Effective Self-Supervised Any-Join Discovery in Tabular Data Repositories (Christos Koutras et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christos Koutras, Jiani Zhang, Xiao Qin, Chuan Lei, Vasileios Ioannidis, Christos Faloutsos, George Karypis, Asterios Katsifodimos. (2024)<br><strong>OmniMatch: Effective Self-Supervised Any-Join Discovery in Tabular Data Repositories</strong><br><button class=copy-to-clipboard title="OmniMatch: Effective Self-Supervised Any-Join Discovery in Tabular Data Repositories" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 38<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Representation Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07653v1.pdf filename=2403.07653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How can we discover join relationships among columns of tabular data in a data repository? Can this be done effectively when metadata is missing? Traditional column matching works mainly rely on similarity measures based on exact value overlaps, hence missing important semantics or failing to handle noise in the data. At the same time, recent dataset discovery methods focusing on deep table <b>representation</b> <b>learning</b> techniques, do not take into consideration the rich set of column similarity signals found in prior matching and discovery methods. Finally, existing methods heavily depend on user-provided similarity thresholds, hindering their deployability in real-world settings. In this paper, we propose OmniMatch, a novel join discovery technique that detects equi-joins and fuzzy-joins betwen columns by combining column-pair similarity measures with <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> OmniMatch&rsquo;s <b>GNN</b> can capture column relatedness leveraging <b>graph</b> <b>transitivity,</b> <b>significantly</b> improving the recall of join discovery tasks. At the same time, OmniMatch also increases the precision by augmenting its training data with negative column join examples through an automated negative example generation process. Most importantly, compared to the state-of-the-art matching and discovery methods, OmniMatch exhibits up to 14% higher effectiveness in F1 score and AUC without relying on metadata or user-provided thresholds for each similarity metric.</p></p class="citation"></blockquote><h3 id=23--253323-couler-unified-machine-learning-workflow-optimization-in-cloud-xiaoda-wang-et-al-2024>(2/3 | 253/323) Couler: Unified Machine Learning Workflow Optimization in Cloud (Xiaoda Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoda Wang, Yuan Tang, Tengda Guo, Bo Sang, Jingji Wu, Jian Sha, Ke Zhang, Jiang Qian, Mingjie Tang. (2024)<br><strong>Couler: Unified Machine Learning Workflow Optimization in Cloud</strong><br><button class=copy-to-clipboard title="Couler: Unified Machine Learning Workflow Optimization in Cloud" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs-LG, cs.DB<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07608v1.pdf filename=2403.07608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines. In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud. Our main insight lies in the ability to generate an ML workflow using natural language (NL) descriptions. We integrate <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into workflow generation, and provide a unified programming interface for various workflow engines. This approach alleviates the need to understand various workflow engines&rsquo; APIs. Moreover, Couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling <b>large</b> <b>workflow</b> <b>auto-parallelization</b> and automatic hyperparameters tuning. These enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training. Couler is extensively deployed in real-world production scenarios at Ant Group, handling approximately 22k workflows daily, and has successfully improved the CPU/Memory utilization by more than 15% and the workflow completion rate by around 17%.</p></p class="citation"></blockquote><h3 id=33--254323-generalised-graph-grammars-for-natural-language-processing-oliver-robert-fox-et-al-2024>(3/3 | 254/323) Generalised Graph Grammars for Natural Language Processing (Oliver Robert Fox et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oliver Robert Fox, Giacomo Bergami. (2024)<br><strong>Generalised Graph Grammars for Natural Language Processing</strong><br><button class=copy-to-clipboard title="Generalised Graph Grammars for Natural Language Processing" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07481v1.pdf filename=2403.07481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This seminal paper proposes a new query language for <b>graph</b> matching and rewriting overcoming {the declarative} limitation of Cypher while outperforming {Neo4j} on <b>graph</b> matching and rewriting by at least one order of magnitude. We exploited columnar databases (KnoBAB) to represent <b>graphs</b> using the Generalised Semistructured Model.</p></p class="citation"></blockquote><h2 id=csdc-5>cs.DC (5)</h2><h3 id=15--255323-measuring-data-similarity-for-efficient-federated-learning-a-feasibility-study-fernanda-famá-et-al-2024>(1/5 | 255/323) Measuring Data Similarity for Efficient Federated Learning: A Feasibility Study (Fernanda Famá et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernanda Famá, Charalampos Kalalas, Sandra Lagen, Paolo Dini. (2024)<br><strong>Measuring Data Similarity for Efficient Federated Learning: A Feasibility Study</strong><br><button class=copy-to-clipboard title="Measuring Data Similarity for Efficient Federated Learning: A Feasibility Study" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 33<br>Keywords: Clustering, Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07450v1.pdf filename=2403.07450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In multiple <b>federated</b> <b>learning</b> schemes, a random subset of clients sends in each round their model updates to the server for aggregation. Although this client selection strategy aims to reduce communication overhead, it remains energy and computationally inefficient, especially when considering resource-constrained devices as clients. This is because conventional random client selection overlooks the content of exchanged information and falls short of providing a mechanism to reduce the transmission of semantically redundant data. To overcome this challenge, we propose <b>clustering</b> the clients with the aid of similarity metrics, where a single client from each of the formed clusters is selected in each round to participate in the <b>federated</b> <b>training.</b> To evaluate our approach, we perform an extensive feasibility study considering the use of nine statistical metrics in the <b>clustering</b> process. <b>Simulation</b> results reveal that, when considering a scenario with high data heterogeneity of clients, similarity-based <b>clustering</b> can reduce the number of required rounds compared to the baseline random client selection. In addition, energy consumption can be notably reduced from 23.93% to 41.61%, for those similarity metrics with an equivalent number of clients per round as the baseline random scheme.</p></p class="citation"></blockquote><h3 id=25--256323-mpcpa-multi-center-privacy-computing-with-predictions-aggregation-based-on-denoising-diffusion-probabilistic-model-guibo-luo-et-al-2024>(2/5 | 256/323) MPCPA: Multi-Center Privacy Computing with Predictions Aggregation based on Denoising Diffusion Probabilistic Model (Guibo Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guibo Luo, Hanwen Zhang, Xiuling Wang, Mingzhi Chen, Yuesheng Zhu. (2024)<br><strong>MPCPA: Multi-Center Privacy Computing with Predictions Aggregation based on Denoising Diffusion Probabilistic Model</strong><br><button class=copy-to-clipboard title="MPCPA: Multi-Center Privacy Computing with Predictions Aggregation based on Denoising Diffusion Probabilistic Model" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 30<br>Keywords: Diffusion Model, Federated Learning, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07838v1.pdf filename=2403.07838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Privacy-preserving computing is crucial for multi-center machine learning in many applications such as healthcare and finance. In this paper a Multi-center Privacy Computing framework with Predictions Aggregation (MPCPA) based on denoising <b>diffusion</b> <b>probabilistic</b> <b>model</b> (DDPM) is proposed, in which conditional <b>diffusion</b> <b>model</b> training, DDPM data generation, a classifier, and strategy of prediction aggregation are included. Compared to <b>federated</b> <b>learning,</b> this framework necessitates fewer communications and leverages high-quality generated data to support robust privacy computing. Experimental validation across multiple datasets demonstrates that the proposed framework outperforms classic <b>federated</b> <b>learning</b> and approaches the performance of centralized learning with original data. Moreover, our approach demonstrates robust security, effectively addressing challenges such as image memorization and membership inference attacks. Our experiments underscore the efficacy of the proposed framework in the realm of privacy computing, with the code set to be released soon.</p></p class="citation"></blockquote><h3 id=35--257323-characterization-of-large-language-model-development-in-the-datacenter-qinghao-hu-et-al-2024>(3/5 | 257/323) Characterization of Large Language Model Development in the Datacenter (Qinghao Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinghao Hu, Zhisheng Ye, Zerui Wang, Guoteng Wang, Meng Zhang, Qiaoling Chen, Peng Sun, Dahua Lin, Xiaolin Wang, Yingwei Luo, Yonggang Wen, Tianwei Zhang. (2024)<br><strong>Characterization of Large Language Model Development in the Datacenter</strong><br><button class=copy-to-clipboard title="Characterization of Large Language Model Development in the Datacenter" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07648v1.pdf filename=2403.07648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have presented impressive performance across several transformative tasks. However, it is non-trivial to efficiently utilize <b>large-scale</b> <b>cluster</b> <b>resources</b> to develop <b>LLMs,</b> often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. In this paper, we present an in-depth characterization study of a six-month <b>LLM</b> development workload trace collected from our GPU datacenter Acme. Specifically, we investigate discrepancies between <b>LLMs</b> and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures. Our analysis <b>summarizes</b> hurdles we encountered and uncovers potential opportunities to optimize systems tailored for <b>LLMs.</b> Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through <b>LLM-involved</b> failure diagnosis and automatic recovery. (2) decoupled scheduling for evaluation, which achieves timely performance feedback via trial decomposition and scheduling optimization.</p></p class="citation"></blockquote><h3 id=45--258323-accelerating-biclique-counting-on-gpu-linshan-qiu-et-al-2024>(4/5 | 258/323) Accelerating Biclique Counting on GPU (Linshan Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linshan Qiu, Zhonggen Li, Xiangyu Ke, Lu Chen, Yunjun Gao. (2024)<br><strong>Accelerating Biclique Counting on GPU</strong><br><button class=copy-to-clipboard title="Accelerating Biclique Counting on GPU" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 13<br>Keywords: Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07858v1.pdf filename=2403.07858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Counting (p,q)-bicliques in bipartite <b>graphs</b> poses a foundational challenge with broad applications, from densest subgraph discovery in algorithmic research to personalized content <b>recommendation</b> in practical scenarios. Despite its significance, current leading (p,q)-biclique counting algorithms fall short, particularly when faced with larger <b>graph</b> sizes and clique scales. Fortunately, the problem&rsquo;s inherent structure, allowing for the independent counting of each biclique starting from every vertex, combined with a substantial set intersections, makes it highly amenable to parallelization. Recent successes in GPU-accelerated algorithms across various domains motivate our exploration into harnessing the parallelism power of GPUs to efficiently address the (p,q)-biclique counting challenge. We introduce GBC (GPU-based Biclique Counting), a novel approach designed to enable efficient and scalable (p,q)-biclique counting on GPUs. To address major bottleneck arising from redundant comparisons in set intersections (occupying an average of 90% of the runtime), we introduce a novel data structure that hashes adjacency lists into truncated bitmaps to enable efficient set intersection on GPUs via bit-wise AND operations. Our innovative hybrid DFS-BFS exploration strategy further enhances thread utilization and effectively manages memory constraints. A composite load balancing strategy, integrating pre-runtime and runtime workload allocation, ensures equitable distribution among threads. Additionally, we employ vertex reordering and <b>graph</b> partitioning strategies for improved compactness and scalability. Experimental evaluations on eight real-life and two synthetic datasets demonstrate that GBC outperforms state-of-the-art algorithms by a substantial margin. In particular, GBC achieves an average speedup of 497.8x, with the largest instance achieving a remarkable 1217.7x speedup when p = q = 8.</p></p class="citation"></blockquote><h3 id=55--259323-efficient-fault-tolerance-for-pipelined-query-engines-via-write-ahead-lineage-ziheng-wang-et-al-2024>(5/5 | 259/323) Efficient Fault Tolerance for Pipelined Query Engines via Write-ahead Lineage (Ziheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziheng Wang, Alex Aiken. (2024)<br><strong>Efficient Fault Tolerance for Pipelined Query Engines via Write-ahead Lineage</strong><br><button class=copy-to-clipboard title="Efficient Fault Tolerance for Pipelined Query Engines via Write-ahead Lineage" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DB, cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08062v1.pdf filename=2403.08062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern distributed pipelined query engines either do not support intra-query fault tolerance or employ high-overhead approaches such as persisting intermediate outputs or checkpointing state. In this work, we present write-ahead lineage, a novel fault recovery technique that combines Spark&rsquo;s lineage-based replay and write-ahead logging. Unlike Spark, where the lineage is determined before query execution, write-ahead lineage persistently logs lineage at runtime to support dynamic task dependencies in pipelined query engines. Since only KB-sized lineages are persisted instead of MB-sized intermediate outputs, the normal execution overhead is minimal compared to spooling or checkpointing based approaches. To ensure fast fault recovery times, tasks only consume intermediate outputs with persisted lineage, preventing global rollbacks upon failure. In addition, lost tasks from different stages can be recovered in a pipelined parallel manner. We implement write-ahead lineage in a distributed pipelined query engine called Quokka. We show that Quokka is around 2x faster than SparkSQL on the TPC-H <b>benchmark</b> with similar fault recovery performance.</p></p class="citation"></blockquote><h2 id=eesssy-11>eess.SY (11)</h2><h3 id=111--260323-learning-based-prescribed-time-safety-for-control-of-unknown-systems-with-control-barrier-functions-tzu-yuan-huang-et-al-2024>(1/11 | 260/323) Learning-based Prescribed-Time Safety for Control of Unknown Systems with Control Barrier Functions (Tzu-Yuan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tzu-Yuan Huang, Xiaobing Dai, Sihua Zhang, Alexandre Capone, Velimir Todorovski, Stefan Sosnowski, Sandra Hirche. (2024)<br><strong>Learning-based Prescribed-Time Safety for Control of Unknown Systems with Control Barrier Functions</strong><br><button class=copy-to-clipboard title="Learning-based Prescribed-Time Safety for Control of Unknown Systems with Control Barrier Functions" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08054v1.pdf filename=2403.08054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many control system applications, state constraint satisfaction needs to be guaranteed within a prescribed time. While this issue has been partially addressed for systems with known dynamics, it remains largely unaddressed for systems with unknown dynamics. In this paper, we propose a <b>Gaussian</b> <b>process-based</b> time-varying control method that leverages backstepping and control barrier functions to achieve safety requirements within prescribed time windows. It can be used to keep a system within a safe region or to make it return to a safe region within a limited time window. These properties are cemented by rigorous theoretical results. The effectiveness of the proposed controller is demonstrated in a <b>simulation</b> of a robotic manipulator.</p></p class="citation"></blockquote><h3 id=211--261323-configuration-and-emt-simulation-of-the-240-bus-miniwecc-system-integrating-offshore-wind-farms-owfs-buxin-she-et-al-2024>(2/11 | 261/323) Configuration and EMT Simulation of the 240-bus MiniWECC System Integrating Offshore Wind Farms (OWFs) (Buxin She et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Buxin She, Hisham Mahmood, Marcelo Elizondo, Veronica Adetola, Yuqing Dong. (2024)<br><strong>Configuration and EMT Simulation of the 240-bus MiniWECC System Integrating Offshore Wind Farms (OWFs)</strong><br><button class=copy-to-clipboard title="Configuration and EMT Simulation of the 240-bus MiniWECC System Integrating Offshore Wind Farms (OWFs)" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07988v1.pdf filename=2403.07988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As offshore wind farms (OWFs) become increasingly prevalent in Northern California and Southern Oregon, they introduce faster dynamics into the Western Electricity Coordinating Council (WECC) system, reshaping its dynamic behavior. Accordingly, electromagnetic transient (EMT) <b>simulation</b> is essential to assess high frequency dynamics of the WECC system with integrated OWFs. Against this background, this paper presents the integration of detailed dynamic models of OWFs into a 240-bus miniWECC system in PSCAD software. The sequential initialization technique is employed to facilitate the smooth initiation of a large-scale system in an EMT <b>simulation.</b> The performance of the configured model is assessed under wind speed variations and grounded faults, demonstrating the effectiveness of the miniWECC system with OWFs. This system serves as a valuable basic use case for validating the fast dynamic performance of future WECC systems with high penetration of wind energy.</p></p class="citation"></blockquote><h3 id=311--262323-humans-in-the-building-getting-rid-of-thermostats-for-optimal-thermal-comfort-control-in-energy-management-systems-jiali-wang-et-al-2024>(3/11 | 262/323) Humans-in-the-Building: Getting Rid of Thermostats for Optimal Thermal Comfort Control in Energy Management Systems (Jiali Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiali Wang, Yang Tang, Luca Schenato. (2024)<br><strong>Humans-in-the-Building: Getting Rid of Thermostats for Optimal Thermal Comfort Control in Energy Management Systems</strong><br><button class=copy-to-clipboard title="Humans-in-the-Building: Getting Rid of Thermostats for Optimal Thermal Comfort Control in Energy Management Systems" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07453v1.pdf filename=2403.07453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the widespread attention to individual thermal comfort, coupled with significant energy-saving potential inherent in energy management systems for optimizing indoor environments, this paper aims to introduce advanced &ldquo;Humans-in-the-building&rdquo; control techniques to redefine the paradigm of indoor temperature design. Firstly, we innovatively redefine the role of individuals in the control loop, establishing a model for users&rsquo; thermal comfort and constructing discomfort signals based on individual preferences. Unlike traditional temperature-centric approaches, &ldquo;thermal comfort control&rdquo; prioritizes personalized comfort. Then, considering the diversity among users, we propose a novel method to determine the optimal indoor temperature range, thus minimizing discomfort for various users and reducing building energy consumption. Finally, the efficacy of the &ldquo;thermal comfort control&rdquo; approach is substantiated through <b>simulations</b> conducted using Matlab.</p></p class="citation"></blockquote><h3 id=411--263323-gmpc-geometric-model-predictive-control-for-wheeled-mobile-robot-trajectory-tracking-jiawei-tang-et-al-2024>(4/11 | 263/323) GMPC: Geometric Model Predictive Control for Wheeled Mobile Robot Trajectory Tracking (Jiawei Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Tang, Shuang Wu, Bo Lan, Yahui Dong, Yuqiang Jin, Guangjian Tian, Wen-An Zhang, Ling Shi. (2024)<br><strong>GMPC: Geometric Model Predictive Control for Wheeled Mobile Robot Trajectory Tracking</strong><br><button class=copy-to-clipboard title="GMPC: Geometric Model Predictive Control for Wheeled Mobile Robot Trajectory Tracking" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07317v1.pdf filename=2403.07317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The configuration of most robotic systems lies in continuous transformation groups. However, in mobile robot trajectory tracking, many recent works still naively utilize optimization methods for elements in vector space without considering the manifold constraint of the robot configuration. In this letter, we propose a geometric model predictive control (MPC) framework for wheeled mobile robot trajectory tracking. We first derive the error dynamics of the wheeled mobile robot trajectory tracking by considering its manifold constraint and kinematic constraint simultaneously. After that, we utilize the relationship between the Lie group and Lie algebra to convexify the tracking control problem, which enables us to solve the problem efficiently. Thanks to the Lie group formulation, our method tracks the trajectory more smoothly than existing nonlinear MPC. <b>Simulations</b> and physical experiments verify the effectiveness of our proposed methods. Our pure Python-based <b>simulation</b> platform is publicly available to benefit further research in the community.</p></p class="citation"></blockquote><h3 id=511--264323-improving-fairness-in-photovoltaic-curtailments-via-daily-topology-reconfiguration-for-voltage-control-in-power-distribution-networks-rahul-k-gupta-et-al-2024>(5/11 | 264/323) Improving Fairness in Photovoltaic Curtailments via Daily Topology Reconfiguration for Voltage Control in Power Distribution Networks (Rahul K. Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul K. Gupta, Daniel K. Molzahn. (2024)<br><strong>Improving Fairness in Photovoltaic Curtailments via Daily Topology Reconfiguration for Voltage Control in Power Distribution Networks</strong><br><button class=copy-to-clipboard title="Improving Fairness in Photovoltaic Curtailments via Daily Topology Reconfiguration for Voltage Control in Power Distribution Networks" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07853v1.pdf filename=2403.07853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In PV-rich power distribution systems, over-voltage issues are often addressed by curtailing excess generation from PV plants (in addition to reactive power control), raising <b>fairness</b> concerns. Existing <b>fairness-aware</b> control schemes tackle this problem by incorporating <b>fairness</b> objectives into the cost function. However, such schemes result in increased overall curtailments. This paper proposes a solution through daily topology reconfiguration, ensuring that different PV plants face varying grid conditions each day, leading to different curtailment levels and enhancing <b>fairness.</b> We illustrate that implementing this approach enhances overall <b>fairness</b> without significantly increasing overall curtailments. The optimization problem involves two stages. The day-ahead stage optimizes the network topology using day-ahead forecasts of PV generation and demand, minimizing net curtailment and accounting for <b>fairness</b> based on curtailments from prior days. The real-time stage implements the optimized topology and computes active and reactive power setpoints for the PV plants. Day-ahead grid constraints are modeled using LinDistFlow, and real-time control employs a linearized model with a first-order Taylor approximation. The proposed scheme is numerically validated on several <b>benchmark</b> test cases. Results are compared using the Jain <b>Fairness</b> Index, considering <b>fairness</b> and reconfiguration scenarios.</p></p class="citation"></blockquote><h3 id=611--265323-utilizing-load-shifting-for-optimal-compressor-sequencing-in-industrial-refrigeration-rohit-konda-et-al-2024>(6/11 | 265/323) Utilizing Load Shifting for Optimal Compressor Sequencing in Industrial Refrigeration (Rohit Konda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohit Konda, Vikas Chandan, Jesse Crossno, Blake Pollard, Dan Walsh, Rick Bohonek, Jason R. Marden. (2024)<br><strong>Utilizing Load Shifting for Optimal Compressor Sequencing in Industrial Refrigeration</strong><br><button class=copy-to-clipboard title="Utilizing Load Shifting for Optimal Compressor Sequencing in Industrial Refrigeration" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07831v1.pdf filename=2403.07831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ubiquity and energy needs of industrial refrigeration has <b>prompted</b> several research studies investigating various control opportunities for reducing energy demand. This work focuses on one such opportunity, termed compressor sequencing, which entails intelligently selecting the operational state of the compressors to service the required refrigeration load with the least possible work. We first study the static compressor sequencing problem and observe that deriving the optimal compressor operational state is computationally challenging and can vary dramatically based on the refrigeration load. Thus we introduce load shifting in conjunction with compressor sequencing, which entails strategically precooling the facility to allow for more efficient compressor operation. Interestingly, we show that load shifting not only provides benefits in computing the optimal compressor operational state, but also can lead to significant energy savings. Our results are based on and compared to real-world sensor data from an operating industrial refrigeration site of Butterball LLC located in Huntsville, AR, which demonstrated that without load shifting, even optimal compressor operation results in compressors often running at intermediate capacity levels, which can lead to inefficiencies. Through collected data, we demonstrate that a load shifting approach for compressor sequencing has the potential to reduce energy use of the compressors up to 20% compared to optimal sequencing without load shifting.</p></p class="citation"></blockquote><h3 id=711--266323-distributed-estimation-by-two-agents-with-different-feature-spaces-aneesh-raghavan-et-al-2024>(7/11 | 266/323) Distributed Estimation by Two Agents with Different Feature Spaces (Aneesh Raghavan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aneesh Raghavan, Karl Henrik Johansson. (2024)<br><strong>Distributed Estimation by Two Agents with Different Feature Spaces</strong><br><button class=copy-to-clipboard title="Distributed Estimation by Two Agents with Different Feature Spaces" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: 62J07 93A16 46N30, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07749v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07749v2.pdf filename=2403.07749v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of estimation of a function by a system consisting of two agents and a fusion center. The two agents collect data comprising of samples of an independent variable and the corresponding value of a dependent variable. The objective of the system is to collaboratively estimate the function without any exchange of data among the members of the system. To this end, we propose the following framework. The agents are given a set of features using which they construct suitable function spaces to formulate and solve the estimation problems locally. The estimated functions are uploaded to a fusion space where an optimization problem is solved to fuse the estimates (also known as <b>meta-learning)</b> <b>to</b> obtain the system estimate of the mapping. The fused function is then downloaded by the agents to gather knowledge about the other agents estimate of the function. With respect to the framework, we present the following: a systematic construction of fusion space given the features of the agents; the derivation of an uploading operator for the agents to upload their estimated functions to a fusion space; the derivation of a downloading operator for the fused function to be downloaded. Through an example on least squares regression, we illustrate the distributed estimation architecture that has been developed.</p></p class="citation"></blockquote><h3 id=811--267323-on-the-locomotion-of-the-slider-within-a-self-adaptive-beam-slider-system-florian-müller-et-al-2024>(8/11 | 267/323) On the locomotion of the slider within a self-adaptive beam-slider system (Florian Müller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Müller, Malte Krack. (2024)<br><strong>On the locomotion of the slider within a self-adaptive beam-slider system</strong><br><button class=copy-to-clipboard title="On the locomotion of the slider within a self-adaptive beam-slider system" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Self-adaption<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07423v1.pdf filename=2403.07423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A beam-slider system is considered whose passive <b>self-adaption</b> relies on an intricate locomotion process involving both frictional and unilateral contact. The system also exploits geometric nonlinearity to achieve broadband efficacy. The dynamics of the system take place on three distinct time scales: On the fast time scale of the harmonic base excitation are the vibrations and the locomotion cycle. On the slow time scale, the slider changes its position along the beam, and the overall vibration level varies. Finally, on an intermediate time scale, strong modulations of the vibration amplitude may take place. In the present work, first, an analytical approximation of the beam&rsquo;s response on the slow time scale is derived as function of the slider position, which is a crucial prerequisite for identifying the main drivers of the slider&rsquo;s locomotion. Then, the most important forms of locomotion are described and approximations of their individual contribution to the overall slider transport are estimated. Finally, the theoretical results are compared against numerical results obtained from an experimentally validated model.</p></p class="citation"></blockquote><h3 id=911--268323-adaptive-gain-scheduling-using-reinforcement-learning-for-quadcopter-control-mike-timmerman-et-al-2024>(9/11 | 268/323) Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control (Mike Timmerman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mike Timmerman, Aryan Patel, Tim Reinhart. (2024)<br><strong>Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control</strong><br><button class=copy-to-clipboard title="Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07216v1.pdf filename=2403.07216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper presents a technique using <b>reinforcement</b> <b>learning</b> (RL) to adapt the control gains of a quadcopter controller. Specifically, we employed Proximal Policy Optimization (PPO) to train a policy which adapts the gains of a cascaded feedback controller in-flight. The primary goal of this controller is to minimize tracking error while following a specified trajectory. The paper&rsquo;s key objective is to analyze the effectiveness of the adaptive gain policy and compare it to the performance of a static gain control algorithm, where the Integral Squared Error and Integral Time Squared Error are used as metrics. The results show that the adaptive gain scheme achieves over 40$%$ decrease in tracking error as compared to the static gain controller.</p></p class="citation"></blockquote><h3 id=1011--269323-on-modeling-adequacy-and-stability-analysis-of-ibr-related-subsynchronous-oscillations-in-multimachine-systems-lilan-karunaratne-et-al-2024>(10/11 | 269/323) On Modeling Adequacy and Stability Analysis of IBR-related Subsynchronous Oscillations in Multimachine Systems (Lilan Karunaratne et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lilan Karunaratne, Nilanjan Ray Chaudhuri, Amirthagunaraj Yogarathnam, Meng Yue. (2024)<br><strong>On Modeling Adequacy and Stability Analysis of IBR-related Subsynchronous Oscillations in Multimachine Systems</strong><br><button class=copy-to-clipboard title="On Modeling Adequacy and Stability Analysis of IBR-related Subsynchronous Oscillations in Multimachine Systems" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07835v1.pdf filename=2403.07835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time-varying phasor-based analysis of subsynchronous oscillations (SSOs) involving grid-following converters (GFLCs) and its <b>benchmarking</b> with electromagnetic transient (EMT) models have so far been restricted to highly simplified grid models with constant voltage sources behind series R-L circuits. In this paper, modeling adequacy of bulk power systems with synchronous generators (SGs), transmission systems, loads, and GFLCs are considered. To this end, we revisit the notions of time-varying phasor calculus, highlighting the distinction between space-phasor-calculus (SPC) and two often interchangeably used frameworks namely baseband-abc and generalized averaging. We present the models of grids in SPC framework that include transmission line dynamics, load dynamics, and SG stator transients. Next, we propose a generic approach to study modeling adequacy in small-signal sense by (a) identifying critical modes through eigenvalue and singular value analysis followed by (b) using weighted maximum singular value error magnitudes as metrics, and (c) further cross-validation. Using a modified 4-machine IEEE <b>benchmark</b> model with up to 3 GFLCs we show that SPC framework can be used for analysis of SSOs. Further, we consider the quasistationary phasor calculus (QPC) framework that neglects transmission line, load, and SG stator dynamics to show its adequacy in SSO modeling and analysis. Time-domain and frequency-domain results with EMT models are also presented.</p></p class="citation"></blockquote><h3 id=1111--270323-energy-versus-output-quality-of-non-volatile-writes-in-intermittent-computing-rei-barjami-et-al-2024>(11/11 | 270/323) Energy versus Output Quality of Non-volatile Writes in Intermittent Computing (Rei Barjami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rei Barjami, Antonio Miele, Luca Mottola. (2024)<br><strong>Energy versus Output Quality of Non-volatile Writes in Intermittent Computing</strong><br><button class=copy-to-clipboard title="Energy versus Output Quality of Non-volatile Writes in Intermittent Computing" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07517v1.pdf filename=2403.07517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore how to improve the energy performance of battery-less Internet of Things (IoT) devices at the cost of a reduction in the quality of the output. Battery-less IoT devices are extremely resource-constrained energy-harvesting devices. Due to erratic energy patterns from the ambient, their executions become intermittent; periods of active computation are interleaved by periods of recharging small energy buffers. To cross periods of energy unavailability, a device persists application and system state onto Non-Volatile Memory (NVM) in anticipation of energy failures. We purposely control the energy invested in these operations, representing a major energy overhead, when using Spin-Transfer Torque Magnetic Random-Access Memory (STT-MRAM) as NVM. As a result, we abate the corresponding overhead, yet introduce write errors. Based on 1.9+ trillion experimental data points, we illustrate whether this is a gamble worth taking, when, and where. We measure the energy consumption and quality of output obtained from the execution of nine diverse <b>benchmarks</b> on top of seven different platforms. Our results allow us to draw three key observations: i) the trade-off between energy saving and reduction of output quality is program-specific; ii) the same trade-off is a function of a platform&rsquo;s specific compute efficiency and power figures; and iii) data encoding and input size impact a program&rsquo;s resilience to errors. As a paradigmatic example, we reveal cases where we achieve up to 50% reduction in energy consumption with negligible effects on output quality, as opposed to settings where a minimal energy gain causes drastic drops in output quality.</p></p class="citation"></blockquote><h2 id=csai-6>cs.AI (6)</h2><h3 id=16--271323-transforming-competition-into-collaboration-the-revolutionary-role-of-multi-agent-systems-and-language-models-in-modern-organizations-carlos-jose-xavier-cruz-2024>(1/6 | 271/323) Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations (Carlos Jose Xavier Cruz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Jose Xavier Cruz. (2024)<br><strong>Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations</strong><br><button class=copy-to-clipboard title="Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs-MA, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07769v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07769v3.pdf filename=2403.07769v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with <b>large</b> <b>language</b> <b>models</b> <b>(LLM),</b> which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical <b>reasoning</b> and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from <b>large</b> <b>language</b> <b>models</b> <b>(LLM),</b> each with distinct prototyping that considers behavioral elements, driven by strategies that stimulate the generation of knowledge based on the use case proposed in the scenario (role-play) business, using a discussion approach between agents (guided conversation). We demonstrate the potential of developing agents useful for organizational strategies, based on multi-agent system theories (SMA) and innovative uses based on <b>large</b> <b>language</b> <b>models</b> <b>(LLM</b> based), offering a differentiated and adaptable experiment to different applications, complexities, domains, and capabilities from <b>LLM.</b></p></p class="citation"></blockquote><h3 id=26--272323-an-improved-strategy-for-blood-glucose-control-using-multi-step-deep-reinforcement-learning-weiwei-gu-et-al-2024>(2/6 | 272/323) An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning (Weiwei Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiwei Gu, Senquan Wang. (2024)<br><strong>An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Benchmarking, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07566v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07566v2.pdf filename=2403.07566v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blood Glucose (BG) control involves keeping an individual&rsquo;s BG within a healthy range through extracorporeal insulin injections is an important task for people with type 1 diabetes. However,traditional patient self-management is cumbersome and risky. Recent research has been devoted to exploring individualized and automated BG control approaches, among which Deep <b>Reinforcement</b> <b>Learning</b> (DRL) shows potential as an emerging approach. In this paper, we use an exponential decay model of drug concentration to convert the formalization of the BG control problem, which takes into account the delay and prolongedness of drug effects, from a PAE-POMDP (Prolonged Action Effect-Partially Observable <b>Markov</b> <b>Decision</b> <b>Process)</b> to a MDP, and we propose a novel multi-step DRL-based algorithm to solve the problem. The Prioritized Experience Replay (PER) sampling method is also used in it. Compared to single-step bootstrapped updates, multi-step learning is more efficient and reduces the influence from biasing targets. Our proposed method converges faster and achieves higher cumulative rewards compared to the <b>benchmark</b> in the same training environment, and improves the time-in-range (TIR), the percentage of time the patient&rsquo;s BG is within the target range, in the evaluation phase. Our work validates the effectiveness of multi-step <b>reinforcement</b> <b>learning</b> in BG control, which may help to explore the optimal glycemic control measure and improve the survival of diabetic patients.</p></p class="citation"></blockquote><h3 id=36--273323-online-continual-learning-for-interactive-instruction-following-agents-byeonghwi-kim-et-al-2024>(3/6 | 273/323) Online Continual Learning For Interactive Instruction Following Agents (Byeonghwi Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Byeonghwi Kim, Minhyuk Seo, Jonghyun Choi. (2024)<br><strong>Online Continual Learning For Interactive Instruction Following Agents</strong><br><button class=copy-to-clipboard title="Online Continual Learning For Interactive Instruction Following Agents" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-RO, cs.AI<br>Keyword Score: 20<br>Keywords: Continual Learning, Instruction Following<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07548v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07548v2.pdf filename=2403.07548v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two <b>continual</b> <b>learning</b> setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous &lsquo;data prior&rsquo; based <b>continual</b> <b>learning</b> methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information during training (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state of the art in our empirical validations by noticeable margins. The project page including codes is <a href=https://github.com/snumprlab/cl-alfred>https://github.com/snumprlab/cl-alfred</a>.</p></p class="citation"></blockquote><h3 id=46--274323-perennial-semantic-data-terms-of-use-for-decentralized-web-rui-zhao-et-al-2024>(4/6 | 274/323) Perennial Semantic Data Terms of Use for Decentralized Web (Rui Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Zhao, Jun Zhao. (2024)<br><strong>Perennial Semantic Data Terms of Use for Decentralized Web</strong><br><button class=copy-to-clipboard title="Perennial Semantic Data Terms of Use for Decentralized Web" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs-LO, cs.AI<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07587v1.pdf filename=2403.07587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods&rsquo;. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated <b>reasoning</b> verifies compliance, and also derives policies for output data. This constitutes a ``perennial&rsquo;&rsquo; DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the <b>reasoning</b> engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance <b>benchmark.</b> We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability.</p></p class="citation"></blockquote><h3 id=56--275323-optimal-design-and-implementation-of-an-open-source-emulation-platform-for-user-centric-shared-e-mobility-services-maqsood-hussain-shah-et-al-2024>(5/6 | 275/323) Optimal Design and Implementation of an Open-source Emulation Platform for User-Centric Shared E-mobility Services (Maqsood Hussain Shah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maqsood Hussain Shah, Yue Ding, Shaoshu Zhu, Yingqi Gu, Mingming Liu. (2024)<br><strong>Optimal Design and Implementation of an Open-source Emulation Platform for User-Centric Shared E-mobility Services</strong><br><button class=copy-to-clipboard title="Optimal Design and Implementation of an Open-source Emulation Platform for User-Centric Shared E-mobility Services" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07964v1.pdf filename=2403.07964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In response to the escalating global challenge of increasing emissions and pollution in transportation, shared electric mobility services, encompassing e-cars, e-bikes, and e-scooters, have emerged as a popular strategy. However, existingshared electric mobility services exhibit critical design deficiencies, including insufficient service integration, imprecise energy consumption forecasting, limited scalability and geographical coverage, and a notable absence of a user-centric perspective, particularly in the context of <b>multi-modal</b> transportation. More importantly, there is no consolidated open-source framework which could benefit the e-mobility research community. This paper aims to bridge this gap by providing a pioneering open-source framework for shared e-mobility. The proposed framework, with an agent-in-the-loop approach and modular architecture, is tailored to diverse user preferences and offers enhanced customization. We demonstrate the viability of this framework by solving an integrated <b>multi-modal</b> route-optimization problem using the modified Ant Colony Optimization (ACO) algorithm. The primary contribution of this work is to provide a collaborative and transparent framework to tackle the dynamic challenges in the field of e-mobility research using a consolidated approach.</p></p class="citation"></blockquote><h3 id=66--276323-relevance-score-a-landmark-like-heuristic-for-planning-oliver-kim-et-al-2024>(6/6 | 276/323) Relevance Score: A Landmark-Like Heuristic for Planning (Oliver Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oliver Kim, Mohan Sridharan. (2024)<br><strong>Relevance Score: A Landmark-Like Heuristic for Planning</strong><br><button class=copy-to-clipboard title="Relevance Score: A Landmark-Like Heuristic for Planning" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-8, cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07510v1.pdf filename=2403.07510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Landmarks are facts or actions that appear in all valid solutions of a planning problem. They have been used successfully to calculate heuristics that guide the search for a plan. We investigate an extension to this concept by defining a novel &ldquo;relevance score&rdquo; that helps identify facts or actions that appear in most but not all plans to achieve any given goal. We describe an approach to compute this relevance score and use it as a heuristic in the search for a plan. We experimentally compare the performance of our approach with that of a state of the art landmark-based heuristic planning approach using <b>benchmark</b> planning problems. While the original landmark-based heuristic leads to better performance on problems with well-defined landmarks, our approach substantially improves performance on problems that lack non-trivial landmarks.</p></p class="citation"></blockquote><h2 id=cssd-3>cs.SD (3)</h2><h3 id=13--277323-multichannel-long-term-streaming-neural-speech-enhancement-for-static-and-moving-speakers-changsheng-quan-et-al-2024>(1/3 | 277/323) Multichannel Long-Term Streaming Neural Speech Enhancement for Static and Moving Speakers (Changsheng Quan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changsheng Quan, Xiaofei Li. (2024)<br><strong>Multichannel Long-Term Streaming Neural Speech Enhancement for Static and Moving Speakers</strong><br><button class=copy-to-clipboard title="Multichannel Long-Term Streaming Neural Speech Enhancement for Static and Moving Speakers" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Fine-tuning, Recurrent Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07675v1.pdf filename=2403.07675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we extend our previously proposed offline SpatialNet for long-term streaming multichannel speech enhancement in both static and moving speaker scenarios. SpatialNet exploits spatial information, such as the spatial/steering direction of speech, for discriminating between target speech and interferences, and achieved outstanding performance. The core of SpatialNet is a narrow-band <b>self-attention</b> module used for learning the temporal dynamic of spatial vectors. Towards long-term streaming speech enhancement, we propose to replace the offline <b>self-attention</b> network with online networks that have linear inference complexity w.r.t signal length and meanwhile maintain the capability of learning long-term information. Three variants are developed based on (i) masked <b>self-attention,</b> (ii) Retention, a <b>self-attention</b> variant with linear inference complexity, and (iii) Mamba, a structured-state-space-based <b>RNN-like</b> network. Moreover, we investigate the length extrapolation ability of different networks, namely test on signals that are much longer than training signals, and propose a short-signal training plus long-signal <b>fine-tuning</b> strategy, which largely improves the length extrapolation ability of the networks within limited training time. Overall, the proposed online SpatialNet achieves outstanding speech enhancement performance for long audio streams, and for both static and moving speakers. The proposed method will be open-sourced in <a href=https://github.com/Audio-WestlakeU/NBSS>https://github.com/Audio-WestlakeU/NBSS</a>.</p></p class="citation"></blockquote><h3 id=23--278323-boosting-keyword-spotting-through-on-device-learnable-user-speech-characteristics-cristian-cioflan-et-al-2024>(2/3 | 278/323) Boosting keyword spotting through on-device learnable user speech characteristics (Cristian Cioflan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristian Cioflan, Lukas Cavigelli, Luca Benini. (2024)<br><strong>Boosting keyword spotting through on-device learnable user speech characteristics</strong><br><button class=copy-to-clipboard title="Boosting keyword spotting through on-device learnable user speech characteristics" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07802v1.pdf filename=2403.07802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Keyword spotting systems for always-on TinyML-constrained applications require on-site tuning to boost the accuracy of offline trained classifiers when deployed in unseen inference conditions. Adapting to the speech peculiarities of target users requires many in-domain samples, often unavailable in real-world scenarios. Furthermore, current on-device learning techniques rely on computationally intensive and memory-hungry backbone update schemes, unfit for always-on, battery-powered devices. In this work, we propose a novel on-device learning architecture, composed of a pretrained backbone and a user-aware embedding learning the user&rsquo;s speech characteristics. The so-generated features are fused and used to classify the input utterance. For domain shifts generated by unseen speakers, we measure error rate reductions of up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google Speech Commands dataset, through the inexpensive update of the user projections. We moreover demonstrate the <b>few-shot</b> <b>learning</b> capabilities of our proposed architecture in sample- and class-scarce learning conditions. With 23.7 kparameters and 1 MFLOP per epoch required for on-device training, our system is feasible for TinyML applications aimed at battery-powered microcontrollers.</p></p class="citation"></blockquote><h3 id=33--279323-on-device-domain-learning-for-keyword-spotting-on-low-power-extreme-edge-embedded-systems-cristian-cioflan-et-al-2024>(3/3 | 279/323) On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge Embedded Systems (Cristian Cioflan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristian Cioflan, Lukas Cavigelli, Manuele Rusci, Miguel de Prado, Luca Benini. (2024)<br><strong>On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge Embedded Systems</strong><br><button class=copy-to-clipboard title="On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge Embedded Systems" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10549v1.pdf filename=2403.10549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Keyword spotting accuracy degrades when neural networks are exposed to noisy environments. On-site adaptation to previously unseen noise is crucial to recovering accuracy loss, and on-device learning is required to ensure that the adaptation process happens entirely on the edge device. In this work, we propose a fully on-device <b>domain</b> <b>adaptation</b> system achieving up to 14% accuracy gains over already-robust keyword spotting models. We enable on-device learning with less than 10 kB of memory, using only 100 labeled utterances to recover 5% accuracy after adapting to the complex speech noise. We demonstrate that <b>domain</b> <b>adaptation</b> can be achieved on ultra-low-power microcontrollers with as little as 806 mJ in only 14 s on always-on, battery-operated devices.</p></p class="citation"></blockquote><h2 id=csce-5>cs.CE (5)</h2><h3 id=15--280323-optimization-of-pressure-management-strategies-for-geological-co2-sequestration-using-surrogate-model-based-reinforcement-learning-jungang-chen-et-al-2024>(1/5 | 280/323) Optimization of Pressure Management Strategies for Geological CO2 Sequestration Using Surrogate Model-based Reinforcement Learning (Jungang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jungang Chen, Eduardo Gildin, John E. Killough. (2024)<br><strong>Optimization of Pressure Management Strategies for Geological CO2 Sequestration Using Surrogate Model-based Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Optimization of Pressure Management Strategies for Geological CO2 Sequestration Using Surrogate Model-based Reinforcement Learning" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07360v1.pdf filename=2403.07360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Injecting greenhouse gas into deep underground reservoirs for permanent storage can inadvertently lead to fault reactivation, caprock fracturing and greenhouse gas leakage when the injection-induced stress exceeds the critical threshold. Extraction of pre-existing fluids at various stages of injection process, referred as pressure management, can mitigate associated risks and lessen environmental impact. However, identifying optimal pressure management strategies typically requires thousands of full-order <b>simulations</b> due to the need for function evaluations, making the process computationally prohibitive. This paper introduces a novel surrogate model-based <b>reinforcement</b> <b>learning</b> method for devising optimal pressure management strategies for geological CO2 sequestration efficiently. Our approach comprises two steps. Firstly, a surrogate model is developed through the embed to control method, which employs an encoder-transition-decoder structure to learn latent dynamics. Leveraging this proxy model, <b>reinforcement</b> <b>learning</b> is utilized to find an optimal strategy that maximizes economic benefits while satisfying various control constraints. The <b>reinforcement</b> <b>learning</b> agent receives the latent state space representation and immediate reward tailored for CO2 sequestration and choose real-time controls which are subject to predefined engineering constraints in order to maximize the long-term cumulative rewards. To demonstrate its effectiveness, this framework is applied to a compositional <b>simulation</b> model where CO2 is injected into saline aquifer. The results reveal that our surrogate model-based <b>reinforcement</b> <b>learning</b> approach significantly optimizes CO2 sequestration strategies, leading to notable economic gains compared to baseline scenarios.</p></p class="citation"></blockquote><h3 id=25--281323-a-boundary-integral-based-particle-initialization-algorithm-for-smooth-particle-hydrodynamics-parikshit-boregowda-et-al-2024>(2/5 | 281/323) A boundary integral based particle initialization algorithm for Smooth Particle Hydrodynamics (Parikshit Boregowda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parikshit Boregowda, G R Liu. (2024)<br><strong>A boundary integral based particle initialization algorithm for Smooth Particle Hydrodynamics</strong><br><button class=copy-to-clipboard title="A boundary integral based particle initialization algorithm for Smooth Particle Hydrodynamics" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-NA, cs.CE, math-NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07779v1.pdf filename=2403.07779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algorithms for initializing particle distribution in SPH <b>simulations</b> of complex geometries have been proven essential for improving the accuracy of SPH <b>simulations.</b> However, no such algorithms exist for boundary integral SPH models, which can model complex geometries without needing virtual particle layers. This study introduces a Boundary Integral based Particle Initialization (BIPI) algorithm. It consists of a particle-shifting technique carefully designed to redistribute particles to fit the boundary by using the boundary integral formulation for particles adjacent to the boundary. The proposed BIPI algorithm gives special consideration to particles adjacent to the boundary to prevent artificial volume compression. It can automatically produce a &ldquo;uniform&rdquo; particle distribution with reduced and stabilized concentration gradient for domains with complex geometrical shapes. Finally, a number of examples are presented to demonstrate the effectiveness of the proposed algorithm.</p></p class="citation"></blockquote><h3 id=35--282323-towards-code-generation-for-octree-based-multigrid-solvers-richard-angersbach-et-al-2024>(3/5 | 282/323) Towards Code Generation for Octree-Based Multigrid Solvers (Richard Angersbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard Angersbach, Sebastian Kuckuck, Harald Köstler. (2024)<br><strong>Towards Code Generation for Octree-Based Multigrid Solvers</strong><br><button class=copy-to-clipboard title="Towards Code Generation for Octree-Based Multigrid Solvers" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 13<br>Keywords: Benchmarking, Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08063v1.pdf filename=2403.08063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel method designed to generate multigrid solvers optimized for octree-based software frameworks. Our approach focuses on accurately capturing local features within a domain while leveraging the efficiency inherent in multigrid techniques. We outline the essential steps involved in generating specialized kernels for local refinement and communication routines, integrating on-the-fly interpolations to seamlessly transfer information between refinement levels. For this purpose, we established a software coupling via an automatic fusion of generated multigrid solvers and communication kernels with manual implementations of complex octree data structures and algorithms often found in established software frameworks. We demonstrate the effectiveness of our method through numerical experiments with different interpolation orders. Large-scale <b>benchmarks</b> conducted on the SuperMUC-NG CPU cluster underscore the advantages of our approach, offering a comparison against a reference implementation to highlight the benefits of our method and <b>code</b> <b>generation</b> in general.</p></p class="citation"></blockquote><h3 id=45--283323-a-time-adaptive-finite-element-phase-field-model-suitable-for-rate-independent-fracture-mechanics-felix-rörentrop-et-al-2024>(4/5 | 283/323) A time-adaptive finite element phase-field model suitable for rate-independent fracture mechanics (Felix Rörentrop et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Rörentrop, Samira Boddin, Dorothee Knees, Jörn Mosler. (2024)<br><strong>A time-adaptive finite element phase-field model suitable for rate-independent fracture mechanics</strong><br><button class=copy-to-clipboard title="A time-adaptive finite element phase-field model suitable for rate-independent fracture mechanics" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: 35D40, 35Q74, 65M12, 74H15, 74R10, 74R05, cs-CE, cs.CE<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07461v1.pdf filename=2403.07461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The modeling of cracks is an important topic - both in engineering as well as in mathematics. Since crack propagation is characterized by a free boundary value problem (the <b>geometry</b> of the crack is not known beforehand, but part of the solution), approximations of the underlying sharp-interface problem based on phase-field models are often considered. Focusing on a rate-independent setting, these models are defined by a unidirectional gradient-flow of an energy functional. Since this energy functional is non-convex, the evolution of the variables such as the displacement field and the phase-field variable might be discontinuous in time leading to so-called brutal crack growth. For this reason, solution concepts have to be carefully chosen in order to predict discontinuities that are physically reasonable. One such concept is that of Balanced Viscosity solutions (BV solutions). This concept predicts physically sound energy trajectories that do not jump across energy barriers. The paper deals with a time-adaptive finite element phase-field model for rate-independent fracture which converges to BV solutions. The model is motivated by constraining the pseudo-velocity of the crack tip. The resulting constrained minimization problem is solved by the augmented Lagrangian method. Numerical examples highlight the predictive capabilities of the model and furthermore show the efficiency and the robustness of the final algorithm.</p></p class="citation"></blockquote><h3 id=55--284323-towards-full-automation-of-geometry-extraction-for-biomechanical-analysis-of-abdominal-aortic-aneurysm-neural-network-based-versus-classical-methodologies-farah-alkhatib-et-al-2024>(5/5 | 284/323) Towards Full Automation of Geometry Extraction for Biomechanical Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical Methodologies (Farah Alkhatib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Farah Alkhatib, Mostafa Jamshidian, Donatien Le Liepvre, Florian Bernard, Ludovic Minvielle, Adam Wittek, Karol Miller. (2024)<br><strong>Towards Full Automation of Geometry Extraction for Biomechanical Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical Methodologies</strong><br><button class=copy-to-clipboard title="Towards Full Automation of Geometry Extraction for Biomechanical Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical Methodologies" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07238v1.pdf filename=2403.07238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study we investigated the impact of image segmentation methods on the results of stress computation in the wall of abdominal aortic aneurysms (AAAs). We compared wall stress distributions and magnitudes calculated from <b>geometry</b> models obtained from classical semi-automated segmentation versus automated neural network-based segmentation. Ten different AAA contrast-enhanced computed tomography (CT) images were semi-automatically segmented by an analyst, taking, depending on the quality of an image, between 15 and 40 minutes of human effort per patient. The same images were automatically segmented using PRAEVAorta 2, commercial software by NUREA (<a href=https://www.nurea-soft.com/)>https://www.nurea-soft.com/)</a>, developed based on artificial intelligence (AI) algorithms, requiring only 1-2 minutes of computer time per patient. Aneurysm wall stress calculations performed using the BioPARR software (<a href=https://bioparr.mech.uwa.edu.au/>https://bioparr.mech.uwa.edu.au/</a>) revealed that, compared to the classical semi-automated segmentation, the automatic neural network-based segmentation leads to equivalent stress distributions, and slightly higher peak and 99th percentile maximum principal stress values. This difference is due to consistently larger lumen surface areas in automatically segmented models as compared to classical semi-automated segmentations, resulting in greater total pressure load on the wall. Our findings are a steppingstone toward a fully automated pipeline for biomechanical analysis of AAAs, starting with CT scans and concluding with wall stress assessment, while at the same time highlighting the critical importance of the repeatable and accurate segmentation of the lumen, the difficult problem often underestimated by the literature.</p></p class="citation"></blockquote><h2 id=csit-5>cs.IT (5)</h2><h3 id=15--285323-6d-movable-antenna-based-on-user-distribution-modeling-and-optimization-xiaodan-shao-et-al-2024>(1/5 | 285/323) 6D Movable Antenna Based on User Distribution: Modeling and Optimization (Xiaodan Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaodan Shao, Qijun Jiang, Rui Zhang. (2024)<br><strong>6D Movable Antenna Based on User Distribution: Modeling and Optimization</strong><br><button class=copy-to-clipboard title="6D Movable Antenna Based on User Distribution: Modeling and Optimization" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08123v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08123v2.pdf filename=2403.08123v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a new six-dimensional (6D) movable antenna (6DMA) system for future wireless networks to improve the communication performance. Unlike the traditional fixed-position antenna (FPA) and existing fluid antenna/two-dimensional (2D) movable antenna (FA/2DMA) systems that adjust the positions of antennas only, the proposed 6DMA system consists of distributed antenna surfaces with independently adjustable three-dimensional (3D) positions as well as 3D rotations within a given space. In particular, this paper applies the 6DMA to the base station (BS) in wireless networks to provide full degrees of freedom (DoFs) for the BS to adapt to the dynamic user spatial distribution in the network. However, a challenging new problem arises on how to optimally control the 6D positions and rotations of all 6DMA surfaces at the BS to maximize the network capacity based on the user spatial distribution, subject to the practical constraints on 6D antennas&rsquo; movement. To tackle this problem, we first model the 6DMA-enabled BS and the user channels with the BS in terms of 6D positions and rotations of all 6DMA surfaces. Next, we propose an efficient alternating optimization algorithm to search for the best 6D positions and rotations of all 6DMA surfaces by leveraging the Monte Carlo <b>simulation</b> technique. Specifically, we sequentially optimize the 3D position/3D rotation of each 6DMA surface with those of the other surfaces fixed in an iterative manner. Numerical results show that our proposed 6DMA-BS can significantly improve the network capacity as compared to the <b>benchmark</b> BS architectures with FPAs or 6DMAs with limited/partial movability, especially when the user distribution is more spatially non-uniform.</p></p class="citation"></blockquote><h3 id=25--286323-multi-source-scheduling-and-resource-allocation-for-age-of-semantic-importance-optimization-in-status-update-systems-lunyuan-chen-et-al-2024>(2/5 | 286/323) Multi-source Scheduling and Resource Allocation for Age-of-Semantic-Importance Optimization in Status Update Systems (Lunyuan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lunyuan Chen, Jie Gong. (2024)<br><strong>Multi-source Scheduling and Resource Allocation for Age-of-Semantic-Importance Optimization in Status Update Systems</strong><br><button class=copy-to-clipboard title="Multi-source Scheduling and Resource Allocation for Age-of-Semantic-Importance Optimization in Status Update Systems" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07386v1.pdf filename=2403.07386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, semantic communication is progressively emerging as an effective means of facilitating intelligent and context-aware communication. However, current researches seldom simultaneously consider the reliability and timeliness of semantic communication, where scheduling and resource allocation (SRA) plays a crucial role. In contrast, conventional age-based approaches cannot seamlessly extend to semantic communication due to their oversight of semantic importance. To bridge this gap, we introduce a novel metric: Age of Semantic Importance (AoSI), which adaptly captures both the freshness of information and its semantic importance. Utilizing AoSI, we formulate an average AoSI minimization problem by optimizing multi-source SRA. To address this problem, we proposed a AoSI-aware joint SRA algorithm based on Deep Q-Network (DQN). <b>Simulation</b> results validate the effectiveness of our proposed method, demonstrating its ability to facilitate timely and reliable semantic communication.</p></p class="citation"></blockquote><h3 id=35--287323-d2-jscc-digital-deep-joint-source-channel-coding-for-semantic-communications-jianhao-huang-et-al-2024>(3/5 | 287/323) D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic Communications (Jianhao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhao Huang, Kai Yuan, Chuan Huang, Kaibin Huang. (2024)<br><strong>D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic Communications</strong><br><button class=copy-to-clipboard title="D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic Communications" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-MM, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07338v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07338v3.pdf filename=2403.07338v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic communications (SemCom) have emerged as a new paradigm for supporting sixth-generation applications, where semantic features of data are transmitted using artificial intelligence algorithms to attain high communication efficiencies. Most existing SemCom techniques utilize deep neural networks (DNNs) to implement analog source-channel mappings, which are incompatible with existing digital communication architectures. To address this issue, this paper proposes a novel framework of digital deep joint source-channel coding (D$^2$-JSCC) targeting image transmission in SemCom. The framework features digital source and channel codings that are jointly optimized to reduce the end-to-end (E2E) distortion. First, deep source coding with an adaptive density model is designed to encode semantic features according to their distributions. Second, digital channel coding is employed to protect encoded features against channel distortion. To facilitate their joint design, the E2E distortion is characterized as a function of the source and channel rates via the analysis of the Bayesian model and Lipschitz assumption on the DNNs. Then to minimize the E2E distortion, a two-step algorithm is proposed to control the source-channel rates for a given channel signal-to-noise ratio. <b>Simulation</b> results reveal that the proposed framework outperforms classic deep JSCC and mitigates the cliff and leveling-off effects, which commonly exist for separation-based approaches.</p></p class="citation"></blockquote><h3 id=45--288323-achievable-rate-analysis-and-optimization-of-double-ris-assisted-spatially-correlated-mimo-with-statistical-csi-kaizhe-xu-et-al-2024>(4/5 | 288/323) Achievable Rate Analysis and Optimization of Double-RIS Assisted Spatially Correlated MIMO with Statistical CSI (Kaizhe Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaizhe Xu, Jiajia Guo, Jun Zhang, Shi Jin, Shaodan Ma. (2024)<br><strong>Achievable Rate Analysis and Optimization of Double-RIS Assisted Spatially Correlated MIMO with Statistical CSI</strong><br><button class=copy-to-clipboard title="Achievable Rate Analysis and Optimization of Double-RIS Assisted Spatially Correlated MIMO with Statistical CSI" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07274v1.pdf filename=2403.07274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS) is a novel meta-material which can form a smart radio environment by dynamically altering reflection directions of the impinging electromagnetic waves. In the prior literature, the inter-RIS links which also contribute to the performance of the whole system are usually neglected when multiple RISs are deployed. In this paper we investigate a general double-RIS assisted multiple-input multiple-output (MIMO) wireless communication system under spatially correlated non line-of-sight propagation channels, where the cooperation of the double RISs is also considered. The design objective is to maximize the achievable ergodic rate based on full statistical channel state information (CSI). Specifically, we firstly present a closed-form asymptotic expression for the achievable ergodic rate by utilizing replica method from statistical physics. Then a full statistical CSI-enabled optimal design is proposed which avoids high pilot training overhead compared to instantaneous CSI-enabled design. To further reduce the signal processing overhead and lower the complexity for practical realization, a common-phase scheme is proposed to design the double RISs. <b>Simulation</b> results show that the derived asymptotic ergodic rate is quite accurate even for small-sized antenna arrays. And the proposed optimization algorithm can achieve substantial gain at the expense of a low overhead and complexity. Furthermore, the cooperative double-RIS assisted MIMO framework is proven to achieve superior ergodic rate performance and high communication reliability under harsh propagation environment.</p></p class="citation"></blockquote><h3 id=55--289323-approaching-rate-distortion-limits-in-neural-compression-with-lattice-transform-coding-eric-lei-et-al-2024>(5/5 | 289/323) Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding (Eric Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti. (2024)<br><strong>Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding</strong><br><button class=copy-to-clipboard title="Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07320v1.pdf filename=2403.07320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal in a one-shot sense on certain sources, we show that it is highly sub-optimal on i.i.d. sequences, and in fact always recovers scalar <b>quantization</b> of the original source sequence. We demonstrate that the sub-optimality is due to the choice of <b>quantization</b> scheme in the latent space, and not the transform design. By employing lattice <b>quantization</b> instead of scalar <b>quantization</b> in the latent space, we demonstrate that Lattice Transform Coding (LTC) is able to recover optimal vector <b>quantization</b> at various dimensions and approach the asymptotically-achievable rate-distortion function at reasonable complexity. On general vector sources, LTC improves upon standard neural compressors in one-shot coding performance. LTC also enables neural compressors that perform block coding on i.i.d. vector sources, which yields coding gain over optimal one-shot coding.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--290323-robocertprob-property-specification-for-probabilistic-robochart-models-kangfeng-ye-et-al-2024>(1/1 | 290/323) RoboCertProb: Property Specification for Probabilistic RoboChart Models (Kangfeng Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kangfeng Ye, Jim Woodcock. (2024)<br><strong>RoboCertProb: Property Specification for Probabilistic RoboChart Models</strong><br><button class=copy-to-clipboard title="RoboCertProb: Property Specification for Probabilistic RoboChart Models" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-AI, cs-LO, cs.LO<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08136v1.pdf filename=2403.08136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>RoboChart is a core notation in the RoboStar framework which brings modern modelling and formal verification technologies into software engineering for robotics. It is a timed and <b>probabilistic</b> <b>domain-specific</b> language for robotics and provides a UML-like architectural and state machine modelling. This work presents RoboCertProb for specifying quantitative properties of <b>probabilistic</b> <b>robotic</b> systems modelled in RoboChart. RoboCertProb&rsquo;s semantics is based on PCTL*. To interpret RoboCertProb over RoboChart models, we give a Markov semantics (DTMCs and <b>MDPs)</b> to RoboChart, derived from its existing transformation semantics to the PRISM language. In addition to property specification, RoboCertProb also entitles us to configure loose constants and unspecified functions and operations in RoboChart models. It allows us to set up environmental inputs to verify reactive <b>probabilistic</b> <b>systems</b> not directly supported in <b>probabilistic</b> <b>model</b> checkers like PRISM because they employ a closed-world assumption. We implement RoboCertProb in an accompanying tool of RoboChart, RoboTool, for specifying properties and automatically generating PRISM properties from them to formally verify RoboChart models using PRISM. We have used it to analyse the behaviour of software controllers for two real robots: an industrial painting robot and an agricultural robot for treating plants with UV lights.</p></p class="citation"></blockquote><h2 id=mathna-3>math.NA (3)</h2><h3 id=13--291323-preconditioners-based-on-voronoi-quantizers-of-random-variable-coefficients-for-stochastic-elliptic-partial-differential-equations-nicolas-venkovic-et-al-2024>(1/3 | 291/323) Preconditioners based on Voronoi quantizers of random variable coefficients for stochastic elliptic partial differential equations (Nicolas Venkovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Venkovic, Paul Mycek, Olivier Le Maître, Luc Giraud. (2024)<br><strong>Preconditioners based on Voronoi quantizers of random variable coefficients for stochastic elliptic partial differential equations</strong><br><button class=copy-to-clipboard title="Preconditioners based on Voronoi quantizers of random variable coefficients for stochastic elliptic partial differential equations" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07824v1.pdf filename=2403.07824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A preconditioning strategy is proposed for the iterative solve of large numbers of linear systems with variable matrix and right-hand side which arise during the computation of solution statistics of stochastic elliptic partial differential equations with random variable coefficients sampled by Monte Carlo. Building on the assumption that a truncated Karhunen-Lo`{e}ve expansion of a known transform of the random variable coefficient is known, we introduce a compact representation of the random coefficient in the form of a Voronoi quantizer. The number of Voronoi cells, each of which is represented by a centroidal variable coefficient, is set to the prescribed number $P$ of preconditioners. Upon sampling the random variable coefficient, the linear system assembled with a given realization of the coefficient is solved with the preconditioner whose centroidal variable coefficient is the closest to the realization. We consider different ways to define and obtain the centroidal variable coefficients, and we investigate the properties of the induced preconditioning strategies in terms of average number of solver iterations for sequential <b>simulations,</b> and of load balancing for parallel <b>simulations.</b> Another approach, which is based on deterministic grids on the system of stochastic coordinates of the truncated representation of the random variable coefficient, is proposed with a stochastic dimension which increases with the number $P$ of preconditioners. This approach allows to bypass the need for preliminary computations in order to determine the optimal stochastic dimension of the truncated approximation of the random variable coefficient for a given number of preconditioners.</p></p class="citation"></blockquote><h3 id=23--292323-a-novel-fast-iterative-moment-method-for-near-continuum-flows-guanghan-li-et-al-2024>(2/3 | 292/323) A novel fast iterative moment method for near-continuum flows (Guanghan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanghan Li, Chunwu Wang, Zhicheng Hu. (2024)<br><strong>A novel fast iterative moment method for near-continuum flows</strong><br><button class=copy-to-clipboard title="A novel fast iterative moment method for near-continuum flows" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 76P05, 65B99, 65M55, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07358v1.pdf filename=2403.07358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we develop a novel fast iterative moment method for the steady-state <b>simulation</b> of near-continuum flows, which are modeled by the high-order moment system derived from the Boltzmann-BGK equation. The fast convergence of the present method is mainly achieved by alternately solving the moment system and the hydrodynamic equations with compatible constitutive relations and boundary conditions. To be specific, the compatible hydrodynamic equations are solved in each iteration to get improved predictions of macroscopic quantities, which are subsequently utilized to expedite the evolution of the moment system. Additionally, a semi-implicit scheme treating the collision term implicitly is introduced for the moment system. With cell-by-cell sweeping strategy, the resulting alternating iteration can be further accelerated for steady-state computation. It is also worth mentioning that such an alternating iteration works well with the nonlinear multigrid method. Numerical experiments for planar Couette flow, shock structure, and lid-driven cavity flow are carried out to investigate the performance of the proposed fast iterative moment method, and all results show wonderful efficiency and robustness.</p></p class="citation"></blockquote><h3 id=33--293323-transparent-boundary-condition-and-its-effectively-local-approximation-for-the-schrödinger-equation-on-a-rectangular-computational-domain-samardhi-yadav-et-al-2024>(3/3 | 293/323) Transparent boundary condition and its effectively local approximation for the Schrödinger equation on a rectangular computational domain (Samardhi Yadav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samardhi Yadav, Vishal Vaibhav. (2024)<br><strong>Transparent boundary condition and its effectively local approximation for the Schrödinger equation on a rectangular computational domain</strong><br><button class=copy-to-clipboard title="Transparent boundary condition and its effectively local approximation for the Schrödinger equation on a rectangular computational domain" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-comp-ph<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07787v1.pdf filename=2403.07787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The transparent boundary condition for the free Schr"{o}dinger equation on a rectangular computational domain requires implementation of an operator of the form $\sqrt{\partial_t-i\triangle_{\Gamma}}$ where $\triangle_{\Gamma}$ is the Laplace-Beltrami operator. It is known that this operator is nonlocal in time as well as space which poses a significant challenge in developing an efficient numerical method of solution. The computational complexity of the existing methods scale with the number of time-steps which can be attributed to the nonlocal nature of the boundary operator. In this work, we report an effectively local approximation for the boundary operator such that the resulting complexity remains independent of number of time-steps. At the heart of this algorithm is a Pad'e approximant based rational approximation of certain fractional operators that handles corners of the domain adequately. For the spatial discretization, we use a Legendre-Galerkin spectral method with a new boundary adapted basis which ensures that the resulting linear system is banded. A compatible boundary-lifting procedure is also presented which accommodates the segments as well as the corners on the boundary. The proposed novel scheme can be implemented within the framework of any one-step time marching schemes. In particular, we demonstrate these ideas for two one-step methods, namely, the backward-differentiation formula of order 1 (BDF1) and the trapezoidal rule (TR). For the sake of comparison, we also present a <b>convolution</b> quadrature based scheme conforming to the one-step methods which is computationally expensive but serves as a golden standard. Finally, several numerical tests are presented to demonstrate the effectiveness of our novel method as well as to verify the order of convergence empirically.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=11--294323-generative-deep-learning-enabled-ultra-large-field-of-view-lens-free-imaging-ronald-b-liu-et-al-2024>(1/1 | 294/323) Generative deep learning-enabled ultra-large field-of-view lens-free imaging (Ronald B. Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ronald B. Liu, Zhe Liu, Max G. A. Wolf, Krishna P. Purohit, Gregor Fritz, Yi Feng, Carsten G. Hansen, Pierre O. Bagnaninchi, Xavier Casadevall i Solvas, Yunjie Yang. (2024)<br><strong>Generative deep learning-enabled ultra-large field-of-view lens-free imaging</strong><br><button class=copy-to-clipboard title="Generative deep learning-enabled ultra-large field-of-view lens-free imaging" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-CV, physics-optics, physics.optics<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07786v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07786v2.pdf filename=2403.07786v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advancements in high-throughput biomedical applications necessitate real-time, large field-of-view (FOV) imaging capabilities. Conventional lens-free imaging (LFI) systems, while addressing the limitations of physical lenses, have been constrained by dynamic, hard-to-model optical fields, resulting in a limited one-shot FOV of approximately 20 $mm^2$. This restriction has been a major bottleneck in applications like live-cell imaging and automation of microfluidic systems for biomedical research. Here, we present a deep-learning(DL)-based imaging framework - GenLFI - leveraging generative artificial intelligence (AI) for holographic image reconstruction. We demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$, surpassing the current LFI system by more than 20-fold, and even larger than the world&rsquo;s largest confocal microscope by 1.76 times. The resolution is at the sub-pixel level of 5.52 $\mu m$, without the need for a shifting light source. The <b>unsupervised</b> <b>learning-based</b> reconstruction does not require optical field modeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics and 3D cell models) in complex optical fields possible. This GenLFI framework unlocks the potential of LFI systems, offering a robust tool to tackle new frontiers in high-throughput biomedical applications such as drug discovery.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--295323-towards-a-dynamic-future-with-adaptable-computing-and-network-convergence-acnc-masoud-shokrnezhad-et-al-2024>(1/3 | 295/323) Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC) (Masoud Shokrnezhad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masoud Shokrnezhad, Hao Yu, Tarik Taleb, Richard Li, Kyunghan Lee, Jaeseung Song, Cedric Westphal. (2024)<br><strong>Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)</strong><br><button class=copy-to-clipboard title="Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-DC, cs-ET, cs-LG, cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Continual Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07573v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07573v1.pdf filename=2403.07573v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, <b>prompting</b> a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent requirements. ACNC encompasses two primary functionalities: state recognition and context detection. Given the intricate nature of the user-service-computing-network space, the paper employs dimension reduction to generate live, holistic, abstract system states in a hierarchical structure. To address the challenges posed by dynamic changes, <b>Continual</b> <b>Learning</b> (CL) is employed, classifying the system state into contexts controlled by dedicated ML agents, enabling them to operate efficiently. These two functionalities are intricately linked within a closed loop overseen by the End-to-End (E2E) orchestrator to allocate resources. The paper introduces the components of ACNC, proposes a Metaverse scenario to exemplify ACNC&rsquo;s role in resource provisioning with Segment Routing v6 (SRv6), outlines ACNC&rsquo;s workflow, details a numerical analysis for efficiency assessment, and concludes with discussions on relevant challenges and potential avenues for future research.</p></p class="citation"></blockquote><h3 id=23--296323-a-survey-on-federated-learning-in-intelligent-transportation-systems-rongqing-zhang-et-al-2024>(2/3 | 296/323) A Survey on Federated Learning in Intelligent Transportation Systems (Rongqing Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rongqing Zhang, Hanqiu Wang, Bing Li, Xiang Cheng, Liuqing Yang. (2024)<br><strong>A Survey on Federated Learning in Intelligent Transportation Systems</strong><br><button class=copy-to-clipboard title="A Survey on Federated Learning in Intelligent Transportation Systems" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07444v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07444v2.pdf filename=2403.07444v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of Intelligent Transportation System (ITS) has brought about comprehensive urban traffic information that not only provides convenience to urban residents in their daily lives but also enhances the efficiency of urban road usage, leading to a more harmonious and sustainable urban life. Typical scenarios in ITS mainly include traffic flow prediction, traffic target recognition, and vehicular edge computing. However, most current ITS applications rely on a centralized training approach where users upload source data to a cloud server with high computing power for management and centralized training. This approach has limitations such as poor real-time performance, data silos, and difficulty in guaranteeing data privacy. To address these limitations, <b>federated</b> <b>learning</b> (FL) has been proposed as a promising solution. In this paper, we present a comprehensive review of the application of FL in ITS, with a particular focus on three key scenarios: traffic flow prediction, traffic target recognition, and vehicular edge computing. For each scenario, we provide an in-depth analysis of its key characteristics, current challenges, and specific manners in which FL is leveraged. Moreover, we discuss the benefits that FL can offer as a potential solution to the limitations of the centralized training approach currently used in ITS applications.</p></p class="citation"></blockquote><h3 id=33--297323-online-digital-twin-empowered-content-resale-mechanism-in-age-of-information-aware-edge-caching-networks-yuhan-yi-et-al-2024>(3/3 | 297/323) Online Digital Twin-Empowered Content Resale Mechanism in Age of Information-Aware Edge Caching Networks (Yuhan Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhan Yi, Guanglin Zhang, Hai Jiang. (2024)<br><strong>Online Digital Twin-Empowered Content Resale Mechanism in Age of Information-Aware Edge Caching Networks</strong><br><button class=copy-to-clipboard title="Online Digital Twin-Empowered Content Resale Mechanism in Age of Information-Aware Edge Caching Networks" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-IT, cs-NI, cs.NI, math-IT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07868v1.pdf filename=2403.07868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For users requesting popular contents from content providers, edge caching can alleviate backhaul pressure and enhance the quality of experience of users. Recently there is also a growing concern about content freshness that is quantified by age of information (AoI). Therefore, AoI-aware online caching algorithms are required, which is challenging because the content popularity is usually unknown in advance and may vary over time. In this paper, we propose an online digital twin (DT) empowered content resale mechanism in AoI-aware edge caching networks. We aim to design an optimal two-timescale caching strategy to maximize the utility of an edge network service provider (ENSP). The formulated optimization problem is non-convex and NP-hard. To tackle this intractable problem, we propose a DT-assisted Online Caching Algorithm (DT-OCA). In specific, we first decompose our formulated problem into a series of subproblems, each handling a cache period. For each cache period, we use a DT-based prediction method to effectively capture future content popularity, and develop online caching strategy. Competitive ratio analysis and extensive experimental results demonstrate that our algorithm has promising performance, and outperforms other <b>benchmark</b> algorithms. Insightful observations are also found and discussed.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--298323-physics-transfer-learning-for-material-strength-screening-yingjie-zhao-et-al-2024>(1/1 | 298/323) Physics-Transfer Learning for Material Strength Screening (Yingjie Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingjie Zhao, Zian Zhang, Zhiping Xu. (2024)<br><strong>Physics-Transfer Learning for Material Strength Screening</strong><br><button class=copy-to-clipboard title="Physics-Transfer Learning for Material Strength Screening" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG, physics-comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07526v1.pdf filename=2403.07526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The strength of materials, like many problems in the natural sciences, spans multiple length and time scales, and the solution has to balance accuracy and performance. Peierls stress is one of the central concepts in crystal plasticity that measures the strength through the resistance of a dislocation to plastic flow. The determination of Peierls stress involves a multiscale nature depending on both elastic lattice responses and the energy landscape of crystal slips. Material screening by strength via the Peierls stress from first-principles calculations is computationally intractable for the nonlocal characteristics of dislocations, and not included in the state-of-the-art computational material databases. In this work, we propose a physics-transfer framework to learn the physics of crystal plasticity from empirical atomistic <b>simulations</b> and then predict the Peierls stress from chemically accurate density functional theory-based calculations of material parameters. Notably, the strengths of single-crystalline metals can be predicted from a few single-point calculations for the deformed lattice and on the {\gamma} surface, allowing efficient, high-throughput screening for material discovery. Uncertainty quantification is carried out to assess the accuracy of models and sources of errors, showing reduced physical and system uncertainties in the predictions by elevating the fidelity of training models. This physics-transfer framework can be generalized to other problems facing the accuracy-performance dilemma, by harnessing the hierarchy of physics in the multiscale models of materials science.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--299323-long-term-hydrothermal-bid-based-market-simulator-joaquim-dias-garcia-et-al-2024>(1/2 | 299/323) Long-term Hydrothermal Bid-based Market Simulator (Joaquim Dias Garcia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joaquim Dias Garcia, Alexandre Street, Mario Veiga Pereira. (2024)<br><strong>Long-term Hydrothermal Bid-based Market Simulator</strong><br><button class=copy-to-clipboard title="Long-term Hydrothermal Bid-based Market Simulator" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07270v1.pdf filename=2403.07270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simulating long-term hydrothermal bid-based markets considering strategic agents is a challenging task. The representation of strategic agents considering inter-temporal constraints within a stochastic framework brings additional complexity to the already difficult single-period bilevel, thus, non-convex, optimal bidding problem. Thus, we propose a <b>simulation</b> methodology that effectively addresses these challenges for large-scale hydrothermal power systems. We demonstrate the effectiveness of the framework through a case study with real data from the large-scale Brazilian power system. In the case studies, we show the effects of market concentration in power systems and how contracts can be used to mitigate them. In particular, we show how market power might affect the current setting in Brazil. The developed method can strongly benefit policy makers, market monitors, and market designers as <b>simulations</b> can be used to understand existing power systems and experiment with alternative designs.</p></p class="citation"></blockquote><h3 id=22--300323-pmbo-enhancing-black-box-optimization-through-multivariate-polynomial-surrogates-janina-schreiber-et-al-2024>(2/2 | 300/323) PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates (Janina Schreiber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Janina Schreiber, Pau Batlle, Damar Wicaksono, Michael Hecht. (2024)<br><strong>PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates</strong><br><button class=copy-to-clipboard title="PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, cs-MS, math-OC, math.OC<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07485v1.pdf filename=2403.07485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a surrogate-based <b>black-box</b> <b>optimization</b> method, termed Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit. We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions. The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization. Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation &ndash; Evolution Strategy (CMA-ES). This finding suggests that PMBO emerges as the pivotal choice among surrogate-based optimization methods when addressing low-dimensional optimization problems. Hereby, the simple nature of polynomials opens the opportunity for interpretation and analysis of the inferred surrogate model, providing a macroscopic perspective on the landscape of the objective function.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--301323-the-dawn-of-ai-native-eda-promises-and-challenges-of-large-circuit-models-lei-chen-et-al-2024>(1/1 | 301/323) The Dawn of AI-Native EDA: Promises and Challenges of Large Circuit Models (Lei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Chen, Yiqi Chen, Zhufei Chu, Wenji Fang, Tsung-Yi Ho, Yu Huang, Sadaf Khan, Min Li, Xingquan Li, Yun Liang, Yibo Lin, Jinwei Liu, Yi Liu, Guojie Luo, Zhengyuan Shi, Guangyu Sun, Dimitrios Tsaras, Runsheng Wang, Ziyi Wang, Xinming Wei, Zhiyao Xie, Qiang Xu, Chenhao Xue, Evangeline F. Y. Young, Bei Yu, Mingxuan Yuan, Haoyi Zhang, Zuodong Zhang, Yuxiang Zhao, Hui-Ling Zhen, Ziyang Zheng, Binwu Zhu, Keren Zhu, Sunan Zou. (2024)<br><strong>The Dawn of AI-Native EDA: Promises and Challenges of Large Circuit Models</strong><br><button class=copy-to-clipboard title="The Dawn of AI-Native EDA: Promises and Challenges of Large Circuit Models" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-ET, cs.AR<br>Keyword Score: 17<br>Keywords: Graph, Benchmarking, Multi-modal, Multi-modal, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07257v1.pdf filename=2403.07257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Within the Electronic Design Automation (EDA) domain, AI-driven solutions have emerged as formidable tools, yet they typically augment rather than redefine existing methodologies. These solutions often repurpose deep learning models from other domains, such as vision, text, and <b>graph</b> analytics, applying them to circuit design without tailoring to the unique complexities of electronic circuits. Such an AI4EDA approach falls short of achieving a holistic design synthesis and understanding, overlooking the intricate interplay of electrical, logical, and physical facets of circuit data. This perspective paper argues for a paradigm shift from AI4EDA towards AI-native EDA, integrating AI at the core of the design process. Pivotal to this vision is the development of a <b>multimodal</b> circuit <b>representation</b> <b>learning</b> technique, poised to provide a comprehensive understanding by harmonizing and extracting insights from varied data sources, such as functional specifications, RTL designs, circuit netlists, and physical layouts. We champion the creation of large circuit models (LCMs) that are inherently <b>multimodal,</b> crafted to decode and express the rich semantics and structures of circuit data, thus fostering more resilient, efficient, and inventive design methodologies. Embracing this AI-native philosophy, we foresee a trajectory that transcends the current innovation plateau in EDA, igniting a profound shift-left in electronic design methodology. The envisioned advancements herald not just an evolution of existing EDA tools but a revolution, giving rise to novel instruments of design-tools that promise to radically enhance design productivity and inaugurate a new epoch where the optimization of circuit performance, power, and area (PPA) is achieved not incrementally, but through leaps that redefine the <b>benchmarks</b> of electronic systems&rsquo; capabilities.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--302323-a-question-centric-multi-experts-contrastive-learning-framework-for-improving-the-accuracy-and-interpretability-of-deep-sequential-knowledge-tracing-models-hengyuan-zhang-et-al-2024>(1/2 | 302/323) A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models (Hengyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengyuan Zhang, Zitao Liu, Chenming Shang, Dawei Li, Yong Jiang. (2024)<br><strong>A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models</strong><br><button class=copy-to-clipboard title="A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs-LG, cs.CY<br>Keyword Score: 15<br>Keywords: Black Box, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07322v1.pdf filename=2403.07322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge tracing (KT) plays a crucial role in predicting students&rsquo; future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students&rsquo; knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model&rsquo;s prediction results in a manner that teachers find interpretable. This makes teachers accept the rationale behind the prediction results and utilize them to design teaching activities and tailored learning strategies for students. However, the inherent <b>black-box</b> <b>nature</b> of deep learning techniques often poses a hurdle for teachers to fully embrace the model&rsquo;s prediction results. To address these challenges, we propose a Question-centric Multi-experts <b>Contrastive</b> <b>Learning</b> framework for KT called Q-MCKT.</p></p class="citation"></blockquote><h3 id=22--303323-legally-binding-but-unfair-towards-assessing-fairness-of-privacy-policies-vincent-freiberger-et-al-2024>(2/2 | 303/323) Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies (Vincent Freiberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Freiberger, Erik Buchmann. (2024)<br><strong>Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies</strong><br><button class=copy-to-clipboard title="Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: K-4-m, cs-AI, cs-CL, cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08115v1.pdf filename=2403.08115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Privacy policies are expected to inform data subjects about their data protection rights. They should explain the data controller&rsquo;s data management practices, and make facts such as retention periods or data transfers to third parties transparent. Privacy policies only fulfill their purpose, if they are correctly perceived, interpreted, understood, and trusted by the data subject. Amongst others, this requires that a privacy policy is written in a fair way, e.g., it does not use polarizing terms, does not require a certain education, or does not assume a particular social background. In this work-in-progress paper, we outline our approach to assessing <b>fairness</b> in privacy policies. To this end, we identify from fundamental legal sources and <b>fairness</b> research, how the dimensions informational <b>fairness,</b> representational <b>fairness</b> and ethics/morality are related to privacy policies. We propose options to automatically assess policies in these <b>fairness</b> dimensions, based on text statistics, linguistic methods and artificial intelligence. Finally, we conduct initial experiments with German privacy policies to provide evidence that our approach is applicable. Our experiments indicate that there are indeed issues in all three dimensions of <b>fairness.</b> For example, our approach finds out if a policy discriminates against individuals with impaired reading skills or certain demographics, and identifies questionable ethics. This is important, as future privacy policies may be used in a corpus for legal artificial intelligence models.</p></p class="citation"></blockquote><h2 id=csgt-2>cs.GT (2)</h2><h3 id=12--304323-data-monetization-pathways-and-complex-dynamic-game-equilibrium-analysis-in-the-energy-industry-zongxian-wang-et-al-2024>(1/2 | 304/323) Data Monetization Pathways and Complex Dynamic Game Equilibrium Analysis in the Energy Industry (Zongxian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zongxian Wang, Jie Song. (2024)<br><strong>Data Monetization Pathways and Complex Dynamic Game Equilibrium Analysis in the Energy Industry</strong><br><button class=copy-to-clipboard title="Data Monetization Pathways and Complex Dynamic Game Equilibrium Analysis in the Energy Industry" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08082v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08082v3.pdf filename=2403.08082v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the most critical production factor in the era of the digital economy, data will have a significant impact on social production and development. Energy enterprises possess data that is interconnected with multiple industries, characterized by diverse needs, sensitivity, and long-term nature. The path to monetizing energy enterprises&rsquo; data is challenging yet crucial. This paper explores the game-theoretic aspects of the data monetization process in energy enterprises by considering the relationships between enterprises and trading platforms. We construct a class of game decision models and study their equilibrium strategies. Our analysis shows that enterprises and platforms can adjust respective benefits by regulating the wholesale price of data and the intensity of data value mining to form a benign equilibrium state. Furthermore, by integrating nonlinear dynamical theory, we discuss the dynamic characteristics present in multi-period repeated game processes. We find that decision-makers should keep the adjustment parameters and initial states within reasonable ranges in multi-period dynamic decision-making to avoid market failure. Finally, based on the theoretical and numerical analysis, we provide decision insights and <b>recommendations</b> for enterprise decision-making to facilitate data monetization through strategic interactions with trading platforms.</p></p class="citation"></blockquote><h3 id=22--305323-multi-apartment-rent-division-ariel-d-procaccia-et-al-2024>(2/2 | 305/323) Multi-Apartment Rent Division (Ariel D. Procaccia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang. (2024)<br><strong>Multi-Apartment Rent Division</strong><br><button class=copy-to-clipboard title="Multi-Apartment Rent Division" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08051v1.pdf filename=2403.08051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rent division is the well-studied problem of fairly assigning rooms and dividing rent among a set of roommates within a single apartment. A shortcoming of existing solutions is that renters are assumed to be considering apartments in isolation, whereas in reality, renters can choose among multiple apartments. In this paper, we generalize the rent division problem to the multi-apartment setting, where the goal is to both fairly choose an apartment among a set of alternatives and fairly assign rooms and rents within the chosen apartment. Our main contribution is a generalization of envy-freeness called rearrangeable envy-freeness. We show that a solution satisfying rearrangeable envy-freeness is guaranteed to exist and that it is possible to optimize over all rearrangeable envy-free solutions in polynomial time. We also define an even stronger <b>fairness</b> notion called universal envy-freeness and study its existence when values are drawn randomly.</p></p class="citation"></blockquote><h2 id=cssy-1>cs.SY (1)</h2><h3 id=11--306323-system-design-approach-for-control-of-differentially-private-dynamical-systems-raman-goyal-et-al-2024>(1/1 | 306/323) System Design Approach for Control of Differentially Private Dynamical Systems (Raman Goyal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raman Goyal, Dhrubajit Chowdhury, Shantanu Rane. (2024)<br><strong>System Design Approach for Control of Differentially Private Dynamical Systems</strong><br><button class=copy-to-clipboard title="System Design Approach for Control of Differentially Private Dynamical Systems" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SY<br>Categories: cs-SY, cs.SY, eess-SY, math-OC<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08065v1.pdf filename=2403.08065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel approach to concurrently design dynamic controllers and correlated <b>differential</b> <b>privacy</b> noise in dynamic control systems. An increase in privacy noise increases the system&rsquo;s privacy but adversely affects the system&rsquo;s performance. Our approach optimizes the noise distribution while shaping closed-loop system dynamics such that the privacy noise has the least impact on system performance and the most effect on system privacy. We further add privacy noise to both control input and system output to privatize the system&rsquo;s state for an adversary with access to both communication channels and direct output measurements. The study also suggests tailored privacy bounds for different states, providing a comprehensive framework for jointly optimizing system performance and privacy in the context of <b>differential</b> <b>privacy.</b></p></p class="citation"></blockquote><h2 id=statap-1>stat.AP (1)</h2><h3 id=11--307323-fusing-climate-data-products-using-a-spatially-varying-autoencoder-jacob-a-johnson-et-al-2024>(1/1 | 307/323) Fusing Climate Data Products using a Spatially Varying Autoencoder (Jacob A. Johnson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob A. Johnson, Matthew J. Heaton, William F. Christensen, Lynsie R. Warr, Summer B. Rupper. (2024)<br><strong>Fusing Climate Data Products using a Spatially Varying Autoencoder</strong><br><button class=copy-to-clipboard title="Fusing Climate Data Products using a Spatially Varying Autoencoder" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.AP<br>Categories: cs-LG, stat-AP, stat.AP<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07822v1.pdf filename=2403.07822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Autoencoders</b> are powerful machine learning models used to compress information from multiple data sources. However, <b>autoencoders,</b> like all artificial neural networks, are often unidentifiable and uninterpretable. This research focuses on creating an identifiable and interpretable <b>autoencoder</b> that can be used to meld and combine climate data products. The proposed <b>autoencoder</b> utilizes a Bayesian statistical framework, allowing for probabilistic interpretations while also varying spatially to capture useful spatial patterns across the various data products. Constraints are placed on the <b>autoencoder</b> as it learns patterns in the data, creating an interpretable consensus that includes the important features from each input. We demonstrate the utility of the <b>autoencoder</b> by combining information from multiple precipitation products in High Mountain Asia.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--308323-beyond-the-labels-unveiling-text-dependency-in-paralinguistic-speech-recognition-datasets-jan-pešán-et-al-2024>(1/2 | 308/323) Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets (Jan Pešán et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Pešán, Santosh Kesiraju, Lukáš Burget, Jan &lsquo;&lsquo;Honza&rsquo;&rsquo; Černocký. (2024)<br><strong>Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets</strong><br><button class=copy-to-clipboard title="Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, eess-AS, eess-SP, eess.AS<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07767v1.pdf filename=2403.07767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in <b>speech</b> <b>recognition</b> research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features. By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling. Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features. The study serves as a call to action for the research community to reevaluate the reliability of existing datasets and methodologies, ensuring that machine learning models genuinely learn what they are designed to recognize.</p></p class="citation"></blockquote><h3 id=22--309323-gender-ambiguous-voice-generation-through-feminine-speaking-style-transfer-in-male-voices-maria-koutsogiannaki-et-al-2024>(2/2 | 309/323) Gender-ambiguous voice generation through feminine speaking style transfer in male voices (Maria Koutsogiannaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria Koutsogiannaki, Shafel Mc Dowall, Ioannis Agiomyrgiannakis. (2024)<br><strong>Gender-ambiguous voice generation through feminine speaking style transfer in male voices</strong><br><button class=copy-to-clipboard title="Gender-ambiguous voice generation through feminine speaking style transfer in male voices" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07661v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07661v2.pdf filename=2403.07661v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, and under the umbrella of Responsible AI, efforts have been made to develop gender-ambiguous synthetic speech to represent with a single voice all individuals in the gender spectrum. However, research efforts have completely overlooked the speaking <b>style</b> <b>despite</b> differences found among binary and non-binary populations. In this work, we synthesise gender-ambiguous speech by combining the timbre of a male speaker with the manner of speech of a female speaker using voice morphing and pitch shifting towards the male-female boundary. Subjective evaluations indicate that the ambiguity of the morphed samples that convey the female speech <b>style</b> <b>is</b> higher than those that undergo plain pitch transformations suggesting that the speaking <b>style</b> <b>can</b> be a contributing factor in creating gender-ambiguous speech. To our knowledge, this is the first study that explicitly uses the transfer of the speaking <b>style</b> <b>to</b> create gender-ambiguous voices.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--310323-monocentric-or-polycentric-city-an-empirical-perspective-rémi-lemoy-2024>(1/1 | 310/323) Monocentric or polycentric city? An empirical perspective (Rémi Lemoy, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rémi Lemoy. (2024)<br><strong>Monocentric or polycentric city? An empirical perspective</strong><br><button class=copy-to-clipboard title="Monocentric or polycentric city? An empirical perspective" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-CY, physics-soc-ph, physics.soc-ph<br>Keyword Score: 10<br>Keywords: Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07624v1.pdf filename=2403.07624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Do cities have just one or several centers? Studies performing radial or monocentric analyses of cities are usually criticised by researchers stating that cities are actually polycentric, and this has been well known for a long time. Reversely, when cities are studied independently of any center, other researchers will wonder how the variables of interest evolve with the distance to the center, because this distance is known to be a major determinant at the intra-urban scale. Both monocentric and polycentric formalisms have been introduced centuries (respectively, decades) ago for the study of urban areas, and used both on the empirical and the theoretical side in different disciplines (economics, geography, complex systems, physics&mldr;). The present work performs a synthesis of both viewpoints on cities, regarding their use in the literature, and explores with data on European urban areas how some cities considered to be the most polycentric in Europe compare to more standard cities when studied through a combination of radial analysis and <b>scaling</b> <b>laws.</b></p></p class="citation"></blockquote><h2 id=csma-3>cs.MA (3)</h2><h3 id=13--311323-ensembling-prioritized-hybrid-policies-for-multi-agent-pathfinding-huijie-tang-et-al-2024>(1/3 | 311/323) Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding (Huijie Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huijie Tang, Federico Berto, Jinkyoo Park. (2024)<br><strong>Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding</strong><br><button class=copy-to-clipboard title="Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-LG, cs-MA, cs-RO, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07559v1.pdf filename=2403.07559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Secondly, we propose Q value-based methods for prioritized resolution of conflicts as well as deadlock situations. Finally, we introduce a robust ensemble method that can efficiently collect the best out of multiple possible solutions. We empirically evaluate EPH in complex multi-agent environments and demonstrate competitive performance against state-of-the-art neural methods for MAPF.</p></p class="citation"></blockquote><h3 id=23--312323-ariadne-and-theseus-exploration-and-rendezvous-with-two-mobile-agents-in-an-unknown-graph-romain-cosson-2024>(2/3 | 312/323) Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph (Romain Cosson, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Romain Cosson. (2024)<br><strong>Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph</strong><br><button class=copy-to-clipboard title="Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07748v1.pdf filename=2403.07748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate two fundamental problems in mobile computing: exploration and rendezvous, with two distinct mobile agents in an unknown <b>graph.</b> The agents can read and write information on whiteboards that are located at all nodes. They both move along one adjacent edge at every time-step. In the exploration problem, both agents start from the same node of the <b>graph</b> and must traverse all of its edges. We show that a simple variant of depth-first search achieves collective exploration in $m$ synchronous time-steps, where $m$ is the number of edges of the <b>graph.</b> This improves the competitive ratio of collective <b>graph</b> exploration. In the rendezvous problem, the agents start from different nodes of the <b>graph</b> and must meet as fast as possible. We introduce an algorithm guaranteeing rendezvous in at most $\frac{3}{2}m$ time-steps. This improves over the so-called `wait for Mommy&rsquo; algorithm which requires $2m$ time-steps. All our guarantees are derived from a more general asynchronous setting in which the speeds of the agents are controlled by an adversary at all times. Our guarantees also generalize to weighted <b>graphs,</b> if the number of edges $m$ is replaced by the sum of all edge lengths.</p></p class="citation"></blockquote><h3 id=33--313323-asynchronous-approximate-byzantine-consensus-a-multi-hop-relay-method-and-tight-graph-conditions-liwei-yuan-et-al-2024>(3/3 | 313/323) Asynchronous Approximate Byzantine Consensus: A Multi-hop Relay Method and Tight Graph Conditions (Liwei Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liwei Yuan, Hideaki Ishii. (2024)<br><strong>Asynchronous Approximate Byzantine Consensus: A Multi-hop Relay Method and Tight Graph Conditions</strong><br><button class=copy-to-clipboard title="Asynchronous Approximate Byzantine Consensus: A Multi-hop Relay Method and Tight Graph Conditions" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs-SY, cs.MA, eess-SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07640v1.pdf filename=2403.07640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a multi-agent resilient consensus problem, where some agents are of the Byzantine type and try to prevent the normal ones from reaching consensus. In our setting, normal agents communicate with each other asynchronously over multi-hop relay channels with delays. To solve this asynchronous Byzantine consensus problem, we develop the multi-hop weighted mean subsequence reduced (MW-MSR) algorithm. The main contribution is that we characterize a tight <b>graph</b> condition for our algorithm to achieve Byzantine consensus, which is expressed in the novel notion of strictly robust <b>graphs.</b> We show that the multi-hop communication is effective for enhancing the network&rsquo;s resilience against Byzantine agents. As a result, we also obtain novel conditions for resilient consensus under the malicious attack model, which are tighter than those known in the literature. Furthermore, the proposed algorithm can be viewed as a generalization of the conventional flooding-based algorithms, with less computational complexity. Lastly, we provide numerical examples to show the effectiveness of the proposed algorithm.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--314323-unsupervised-self-organising-map-of-prostate-cell-raman-spectra-shows-disease-state-subclustering-daniel-west-et-al-2024>(1/1 | 314/323) Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering (Daniel West et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel West, Susan Stepney, Y. Hancock. (2024)<br><strong>Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering</strong><br><button class=copy-to-clipboard title="Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07960v1.pdf filename=2403.07960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prostate cancer is a disease which poses an interesting clinical question: should it be treated? A small subset of prostate cancers are aggressive and require removal and treatment to prevent metastatic spread. However, conventional diagnostics remain challenged to risk-stratify such patients, hence, new methods of approach to biomolecularly subclassify the disease are needed. Here we use an <b>unsupervised,</b> self-organising map approach to analyse live-cell Raman spectroscopy data obtained from prostate cell-lines; our aim is to test the feasibility of this method to differentiate, at the single-cell-level, cancer from normal using high-dimensional datasets with minimal preprocessing. The results demonstrate not only successful separation of normal prostate and cancer cells, but also a new subclustering of the prostate cancer cell-line into two groups. Initial analysis of the spectra from each of the cancer subclusters demonstrates a differential expression of lipids, which, against the normal control, may be linked to disease-related changes in cellular signalling.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--315323-satisfiability-to-coverage-in-presence-of-fairness-matroid-and-global-constraints-tanmay-inamdar-et-al-2024>(1/3 | 315/323) Satisfiability to Coverage in Presence of Fairness, Matroid, and Global Constraints (Tanmay Inamdar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanmay Inamdar, Pallavi Jain, Daniel Lokshtanov, Abhishek Sahu, Saket Saurabh, Anannya Upasana. (2024)<br><strong>Satisfiability to Coverage in Presence of Fairness, Matroid, and Global Constraints</strong><br><button class=copy-to-clipboard title="Satisfiability to Coverage in Presence of Fairness, Matroid, and Global Constraints" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07328v1.pdf filename=2403.07328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In MaxSAT with Cardinality Constraint problem (CC-MaxSAT), we are given a CNF-formula $\Phi$, and $k \ge 0$, and the goal is to find an assignment $\beta$ with at most $k$ variables set to true (also called a weight $k$-assignment) such that the number of clauses satisfied by $\beta$ is maximized. MaxCov can be seen as a special case of CC-MaxSAT, where the formula $\Phi$ is monotone, i.e., does not contain any negative literals. CC-MaxSAT and MaxCov are extremely well-studied problems in the approximation algorithms as well as parameterized complexity literature. Our first contribution is that the two problems are equivalent to each other in the context of FPT-Approximation parameterized by $k$ (approximation is in terms of number of clauses satisfied/elements covered). We give a randomized reduction from CC-MaxSAT to MaxCov in time $O(1/\epsilon)^{k} \cdot (m+n)^{O(1)}$ that preserves the approximation guarantee up to a factor of $1-\epsilon$. Furthermore, this reduction also works in the presence of <b>fairness</b> and matroid constraints. Armed with this reduction, we focus on designing FPT-Approximation schemes (FPT-ASes) for MaxCov and its generalizations. Our algorithms are based on a novel combination of a variety of ideas, including a carefully designed probability distribution that exploits sparse coverage functions. These algorithms substantially generalize the results in Jain et al. [SODA 2023] for CC-MaxSAT and MaxCov for $K_{d,d}$-free set systems (i.e., no $d$ sets share $d$ elements), as well as a recent FPT-AS for Matroid-Constrained MaxCov by Sellier [ESA 2023] for frequency-$d$ set systems.</p></p class="citation"></blockquote><h3 id=23--316323-maximum-defective-clique-computation-improved-time-complexities-and-practical-performance-lijun-chang-2024>(2/3 | 316/323) Maximum Defective Clique Computation: Improved Time Complexities and Practical Performance (Lijun Chang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lijun Chang. (2024)<br><strong>Maximum Defective Clique Computation: Improved Time Complexities and Practical Performance</strong><br><button class=copy-to-clipboard title="Maximum Defective Clique Computation: Improved Time Complexities and Practical Performance" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs-SI, cs.DS<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07561v1.pdf filename=2403.07561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The concept of $k$-defective clique, a relaxation of clique by allowing up-to $k$ missing edges, has been receiving increasing interests recently. Although the problem of finding the maximum $k$-defective clique is NP-hard, several practical algorithms have been recently proposed in the literature, with kDC being the state of the art. kDC not only runs the fastest in practice, but also achieves the best time complexity. Specifically, it runs in $O^<em>(\gamma_k^n)$ time when ignoring polynomial factors; here, $\gamma_k$ is a constant that is smaller than two and only depends on $k$, and $n$ is the number of vertices in the input <b>graph</b> $G$. In this paper, we propose the kDC-Two algorithm to improve the time complexity as well as practical performance. kDC-Two runs in $O^</em>( (\alpha\Delta)^{k+2} \gamma_{k-1}^\alpha)$ time when the maximum $k$-defective clique size $\omega_k(G)$ is at least $k+2$, and in $O^<em>(\gamma_{k-1}^n)$ time otherwise, where $\alpha$ and $\Delta$ are the degeneracy and maximum degree of $G$, respectively. In addition, with slight modification, kDC-Two also runs in $O^</em>( (\alpha\Delta)^{k+2} (k+1)^{\alpha+k+1-\omega_k(G)})$ time by using the degeneracy gap $\alpha+k+1-\omega_k(G)$ parameterization; this is better than $O^*( (\alpha\Delta)^{k+2}\gamma_{k-1}^\alpha)$ when $\omega_k(G)$ is close to the degeneracy-based upper bound $\alpha+k+1$. Finally, to further improve the practical performance, we propose a new degree-sequence-based reduction rule that can be efficiently applied, and theoretically demonstrate its effectiveness compared with those proposed in the literature. Extensive empirical studies on three <b>benchmark</b> <b>graph</b> collections show that our algorithm outperforms the existing fastest algorithm by several orders of magnitude.</p></p class="citation"></blockquote><h3 id=33--317323-shining-light-on-periodic-dominating-sets-in-bounded-treewidth-graphs-jakob-greilhuber-et-al-2024>(3/3 | 317/323) Shining Light on Periodic Dominating Sets in Bounded-Treewidth Graphs (Jakob Greilhuber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakob Greilhuber, Philipp Schepper, Philip Wellnitz. (2024)<br><strong>Shining Light on Periodic Dominating Sets in Bounded-Treewidth Graphs</strong><br><button class=copy-to-clipboard title="Shining Light on Periodic Dominating Sets in Bounded-Treewidth Graphs" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07524v1.pdf filename=2403.07524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For the vertex selection problem $(\sigma,\rho)$-DomSet one is given two fixed sets $\sigma$ and $\rho$ of integers and the task is to decide whether we can select vertices of the input <b>graph,</b> such that, for every selected vertex, the number of selected neighbors is in $\sigma$ and, for every unselected vertex, the number of selected neighbors is in $\rho$. This framework covers Independent Set and Dominating Set for example. We investigate the case when $\sigma$ and $\rho$ are periodic sets with the same period $m\ge 2$, that is, the sets are two (potentially different) residue classes modulo $m$. We study the problem parameterized by treewidth and present an algorithm that solves in time $m^{tw} \cdot n^{O(1)}$ the decision, minimization and maximization version of the problem. This significantly improves upon the known algorithms where for the case $m \ge 3$ not even an explicit running time is known. We complement our algorithm by providing matching lower bounds which state that there is no $(m-\epsilon)^{pw} \cdot n^{O(1)}$ unless SETH fails. For $m = 2$, we extend these bound to the minimization version as the decision version is efficiently solvable.</p></p class="citation"></blockquote><h2 id=mathmg-1>math.MG (1)</h2><h3 id=11--318323-signed-graphs-in-data-sciences-via-communicability-geometry-fernando-diaz-diaz-et-al-2024>(1/1 | 318/323) Signed graphs in data sciences via communicability geometry (Fernando Diaz-Diaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando Diaz-Diaz, Ernesto Estrada. (2024)<br><strong>Signed graphs in data sciences via communicability geometry</strong><br><button class=copy-to-clipboard title="Signed graphs in data sciences via communicability geometry" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.MG<br>Categories: cs-DM, cs-LG, math-CO, math-MG, math.MG, physics-soc-ph<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07493v1.pdf filename=2403.07493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Signed <b>graphs</b> are an emergent way of representing data in a variety of contexts were conflicting interactions exist. These include data from biological, ecological, and social systems. Here we propose the concept of communicability <b>geometry</b> for signed <b>graphs,</b> proving that metrics in this space, such as the communicability distance and angles, are Euclidean and spherical. We then apply these metrics to solve several problems in data analysis of signed <b>graphs</b> in a unified way. They include the partitioning of signed <b>graphs,</b> dimensionality reduction, finding hierarchies of alliances in signed networks as well as the quantification of the degree of polarization between the existing factions in systems represented by this type of <b>graphs.</b></p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--319323-characterising-harmful-data-sources-when-constructing-multi-fidelity-surrogate-models-nicolau-andrés-thió-et-al-2024>(1/1 | 319/323) Characterising harmful data sources when constructing multi-fidelity surrogate models (Nicolau Andrés-Thió et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolau Andrés-Thió, Mario Andrés Muñoz, Kate Smith-Miles. (2024)<br><strong>Characterising harmful data sources when constructing multi-fidelity surrogate models</strong><br><button class=copy-to-clipboard title="Characterising harmful data sources when constructing multi-fidelity surrogate models" index=319>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-319 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-AI, cs-LG, stat-ME, stat-ML, stat.ME<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08118v1.pdf filename=2403.08118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surrogate modelling techniques have seen growing attention in recent years when applied to both modelling and optimisation of industrial design problems. These techniques are highly relevant when assessing the performance of a particular design carries a high cost, as the overall cost can be mitigated via the construction of a model to be queried in lieu of the available high-cost source. The construction of these models can sometimes employ other sources of information which are both cheaper and less accurate. The existence of these sources however poses the question of which sources should be used when constructing a model. Recent studies have attempted to characterise harmful data sources to guide practitioners in choosing when to ignore a certain source. These studies have done so in a synthetic setting, characterising sources using a large amount of data that is not available in practice. Some of these studies have also been shown to potentially suffer from bias in the <b>benchmarks</b> used in the analysis. In this study, we present a characterisation of harmful low-fidelity sources using only the limited data available to train a surrogate model. We employ recently developed <b>benchmark</b> filtering techniques to conduct a bias-free assessment, providing objectively varied <b>benchmark</b> suites of different sizes for future research. Analysing one of these <b>benchmark</b> suites with the technique known as Instance Space Analysis, we provide an intuitive visualisation of when a low-fidelity source should be used and use this analysis to provide guidelines that can be used in an applied industrial setting.</p></p class="citation"></blockquote><h2 id=mathat-1>math.AT (1)</h2><h3 id=11--320323-computing-generalized-ranks-of-persistence-modules-via-unfolding-to-zigzag-modules-tamal-k-dey-et-al-2024>(1/1 | 320/323) Computing Generalized Ranks of Persistence Modules via Unfolding to Zigzag Modules (Tamal K. Dey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tamal K. Dey, Aman Timalsina, Cheng Xin. (2024)<br><strong>Computing Generalized Ranks of Persistence Modules via Unfolding to Zigzag Modules</strong><br><button class=copy-to-clipboard title="Computing Generalized Ranks of Persistence Modules via Unfolding to Zigzag Modules" index=320>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-320 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AT<br>Categories: cs-CG, math-AT, math.AT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08110v1.pdf filename=2403.08110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For a $P$-indexed persistence module ${\sf M}$, the (generalized) rank of ${\sf M}$ is defined as the rank of the limit-to-colimit map for ${\sf M}$ over the poset $P$. For $2$-parameter persistence modules, recently a zigzag persistence based algorithm has been proposed that takes advantage of the fact that generalized rank for $2$-parameter modules is equal to the number of full intervals in a zigzag module defined on the boundary of the poset. Analogous definition of boundary for $d$-parameter persistence modules or general $P$-indexed persistence modules does not seem plausible. To overcome this difficulty, we first unfold a given $P$-indexed module ${\sf M}$ into a zigzag module ${\sf M}<em>{ZZ}$ and then check how many full interval modules in a decomposition of ${\sf M}</em>{ZZ}$ can be folded back to remain full in ${\sf M}$. This number determines the generalized rank of ${\sf M}$. For special cases of degree-$d$ homology for $d$-complexes, we obtain a more efficient algorithm including a linear time algorithm for degree-$1$ homology in <b>graphs.</b></p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--321323-overlapping-community-detection-algorithms-using-modularity-and-the-cosine-do-duy-hieu-et-al-2024>(1/1 | 321/323) Overlapping community detection algorithms using Modularity and the cosine (Do Duy Hieu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Do Duy Hieu, Phan Thi Ha Duong. (2024)<br><strong>Overlapping community detection algorithms using Modularity and the cosine</strong><br><button class=copy-to-clipboard title="Overlapping community detection algorithms using Modularity and the cosine" index=321>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-321 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: Community detection, cs-DM, cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08000v1.pdf filename=2403.08000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The issue of network community detection has been extensively studied across many fields. Most community detection methods assume that nodes belong to only one community. However, in many cases, nodes can belong to multiple communities simultaneously.This paper presents two overlapping network community detection algorithms that build on the two-step approach, using the extended modularity and cosine function. The applicability of our algorithms extends to both undirected and directed <b>graph</b> structures. To demonstrate the feasibility and effectiveness of these algorithms, we conducted experiments using real data.</p></p class="citation"></blockquote><h2 id=csfl-1>cs.FL (1)</h2><h3 id=11--322323-on-graph-grammars-and-games-jayakrishna-vijayakumar-et-al-2024>(1/1 | 322/323) On Graph Grammars and Games (Jayakrishna Vijayakumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jayakrishna Vijayakumar, Lisa Mathew. (2024)<br><strong>On Graph Grammars and Games</strong><br><button class=copy-to-clipboard title="On Graph Grammars and Games" index=322>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-322 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: 68Q42, 68Q45, 68R10, F-4-2; G-2-2; F-4-3; F-1-1, cs-DM, cs-FL, cs.FL, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07607v1.pdf filename=2403.07607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> grammars form an interesting area of research because of their versatility in modelling diverse situations with <b>graphs</b> as the structures which are to be manipulated. A new class of <b>graph</b> grammars, nc-eNCE <b>Graph</b> Grammars has been introduced recently with an aim of restricting the order of application of <b>graph</b> production rules, thereby generating different <b>graph</b> classes using the same set of rules. On the other hand 2D game design using an algorithmic approach known as procedural content generation has been of interest recently. In this paper we modify the structure of nc-eNCE <b>graph</b> grammars with the aim of generating directed <b>graphs.</b> We show that employing these <b>graph</b> grammars simplifies the design of 2D games. We have also developed an algorithm which makes use of these <b>graph</b> grammars for generating random game level layouts ensuring that the players will get a different gaming experience each time they play.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--323323-the-primal-pathwidth-seth-michael-lampis-2024>(1/1 | 323/323) The Primal Pathwidth SETH (Michael Lampis, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Lampis. (2024)<br><strong>The Primal Pathwidth SETH</strong><br><button class=copy-to-clipboard title="The Primal Pathwidth SETH" index=323>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-323 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DS, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07239v1.pdf filename=2403.07239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the importance of dynamic programming (DP) in parameterized complexity, we consider several fine-grained questions, such as the following examples: (i) can Dominating Set be solved in time $(3-\epsilon)^{pw}n^{O(1)}$? (where $pw$ is the pathwidth) (ii) can Coloring be solved in time $pw^{(1-\epsilon)pw}n^{O(1)}$? (iii) can a short reconfiguration between two size-$k$ independent sets be found in time $n^{(1-\epsilon)k}$? Such questions are well-studied: in some cases the answer is No under the SETH, while in others coarse-grained lower bounds are known under the ETH. Even though questions such as the above seem &ldquo;morally equivalent&rdquo; as they all ask if a simple DP can be improved, the problems concerned have wildly varying time complexities, ranging from single-exponential FPT to XNLP-complete. This paper&rsquo;s main contribution is to show that, despite their varying complexities, these questions are not just morally equivalent, but in fact they are the same question in disguise. We achieve this by putting forth a natural complexity assumption which we call the Primal Pathwidth-Strong Exponential Time Hypothesis (PP-SETH) and which states that 3-SAT cannot be solved in time $(2-\epsilon)^{pw}n^{O(1)}$, for any $\epsilon>0$, where $pw$ is the pathwidth of the primal <b>graph</b> of the input. We then show that numerous fine-grained questions in parameterized complexity, including the ones above, are equivalent to the PP-SETH, and hence to each other. This allows us to obtain sharp fine-grained lower bounds for problems for which previous lower bounds left a constant in the exponent undetermined, but also to increase our confidence in bounds which were previously known under the SETH, because we show that breaking any one such bound requires breaking all (old and new) bounds; and because we show that the PP-SETH is more plausible than the SETH.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.13</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.15</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-48>cs.CL (48)</a><ul><li><a href=#148--1323-investigating-the-performance-of-retrieval-augmented-generation-and-fine-tuning-for-the-development-of-ai-driven-knowledge-based-systems-robert-lakatos-et-al-2024>(1/48 | 1/323) Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems (Robert Lakatos et al., 2024)</a></li><li><a href=#248--2323-exploring-safety-generalization-challenges-of-large-language-models-via-code-qibing-ren-et-al-2024>(2/48 | 2/323) Exploring Safety Generalization Challenges of Large Language Models via Code (Qibing Ren et al., 2024)</a></li><li><a href=#348--3323-rethinking-aste-a-minimalist-tagging-scheme-alongside-contrastive-learning-qiao-sun-et-al-2024>(3/48 | 3/323) Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning (Qiao Sun et al., 2024)</a></li><li><a href=#448--4323-rethinking-generative-large-language-model-evaluation-for-semantic-comprehension-fangyun-wei-et-al-2024>(4/48 | 4/323) Rethinking Generative Large Language Model Evaluation for Semantic Comprehension (Fangyun Wei et al., 2024)</a></li><li><a href=#548--5323-knowledge-graph-large-language-model-kg-llm-for-link-prediction-dong-shu-et-al-2024>(5/48 | 5/323) Knowledge Graph Large Language Model (KG-LLM) for Link Prediction (Dong Shu et al., 2024)</a></li><li><a href=#648--6323-fine-tuning-large-language-models-with-sequential-instructions-hanxu-hu-et-al-2024>(6/48 | 6/323) Fine-tuning Large Language Models with Sequential Instructions (Hanxu Hu et al., 2024)</a></li><li><a href=#748--7323-rad-phi2-instruction-tuning-phi-2-for-radiology-mercy-ranjit-et-al-2024>(7/48 | 7/323) RAD-PHI2: Instruction Tuning PHI-2 for Radiology (Mercy Ranjit et al., 2024)</a></li><li><a href=#848--8323-sifid-reassess-summary-factual-inconsistency-detection-with-llm-jiuding-yang-et-al-2024>(8/48 | 8/323) SIFiD: Reassess Summary Factual Inconsistency Detection with LLM (Jiuding Yang et al., 2024)</a></li><li><a href=#948--9323-training-small-multimodal-models-to-bridge-biomedical-competency-gap-a-case-study-in-radiology-imaging-juan-manuel-zambrano-chaves-et-al-2024>(9/48 | 9/323) Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging (Juan Manuel Zambrano Chaves et al., 2024)</a></li><li><a href=#1048--10323-complex-reasoning-over-logical-queries-on-commonsense-knowledge-graphs-tianqing-fang-et-al-2024>(10/48 | 10/323) Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs (Tianqing Fang et al., 2024)</a></li><li><a href=#1148--11323-llmvssmall-model-large-language-model-based-text-augmentation-enhanced-personality-detection-model-linmei-hu-et-al-2024>(11/48 | 11/323) LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model (Linmei Hu et al., 2024)</a></li><li><a href=#1248--12323-matrix-transformation-based-low-rank-adaptation-mtlora-a-brain-inspired-method-for-parameter-efficient-fine-tuning-yao-liang-et-al-2024>(12/48 | 12/323) Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning (Yao Liang et al., 2024)</a></li><li><a href=#1348--13323-harnessing-artificial-intelligence-to-combat-online-hate-exploring-the-challenges-and-opportunities-of-large-language-models-in-hate-speech-detection-tharindu-kumarage-et-al-2024>(13/48 | 13/323) Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection (Tharindu Kumarage et al., 2024)</a></li><li><a href=#1448--14323-semeval-2024-shared-task-6-shroom-a-shared-task-on-hallucinations-and-related-observable-overgeneration-mistakes-timothee-mickus-et-al-2024>(14/48 | 14/323) SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes (Timothee Mickus et al., 2024)</a></li><li><a href=#1548--15323-improving-reinforcement-learning-from-human-feedback-using-contrastive-rewards-wei-shen-et-al-2024>(15/48 | 15/323) Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards (Wei Shen et al., 2024)</a></li><li><a href=#1648--16323-smalltolarge-s2l-scalable-data-selection-for-fine-tuning-large-language-models-by-summarizing-training-trajectories-of-small-models-yu-yang-et-al-2024>(16/48 | 16/323) SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models (Yu Yang et al., 2024)</a></li><li><a href=#1748--17323-fine-tuning-vs-prompting-can-language-models-understand-human-values-pingwei-sun-2024>(17/48 | 17/323) Fine-tuning vs Prompting, Can Language Models Understand Human Values? (Pingwei Sun, 2024)</a></li><li><a href=#1848--18323-ckerc--joint-large-language-models-with-commonsense-knowledge-for-emotion-recognition-in-conversation-yumeng-fu-2024>(18/48 | 18/323) CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation (Yumeng Fu, 2024)</a></li><li><a href=#1948--19323-stabletoolbench-towards-stable-large-scale-benchmarking-on-tool-learning-of-large-language-models-zhicheng-guo-et-al-2024>(19/48 | 19/323) StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models (Zhicheng Guo et al., 2024)</a></li><li><a href=#2048--20323-a-semantic-mention-graph-augmented-model-for-document-level-event-argument-extraction-jian-zhang-et-al-2024>(20/48 | 20/323) A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction (Jian Zhang et al., 2024)</a></li><li><a href=#2148--21323-orpo-monolithic-preference-optimization-without-reference-model-jiwoo-hong-et-al-2024>(21/48 | 21/323) ORPO: Monolithic Preference Optimization without Reference Model (Jiwoo Hong et al., 2024)</a></li><li><a href=#2248--22323-moralbert-detecting-moral-values-in-social-discourse-vjosa-preniqi-et-al-2024>(22/48 | 22/323) MoralBERT: Detecting Moral Values in Social Discourse (Vjosa Preniqi et al., 2024)</a></li><li><a href=#2348--23323-triples-to-isixhosa-t2x-addressing-the-challenges-of-low-resource-agglutinative-data-to-text-generation-francois-meyer-et-al-2024>(23/48 | 23/323) Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation (Francois Meyer et al., 2024)</a></li><li><a href=#2448--24323-curry-dpo-enhancing-alignment-using-curriculum-learning--ranked-preferences-pulkit-pattnaik-et-al-2024>(24/48 | 24/323) Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences (Pulkit Pattnaik et al., 2024)</a></li><li><a href=#2548--25323-finemath-a-fine-grained-mathematical-evaluation-benchmark-for-chinese-large-language-models-yan-liu-et-al-2024>(25/48 | 25/323) FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models (Yan Liu et al., 2024)</a></li><li><a href=#2648--26323-contextual-clarity-generating-sentences-with-transformer-models-using-context-reverso-data-ruslan-musaev-2024>(26/48 | 26/323) Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data (Ruslan Musaev, 2024)</a></li><li><a href=#2748--27323-gujarati-english-code-switching-speech-recognition-using-ensemble-prediction-of-spoken-language-yash-sharma-et-al-2024>(27/48 | 27/323) Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language (Yash Sharma et al., 2024)</a></li><li><a href=#2848--28323-branch-train-mix-mixing-expert-llms-into-a-mixture-of-experts-llm-sainbayar-sukhbaatar-et-al-2024>(28/48 | 28/323) Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM (Sainbayar Sukhbaatar et al., 2024)</a></li><li><a href=#2948--29323-beyond-memorization-the-challenge-of-random-memory-access-in-language-models-tongyao-zhu-et-al-2024>(29/48 | 29/323) Beyond Memorization: The Challenge of Random Memory Access in Language Models (Tongyao Zhu et al., 2024)</a></li><li><a href=#3048--30323-large-small-or-both-a-novel-data-augmentation-framework-based-on-language-models-for-debiasing-opinion-summarization-yanyue-zhang-et-al-2024>(30/48 | 30/323) Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization (Yanyue Zhang et al., 2024)</a></li><li><a href=#3148--31323-truth-aware-context-selection-mitigating-the-hallucinations-of-large-language-models-being-misled-by-untruthful-contexts-tian-yu-et-al-2024>(31/48 | 31/323) Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts (Tian Yu et al., 2024)</a></li><li><a href=#3248--32323-comprehensive-implementation-of-textcnn-for-enhanced-collaboration-between-natural-language-processing-and-system-recommendation-xiaonan-xu-et-al-2024>(32/48 | 32/323) Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation (Xiaonan Xu et al., 2024)</a></li><li><a href=#3348--33323-big-city-bias-evaluating-the-impact-of-metropolitan-size-on-computational-job-market-abilities-of-language-models-charlie-campanella-et-al-2024>(33/48 | 33/323) Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models (Charlie Campanella et al., 2024)</a></li><li><a href=#3448--34323-gpt-generated-text-detection-benchmark-dataset-and-tensor-based-detection-method-zubair-qazi-et-al-2024>(34/48 | 34/323) GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method (Zubair Qazi et al., 2024)</a></li><li><a href=#3548--35323-bagel-bootstrapping-agents-by-guiding-exploration-with-language-shikhar-murty-et-al-2024>(35/48 | 35/323) BAGEL: Bootstrapping Agents by Guiding Exploration with Language (Shikhar Murty et al., 2024)</a></li><li><a href=#3648--36323-simulating-weighted-automata-over-sequences-and-trees-with-transformers-michael-rizvi-et-al-2024>(36/48 | 36/323) Simulating Weighted Automata over Sequences and Trees with Transformers (Michael Rizvi et al., 2024)</a></li><li><a href=#3748--37323-authorship-style-transfer-with-policy-optimization-shuai-liu-et-al-2024>(37/48 | 37/323) Authorship Style Transfer with Policy Optimization (Shuai Liu et al., 2024)</a></li><li><a href=#3848--38323-the-missing-piece-in-model-editing-a-deep-dive-into-the-hidden-damage-brought-by-model-editing-jianchen-wang-et-al-2024>(38/48 | 38/323) The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing (Jianchen Wang et al., 2024)</a></li><li><a href=#3948--39323-svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression-xin-wang-et-al-2024>(39/48 | 39/323) SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression (Xin Wang et al., 2024)</a></li><li><a href=#4048--40323-kebench-a-benchmark-on-knowledge-editing-for-large-vision-language-models-han-huang-et-al-2024>(40/48 | 40/323) KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models (Han Huang et al., 2024)</a></li><li><a href=#4148--41323-debatrix-multi-dimensinal-debate-judge-with-iterative-chronological-analysis-based-on-llm-jingcong-liang-et-al-2024>(41/48 | 41/323) Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM (Jingcong Liang et al., 2024)</a></li><li><a href=#4248--42323-claimver-explainable-claim-level-verification-and-evidence-attribution-of-text-through-knowledge-graphs-preetam-prabhu-srikar-dammu-et-al-2024>(42/48 | 42/323) ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs (Preetam Prabhu Srikar Dammu et al., 2024)</a></li><li><a href=#4348--43323-generating-clarification-questions-for-disambiguating-contracts-anmol-singhal-et-al-2024>(43/48 | 43/323) Generating Clarification Questions for Disambiguating Contracts (Anmol Singhal et al., 2024)</a></li><li><a href=#4448--44323-pix2pix-onthefly-leveraging-llms-for-instruction-guided-image-editing-rodrigo-santos-et-al-2024>(44/48 | 44/323) Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing (Rodrigo Santos et al., 2024)</a></li><li><a href=#4548--45323-mammoth-massively-multilingual-modular-open-translation--helsinki-timothee-mickus-et-al-2024>(45/48 | 45/323) MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki (Timothee Mickus et al., 2024)</a></li><li><a href=#4648--46323-prediction-of-readmission-of-patients-by-extracting-biomedical-concepts-from-clinical-texts-rasoul-samani-et-al-2024>(46/48 | 46/323) Prediction of readmission of patients by extracting biomedical concepts from clinical texts (Rasoul Samani et al., 2024)</a></li><li><a href=#4748--47323-mevaker-conclusion-extraction-and-allocation-resources-for-the-hebrew-language-vitaly-shalumov-et-al-2024>(47/48 | 47/323) Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language (Vitaly Shalumov et al., 2024)</a></li><li><a href=#4848--48323-a-survey-of-explainable-knowledge-tracing-yanhong-bai-et-al-2024>(48/48 | 48/323) A Survey of Explainable Knowledge Tracing (Yanhong Bai et al., 2024)</a></li></ul></li><li><a href=#cscv-88>cs.CV (88)</a><ul><li><a href=#188--49323-in-context-learning-enables-multimodal-large-language-models-to-classify-cancer-pathology-images-dyke-ferber-et-al-2024>(1/88 | 49/323) In-context learning enables multimodal large language models to classify cancer pathology images (Dyke Ferber et al., 2024)</a></li><li><a href=#288--50323-a-survey-of-vision-transformers-in-autonomous-driving-current-trends-and-future-directions-quoc-vinh-lai-dang-2024>(2/88 | 50/323) A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions (Quoc-Vinh Lai-Dang, 2024)</a></li><li><a href=#388--51323-moai-mixture-of-all-intelligence-for-large-language-and-vision-models-byung-kwan-lee-et-al-2024>(3/88 | 51/323) MoAI: Mixture of All Intelligence for Large Language and Vision Models (Byung-Kwan Lee et al., 2024)</a></li><li><a href=#488--52323-navcot-boosting-llm-based-vision-and-language-navigation-via-learning-disentangled-reasoning-bingqian-lin-et-al-2024>(4/88 | 52/323) NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning (Bingqian Lin et al., 2024)</a></li><li><a href=#588--53323-pelk-parameter-efficient-large-kernel-convnets-with-peripheral-convolution-honghao-chen-et-al-2024>(5/88 | 53/323) PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution (Honghao Chen et al., 2024)</a></li><li><a href=#688--54323-multi-modal-auto-regressive-modeling-via-visual-words-tianshuo-peng-et-al-2024>(6/88 | 54/323) Multi-modal Auto-regressive Modeling via Visual Words (Tianshuo Peng et al., 2024)</a></li><li><a href=#788--55323-distilling-the-knowledge-in-data-pruning-emanuel-ben-baruch-et-al-2024>(7/88 | 55/323) Distilling the Knowledge in Data Pruning (Emanuel Ben-Baruch et al., 2024)</a></li><li><a href=#888--56323-towards-zero-shot-human-object-interaction-detection-via-vision-language-integration-weiying-xue-et-al-2024>(8/88 | 56/323) Towards Zero-shot Human-Object Interaction Detection via Vision-Language Integration (Weiying Xue et al., 2024)</a></li><li><a href=#988--57323-beyond-text-frozen-large-language-models-in-visual-signal-comprehension-lei-zhu-et-al-2024>(9/88 | 57/323) Beyond Text: Frozen Large Language Models in Visual Signal Comprehension (Lei Zhu et al., 2024)</a></li><li><a href=#1088--58323-genuine-knowledge-from-practice-diffusion-test-time-adaptation-for-video-adverse-weather-removal-yijun-yang-et-al-2024>(10/88 | 58/323) Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal (Yijun Yang et al., 2024)</a></li><li><a href=#1188--59323-gabor-guided-transformer-for-single-image-deraining-sijin-he-et-al-2024>(11/88 | 59/323) Gabor-guided transformer for single image deraining (Sijin He et al., 2024)</a></li><li><a href=#1288--60323-optimizing-negative-prompts-for-enhanced-aesthetics-and-fidelity-in-text-to-image-generation-michael-ogezi-et-al-2024>(12/88 | 60/323) Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation (Michael Ogezi et al., 2024)</a></li><li><a href=#1388--61323-vit-comer-vision-transformer-with-convolutional-multi-scale-feature-interaction-for-dense-predictions-chunlong-xia-et-al-2024>(13/88 | 61/323) ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions (Chunlong Xia et al., 2024)</a></li><li><a href=#1488--62323-large-window-based-mamba-unet-for-medical-image-segmentation-beyond-convolution-and-self-attention-jinhong-wang-et-al-2024>(14/88 | 62/323) Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention (Jinhong Wang et al., 2024)</a></li><li><a href=#1588--63323-mentor-multilingual-text-detection-toward-learning-by-analogy-hsin-ju-lin-et-al-2024>(15/88 | 63/323) MENTOR: Multilingual tExt detectioN TOward leaRning by analogy (Hsin-Ju Lin et al., 2024)</a></li><li><a href=#1688--64323-cuvler-enhanced-unsupervised-object-discoveries-through-exhaustive-self-supervised-transformers-shahaf-arica-et-al-2024>(16/88 | 64/323) CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers (Shahaf Arica et al., 2024)</a></li><li><a href=#1788--65323-aesopagent-agent-driven-evolutionary-system-on-story-to-video-production-jiuniu-wang-et-al-2024>(17/88 | 65/323) AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production (Jiuniu Wang et al., 2024)</a></li><li><a href=#1888--66323-text-to-image-diffusion-models-are-great-sketch-photo-matchmakers-subhadeep-koley-et-al-2024>(18/88 | 66/323) Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers (Subhadeep Koley et al., 2024)</a></li><li><a href=#1988--67323-lg-traj-llm-guided-pedestrian-trajectory-prediction-pranav-singh-chib-et-al-2024>(19/88 | 67/323) LG-Traj: LLM Guided Pedestrian Trajectory Prediction (Pranav Singh Chib et al., 2024)</a></li><li><a href=#2088--68323-taskclip-extend-large-vision-language-model-for-task-oriented-object-detection-hanning-chen-et-al-2024>(20/88 | 68/323) TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection (Hanning Chen et al., 2024)</a></li><li><a href=#2188--69323-real-time-surgical-instrument-segmentation-in-video-using-point-tracking-and-segment-anything-zijian-wu-et-al-2024>(21/88 | 69/323) Real-time Surgical Instrument Segmentation in Video Using Point Tracking and Segment Anything (Zijian Wu et al., 2024)</a></li><li><a href=#2288--70323-bridging-different-language-models-and-generative-vision-models-for-text-to-image-generation-shihao-zhao-et-al-2024>(22/88 | 70/323) Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation (Shihao Zhao et al., 2024)</a></li><li><a href=#2388--71323-mope-clip-structured-pruning-for-efficient-vision-language-models-with-module-wise-pruning-error-metric-haokun-lin-et-al-2024>(23/88 | 71/323) MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric (Haokun Lin et al., 2024)</a></li><li><a href=#2488--72323-synth2-boosting-visual-language-models-with-synthetic-captions-and-image-embeddings-sahand-sharifzadeh-et-al-2024>(24/88 | 72/323) Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings (Sahand Sharifzadeh et al., 2024)</a></li><li><a href=#2588--73323-decomposing-disease-descriptions-for-enhanced-pathology-detection-a-multi-aspect-vision-language-matching-framework-minh-hieu-phan-et-al-2024>(25/88 | 73/323) Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Matching Framework (Minh Hieu Phan et al., 2024)</a></li><li><a href=#2688--74323-block-wise-lora-revisiting-fine-grained-lora-for-effective-personalization-and-stylization-in-text-to-image-generation-likun-li-et-al-2024>(26/88 | 74/323) Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation (Likun Li et al., 2024)</a></li><li><a href=#2788--75323-calibrating-multi-modal-representations-a-pursuit-of-group-robustness-without-annotations-chenyu-you-et-al-2024>(27/88 | 75/323) Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations (Chenyu You et al., 2024)</a></li><li><a href=#2888--76323-premonition-using-generative-models-to-preempt-future-data-changes-in-continual-learning-mark-d-mcdonnell-et-al-2024>(28/88 | 76/323) Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning (Mark D. McDonnell et al., 2024)</a></li><li><a href=#2988--77323-mitigating-the-impact-of-attribute-editing-on-face-recognition-sudipta-banerjee-et-al-2024>(29/88 | 77/323) Mitigating the Impact of Attribute Editing on Face Recognition (Sudipta Banerjee et al., 2024)</a></li><li><a href=#3088--78323-fluorosam-a-language-aligned-foundation-model-for-x-ray-image-segmentation-benjamin-d-killeen-et-al-2024>(30/88 | 78/323) FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation (Benjamin D. Killeen et al., 2024)</a></li><li><a href=#3188--79323-unified-source-free-domain-adaptation-song-tang-et-al-2024>(31/88 | 79/323) Unified Source-Free Domain Adaptation (Song Tang et al., 2024)</a></li><li><a href=#3288--80323-fpt-fine-grained-prompt-tuning-for-parameter-and-memory-efficient-fine-tuning-in-high-resolution-medical-image-classification-yijin-huang-et-al-2024>(32/88 | 80/323) FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine Tuning in High-resolution Medical Image Classification (Yijin Huang et al., 2024)</a></li><li><a href=#3388--81323-rsbuilding-towards-general-remote-sensing-image-building-extraction-and-change-detection-with-foundation-model-mingze-wang-et-al-2024>(33/88 | 81/323) RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model (Mingze Wang et al., 2024)</a></li><li><a href=#3488--82323-continual-all-in-one-adverse-weather-removal-with-knowledge-replay-on-a-unified-network-structure-de-cheng-et-al-2024>(34/88 | 82/323) Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure (De Cheng et al., 2024)</a></li><li><a href=#3588--83323-its-all-about-your-sketch-democratising-sketch-control-in-diffusion-models-subhadeep-koley-et-al-2024>(35/88 | 83/323) It&rsquo;s All About Your Sketch: Democratising Sketch Control in Diffusion Models (Subhadeep Koley et al., 2024)</a></li><li><a href=#3688--84323-dynamic-graph-representation-with-knowledge-aware-attention-for-histopathology-whole-slide-image-analysis-jiawen-li-et-al-2024>(36/88 | 84/323) Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis (Jiawen Li et al., 2024)</a></li><li><a href=#3788--85323-let-storytelling-tell-vivid-stories-an-expressive-and-fluent-multimodal-storyteller-chuanqi-zang-et-al-2024>(37/88 | 85/323) Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller (Chuanqi Zang et al., 2024)</a></li><li><a href=#3888--86323-a-fourier-transform-framework-for-domain-adaptation-le-luo-et-al-2024>(38/88 | 86/323) A Fourier Transform Framework for Domain Adaptation (Le Luo et al., 2024)</a></li><li><a href=#3988--87323-eliminating-cross-modal-conflicts-in-bev-space-for-lidar-camera-3d-object-detection-jiahui-fu-et-al-2024>(39/88 | 87/323) Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D Object Detection (Jiahui Fu et al., 2024)</a></li><li><a href=#4088--88323-a-bayesian-approach-to-ood-robustness-in-image-classification-prakhar-kaushik-et-al-2024>(40/88 | 88/323) A Bayesian Approach to OOD Robustness in Image Classification (Prakhar Kaushik et al., 2024)</a></li><li><a href=#4188--89323-stylegaussian-instant-3d-style-transfer-with-gaussian-splatting-kunhao-liu-et-al-2024>(41/88 | 89/323) StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting (Kunhao Liu et al., 2024)</a></li><li><a href=#4288--90323-stable-makeup-when-real-world-makeup-transfer-meets-diffusion-model-yuxuan-zhang-et-al-2024>(42/88 | 90/323) Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model (Yuxuan Zhang et al., 2024)</a></li><li><a href=#4388--91323-fast-and-simple-explainability-for-point-cloud-networks-meir-yossef-levi-et-al-2024>(43/88 | 91/323) Fast and Simple Explainability for Point Cloud Networks (Meir Yossef Levi et al., 2024)</a></li><li><a href=#4488--92323-annotations-on-a-budget-leveraging-geo-data-similarity-to-balance-model-performance-and-annotation-cost-oana-ignat-et-al-2024>(44/88 | 92/323) Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost (Oana Ignat et al., 2024)</a></li><li><a href=#4588--93323-hunting-attributes-context-prototype-aware-learning-for-weakly-supervised-semantic-segmentation-feilong-tang-et-al-2024>(45/88 | 93/323) Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation (Feilong Tang et al., 2024)</a></li><li><a href=#4688--94323-minkunext-point-cloud-based-large-scale-place-recognition-using-3d-sparse-convolutions-j-j-cabrera-et-al-2024>(46/88 | 94/323) MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D Sparse Convolutions (J. J. Cabrera et al., 2024)</a></li><li><a href=#4788--95323-aacp-aesthetics-assessment-of-childrens-paintings-based-on-self-supervised-learning-shiqi-jiang-et-al-2024>(47/88 | 95/323) AACP: Aesthetics assessment of children&rsquo;s paintings based on self-supervised learning (Shiqi Jiang et al., 2024)</a></li><li><a href=#4888--96323-d4d-an-rgbd-diffusion-model-to-boost-monocular-depth-estimation-l-papa-et-al-2024>(48/88 | 96/323) D4D: An RGBD diffusion model to boost monocular depth estimation (L. Papa et al., 2024)</a></li><li><a href=#4988--97323-a-comprehensive-survey-of-3d-dense-captioning-localizing-and-describing-objects-in-3d-scenes-ting-yu-et-al-2024>(49/88 | 97/323) A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing Objects in 3D Scenes (Ting Yu et al., 2024)</a></li><li><a href=#5088--98323-jstr-joint-spatio-temporal-reasoning-for-event-based-moving-object-detection-hanyu-zhou-et-al-2024>(50/88 | 98/323) JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection (Hanyu Zhou et al., 2024)</a></li><li><a href=#5188--99323-auxiliary-cyclegan-guidance-for-task-aware-domain-translation-from-duplex-to-monoplex-ihc-images-nicolas-brieu-et-al-2024>(51/88 | 99/323) Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images (Nicolas Brieu et al., 2024)</a></li><li><a href=#5288--100323-textual-knowledge-matters-cross-modality-co-teaching-for-generalized-visual-class-discovery-haiyang-zheng-et-al-2024>(52/88 | 100/323) Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery (Haiyang Zheng et al., 2024)</a></li><li><a href=#5388--101323-bid-boundary-interior-decoding-for-unsupervised-temporal-action-localization-pre-trainin-qihang-fang-et-al-2024>(53/88 | 101/323) BID: Boundary-Interior Decoding for Unsupervised Temporal Action Localization Pre-Trainin (Qihang Fang et al., 2024)</a></li><li><a href=#5488--102323-frequency-aware-deepfake-detection-improving-generalizability-through-frequency-space-learning-chuangchuang-tan-et-al-2024>(54/88 | 102/323) Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Learning (Chuangchuang Tan et al., 2024)</a></li><li><a href=#5588--103323-learn-and-search-an-elegant-technique-for-object-lookup-using-contrastive-learning-chandan-kumar-et-al-2024>(55/88 | 103/323) Learn and Search: An Elegant Technique for Object Lookup using Contrastive Learning (Chandan Kumar et al., 2024)</a></li><li><a href=#5688--104323-lumen-unleashing-versatile-vision-centric-capabilities-of-large-multimodal-models-yang-jiao-et-al-2024>(56/88 | 104/323) Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models (Yang Jiao et al., 2024)</a></li><li><a href=#5788--105323-q-slam-quadric-representations-for-monocular-slam-chensheng-peng-et-al-2024>(57/88 | 105/323) Q-SLAM: Quadric Representations for Monocular SLAM (Chensheng Peng et al., 2024)</a></li><li><a href=#5888--106323-spatiotemporal-representation-learning-for-short-and-long-medical-image-time-series-chengzhi-shen-et-al-2024>(58/88 | 106/323) Spatiotemporal Representation Learning for Short and Long Medical Image Time Series (Chengzhi Shen et al., 2024)</a></li><li><a href=#5988--107323-unleashing-hydra-hybrid-fusion-depth-consistency-and-radar-for-unified-3d-perception-philipp-wolters-et-al-2024>(59/88 | 107/323) Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception (Philipp Wolters et al., 2024)</a></li><li><a href=#6088--108323-ssm-meets-video-diffusion-models-efficient-video-generation-with-structured-state-spaces-yuta-oshima-et-al-2024>(60/88 | 108/323) SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces (Yuta Oshima et al., 2024)</a></li><li><a href=#6188--109323-unleashing-network-potentials-for-semantic-scene-completion-fengyun-wang-et-al-2024>(61/88 | 109/323) Unleashing Network Potentials for Semantic Scene Completion (Fengyun Wang et al., 2024)</a></li><li><a href=#6288--110323-sparselif-high-performance-sparse-lidar-camera-fusion-for-3d-object-detection-hongcheng-zhang-et-al-2024>(62/88 | 110/323) SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection (Hongcheng Zhang et al., 2024)</a></li><li><a href=#6388--111323-learning-data-association-for-multi-object-tracking-using-only-coordinates-mehdi-miah-et-al-2024>(63/88 | 111/323) Learning Data Association for Multi-Object Tracking using Only Coordinates (Mehdi Miah et al., 2024)</a></li><li><a href=#6488--112323-red-teaming-models-for-hyperspectral-image-analysis-using-explainable-ai-vladimir-zaigrajew-et-al-2024>(64/88 | 112/323) Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI (Vladimir Zaigrajew et al., 2024)</a></li><li><a href=#6588--113323-semcity-semantic-scene-generation-with-triplane-diffusion-jumin-lee-et-al-2024>(65/88 | 113/323) SemCity: Semantic Scene Generation with Triplane Diffusion (Jumin Lee et al., 2024)</a></li><li><a href=#6688--114323-robust-synthetic-to-real-transfer-for-stereo-matching-jiawei-zhang-et-al-2024>(66/88 | 114/323) Robust Synthetic-to-Real Transfer for Stereo Matching (Jiawei Zhang et al., 2024)</a></li><li><a href=#6788--115323-masked-autodecoder-is-effective-multi-task-vision-generalist-han-qiu-et-al-2024>(67/88 | 115/323) Masked AutoDecoder is Effective Multi-Task Vision Generalist (Han Qiu et al., 2024)</a></li><li><a href=#6888--116323-smartphone-region-wise-image-indoor-localization-using-deep-learning-for-indoor-tourist-attraction-gabriel-toshio-hirokawa-higa-et-al-2024>(68/88 | 116/323) Smartphone region-wise image indoor localization using deep learning for indoor tourist attraction (Gabriel Toshio Hirokawa Higa et al., 2024)</a></li><li><a href=#6988--117323-mondrian-on-device-high-performance-video-analytics-with-compressive-packed-inference-changmin-jeon-et-al-2024>(69/88 | 117/323) Mondrian: On-Device High-Performance Video Analytics with Compressive Packed Inference (Changmin Jeon et al., 2024)</a></li><li><a href=#7088--118323-lab-gatr-geometric-algebra-transformers-for-large-biomedical-surface-and-volume-meshes-julian-suk-et-al-2024>(70/88 | 118/323) LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes (Julian Suk et al., 2024)</a></li><li><a href=#7188--119323-uncertainty-guided-contrastive-learning-for-single-source-domain-generalisation-anastasios-arsenos-et-al-2024>(71/88 | 119/323) Uncertainty-guided Contrastive Learning for Single Source Domain Generalisation (Anastasios Arsenos et al., 2024)</a></li><li><a href=#7288--120323-nighthaze-nighttime-image-dehazing-via-self-prior-learning-beibei-lin-et-al-2024>(72/88 | 120/323) NightHaze: Nighttime Image Dehazing via Self-Prior Learning (Beibei Lin et al., 2024)</a></li><li><a href=#7388--121323-fetril-feature-translation-for-exemplar-free-class-incremental-learning-with-hill-climbing-eduard-hogea-et-al-2024>(73/88 | 121/323) FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental Learning with Hill-Climbing (Eduard Hogea et al., 2024)</a></li><li><a href=#7488--122323-time-efficient-and-identity-consistent-virtual-try-on-using-a-variant-of-altered-diffusion-models-phuong-dam-et-al-2024>(74/88 | 122/323) Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of Altered Diffusion Models (Phuong Dam et al., 2024)</a></li><li><a href=#7588--123323-entropy-is-not-enough-for-test-time-adaptation-from-the-perspective-of-disentangled-factors-jonghyun-lee-et-al-2024>(75/88 | 123/323) Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors (Jonghyun Lee et al., 2024)</a></li><li><a href=#7688--124323-efficient-diffusion-model-for-image-restoration-by-residual-shifting-zongsheng-yue-et-al-2024>(76/88 | 124/323) Efficient Diffusion Model for Image Restoration by Residual Shifting (Zongsheng Yue et al., 2024)</a></li><li><a href=#7788--125323-rediscovering-bce-loss-for-uniform-classification-qiufu-li-et-al-2024>(77/88 | 125/323) Rediscovering BCE Loss for Uniform Classification (Qiufu Li et al., 2024)</a></li><li><a href=#7888--126323-a-multimodal-intermediate-fusion-network-with-manifold-learning-for-stress-detection-morteza-bodaghi-et-al-2024>(78/88 | 126/323) A Multimodal Intermediate Fusion Network with Manifold Learning for Stress Detection (Morteza Bodaghi et al., 2024)</a></li><li><a href=#7988--127323-indicstr12-a-dataset-for-indic-scene-text-recognition-harsh-lunia-et-al-2024>(79/88 | 127/323) IndicSTR12: A Dataset for Indic Scene Text Recognition (Harsh Lunia et al., 2024)</a></li><li><a href=#8088--128323-bring-event-into-rgb-and-lidar-hierarchical-visual-motion-fusion-for-scene-flow-hanyu-zhou-et-al-2024>(80/88 | 128/323) Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow (Hanyu Zhou et al., 2024)</a></li><li><a href=#8188--129323-frequency-decoupling-for-motion-magnification-via-multi-level-isomorphic-architecture-fei-wang-et-al-2024>(81/88 | 129/323) Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture (Fei Wang et al., 2024)</a></li><li><a href=#8288--130323-mrc-net-6-dof-pose-estimation-with-multiscale-residual-correlation-yuelong-li-et-al-2024>(82/88 | 130/323) MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation (Yuelong Li et al., 2024)</a></li><li><a href=#8388--131323-dseg-lime---improving-image-explanation-by-hierarchical-data-driven-segmentation-patrick-knab-et-al-2024>(83/88 | 131/323) DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven Segmentation (Patrick Knab et al., 2024)</a></li><li><a href=#8488--132323-accurate-spatial-gene-expression-prediction-by-integrating-multi-resolution-features-youngmin-chung-et-al-2024>(84/88 | 132/323) Accurate Spatial Gene Expression Prediction by integrating Multi-resolution features (Youngmin Chung et al., 2024)</a></li><li><a href=#8588--133323-smurf-continuous-dynamics-for-motion-deblurring-radiance-fields-jungho-lee-et-al-2024>(85/88 | 133/323) SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields (Jungho Lee et al., 2024)</a></li><li><a href=#8688--134323-adaptive-fusion-of-single-view-and-multi-view-depth-for-autonomous-driving-junda-cheng-et-al-2024>(86/88 | 134/323) Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving (JunDa Cheng et al., 2024)</a></li><li><a href=#8788--135323-from-canteen-food-to-daily-meals-generalizing-food-recognition-to-more-practical-scenarios-guoshan-liu-et-al-2024>(87/88 | 135/323) From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios (Guoshan Liu et al., 2024)</a></li><li><a href=#8888--136323-learning-hierarchical-color-guidance-for-depth-map-super-resolution-runmin-cong-et-al-2024>(88/88 | 136/323) Learning Hierarchical Color Guidance for Depth Map Super-Resolution (Runmin Cong et al., 2024)</a></li></ul></li><li><a href=#cslg-55>cs.LG (55)</a><ul><li><a href=#155--137323-knowcoder-coding-structured-knowledge-into-llms-for-universal-information-extraction-zixuan-li-et-al-2024>(1/55 | 137/323) KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction (Zixuan Li et al., 2024)</a></li><li><a href=#255--138323-propml-probability-partial-multi-label-learning-łukasz-struski-et-al-2024>(2/55 | 138/323) ProPML: Probability Partial Multi-label Learning (Łukasz Struski et al., 2024)</a></li><li><a href=#355--139323-chai-clustered-head-attention-for-efficient-llm-inference-saurabh-agarwal-et-al-2024>(3/55 | 139/323) CHAI: Clustered Head Attention for Efficient LLM Inference (Saurabh Agarwal et al., 2024)</a></li><li><a href=#455--140323-chronos-learning-the-language-of-time-series-abdul-fatir-ansari-et-al-2024>(4/55 | 140/323) Chronos: Learning the Language of Time Series (Abdul Fatir Ansari et al., 2024)</a></li><li><a href=#555--141323-efficient-language-model-architectures-for-differentially-private-federated-learning-jae-hun-ro-et-al-2024>(5/55 | 141/323) Efficient Language Model Architectures for Differentially Private Federated Learning (Jae Hun Ro et al., 2024)</a></li><li><a href=#655--142323-fairness-feedback-loops-training-on-synthetic-data-amplifies-bias-sierra-wyllie-et-al-2024>(6/55 | 142/323) Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias (Sierra Wyllie et al., 2024)</a></li><li><a href=#755--143323-quantifying-and-mitigating-privacy-risks-for-tabular-generative-models-chaoyi-zhu-et-al-2024>(7/55 | 143/323) Quantifying and Mitigating Privacy Risks for Tabular Generative Models (Chaoyi Zhu et al., 2024)</a></li><li><a href=#855--144323-taming-pre-trained-llms-for-generalised-time-series-forecasting-via-cross-modal-knowledge-distillation-peiyuan-liu-et-al-2024>(8/55 | 144/323) Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation (Peiyuan Liu et al., 2024)</a></li><li><a href=#955--145323-disentangling-policy-from-offline-task-representation-learning-via-adversarial-data-augmentation-chengxing-jia-et-al-2024>(9/55 | 145/323) Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation (Chengxing Jia et al., 2024)</a></li><li><a href=#1055--146323-iterative-graph-neural-network-enhancement-via-frequent-subgraph-mining-of-explanations-harish-g-naik-et-al-2024>(10/55 | 146/323) Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations (Harish G. Naik et al., 2024)</a></li><li><a href=#1155--147323-conditional-computation-in-neural-networks-principles-and-research-trends-simone-scardapane-et-al-2024>(11/55 | 147/323) Conditional computation in neural networks: principles and research trends (Simone Scardapane et al., 2024)</a></li><li><a href=#1255--148323-advantage-aware-policy-optimization-for-offline-reinforcement-learning-yunpeng-qing-et-al-2024>(12/55 | 148/323) Advantage-Aware Policy Optimization for Offline Reinforcement Learning (Yunpeng Qing et al., 2024)</a></li><li><a href=#1355--149323-microt-low-energy-and-adaptive-models-for-mcus-yushan-huang-et-al-2024>(13/55 | 149/323) MicroT: Low-Energy and Adaptive Models for MCUs (Yushan Huang et al., 2024)</a></li><li><a href=#1455--150323-maxwells-demon-at-work-efficient-pruning-by-leveraging-saturation-of-neurons-simon-dufort-labbé-et-al-2024>(14/55 | 150/323) Maxwell&rsquo;s Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons (Simon Dufort-Labbé et al., 2024)</a></li><li><a href=#1555--151323-efficient-knowledge-deletion-from-trained-models-through-layer-wise-partial-machine-unlearning-vinay-chakravarthi-gogineni-et-al-2024>(15/55 | 151/323) Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning (Vinay Chakravarthi Gogineni et al., 2024)</a></li><li><a href=#1655--152323-hallmarks-of-optimization-trajectories-in-neural-networks-and-llms-the-lengths-bends-and-dead-ends-sidak-pal-singh-et-al-2024>(16/55 | 152/323) Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends (Sidak Pal Singh et al., 2024)</a></li><li><a href=#1755--153323-abstracting-sparse-dnn-acceleration-via-structured-sparse-tensor-decomposition-geonhwa-jeong-et-al-2024>(17/55 | 153/323) Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition (Geonhwa Jeong et al., 2024)</a></li><li><a href=#1855--154323-verification-aided-learning-of-neural-network-barrier-functions-with-termination-guarantees-shaoru-chen-et-al-2024>(18/55 | 154/323) Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees (Shaoru Chen et al., 2024)</a></li><li><a href=#1955--155323-workarena-how-capable-are-web-agents-at-solving-common-knowledge-work-tasks-alexandre-drouin-et-al-2024>(19/55 | 155/323) WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks? (Alexandre Drouin et al., 2024)</a></li><li><a href=#2055--156323-early-directional-convergence-in-deep-homogeneous-neural-networks-for-small-initializations-akshay-kumar-et-al-2024>(20/55 | 156/323) Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations (Akshay Kumar et al., 2024)</a></li><li><a href=#2155--157323-mechanics-of-next-token-prediction-with-self-attention-yingcong-li-et-al-2024>(21/55 | 157/323) Mechanics of Next Token Prediction with Self-Attention (Yingcong Li et al., 2024)</a></li><li><a href=#2255--158323-drivaernet-a-parametric-car-dataset-for-data-driven-aerodynamic-design-and-graph-based-drag-prediction-mohamed-elrefaie-et-al-2024>(22/55 | 158/323) DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction (Mohamed Elrefaie et al., 2024)</a></li><li><a href=#2355--159323-symmetric-q-learning-reducing-skewness-of-bellman-error-in-online-reinforcement-learning-motoki-omura-et-al-2024>(23/55 | 159/323) Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning (Motoki Omura et al., 2024)</a></li><li><a href=#2455--160323-harder-tasks-need-more-experts-dynamic-routing-in-moe-models-quzhe-huang-et-al-2024>(24/55 | 160/323) Harder Tasks Need More Experts: Dynamic Routing in MoE Models (Quzhe Huang et al., 2024)</a></li><li><a href=#2555--161323-federated-learning-of-socially-appropriate-agent-behaviours-in-simulated-home-environments-saksham-checker-et-al-2024>(25/55 | 161/323) Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments (Saksham Checker et al., 2024)</a></li><li><a href=#2655--162323-optimizing-polynomial-graph-filters-a-novel-adaptive-krylov-subspace-approach-keke-huang-et-al-2024>(26/55 | 162/323) Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach (Keke Huang et al., 2024)</a></li><li><a href=#2755--163323-graph-unlearning-with-efficient-partial-retraining-jiahao-zhang-et-al-2024>(27/55 | 163/323) Graph Unlearning with Efficient Partial Retraining (Jiahao Zhang et al., 2024)</a></li><li><a href=#2855--164323-graph-data-condensation-via-self-expressive-graph-structure-reconstruction-zhanyu-liu-et-al-2024>(28/55 | 164/323) Graph Data Condensation via Self-expressive Graph Structure Reconstruction (Zhanyu Liu et al., 2024)</a></li><li><a href=#2955--165323-supervised-time-series-classification-for-anomaly-detection-in-subsea-engineering-ergys-çokaj-et-al-2024>(29/55 | 165/323) Supervised Time Series Classification for Anomaly Detection in Subsea Engineering (Ergys Çokaj et al., 2024)</a></li><li><a href=#3055--166323-on-the-last-iterate-convergence-of-shuffling-gradient-methods-zijian-liu-et-al-2024>(30/55 | 166/323) On the Last-Iterate Convergence of Shuffling Gradient Methods (Zijian Liu et al., 2024)</a></li><li><a href=#3155--167323-do-deep-neural-network-solutions-form-a-star-domain-ankit-sonthalia-et-al-2024>(31/55 | 167/323) Do Deep Neural Network Solutions Form a Star Domain? (Ankit Sonthalia et al., 2024)</a></li><li><a href=#3255--168323-visual-privacy-auditing-with-diffusion-models-kristian-schwethelm-et-al-2024>(32/55 | 168/323) Visual Privacy Auditing with Diffusion Models (Kristian Schwethelm et al., 2024)</a></li><li><a href=#3355--169323-experimental-comparison-of-ensemble-methods-and-time-to-event-analysis-models-through-integrated-brier-score-and-concordance-index-camila-fernandez-et-al-2024>(33/55 | 169/323) Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index (Camila Fernandez et al., 2024)</a></li><li><a href=#3455--170323-proxy-methods-for-domain-adaptation-katherine-tsai-et-al-2024>(34/55 | 170/323) Proxy Methods for Domain Adaptation (Katherine Tsai et al., 2024)</a></li><li><a href=#3555--171323-enhancing-transfer-learning-with-flexible-nonparametric-posterior-sampling-hyungi-lee-et-al-2024>(35/55 | 171/323) Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling (Hyungi Lee et al., 2024)</a></li><li><a href=#3655--172323-lookupffn-making-transformers-compute-lite-for-cpu-inference-zhanpeng-zeng-et-al-2024>(36/55 | 172/323) LookupFFN: Making Transformers Compute-lite for CPU inference (Zhanpeng Zeng et al., 2024)</a></li><li><a href=#3755--173323-a-tutorial-on-multi-view-autoencoders-using-the-multi-view-ae-library-ana-lawry-aguila-et-al-2024>(37/55 | 173/323) A tutorial on multi-view autoencoders using the multi-view-AE library (Ana Lawry Aguila et al., 2024)</a></li><li><a href=#3855--174323-12-mj-per-class-on-device-online-few-shot-class-incremental-learning-yoga-esa-wibowo-et-al-2024>(38/55 | 174/323) 12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning (Yoga Esa Wibowo et al., 2024)</a></li><li><a href=#3955--175323-accelerated-inference-and-reduced-forgetting-the-dual-benefits-of-early-exit-networks-in-continual-learning-filip-szatkowski-et-al-2024>(39/55 | 175/323) Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning (Filip Szatkowski et al., 2024)</a></li><li><a href=#4055--176323-dataset-condensation-for-time-series-classification-via-dual-domain-matching-zhanyu-liu-et-al-2024>(40/55 | 176/323) Dataset Condensation for Time Series Classification via Dual Domain Matching (Zhanyu Liu et al., 2024)</a></li><li><a href=#4155--177323-towards-independence-criterion-in-machine-unlearning-of-features-and-labels-ling-han-et-al-2024>(41/55 | 177/323) Towards Independence Criterion in Machine Unlearning of Features and Labels (Ling Han et al., 2024)</a></li><li><a href=#4255--178323-do-agents-dream-of-electric-sheep-improving-generalization-in-reinforcement-learning-through-generative-learning-giorgio-franceschelli-et-al-2024>(42/55 | 178/323) Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning (Giorgio Franceschelli et al., 2024)</a></li><li><a href=#4355--179323-balancing-fairness-and-accuracy-in-data-restricted-binary-classification-zachary-mcbride-lazri-et-al-2024>(43/55 | 179/323) Balancing Fairness and Accuracy in Data-Restricted Binary Classification (Zachary McBride Lazri et al., 2024)</a></li><li><a href=#4455--180323-scalable-spatiotemporal-prediction-with-bayesian-neural-fields-feras-saad-et-al-2024>(44/55 | 180/323) Scalable Spatiotemporal Prediction with Bayesian Neural Fields (Feras Saad et al., 2024)</a></li><li><a href=#4555--181323-constrained-optimal-fuel-consumption-of-hev-a-constrained-reinforcement-learning-approach-shuchang-yan-2024>(45/55 | 181/323) Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach (Shuchang Yan, 2024)</a></li><li><a href=#4655--182323-xpertai-uncovering-model-strategies-for-sub-manifolds-simon-letzgus-et-al-2024>(46/55 | 182/323) XpertAI: uncovering model strategies for sub-manifolds (Simon Letzgus et al., 2024)</a></li><li><a href=#4755--183323-deepcdcl-an-cdcl-based-neural-network-verification-framework-zongxin-liu-et-al-2024>(47/55 | 183/323) DeepCDCL: An CDCL-based Neural Network Verification Framework (Zongxin Liu et al., 2024)</a></li><li><a href=#4855--184323-towards-faithful-explanations-boosting-rationalization-with-shortcuts-discovery-linan-yue-et-al-2024>(48/55 | 184/323) Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery (Linan Yue et al., 2024)</a></li><li><a href=#4955--185323-challenging-forgets-unveiling-the-worst-case-forget-sets-in-machine-unlearning-chongyu-fan-et-al-2024>(49/55 | 185/323) Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning (Chongyu Fan et al., 2024)</a></li><li><a href=#5055--186323-im-unpack-training-and-inference-with-arbitrarily-low-precision-integers-zhanpeng-zeng-et-al-2024>(50/55 | 186/323) IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers (Zhanpeng Zeng et al., 2024)</a></li><li><a href=#5155--187323-reinforced-sequential-decision-making-for-sepsis-treatment-the-posnegdm-framework-with-mortality-classifier-and-transformer-dipesh-tamboli-et-al-2024>(51/55 | 187/323) Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer (Dipesh Tamboli et al., 2024)</a></li><li><a href=#5255--188323-learning-augmented-algorithms-with-explicit-predictors-marek-elias-et-al-2024>(52/55 | 188/323) Learning-Augmented Algorithms with Explicit Predictors (Marek Elias et al., 2024)</a></li><li><a href=#5355--189323-mccatch-scalable-microcluster-detection-in-dimensional-and-nondimensional-datasets-braulio-v-sánchez-vinces-et-al-2024>(53/55 | 189/323) McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets (Braulio V. Sánchez Vinces et al., 2024)</a></li><li><a href=#5455--190323-robustifying-and-boosting-training-free-neural-architecture-search-zhenfeng-he-et-al-2024>(54/55 | 190/323) Robustifying and Boosting Training-Free Neural Architecture Search (Zhenfeng He et al., 2024)</a></li><li><a href=#5555--191323-unknown-domain-inconsistency-minimization-for-domain-generalization-seungjae-shin-et-al-2024>(55/55 | 191/323) Unknown Domain Inconsistency Minimization for Domain Generalization (Seungjae Shin et al., 2024)</a></li></ul></li><li><a href=#statml-5>stat.ML (5)</a><ul><li><a href=#15--192323-knowledge-transfer-across-multiple-principal-component-analysis-studies-zeyu-li-et-al-2024>(1/5 | 192/323) Knowledge Transfer across Multiple Principal Component Analysis Studies (Zeyu Li et al., 2024)</a></li><li><a href=#25--193323-fast-accurate-and-lightweight-sequential-simulation-based-inference-using-gaussian-locally-linear-mappings-henrik-häggström-et-al-2024>(2/5 | 193/323) Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings (Henrik Häggström et al., 2024)</a></li><li><a href=#35--194323-fairrr-pre-processing-for-group-fairness-through-randomized-response-xianli-zeng-et-al-2024>(3/5 | 194/323) FairRR: Pre-Processing for Group Fairness through Randomized Response (Xianli Zeng et al., 2024)</a></li><li><a href=#45--195323-cas-a-general-algorithm-for-online-selective-conformal-prediction-with-fcr-control-yajie-bao-et-al-2024>(4/5 | 195/323) CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control (Yajie Bao et al., 2024)</a></li><li><a href=#55--196323-on-the-nonconvexity-of-some-push-forward-constraints-and-its-consequences-in-machine-learning-lucas-de-lara-et-al-2024>(5/5 | 196/323) On the nonconvexity of some push-forward constraints and its consequences in machine learning (Lucas de Lara et al., 2024)</a></li></ul></li><li><a href=#csir-8>cs.IR (8)</a><ul><li><a href=#18--197323-towards-graph-foundation-models-for-personalization-andreas-damianou-et-al-2024>(1/8 | 197/323) Towards Graph Foundation Models for Personalization (Andreas Damianou et al., 2024)</a></li><li><a href=#28--198323-the-future-of-document-indexing-gpt-and-donut-revolutionize-table-of-content-processing-degaga-wolde-feyisa-et-al-2024>(2/8 | 198/323) The future of document indexing: GPT and Donut revolutionize table of content processing (Degaga Wolde Feyisa et al., 2024)</a></li><li><a href=#38--199323-analyzing-adversarial-attacks-on-sequence-to-sequence-relevance-models-andrew-parry-et-al-2024>(3/8 | 199/323) Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models (Andrew Parry et al., 2024)</a></li><li><a href=#48--200323-self-supervised-contrastive-learning-for-implicit-collaborative-filtering-shipeng-song-et-al-2024>(4/8 | 200/323) Self-supervised Contrastive Learning for Implicit Collaborative Filtering (Shipeng Song et al., 2024)</a></li><li><a href=#58--201323-empowering-sequential-recommendation-from-collaborative-signals-and-semantic-relatedness-mingyue-cheng-et-al-2024>(5/8 | 201/323) Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness (Mingyue Cheng et al., 2024)</a></li><li><a href=#68--202323-proactive-recommendation-with-iterative-preference-guidance-shuxian-bi-et-al-2024>(6/8 | 202/323) Proactive Recommendation with Iterative Preference Guidance (Shuxian Bi et al., 2024)</a></li><li><a href=#78--203323-desere-the-1st-workshop-on-decentralised-search-and-recommendation-mohamed-ragab-et-al-2024>(7/8 | 203/323) DESERE: The 1st Workshop on Decentralised Search and Recommendation (Mohamed Ragab et al., 2024)</a></li><li><a href=#88--204323-list-learning-to-index-spatio-textual-data-for-embedding-based-spatial-keyword-queries-ziqi-yin-et-al-2024>(8/8 | 204/323) LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries (Ziqi Yin et al., 2024)</a></li></ul></li><li><a href=#eesssp-4>eess.SP (4)</a><ul><li><a href=#14--205323-vector-quantization-for-deep-learning-based-csi-feedback-in-massive-mimo-systems-junyong-shin-et-al-2024>(1/4 | 205/323) Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems (Junyong Shin et al., 2024)</a></li><li><a href=#24--206323-deep-learning-assisted-parallel-interference-cancellation-for-grant-free-noma-in-machine-type-communication-yongjeong-oh-et-al-2024>(2/4 | 206/323) Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication (Yongjeong Oh et al., 2024)</a></li><li><a href=#34--207323-discrete-time-modeling-and-handover-analysis-of-intelligent-reflecting-surface-assisted-networks-hongtao-zhang-et-al-2024>(3/4 | 207/323) Discrete-Time Modeling and Handover Analysis of Intelligent Reflecting Surface-Assisted Networks (Hongtao Zhang et al., 2024)</a></li><li><a href=#44--208323-advancements-in-continuous-glucose-monitoring-integrating-deep-learning-and-ecg-signal-mohammadreza-hosseinzadehketilateh-et-al-2024>(4/4 | 208/323) Advancements in Continuous Glucose Monitoring: Integrating Deep Learning and ECG Signal (MohammadReza Hosseinzadehketilateh et al., 2024)</a></li></ul></li><li><a href=#csro-16>cs.RO (16)</a><ul><li><a href=#116--209323-vanp-learning-where-to-see-for-navigation-with-self-supervised-vision-action-pre-training-mohammad-nazeri-et-al-2024>(1/16 | 209/323) VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training (Mohammad Nazeri et al., 2024)</a></li><li><a href=#216--210323-deligrasp-inferring-object-mass-friction-and-compliance-with-llms-for-adaptive-and-minimally-deforming-grasp-policies-william-xie-et-al-2024>(2/16 | 210/323) DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies (William Xie et al., 2024)</a></li><li><a href=#316--211323-drplanner-diagnosis-and-repair-of-motion-planners-using-large-language-models-yuanfei-lin-et-al-2024>(3/16 | 211/323) DrPlanner: Diagnosis and Repair of Motion Planners Using Large Language Models (Yuanfei Lin et al., 2024)</a></li><li><a href=#416--212323-toward-an-analytic-theory-of-intrinsic-robustness-for-dexterous-grasping-albert-h-li-et-al-2024>(4/16 | 212/323) Toward An Analytic Theory of Intrinsic Robustness for Dexterous Grasping (Albert H. Li et al., 2024)</a></li><li><a href=#516--213323-multi-task-manipulation-policy-modeling-with-visuomotor-latent-diffusion-wenhui-tan-et-al-2024>(5/16 | 213/323) Multi-task Manipulation Policy Modeling with Visuomotor Latent Diffusion (Wenhui Tan et al., 2024)</a></li><li><a href=#616--214323-telemoma-a-modular-and-versatile-teleoperation-system-for-mobile-manipulation-shivin-dass-et-al-2024>(6/16 | 214/323) TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation (Shivin Dass et al., 2024)</a></li><li><a href=#716--215323-the-virtues-of-laziness-multi-query-kinodynamic-motion-planning-with-lazy-methods-anuj-pasricha-et-al-2024>(7/16 | 215/323) The Virtues of Laziness: Multi-Query Kinodynamic Motion Planning with Lazy Methods (Anuj Pasricha et al., 2024)</a></li><li><a href=#816--216323-a-framework-for-controlling-multiple-industrial-robots-using-mobile-applications-daniela-alvarado-et-al-2024>(8/16 | 216/323) A Framework for Controlling Multiple Industrial Robots using Mobile Applications (Daniela Alvarado et al., 2024)</a></li><li><a href=#916--217323-online-adaptation-of-sampling-based-motion-planning-with-inaccurate-models-marco-faroni-et-al-2024>(9/16 | 217/323) Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models (Marco Faroni et al., 2024)</a></li><li><a href=#1016--218323-efficient-global-navigational-planning-in-3d-structures-based-on-point-cloud-tomography-bowen-yang-et-al-2024>(10/16 | 218/323) Efficient Global Navigational Planning in 3D Structures based on Point Cloud Tomography (Bowen Yang et al., 2024)</a></li><li><a href=#1116--219323-learning-generalizable-feature-fields-for-mobile-manipulation-ri-zhao-qiu-et-al-2024>(11/16 | 219/323) Learning Generalizable Feature Fields for Mobile Manipulation (Ri-Zhao Qiu et al., 2024)</a></li><li><a href=#1216--220323-dexcap-scalable-and-portable-mocap-data-collection-system-for-dexterous-manipulation-chen-wang-et-al-2024>(12/16 | 220/323) DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation (Chen Wang et al., 2024)</a></li><li><a href=#1316--221323-3d-uncertain-distance-field-mapping-using-gmm-and-gp-qianqian-zou-et-al-2024>(13/16 | 221/323) 3D Uncertain Distance Field Mapping using GMM and GP (Qianqian Zou et al., 2024)</a></li><li><a href=#1416--222323-tractable-joint-prediction-and-planning-over-discrete-behavior-modes-for-urban-driving-adam-villaflor-et-al-2024>(14/16 | 222/323) Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving (Adam Villaflor et al., 2024)</a></li><li><a href=#1516--223323-task-and-motion-planning-in-hierarchical-3d-scene-graphs-aaron-ray-et-al-2024>(15/16 | 223/323) Task and Motion Planning in Hierarchical 3D Scene Graphs (Aaron Ray et al., 2024)</a></li><li><a href=#1616--224323-mps-a-new-method-for-selecting-the-stable-closed-loop-equilibrium-attitude-error-quaternion-of-a-uav-during-flight-francisco-m-f-r-gonçalves-et-al-2024>(16/16 | 224/323) MPS: A New Method for Selecting the Stable Closed-Loop Equilibrium Attitude-Error Quaternion of a UAV During Flight (Francisco M. F. R. Gonçalves et al., 2024)</a></li></ul></li><li><a href=#eessiv-9>eess.IV (9)</a><ul><li><a href=#19--225323-equipping-computational-pathology-systems-with-artifact-processing-pipelines-a-showcase-for-computation-and-performance-trade-offs-neel-kanwal-et-al-2024>(1/9 | 225/323) Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs (Neel Kanwal et al., 2024)</a></li><li><a href=#29--226323-dalsa-domain-adaptation-for-supervised-learning-from-sparsely-annotated-mr-images-michael-götz-et-al-2024>(2/9 | 226/323) DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images (Michael Götz et al., 2024)</a></li><li><a href=#39--227323-samda-leveraging-sam-on-few-shot-domain-adaptation-for-electronic-microscopy-segmentation-yiran-wang-et-al-2024>(3/9 | 227/323) SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation (Yiran Wang et al., 2024)</a></li><li><a href=#49--228323-ct-evaluation-of-2d-and-3d-holistic-deep-learning-methods-for-the-volumetric-segmentation-of-airway-lesions-amel-imene-hadj-bouzid-et-al-2024>(4/9 | 228/323) CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions (Amel Imene Hadj Bouzid et al., 2024)</a></li><li><a href=#59--229323-intra-video-positive-pairs-in-self-supervised-learning-for-ultrasound-blake-vanberlo-et-al-2024>(5/9 | 229/323) Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound (Blake VanBerlo et al., 2024)</a></li><li><a href=#69--230323-learning-correction-errors-via-frequency-self-attention-for-blind-image-super-resolution-haochen-sun-et-al-2024>(6/9 | 230/323) Learning Correction Errors via Frequency-Self Attention for Blind Image Super-Resolution (Haochen Sun et al., 2024)</a></li><li><a href=#79--231323-dynamic-u-net-adaptively-calibrate-features-for-abdominal-multi-organ-segmentation-jin-yang-et-al-2024>(7/9 | 231/323) Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ Segmentation (Jin Yang et al., 2024)</a></li><li><a href=#89--232323-aedes-aegypti-egg-counting-with-neural-networks-for-object-detection-micheli-nayara-de-oliveira-vicente-et-al-2024>(8/9 | 232/323) Aedes aegypti Egg Counting with Neural Networks for Object Detection (Micheli Nayara de Oliveira Vicente et al., 2024)</a></li><li><a href=#99--233323-guidegen-a-text-guided-framework-for-joint-ct-volume-and-anatomical-structure-generation-linrui-dai-et-al-2024>(9/9 | 233/323) GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation (Linrui Dai et al., 2024)</a></li></ul></li><li><a href=#csse-7>cs.SE (7)</a><ul><li><a href=#17--234323-livecodebench-holistic-and-contamination-free-evaluation-of-large-language-models-for-code-naman-jain-et-al-2024>(1/7 | 234/323) LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code (Naman Jain et al., 2024)</a></li><li><a href=#27--235323-process-modeling-with-large-language-models-humam-kourani-et-al-2024>(2/7 | 235/323) Process Modeling With Large Language Models (Humam Kourani et al., 2024)</a></li><li><a href=#37--236323-bus-factor-explorer-egor-klimov-et-al-2024>(3/7 | 236/323) Bus Factor Explorer (Egor Klimov et al., 2024)</a></li><li><a href=#47--237323-robustness-security-privacy-explainability-efficiency-and-usability-of-large-language-models-for-code-zhou-yang-et-al-2024>(4/7 | 237/323) Robustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code (Zhou Yang et al., 2024)</a></li><li><a href=#57--238323-fixing-smart-contract-vulnerabilities-a-comparative-analysis-of-literature-and-developers-practices-francesco-salzano-et-al-2024>(5/7 | 238/323) Fixing Smart Contract Vulnerabilities: A Comparative Analysis of Literature and Developer&rsquo;s Practices (Francesco Salzano et al., 2024)</a></li><li><a href=#67--239323-bayesflo-bayesian-fault-localization-of-complex-software-systems-yi-ji-et-al-2024>(6/7 | 239/323) BayesFLo: Bayesian fault localization of complex software systems (Yi Ji et al., 2024)</a></li><li><a href=#77--240323-supporting-error-chains-in-static-analysis-for-precise-evaluation-results-and-enhanced-usability-anna-katharina-wickert-et-al-2024>(7/7 | 240/323) Supporting Error Chains in Static Analysis for Precise Evaluation Results and Enhanced Usability (Anna-Katharina Wickert et al., 2024)</a></li></ul></li><li><a href=#cscr-6>cs.CR (6)</a><ul><li><a href=#16--241323-towards-model-extraction-attacks-in-gan-based-image-translation-via-domain-shift-mitigation-di-mi-et-al-2024>(1/6 | 241/323) Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation (Di Mi et al., 2024)</a></li><li><a href=#26--242323-one-for-all-and-all-for-one-gnn-based-control-flow-attestation-for-embedded-devices-marco-chilese-et-al-2024>(2/6 | 242/323) One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices (Marco Chilese et al., 2024)</a></li><li><a href=#36--243323-a-framework-for-cost-effective-and-self-adaptive-llm-shaking-and-recovery-mechanism-zhiyu-chen-et-al-2024>(3/6 | 243/323) A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism (Zhiyu Chen et al., 2024)</a></li><li><a href=#46--244323-backdoor-attack-with-mode-mixture-latent-modification-hongwei-zhang-et-al-2024>(4/6 | 244/323) Backdoor Attack with Mode Mixture Latent Modification (Hongwei Zhang et al., 2024)</a></li><li><a href=#56--245323-wannalaugh-a-configurable-ransomware-emulator----learning-to-mimic-malicious-storage-traces-dionysios-diamantopolous-et-al-2024>(5/6 | 245/323) WannaLaugh: A Configurable Ransomware Emulator &ndash; Learning to Mimic Malicious Storage Traces (Dionysios Diamantopolous et al., 2024)</a></li><li><a href=#66--246323-an-interpretable-generalization-mechanism-for-accurately-detecting-anomaly-and-identifying-networking-intrusion-techniques-hao-ting-pai-et-al-2024>(6/6 | 246/323) An Interpretable Generalization Mechanism for Accurately Detecting Anomaly and Identifying Networking Intrusion Techniques (Hao-Ting Pai et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--247323-generaitor-tree-in-the-loop-text-generation-for-language-model-explainability-and-adaptation-thilo-spinner-et-al-2024>(1/5 | 247/323) generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation (Thilo Spinner et al., 2024)</a></li><li><a href=#25--248323-enhancing-depression-diagnosis-oriented-chat-with-psychological-state-tracking-yiyang-gu-et-al-2024>(2/5 | 248/323) Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking (Yiyang Gu et al., 2024)</a></li><li><a href=#35--249323-from-paper-to-card-transforming-design-implications-with-generative-ai-donghoon-shin-et-al-2024>(3/5 | 249/323) From Paper to Card: Transforming Design Implications with Generative AI (Donghoon Shin et al., 2024)</a></li><li><a href=#45--250323-visual-decoding-and-reconstruction-via-eeg-embeddings-with-guided-diffusion-dongyang-li-et-al-2024>(4/5 | 250/323) Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion (Dongyang Li et al., 2024)</a></li><li><a href=#55--251323-tutoai-a-cross-domain-framework-for-ai-assisted-mixed-media-tutorial-creation-on-physical-tasks-yuexi-chen-et-al-2024>(5/5 | 251/323) TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks (Yuexi Chen et al., 2024)</a></li></ul></li><li><a href=#csdb-3>cs.DB (3)</a><ul><li><a href=#13--252323-omnimatch-effective-self-supervised-any-join-discovery-in-tabular-data-repositories-christos-koutras-et-al-2024>(1/3 | 252/323) OmniMatch: Effective Self-Supervised Any-Join Discovery in Tabular Data Repositories (Christos Koutras et al., 2024)</a></li><li><a href=#23--253323-couler-unified-machine-learning-workflow-optimization-in-cloud-xiaoda-wang-et-al-2024>(2/3 | 253/323) Couler: Unified Machine Learning Workflow Optimization in Cloud (Xiaoda Wang et al., 2024)</a></li><li><a href=#33--254323-generalised-graph-grammars-for-natural-language-processing-oliver-robert-fox-et-al-2024>(3/3 | 254/323) Generalised Graph Grammars for Natural Language Processing (Oliver Robert Fox et al., 2024)</a></li></ul></li><li><a href=#csdc-5>cs.DC (5)</a><ul><li><a href=#15--255323-measuring-data-similarity-for-efficient-federated-learning-a-feasibility-study-fernanda-famá-et-al-2024>(1/5 | 255/323) Measuring Data Similarity for Efficient Federated Learning: A Feasibility Study (Fernanda Famá et al., 2024)</a></li><li><a href=#25--256323-mpcpa-multi-center-privacy-computing-with-predictions-aggregation-based-on-denoising-diffusion-probabilistic-model-guibo-luo-et-al-2024>(2/5 | 256/323) MPCPA: Multi-Center Privacy Computing with Predictions Aggregation based on Denoising Diffusion Probabilistic Model (Guibo Luo et al., 2024)</a></li><li><a href=#35--257323-characterization-of-large-language-model-development-in-the-datacenter-qinghao-hu-et-al-2024>(3/5 | 257/323) Characterization of Large Language Model Development in the Datacenter (Qinghao Hu et al., 2024)</a></li><li><a href=#45--258323-accelerating-biclique-counting-on-gpu-linshan-qiu-et-al-2024>(4/5 | 258/323) Accelerating Biclique Counting on GPU (Linshan Qiu et al., 2024)</a></li><li><a href=#55--259323-efficient-fault-tolerance-for-pipelined-query-engines-via-write-ahead-lineage-ziheng-wang-et-al-2024>(5/5 | 259/323) Efficient Fault Tolerance for Pipelined Query Engines via Write-ahead Lineage (Ziheng Wang et al., 2024)</a></li></ul></li><li><a href=#eesssy-11>eess.SY (11)</a><ul><li><a href=#111--260323-learning-based-prescribed-time-safety-for-control-of-unknown-systems-with-control-barrier-functions-tzu-yuan-huang-et-al-2024>(1/11 | 260/323) Learning-based Prescribed-Time Safety for Control of Unknown Systems with Control Barrier Functions (Tzu-Yuan Huang et al., 2024)</a></li><li><a href=#211--261323-configuration-and-emt-simulation-of-the-240-bus-miniwecc-system-integrating-offshore-wind-farms-owfs-buxin-she-et-al-2024>(2/11 | 261/323) Configuration and EMT Simulation of the 240-bus MiniWECC System Integrating Offshore Wind Farms (OWFs) (Buxin She et al., 2024)</a></li><li><a href=#311--262323-humans-in-the-building-getting-rid-of-thermostats-for-optimal-thermal-comfort-control-in-energy-management-systems-jiali-wang-et-al-2024>(3/11 | 262/323) Humans-in-the-Building: Getting Rid of Thermostats for Optimal Thermal Comfort Control in Energy Management Systems (Jiali Wang et al., 2024)</a></li><li><a href=#411--263323-gmpc-geometric-model-predictive-control-for-wheeled-mobile-robot-trajectory-tracking-jiawei-tang-et-al-2024>(4/11 | 263/323) GMPC: Geometric Model Predictive Control for Wheeled Mobile Robot Trajectory Tracking (Jiawei Tang et al., 2024)</a></li><li><a href=#511--264323-improving-fairness-in-photovoltaic-curtailments-via-daily-topology-reconfiguration-for-voltage-control-in-power-distribution-networks-rahul-k-gupta-et-al-2024>(5/11 | 264/323) Improving Fairness in Photovoltaic Curtailments via Daily Topology Reconfiguration for Voltage Control in Power Distribution Networks (Rahul K. Gupta et al., 2024)</a></li><li><a href=#611--265323-utilizing-load-shifting-for-optimal-compressor-sequencing-in-industrial-refrigeration-rohit-konda-et-al-2024>(6/11 | 265/323) Utilizing Load Shifting for Optimal Compressor Sequencing in Industrial Refrigeration (Rohit Konda et al., 2024)</a></li><li><a href=#711--266323-distributed-estimation-by-two-agents-with-different-feature-spaces-aneesh-raghavan-et-al-2024>(7/11 | 266/323) Distributed Estimation by Two Agents with Different Feature Spaces (Aneesh Raghavan et al., 2024)</a></li><li><a href=#811--267323-on-the-locomotion-of-the-slider-within-a-self-adaptive-beam-slider-system-florian-müller-et-al-2024>(8/11 | 267/323) On the locomotion of the slider within a self-adaptive beam-slider system (Florian Müller et al., 2024)</a></li><li><a href=#911--268323-adaptive-gain-scheduling-using-reinforcement-learning-for-quadcopter-control-mike-timmerman-et-al-2024>(9/11 | 268/323) Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control (Mike Timmerman et al., 2024)</a></li><li><a href=#1011--269323-on-modeling-adequacy-and-stability-analysis-of-ibr-related-subsynchronous-oscillations-in-multimachine-systems-lilan-karunaratne-et-al-2024>(10/11 | 269/323) On Modeling Adequacy and Stability Analysis of IBR-related Subsynchronous Oscillations in Multimachine Systems (Lilan Karunaratne et al., 2024)</a></li><li><a href=#1111--270323-energy-versus-output-quality-of-non-volatile-writes-in-intermittent-computing-rei-barjami-et-al-2024>(11/11 | 270/323) Energy versus Output Quality of Non-volatile Writes in Intermittent Computing (Rei Barjami et al., 2024)</a></li></ul></li><li><a href=#csai-6>cs.AI (6)</a><ul><li><a href=#16--271323-transforming-competition-into-collaboration-the-revolutionary-role-of-multi-agent-systems-and-language-models-in-modern-organizations-carlos-jose-xavier-cruz-2024>(1/6 | 271/323) Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations (Carlos Jose Xavier Cruz, 2024)</a></li><li><a href=#26--272323-an-improved-strategy-for-blood-glucose-control-using-multi-step-deep-reinforcement-learning-weiwei-gu-et-al-2024>(2/6 | 272/323) An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning (Weiwei Gu et al., 2024)</a></li><li><a href=#36--273323-online-continual-learning-for-interactive-instruction-following-agents-byeonghwi-kim-et-al-2024>(3/6 | 273/323) Online Continual Learning For Interactive Instruction Following Agents (Byeonghwi Kim et al., 2024)</a></li><li><a href=#46--274323-perennial-semantic-data-terms-of-use-for-decentralized-web-rui-zhao-et-al-2024>(4/6 | 274/323) Perennial Semantic Data Terms of Use for Decentralized Web (Rui Zhao et al., 2024)</a></li><li><a href=#56--275323-optimal-design-and-implementation-of-an-open-source-emulation-platform-for-user-centric-shared-e-mobility-services-maqsood-hussain-shah-et-al-2024>(5/6 | 275/323) Optimal Design and Implementation of an Open-source Emulation Platform for User-Centric Shared E-mobility Services (Maqsood Hussain Shah et al., 2024)</a></li><li><a href=#66--276323-relevance-score-a-landmark-like-heuristic-for-planning-oliver-kim-et-al-2024>(6/6 | 276/323) Relevance Score: A Landmark-Like Heuristic for Planning (Oliver Kim et al., 2024)</a></li></ul></li><li><a href=#cssd-3>cs.SD (3)</a><ul><li><a href=#13--277323-multichannel-long-term-streaming-neural-speech-enhancement-for-static-and-moving-speakers-changsheng-quan-et-al-2024>(1/3 | 277/323) Multichannel Long-Term Streaming Neural Speech Enhancement for Static and Moving Speakers (Changsheng Quan et al., 2024)</a></li><li><a href=#23--278323-boosting-keyword-spotting-through-on-device-learnable-user-speech-characteristics-cristian-cioflan-et-al-2024>(2/3 | 278/323) Boosting keyword spotting through on-device learnable user speech characteristics (Cristian Cioflan et al., 2024)</a></li><li><a href=#33--279323-on-device-domain-learning-for-keyword-spotting-on-low-power-extreme-edge-embedded-systems-cristian-cioflan-et-al-2024>(3/3 | 279/323) On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge Embedded Systems (Cristian Cioflan et al., 2024)</a></li></ul></li><li><a href=#csce-5>cs.CE (5)</a><ul><li><a href=#15--280323-optimization-of-pressure-management-strategies-for-geological-co2-sequestration-using-surrogate-model-based-reinforcement-learning-jungang-chen-et-al-2024>(1/5 | 280/323) Optimization of Pressure Management Strategies for Geological CO2 Sequestration Using Surrogate Model-based Reinforcement Learning (Jungang Chen et al., 2024)</a></li><li><a href=#25--281323-a-boundary-integral-based-particle-initialization-algorithm-for-smooth-particle-hydrodynamics-parikshit-boregowda-et-al-2024>(2/5 | 281/323) A boundary integral based particle initialization algorithm for Smooth Particle Hydrodynamics (Parikshit Boregowda et al., 2024)</a></li><li><a href=#35--282323-towards-code-generation-for-octree-based-multigrid-solvers-richard-angersbach-et-al-2024>(3/5 | 282/323) Towards Code Generation for Octree-Based Multigrid Solvers (Richard Angersbach et al., 2024)</a></li><li><a href=#45--283323-a-time-adaptive-finite-element-phase-field-model-suitable-for-rate-independent-fracture-mechanics-felix-rörentrop-et-al-2024>(4/5 | 283/323) A time-adaptive finite element phase-field model suitable for rate-independent fracture mechanics (Felix Rörentrop et al., 2024)</a></li><li><a href=#55--284323-towards-full-automation-of-geometry-extraction-for-biomechanical-analysis-of-abdominal-aortic-aneurysm-neural-network-based-versus-classical-methodologies-farah-alkhatib-et-al-2024>(5/5 | 284/323) Towards Full Automation of Geometry Extraction for Biomechanical Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical Methodologies (Farah Alkhatib et al., 2024)</a></li></ul></li><li><a href=#csit-5>cs.IT (5)</a><ul><li><a href=#15--285323-6d-movable-antenna-based-on-user-distribution-modeling-and-optimization-xiaodan-shao-et-al-2024>(1/5 | 285/323) 6D Movable Antenna Based on User Distribution: Modeling and Optimization (Xiaodan Shao et al., 2024)</a></li><li><a href=#25--286323-multi-source-scheduling-and-resource-allocation-for-age-of-semantic-importance-optimization-in-status-update-systems-lunyuan-chen-et-al-2024>(2/5 | 286/323) Multi-source Scheduling and Resource Allocation for Age-of-Semantic-Importance Optimization in Status Update Systems (Lunyuan Chen et al., 2024)</a></li><li><a href=#35--287323-d2-jscc-digital-deep-joint-source-channel-coding-for-semantic-communications-jianhao-huang-et-al-2024>(3/5 | 287/323) D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic Communications (Jianhao Huang et al., 2024)</a></li><li><a href=#45--288323-achievable-rate-analysis-and-optimization-of-double-ris-assisted-spatially-correlated-mimo-with-statistical-csi-kaizhe-xu-et-al-2024>(4/5 | 288/323) Achievable Rate Analysis and Optimization of Double-RIS Assisted Spatially Correlated MIMO with Statistical CSI (Kaizhe Xu et al., 2024)</a></li><li><a href=#55--289323-approaching-rate-distortion-limits-in-neural-compression-with-lattice-transform-coding-eric-lei-et-al-2024>(5/5 | 289/323) Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding (Eric Lei et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--290323-robocertprob-property-specification-for-probabilistic-robochart-models-kangfeng-ye-et-al-2024>(1/1 | 290/323) RoboCertProb: Property Specification for Probabilistic RoboChart Models (Kangfeng Ye et al., 2024)</a></li></ul></li><li><a href=#mathna-3>math.NA (3)</a><ul><li><a href=#13--291323-preconditioners-based-on-voronoi-quantizers-of-random-variable-coefficients-for-stochastic-elliptic-partial-differential-equations-nicolas-venkovic-et-al-2024>(1/3 | 291/323) Preconditioners based on Voronoi quantizers of random variable coefficients for stochastic elliptic partial differential equations (Nicolas Venkovic et al., 2024)</a></li><li><a href=#23--292323-a-novel-fast-iterative-moment-method-for-near-continuum-flows-guanghan-li-et-al-2024>(2/3 | 292/323) A novel fast iterative moment method for near-continuum flows (Guanghan Li et al., 2024)</a></li><li><a href=#33--293323-transparent-boundary-condition-and-its-effectively-local-approximation-for-the-schrödinger-equation-on-a-rectangular-computational-domain-samardhi-yadav-et-al-2024>(3/3 | 293/323) Transparent boundary condition and its effectively local approximation for the Schrödinger equation on a rectangular computational domain (Samardhi Yadav et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#11--294323-generative-deep-learning-enabled-ultra-large-field-of-view-lens-free-imaging-ronald-b-liu-et-al-2024>(1/1 | 294/323) Generative deep learning-enabled ultra-large field-of-view lens-free imaging (Ronald B. Liu et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--295323-towards-a-dynamic-future-with-adaptable-computing-and-network-convergence-acnc-masoud-shokrnezhad-et-al-2024>(1/3 | 295/323) Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC) (Masoud Shokrnezhad et al., 2024)</a></li><li><a href=#23--296323-a-survey-on-federated-learning-in-intelligent-transportation-systems-rongqing-zhang-et-al-2024>(2/3 | 296/323) A Survey on Federated Learning in Intelligent Transportation Systems (Rongqing Zhang et al., 2024)</a></li><li><a href=#33--297323-online-digital-twin-empowered-content-resale-mechanism-in-age-of-information-aware-edge-caching-networks-yuhan-yi-et-al-2024>(3/3 | 297/323) Online Digital Twin-Empowered Content Resale Mechanism in Age of Information-Aware Edge Caching Networks (Yuhan Yi et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--298323-physics-transfer-learning-for-material-strength-screening-yingjie-zhao-et-al-2024>(1/1 | 298/323) Physics-Transfer Learning for Material Strength Screening (Yingjie Zhao et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--299323-long-term-hydrothermal-bid-based-market-simulator-joaquim-dias-garcia-et-al-2024>(1/2 | 299/323) Long-term Hydrothermal Bid-based Market Simulator (Joaquim Dias Garcia et al., 2024)</a></li><li><a href=#22--300323-pmbo-enhancing-black-box-optimization-through-multivariate-polynomial-surrogates-janina-schreiber-et-al-2024>(2/2 | 300/323) PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates (Janina Schreiber et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--301323-the-dawn-of-ai-native-eda-promises-and-challenges-of-large-circuit-models-lei-chen-et-al-2024>(1/1 | 301/323) The Dawn of AI-Native EDA: Promises and Challenges of Large Circuit Models (Lei Chen et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--302323-a-question-centric-multi-experts-contrastive-learning-framework-for-improving-the-accuracy-and-interpretability-of-deep-sequential-knowledge-tracing-models-hengyuan-zhang-et-al-2024>(1/2 | 302/323) A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models (Hengyuan Zhang et al., 2024)</a></li><li><a href=#22--303323-legally-binding-but-unfair-towards-assessing-fairness-of-privacy-policies-vincent-freiberger-et-al-2024>(2/2 | 303/323) Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies (Vincent Freiberger et al., 2024)</a></li></ul></li><li><a href=#csgt-2>cs.GT (2)</a><ul><li><a href=#12--304323-data-monetization-pathways-and-complex-dynamic-game-equilibrium-analysis-in-the-energy-industry-zongxian-wang-et-al-2024>(1/2 | 304/323) Data Monetization Pathways and Complex Dynamic Game Equilibrium Analysis in the Energy Industry (Zongxian Wang et al., 2024)</a></li><li><a href=#22--305323-multi-apartment-rent-division-ariel-d-procaccia-et-al-2024>(2/2 | 305/323) Multi-Apartment Rent Division (Ariel D. Procaccia et al., 2024)</a></li></ul></li><li><a href=#cssy-1>cs.SY (1)</a><ul><li><a href=#11--306323-system-design-approach-for-control-of-differentially-private-dynamical-systems-raman-goyal-et-al-2024>(1/1 | 306/323) System Design Approach for Control of Differentially Private Dynamical Systems (Raman Goyal et al., 2024)</a></li></ul></li><li><a href=#statap-1>stat.AP (1)</a><ul><li><a href=#11--307323-fusing-climate-data-products-using-a-spatially-varying-autoencoder-jacob-a-johnson-et-al-2024>(1/1 | 307/323) Fusing Climate Data Products using a Spatially Varying Autoencoder (Jacob A. Johnson et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--308323-beyond-the-labels-unveiling-text-dependency-in-paralinguistic-speech-recognition-datasets-jan-pešán-et-al-2024>(1/2 | 308/323) Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets (Jan Pešán et al., 2024)</a></li><li><a href=#22--309323-gender-ambiguous-voice-generation-through-feminine-speaking-style-transfer-in-male-voices-maria-koutsogiannaki-et-al-2024>(2/2 | 309/323) Gender-ambiguous voice generation through feminine speaking style transfer in male voices (Maria Koutsogiannaki et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--310323-monocentric-or-polycentric-city-an-empirical-perspective-rémi-lemoy-2024>(1/1 | 310/323) Monocentric or polycentric city? An empirical perspective (Rémi Lemoy, 2024)</a></li></ul></li><li><a href=#csma-3>cs.MA (3)</a><ul><li><a href=#13--311323-ensembling-prioritized-hybrid-policies-for-multi-agent-pathfinding-huijie-tang-et-al-2024>(1/3 | 311/323) Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding (Huijie Tang et al., 2024)</a></li><li><a href=#23--312323-ariadne-and-theseus-exploration-and-rendezvous-with-two-mobile-agents-in-an-unknown-graph-romain-cosson-2024>(2/3 | 312/323) Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph (Romain Cosson, 2024)</a></li><li><a href=#33--313323-asynchronous-approximate-byzantine-consensus-a-multi-hop-relay-method-and-tight-graph-conditions-liwei-yuan-et-al-2024>(3/3 | 313/323) Asynchronous Approximate Byzantine Consensus: A Multi-hop Relay Method and Tight Graph Conditions (Liwei Yuan et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--314323-unsupervised-self-organising-map-of-prostate-cell-raman-spectra-shows-disease-state-subclustering-daniel-west-et-al-2024>(1/1 | 314/323) Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering (Daniel West et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--315323-satisfiability-to-coverage-in-presence-of-fairness-matroid-and-global-constraints-tanmay-inamdar-et-al-2024>(1/3 | 315/323) Satisfiability to Coverage in Presence of Fairness, Matroid, and Global Constraints (Tanmay Inamdar et al., 2024)</a></li><li><a href=#23--316323-maximum-defective-clique-computation-improved-time-complexities-and-practical-performance-lijun-chang-2024>(2/3 | 316/323) Maximum Defective Clique Computation: Improved Time Complexities and Practical Performance (Lijun Chang, 2024)</a></li><li><a href=#33--317323-shining-light-on-periodic-dominating-sets-in-bounded-treewidth-graphs-jakob-greilhuber-et-al-2024>(3/3 | 317/323) Shining Light on Periodic Dominating Sets in Bounded-Treewidth Graphs (Jakob Greilhuber et al., 2024)</a></li></ul></li><li><a href=#mathmg-1>math.MG (1)</a><ul><li><a href=#11--318323-signed-graphs-in-data-sciences-via-communicability-geometry-fernando-diaz-diaz-et-al-2024>(1/1 | 318/323) Signed graphs in data sciences via communicability geometry (Fernando Diaz-Diaz et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--319323-characterising-harmful-data-sources-when-constructing-multi-fidelity-surrogate-models-nicolau-andrés-thió-et-al-2024>(1/1 | 319/323) Characterising harmful data sources when constructing multi-fidelity surrogate models (Nicolau Andrés-Thió et al., 2024)</a></li></ul></li><li><a href=#mathat-1>math.AT (1)</a><ul><li><a href=#11--320323-computing-generalized-ranks-of-persistence-modules-via-unfolding-to-zigzag-modules-tamal-k-dey-et-al-2024>(1/1 | 320/323) Computing Generalized Ranks of Persistence Modules via Unfolding to Zigzag Modules (Tamal K. Dey et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--321323-overlapping-community-detection-algorithms-using-modularity-and-the-cosine-do-duy-hieu-et-al-2024>(1/1 | 321/323) Overlapping community detection algorithms using Modularity and the cosine (Do Duy Hieu et al., 2024)</a></li></ul></li><li><a href=#csfl-1>cs.FL (1)</a><ul><li><a href=#11--322323-on-graph-grammars-and-games-jayakrishna-vijayakumar-et-al-2024>(1/1 | 322/323) On Graph Grammars and Games (Jayakrishna Vijayakumar et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--323323-the-primal-pathwidth-seth-michael-lampis-2024>(1/1 | 323/323) The Primal Pathwidth SETH (Michael Lampis, 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>