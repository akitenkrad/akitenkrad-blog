<!doctype html><html><head><title>arXiv @ 2024.03.15</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.15"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cs.AI (4) cs.CE (5) cs.CL (51) cs.CR (3) cs.CV (85) cs.CY (1) cs.DM (3) cs.DS (2) cs.GT (4) cs.HC (5) cs.IR (6) cs.IT (7) cs.LG (53) cs.LO (1) cs.MA (4) cs.NI (2) cs.RO (18) cs.SD (3) cs.SE (9) cs.SI (3) cs.SY (1) econ.TH (1) eess.AS (1) eess.IV (13) eess.SP (1) eess.SY (6) math.CO (1) math.NA (4) math.OC (3) q-bio.GN (1) q-bio.NC (1) quant-ph (2) stat.ME (1) stat."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240315000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-15T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.15"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240315000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Mar 15, 2024</p></div><div class=title><h1>arXiv @ 2024.03.15</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csai-4>cs.AI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csce-5>cs.CE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cscl-51>cs.CL (51)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cscr-3>cs.CR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cscv-85>cs.CV (85)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csdm-3>cs.DM (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csgt-4>cs.GT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csir-6>cs.IR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csit-7>cs.IT (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cslg-53>cs.LG (53)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csma-4>cs.MA (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csro-18>cs.RO (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cssd-3>cs.SD (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#csse-9>cs.SE (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cssi-3>cs.SI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#cssy-1>cs.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#eessiv-13>eess.IV (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#eesssy-6>eess.SY (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#mathna-4>math.NA (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#mathoc-3>math.OC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/#statml-4>stat.ML (4)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th><th>eess.IV</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Alpaca</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>3</td><td></td><td>2</td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Automatic Speech Recognition</td><td>8</td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>11</td><td>18</td><td>11</td><td>1</td><td>2</td></tr><tr><td>Black Box</td><td>1</td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Chatbot</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Code Generation</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>ControlNet</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>9</td><td></td><td>1</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>11</td><td></td><td>1</td><td>3</td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>Data Augmentation</td><td>1</td><td>5</td><td>3</td><td></td><td>1</td></tr><tr><td>Dialogue System</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>18</td><td>2</td><td></td><td>4</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Essay Scoring</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Face Recognition</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Few-shot</td><td>5</td><td>3</td><td>1</td><td></td><td>1</td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Fine-tuning</td><td>12</td><td>7</td><td>4</td><td>2</td><td></td></tr><tr><td>Foundation Model</td><td>1</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>GLUE</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT</td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-2</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Gemini</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative AI</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>8</td><td></td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>9</td><td></td><td>1</td><td></td></tr><tr><td>Graph</td><td>2</td><td>3</td><td>11</td><td>3</td><td></td></tr><tr><td>Graph Attention Networks</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>7</td><td></td><td></td></tr><tr><td>GraphSAGE</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Grounding</td><td>1</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Intent Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Karush-Kuhn-Tucker</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>5</td><td>2</td><td></td><td></td></tr><tr><td>Knowledge Graph</td><td>3</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>LLaMA</td><td>3</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>62</td><td>7</td><td>13</td><td>4</td><td></td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Morphological Analysis</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td>2</td><td>12</td><td>3</td><td></td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>5</td><td></td><td>1</td><td></td></tr><tr><td>Optical Character Recognition</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Out-of-domain</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Outlier Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>11</td><td>8</td><td>2</td><td>2</td><td>1</td></tr><tr><td>Pruning</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td>1</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Question Answering</td><td>3</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>6</td><td>2</td><td>3</td><td>2</td><td></td></tr><tr><td>Recommendation</td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Recommender System</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>1</td><td>7</td><td>4</td><td></td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>1</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>3</td><td>1</td><td></td><td>1</td></tr><tr><td>Rerank</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Scaling Law</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td>2</td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td>3</td><td>3</td><td>9</td><td></td></tr><tr><td>Simulator</td><td>1</td><td>3</td><td>3</td><td>9</td><td></td></tr><tr><td>Stemming</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Style Transfer</td><td>1</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>2</td><td>6</td><td>3</td><td></td><td>4</td></tr><tr><td>T5</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2SQL</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td>3</td><td>10</td><td>1</td><td>1</td><td>2</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td></td><td>2</td><td>1</td><td>2</td></tr><tr><td>Vision Transformer</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>5</td><td></td><td>2</td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>4</td><td>7</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>human-in-the-loop</td><td>1</td><td></td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-51>cs.CL (51)</h2><h3 id=151--1310-zero-shot-and-few-shot-generation-strategies-for-artificial-clinical-records-erlend-frayling-et-al-2024>(1/51 | 1/310) Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records (Erlend Frayling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erlend Frayling, Jake Lever, Graham McDonald. (2024)<br><strong>Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records</strong><br><button class=copy-to-clipboard title="Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Few-shot, Fine-tuning, Fine-tuning, Zero-shot, LLaMA, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08664v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08664v2.pdf filename=2403.08664v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the <b>Llama</b> 2 <b>LLM</b> to create synthetic medical records that accurately reflect real patient information, employing <b>zero-shot</b> and <b>few-shot</b> <b>prompting</b> strategies for comparison against <b>fine-tuned</b> methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, utilising data from the MIMIC-IV dataset for comparison. In this work introduce a novel <b>prompting</b> technique that leverages a <b>chain-of-thought</b> <b>approach,</b> enhancing the model&rsquo;s ability to generate more accurate and contextually relevant medical narratives without prior <b>fine-tuning.</b> Our findings suggest that this <b>chain-of-thought</b> <b>prompted</b> approach allows the <b>zero-shot</b> model to achieve results on par with those of <b>fine-tuned</b> models, based on <b>Rouge</b> metrics evaluation.</p></p class="citation"></blockquote><h3 id=251--2310-rich-semantic-knowledge-enhanced-large-language-models-for-few-shot-chinese-spell-checking-ming-dong-et-al-2024>(2/51 | 2/310) Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking (Ming Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Dong, Yujing Chen, Miao Zhang, Hao Sun, Tingting He. (2024)<br><strong>Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking</strong><br><button class=copy-to-clipboard title="Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Optical Character Recognition, Optical Character Recognition, Few-shot, Foundation Model, BERT, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08492v1.pdf filename=2403.08492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and <b>optical</b> <b>character</b> <b>recognition</b> <b>(OCR).</b> Most of the existing CSC approaches relying on <b>BERT</b> architecture achieve excellent performance. However, limited by the scale of the <b>foundation</b> <b>model,</b> <b>BERT-based</b> method does not work well in <b>few-shot</b> scenarios, showing certain limitations in practical applications. In this paper, we explore using an <b>in-context</b> <b>learning</b> method named RS-LLM (Rich Semantic based <b>LLMs)</b> to introduce <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as the <b>foundation</b> <b>model.</b> Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, <b>LLMs</b> achieve better performance than the <b>BERT-based</b> model on <b>few-shot</b> CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verified the superiority of our proposed framework.</p></p class="citation"></blockquote><h3 id=351--3310-large-language-models-are-contrastive-reasoners-liang-yao-2024>(3/51 | 3/310) Large Language Models are Contrastive Reasoners (Liang Yao, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Yao. (2024)<br><strong>Large Language Models are Contrastive Reasoners</strong><br><button class=copy-to-clipboard title="Large Language Models are Contrastive Reasoners" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Few-shot, Zero-shot, GPT, GPT-4, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08211v1.pdf filename=2403.08211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompting</b> methods play a crucial role in enhancing the capabilities of pre-trained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We explore how contrastive <b>prompting</b> (CP) significantly improves the ability of <b>large</b> <b>language</b> <b>models</b> to perform complex <b>reasoning.</b> We demonstrate that <b>LLMs</b> are decent contrastive reasoners by simply adding &ldquo;Let&rsquo;s give a correct and a wrong answer.&rdquo; before <b>LLMs</b> provide answers. Experiments on two <b>large</b> <b>language</b> <b>models</b> show that <b>zero-shot</b> contrastive <b>prompting</b> improves performance on a range of arithmetic, <b>commonsense,</b> <b>and</b> symbolic <b>reasoning</b> tasks without any hand-crafted <b>few-shot</b> examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art <b>GPT-4</b> model. Our method not only surpasses <b>zero-shot</b> CoT and <b>few-shot</b> CoT in most arithmetic and <b>commonsense</b> <b>reasoning</b> tasks but also can seamlessly integrate with existing <b>prompting</b> methods, resulting in improved or comparable results when compared to state-of-the-art methods. Our code is available at <a href=https://github.com/yao8839836/cp>https://github.com/yao8839836/cp</a></p></p class="citation"></blockquote><h3 id=451--4310-pet-sql-a-prompt-enhanced-two-stage-text-to-sql-framework-with-cross-consistency-zhishuai-li-et-al-2024>(4/51 | 4/310) PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency (Zhishuai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao. (2024)<br><strong>PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency</strong><br><button class=copy-to-clipboard title="PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Few-shot, Text2SQL, Text2SQL, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09732v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09732v2.pdf filename=2403.09732v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Text-to-SQL</b> <b>(Text2SQL)</b> emphasize stimulating the <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> on <b>in-context</b> <b>learning,</b> achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current <b>LLM-based</b> natural language to SQL systems. We first introduce a novel <b>prompt</b> representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct <b>LLMs</b> in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as <b>few-shot</b> demonstrations, <b>prompting</b> the <b>LLM</b> to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the <b>prompt&rsquo;s</b> schema information and instruct the <b>LLM</b> to produce the final SQL. Finally, as the post-refinement module, we propose using cross-consistency across different <b>LLMs</b> rather than self-consistency within a particular <b>LLM.</b> Our methods achieve new SOTA results on the Spider <b>benchmark,</b> with an execution accuracy of 87.6%.</p></p class="citation"></blockquote><h3 id=551--5310-lmstyle-benchmark-evaluating-text-style-transfer-for-chatbots-jianlin-chen-2024>(5/51 | 5/310) LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots (Jianlin Chen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianlin Chen. (2024)<br><strong>LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots</strong><br><button class=copy-to-clipboard title="LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Alpaca, ChatGPT, LLaMA, Chatbot, Style Transfer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08943v1.pdf filename=2403.08943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the breakthrough of <b>ChatGPT,</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have garnered significant attention in the research community. With the development of <b>LLMs,</b> the question of text <b>style</b> <b>transfer</b> for conversational models has emerged as a natural extension, where <b>chatbots</b> may possess their own <b>styles</b> <b>or</b> even characters. However, standard evaluation metrics have not yet been established for this new settings. This paper aims to address this issue by proposing the LMStyle <b>Benchmark,</b> a novel evaluation framework applicable to chat-style text <b>style</b> <b>transfer</b> (C-TST), that can measure the quality of <b>style</b> <b>transfer</b> for <b>LLMs</b> in an automated and scalable manner. In addition to conventional <b>style</b> <b>strength</b> metrics, LMStyle <b>Benchmark</b> further considers a novel aspect of metrics called appropriateness, a high-level metrics take account of coherence, fluency and other implicit factors without the aid of reference samples. Our experiments demonstrate that the new evaluation methods introduced by LMStyle <b>Benchmark</b> have a higher correlation with human judgments in terms of appropriateness. Based on LMStyle <b>Benchmark,</b> we present a comprehensive list of evaluation results for popular <b>LLMs,</b> including <b>LLaMA,</b> <b>Alpaca,</b> and Vicuna, reflecting their stylistic properties, such as formality and sentiment strength, along with their appropriateness.</p></p class="citation"></blockquote><h3 id=651--6310-from-human-experts-to-machines-an-llm-supported-approach-to-ontology-and-knowledge-graph-construction-vamsi-krishna-kommineni-et-al-2024>(6/51 | 6/310) From human experts to machines: An LLM supported approach to ontology and knowledge graph construction (Vamsi Krishna Kommineni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vamsi Krishna Kommineni, Birgitta König-Ries, Sheeba Samuel. (2024)<br><strong>From human experts to machines: An LLM supported approach to ontology and knowledge graph construction</strong><br><button class=copy-to-clipboard title="From human experts to machines: An LLM supported approach to ontology and knowledge graph construction" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, human-in-the-loop, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08345v1.pdf filename=2403.08345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The conventional process of building Ontologies and <b>Knowledge</b> <b>Graphs</b> <b>(KGs)</b> heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (or populate with instances), and ensure data quality (including amongst others accuracy and completeness). On the other hand, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process. This work explores the (semi-)automatic construction of <b>KGs</b> facilitated by open-source <b>LLMs.</b> Our pipeline involves formulating competency questions (CQs), developing an ontology (TBox) based on these CQs, constructing <b>KGs</b> using the developed ontology, and evaluating the resultant <b>KG</b> with minimal to no involvement of human experts. We showcase the feasibility of our semi-automated pipeline by creating a <b>KG</b> on deep learning methodologies by exploiting scholarly publications. To evaluate the answers generated via <b>Retrieval-Augmented-Generation</b> <b>(RAG)</b> <b>as</b> well as the <b>KG</b> concepts automatically extracted using <b>LLMs,</b> we design a judge <b>LLM,</b> which rates the generated content based on ground truth. Our findings suggest that employing <b>LLMs</b> could potentially reduce the human effort involved in the construction of <b>KGs,</b> although a <b>human-in-the-loop</b> approach is recommended to evaluate automatically generated <b>KGs.</b></p></p class="citation"></blockquote><h3 id=751--7310-can-large-language-models-identify-authorship-baixiang-huang-et-al-2024>(7/51 | 7/310) Can Large Language Models Identify Authorship? (Baixiang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baixiang Huang, Canyu Chen, Kai Shu. (2024)<br><strong>Can Large Language Models Identify Authorship?</strong><br><button class=copy-to-clipboard title="Can Large Language Models Identify Authorship?" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Zero-shot, Reasoning, Large Language Model, Large Language Model, Pre-trained Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08213v1.pdf filename=2403.08213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated exceptional capacity for <b>reasoning</b> and problem-solving. However, their potential in authorship analysis, encompassing authorship verification and attribution, remains underexplored. This paper conducts a comprehensive evaluation of <b>LLMs</b> in these critical tasks. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage <b>text</b> <b>embeddings</b> from <b>pre-trained</b> <b>language</b> <b>models.</b> These methods, which typically require <b>fine-tuning</b> on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can <b>LLMs</b> perform <b>zero-shot,</b> end-to-end authorship verification effectively? (2) Are <b>LLMs</b> capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)? (3) How can <b>LLMs</b> provide explainability in authorship analysis, particularly through the role of linguistic features? Moreover, we investigate the integration of explicit linguistic features to guide <b>LLMs</b> in their <b>reasoning</b> processes. Our extensive assessment demonstrates <b>LLMs&rsquo;</b> proficiency in both tasks without the need for domain-specific <b>fine-tuning,</b> providing insights into their decision-making via a detailed analysis of linguistic features. This establishes a new <b>benchmark</b> for future research on <b>LLM-based</b> authorship analysis. The code and data are available at <a href=https://github.com/baixianghuang/authorship-llm>https://github.com/baixianghuang/authorship-llm</a>.</p></p class="citation"></blockquote><h3 id=851--8310-sotopia-π-interactive-learning-of-socially-intelligent-language-agents-ruiyi-wang-et-al-2024>(8/51 | 8/310) SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language Agents (Ruiyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, Hao Zhu. (2024)<br><strong>SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language Agents</strong><br><button class=copy-to-clipboard title="SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language Agents" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-4, Massive Multitask Language Understanding (MMLU), Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08715v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08715v2.pdf filename=2403.08715v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\pi$, improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> ratings. We show that our training method allows a 7B <b>LLM</b> to reach the social goal completion ability of an expert model <b>(GPT-4-based</b> agent), while improving the safety of language agents and maintaining general <b>QA</b> ability on the <b>MMLU</b> <b>benchmark.</b> We also find that this training paradigm uncovers some difficulties in <b>LLM-based</b> evaluation of social intelligence: <b>LLM-based</b> evaluators overestimate the abilities of the language agents trained specifically for social interaction.</p></p class="citation"></blockquote><h3 id=951--9310-devbench-a-comprehensive-benchmark-for-software-development-bowen-li-et-al-2024>(9/51 | 9/310) DevBench: A Comprehensive Benchmark for Software Development (Bowen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, Zhiyin Yu, He Du, Ping Yang, Dahua Lin, Chao Peng, Kai Chen. (2024)<br><strong>DevBench: A Comprehensive Benchmark for Software Development</strong><br><button class=copy-to-clipboard title="DevBench: A Comprehensive Benchmark for Software Development" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SE, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-4, GPT-4 turbo, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08604v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08604v2.pdf filename=2403.08604v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have significantly enhanced their coding capabilities. However, existing <b>benchmarks</b> predominantly focused on simplified or isolated aspects of programming, such as single-file <b>code</b> <b>generation</b> or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive <b>benchmark</b> that evaluates <b>LLMs</b> across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current <b>LLMs,</b> including <b>GPT-4-Turbo,</b> <b>fail</b> to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. Our findings offer actionable insights for the future development of <b>LLMs</b> toward real-world programming applications. Our <b>benchmark</b> is available at <a href=https://github.com/open-compass/DevBench>https://github.com/open-compass/DevBench</a></p></p class="citation"></blockquote><h3 id=1051--10310-teams-rl-teaching-llms-to-teach-themselves-better-instructions-via-reinforcement-learning-shangding-gu-et-al-2024>(10/51 | 10/310) TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning (Shangding Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shangding Gu, Alois Knoll, Ming Jin. (2024)<br><strong>TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning</strong><br><button class=copy-to-clipboard title="TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Stemming, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08694v1.pdf filename=2403.08694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> often confronts challenges <b>stemming</b> from the heavy reliance on human annotators in the <b>reinforcement</b> <b>learning</b> with human feedback <b>(RLHF)</b> framework, or the frequent and costly external queries tied to the self-instruct paradigm. In this work, we pivot to <b>Reinforcement</b> <b>Learning</b> (RL) &ndash; but with a twist. Diverging from the typical <b>RLHF,</b> which refines <b>LLMs</b> following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for <b>fine-tuning.</b> Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets. It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single <b>fine-tuning</b> step and negating the need for subsequent <b>RLHF</b> stages. Our findings highlight key advantages of our approach: reduced need for human involvement and fewer model queries (only $5.73%$ of WizardLM&rsquo;s total), along with enhanced capabilities of <b>LLMs</b> in crafting and comprehending complex instructions compared to strong baselines, and substantially improved model privacy protection.</p></p class="citation"></blockquote><h3 id=1151--11310-generative-pretrained-structured-transformers-unsupervised-syntactic-language-models-at-scale-xiang-hu-et-al-2024>(11/51 | 11/310) Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale (Xiang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Hu, Pengyu Ji, Qingyang Zhu, Wei Wu, Kewei Tu. (2024)<br><strong>Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale</strong><br><button class=copy-to-clipboard title="Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Supervised Learning, Unsupervised Learning, GPT, GPT-2, Transformer, Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08293v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08293v2.pdf filename=2403.08293v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A syntactic <b>language</b> <b>model</b> (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured <b>Transformers</b> (GPST), an <b>unsupervised</b> SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM <b>supervised</b> by a uni-directional <b>language</b> <b>modeling</b> loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, <b>supervised</b> by a bi-directional <b>language</b> <b>modeling</b> loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over <b>GPT-2</b> with a comparable size in numerous tasks covering both <b>language</b> <b>understanding</b> and <b>language</b> <b>generation.</b> Meanwhile, GPST also significantly outperforms existing <b>unsupervised</b> SLMs on left-to-right grammar induction, while holding a substantial acceleration on training.</p></p class="citation"></blockquote><h3 id=1251--12310-steering-llms-towards-unbiased-responses-a-causality-guided-debiasing-framework-jingling-li-et-al-2024>(12/51 | 12/310) Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework (Jingling Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, Yang Liu. (2024)<br><strong>Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework</strong><br><button class=copy-to-clipboard title="Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 55<br>Keywords: Black Box, Reasoning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08743v1.pdf filename=2403.08743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can easily generate biased and discriminative responses. As <b>LLMs</b> tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and <b>LLM</b> outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to <b>LLMs,</b> and (2) the internal <b>reasoning</b> process of <b>LLM</b> inference, to guide the design of <b>prompts</b> for debiasing <b>LLM</b> outputs through selection mechanisms. Our framework unifies existing de-biasing <b>prompting</b> approaches such as inhibitive instructions and <b>in-context</b> contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free <b>reasoning.</b> Our strong empirical performance on real-world datasets demonstrates that our framework provides principled guidelines on debiasing <b>LLM</b> outputs even with only the <b>black-box</b> <b>access.</b></p></p class="citation"></blockquote><h3 id=1351--13310-the-human-factor-in-detecting-errors-of-large-language-models-a-systematic-literature-review-and-future-research-directions-christian-a-schiller-2024>(13/51 | 13/310) The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions (Christian A. Schiller, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian A. Schiller. (2024)<br><strong>The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions</strong><br><button class=copy-to-clipboard title="The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: ChatGPT, Stemming, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09743v1.pdf filename=2403.09743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The launch of <b>ChatGPT</b> by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to the mainstream and setting new records in user adoption. <b>LLMs,</b> particularly <b>ChatGPT,</b> trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - &ldquo;hallucinations&rdquo; and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks. There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in <b>LLM</b> outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essential for organizations aiming to leverage <b>LLM</b> technology efficiently, guiding targeted training and deployment strategies to enhance error detection by users. This approach not only aims to optimize the use of <b>LLMs</b> but also to prevent potential downstream issues <b>stemming</b> from reliance on inaccurate model responses. The research emphasizes the balance between technological advancement and human insight in maximizing the benefits of <b>LLMs</b> while minimizing the risks, particularly in areas where precision is paramount. This paper performs a systematic literature research on this research topic, analyses and synthesizes the findings, and outlines future research directions. Literature selection cut-off date is January 11th 2024.</p></p class="citation"></blockquote><h3 id=1451--14310-detecting-hallucination-and-coverage-errors-in-retrieval-augmented-generation-for-controversial-topics-tyler-a-chang-et-al-2024>(14/51 | 14/310) Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics (Tyler A. Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tyler A. Chang, Katrin Tomanek, Jessica Hoffmann, Nithum Thain, Erin van Liemt, Kathleen Meier-Hellstern, Lucas Dixon. (2024)<br><strong>Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics</strong><br><button class=copy-to-clipboard title="Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Chatbot, Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08904v1.pdf filename=2403.08904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore a strategy to handle controversial topics in <b>LLM-based</b> <b>chatbots</b> based on Wikipedia&rsquo;s Neutral Point of View (NPOV) principle: acknowledge the absence of a single true answer and surface multiple perspectives. We frame this as <b>retrieval</b> <b>augmented</b> <b>generation,</b> where perspectives are retrieved from a knowledge base and the <b>LLM</b> is tasked with generating a fluent and faithful response from the given perspectives. As a starting point, we use a deterministic <b>retrieval</b> <b>system</b> <b>and</b> then focus on common <b>LLM</b> failure modes that arise during this approach to <b>text</b> <b>generation,</b> namely hallucination and coverage errors. We propose and evaluate three methods to detect such errors based on (1) word-overlap, (2) salience, and (3) <b>LLM-based</b> classifiers. Our results demonstrate that <b>LLM-based</b> classifiers, even when trained only on synthetic errors, achieve high error detection performance, with ROC AUC scores of 95.3% for hallucination and 90.5% for coverage error detection on unambiguous error cases. We show that when no training data is available, our other methods still yield good results on hallucination (84.0%) and coverage error (85.2%) detection.</p></p class="citation"></blockquote><h3 id=1551--15310-mastering-text-code-and-math-simultaneously-via-fusing-highly-specialized-language-models-ning-ding-et-al-2024>(15/51 | 15/310) Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models (Ning Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Weilin Zhao, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models</strong><br><button class=copy-to-clipboard title="Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Graph Attention Networks, Supervised Learning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08281v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08281v3.pdf filename=2403.08281v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an <b>LLM</b> within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level <b>gating</b> mechanism is introduced to blend the specialists&rsquo; outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a high-quality <b>supervised</b> <b>instruction</b> <b>tuning</b> dataset, UltraChat 2, which includes text, code, and mathematical content. This dataset comprises approximately 300,000 <b>instructions</b> <b>and</b> covers a wide range of topics in each domain. Experiments show that our model could simultaneously achieve mastery of the three crucial domains.</p></p class="citation"></blockquote><h3 id=1651--16310-recipe4u-student-chatgpt-interaction-dataset-in-efl-writing-education-jieun-han-et-al-2024>(16/51 | 16/310) RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education (Jieun Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Tak Yeon Lee, So-Yeon Ahn, Alice Oh. (2024)<br><strong>RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education</strong><br><button class=copy-to-clipboard title="RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Generative AI, ChatGPT, Dialogue System, Intent Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08272v1.pdf filename=2403.08272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>generative</b> <b>AI</b> in education is expanding, yet empirical analyses of large-scale and real-world interactions between students and AI systems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE for University), a dataset sourced from a semester-long experiment with 212 college students in English as Foreign Language (EFL) writing courses. During the study, students engaged in <b>dialogues</b> <b>with</b> <b>ChatGPT</b> to revise their essays. RECIPE4U includes comprehensive records of these interactions, including conversation logs, students&rsquo; <b>intent,</b> <b>students&rsquo;</b> self-rated satisfaction, and students&rsquo; essay edit histories. In particular, we annotate the students&rsquo; utterances in RECIPE4U with 13 intention labels based on our coding schemes. We establish baseline results for two subtasks in task-oriented <b>dialogue</b> <b>systems</b> within educational contexts: <b>intent</b> <b>detection</b> and satisfaction estimation. As a foundational step, we explore student-ChatGPT interaction patterns through RECIPE4U and analyze them by focusing on students&rsquo; <b>dialogue,</b> <b>essay</b> data statistics, and students&rsquo; essay edits. We further illustrate potential applications of RECIPE4U dataset for enhancing the incorporation of <b>LLMs</b> in educational frameworks. RECIPE4U is publicly available at <a href=https://zeunie.github.io/RECIPE4U/>https://zeunie.github.io/RECIPE4U/</a>.</p></p class="citation"></blockquote><h3 id=1751--17310-boosting-disfluency-detection-with-large-language-model-as-disfluency-generator-zhenrong-cheng-et-al-2024>(17/51 | 17/310) Boosting Disfluency Detection with Large Language Model as Disfluency Generator (Zhenrong Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenrong Cheng, Jiayan Guo, Hao Sun, Yan Zhang. (2024)<br><strong>Boosting Disfluency Detection with Large Language Model as Disfluency Generator</strong><br><button class=copy-to-clipboard title="Boosting Disfluency Detection with Large Language Model as Disfluency Generator" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08229v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08229v1.pdf filename=2403.08229v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current disfluency detection methods heavily rely on costly and scarce human-annotated <b>data.</b> <b>To</b> tackle this issue, some approaches employ heuristic or statistical features to generate disfluent sentences, partially improving detection performance. However, these sentences often deviate from real-life scenarios, constraining overall model enhancement. In this study, we propose a lightweight <b>data</b> <b>augmentation</b> approach for disfluency detection, utilizing the superior generative and semantic understanding capabilities of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to generate disfluent sentences as augmentation <b>data.</b> <b>We</b> leverage <b>LLM</b> to generate diverse and more realistic sentences guided by specific <b>prompts,</b> without the need for <b>fine-tuning</b> the <b>LLM.</b> Subsequently, we apply an uncertainty-aware <b>data</b> <b>filtering</b> approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance. Experiments using enhanced <b>data</b> <b>yielded</b> state-of-the-art results. The results showed that using a small amount of <b>LLM-generated</b> enhanced <b>data</b> <b>can</b> significantly improve performance, thereby further enhancing cost-effectiveness.</p></p class="citation"></blockquote><h3 id=1851--18310-call-me-when-necessary-llms-can-efficiently-and-faithfully-reason-over-structured-environments-sitao-cheng-et-al-2024>(18/51 | 18/310) Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments (Sitao Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang. (2024)<br><strong>Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments</strong><br><button class=copy-to-clipboard title="Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 48<br>Keywords: Graph, Fine-tuning, Knowledge Graph, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08593v1.pdf filename=2403.08593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown potential in <b>reasoning</b> over structured environments, e.g., <b>knowledge</b> <b>graph</b> and table. Such tasks typically require multi-hop <b>reasoning,</b> i.e., match natural language utterance with instances in the environment. Previous methods leverage <b>LLMs</b> to incrementally build a <b>reasoning</b> path, where the <b>LLMs</b> either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose <b>Reasoning-Path-Editing</b> (Readi), a novel framework where <b>LLMs</b> can efficiently and faithfully reason over structured environments. In Readi, <b>LLMs</b> initially generate a <b>reasoning</b> path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all <b>LLM-based</b> methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9% on WTQ), comparable with state-of-the-art <b>fine-tuned</b> methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla <b>LLMs</b> (by 14.9% on CWQ). Our code will be available upon publication.</p></p class="citation"></blockquote><h3 id=1951--19310-data-oriented-dynamic-fine-tuning-parameter-selection-strategy-for-fish-mask-based-efficient-fine-tuning-ming-dong-et-al-2024>(19/51 | 19/310) Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning (Ming Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Dong, Kang Xue, Bolong Zheng, Tingting He. (2024)<br><strong>Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning</strong><br><button class=copy-to-clipboard title="Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, GLUE, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08484v1.pdf filename=2403.08484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In view of the huge number of parameters of <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> , tuning all parameters is very costly, and accordingly <b>fine-tuning</b> specific parameters is more sensible. Most of parameter efficient <b>fine-tuning</b> (PEFT) concentrate on parameter selection strategies, such as additive method, selective method and reparametrization-based method. However, there are few methods that consider the impact of data samples on parameter selecting, such as Fish Mask based method. Fish Mask randomly choose a part of data samples and treat them equally during parameter selection, which is unable to dynamically select optimal parameters for inconstant data distributions. In this work, we adopt a data-oriented perspective, then proposing an IRD ($\mathrm{\underline I}$terative sample-parameter $\mathrm{\underline R}$ange $\mathrm{\underline D}$ecreasing) algorithm to search the best setting of sample-parameter pair for FISH Mask. In each iteration, by searching the set of samples and parameters with larger Fish information, IRD can find better sample-parameter pair in most scale. We demonstrate the effectiveness and rationality of proposed strategy by conducting experiments on <b>GLUE</b> <b>benchmark.</b> Experimental results show our strategy optimizes the parameter selection and achieves preferable performance.</p></p class="citation"></blockquote><h3 id=2051--20310-gemma-open-models-based-on-gemini-research-and-technology-gemma-team-et-al-2024>(20/51 | 20/310) Gemma: Open Models Based on Gemini Research and Technology (Gemma Team et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, Kathleen Kenealy. (2024)<br><strong>Gemma: Open Models Based on Gemini Research and Technology</strong><br><button class=copy-to-clipboard title="Gemma: Open Models Based on Gemini Research and Technology" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Gemini, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08295v1.pdf filename=2403.08295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create <b>Gemini</b> models. Gemma models demonstrate strong performance across academic <b>benchmarks</b> for language understanding, <b>reasoning,</b> and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and <b>fine-tuned</b> checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of <b>LLMs</b> is critical for improving the safety of frontier models, and for enabling the next wave of <b>LLM</b> innovations.</p></p class="citation"></blockquote><h3 id=2151--21310-speechcolab-leaderboard-an-open-source-platform-for-automatic-speech-recognition-evaluation-jiayu-du-et-al-2024>(21/51 | 21/310) SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech Recognition Evaluation (Jiayu Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayu Du, Jinpeng Li, Guoguo Chen, Wei-Qiang Zhang. (2024)<br><strong>SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech Recognition Evaluation</strong><br><button class=copy-to-clipboard title="SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech Recognition Evaluation" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, eess-AS<br>Keyword Score: 43<br>Keywords: Benchmarking, Quantization, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08196v1.pdf filename=2403.08196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the wake of the surging tide of deep learning over the past decade, <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> has garnered substantial attention, leading to the emergence of numerous publicly accessible <b>ASR</b> systems that are actively being integrated into our daily lives. Nonetheless, the impartial and replicable evaluation of these <b>ASR</b> systems encounters challenges due to various crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a general-purpose, open-source platform designed for <b>ASR</b> evaluation. With this platform: (i) We report a comprehensive <b>benchmark,</b> unveiling the current state-of-the-art panorama for <b>ASR</b> systems, covering both open-source models and industrial commercial services. (ii) We <b>quantize</b> how distinct nuances in the scoring pipeline influence the final <b>benchmark</b> outcomes. These include nuances related to capitalization, punctuation, interjection, contraction, synonym usage, compound words, etc. These issues have gained prominence in the context of the transition towards an End-to-End future. (iii) We propose a practical modification to the conventional Token-Error-Rate (TER) evaluation metric, with inspirations from Kolmogorov complexity and Normalized Information Distance (NID). This adaptation, called modified-TER (mTER), achieves proper normalization and symmetrical treatment of reference and hypothesis. By leveraging this platform as a large-scale testing ground, this study demonstrates the robustness and backward compatibility of mTER when compared to TER. The SpeechColab Leaderboard is accessible at <a href=https://github.com/SpeechColab/Leaderboard>https://github.com/SpeechColab/Leaderboard</a></p></p class="citation"></blockquote><h3 id=2251--22310-moleculeqa-a-dataset-to-evaluate-factual-accuracy-in-molecular-comprehension-xingyu-lu-et-al-2024>(22/51 | 22/310) MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension (Xingyu Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Lu, He Cao, Zijing Liu, Shengyuan Bai, Leqing Chen, Yuan Yao, Hai-Tao Zheng, Yu Li. (2024)<br><strong>MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension</strong><br><button class=copy-to-clipboard title="MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, q-bio-BM<br>Keyword Score: 43<br>Keywords: Benchmarking, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08192v1.pdf filename=2403.08192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> are playing an increasingly significant role in molecular research, yet existing models often generate erroneous information, posing challenges to accurate molecular comprehension. Traditional evaluation metrics for generated content fail to assess a model&rsquo;s accuracy in molecular understanding. To rectify the absence of factual evaluation, we present MoleculeQA, a novel <b>question</b> <b>answering</b> <b>(QA)</b> dataset which possesses 62K <b>QA</b> pairs over 23K molecules. Each <b>QA</b> pair, composed of a manual <b>question,</b> <b>a</b> positive option and three negative options, has consistent semantics with a molecular description from authoritative molecular corpus. MoleculeQA is not only the first <b>benchmark</b> for molecular factual bias evaluation but also the largest <b>QA</b> dataset for molecular research. A comprehensive evaluation on MoleculeQA for existing molecular <b>LLMs</b> exposes their deficiencies in specific areas and pinpoints several particularly crucial factors for molecular understanding.</p></p class="citation"></blockquote><h3 id=2351--23310-evaluating-large-language-models-as-generative-user-simulators-for-conversational-recommendation-se-eun-yoon-et-al-2024>(23/51 | 23/310) Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation (Se-eun Yoon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley. (2024)<br><strong>Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation</strong><br><button class=copy-to-clipboard title="Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 40<br>Keywords: Recommendation, Recommender System, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09738v1.pdf filename=2403.09738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synthetic users are cost-effective proxies for real users in the evaluation of conversational <b>recommender</b> <b>systems.</b> <b>Large</b> <b>language</b> <b>models</b> show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational <b>recommendation.</b> This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting <b>recommendations,</b> and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and <b>prompting</b> strategies.</p></p class="citation"></blockquote><h3 id=2451--24310-medinsight-a-multi-source-context-augmentation-framework-for-generating-patient-centric-medical-responses-using-large-language-models-subash-neupane-et-al-2024>(24/51 | 24/310) MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models (Subash Neupane et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subash Neupane, Shaswata Mitra, Sudip Mittal, Noorbakhsh Amiri Golilarz, Shahram Rahimi, Amin Amirlatifi. (2024)<br><strong>MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models</strong><br><button class=copy-to-clipboard title="MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Recommendation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08607v1.pdf filename=2403.08607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown impressive capabilities in generating human-like responses. However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital. To address this challenge and enable the generation of patient-centric responses that are contextually relevant and comprehensive, we propose MedInsight:a novel retrieval augmented framework that augments <b>LLM</b> inputs <b>(prompts)</b> with relevant background information from multiple sources. MedInsight extracts pertinent details from the patient&rsquo;s medical record or consultation transcript. It then integrates information from authoritative medical textbooks and curated web resources based on the patient&rsquo;s health history and condition. By constructing an augmented context combining the patient&rsquo;s record with relevant medical knowledge, MedInsight generates enriched, patient-specific responses tailored for healthcare applications such as diagnosis, treatment <b>recommendations,</b> or patient education. Experiments on the MTSamples dataset validate MedInsight&rsquo;s effectiveness in generating contextually appropriate medical responses. Quantitative evaluation using the Ragas metric and TruLens for answer similarity and answer correctness demonstrates the model&rsquo;s efficacy. Furthermore, human evaluation studies involving Subject Matter Expert (SMEs) confirm MedInsight&rsquo;s utility, with moderate inter-rater agreement on the relevance and correctness of the generated responses.</p></p class="citation"></blockquote><h3 id=2551--25310-automatic-interactive-evaluation-for-large-language-models-with-state-aware-patient-simulator-yusheng-liao-et-al-2024>(25/51 | 25/310) Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator (Yusheng Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusheng Liao, Yutong Meng, Yuhao Wang, Hongcheng Liu, Yanfeng Wang, Yu Wang. (2024)<br><strong>Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator</strong><br><button class=copy-to-clipboard title="Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08495v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08495v2.pdf filename=2403.08495v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable proficiency in human interactions, yet their application within the medical field remains insufficiently explored. Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of <b>LLMs</b> on clinical tasks. In the quest to enhance the application of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional <b>LLM</b> evaluations and the nuanced demands of clinical practice. Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing <b>LLMs</b> through multi-turn doctor-patient <b>simulations.</b> This approach offers a closer approximation to real clinical scenarios and allows for a detailed analysis of <b>LLM</b> behaviors in response to complex patient interactions. Our extensive experimental validation demonstrates the effectiveness of the AIE framework, with outcomes that align well with human evaluations, underscoring its potential to revolutionize medical <b>LLM</b> testing for improved healthcare delivery.</p></p class="citation"></blockquote><h3 id=2651--26310-autoregressive-score-generation-for-multi-trait-essay-scoring-heejin-do-et-al-2024>(26/51 | 26/310) Autoregressive Score Generation for Multi-trait Essay Scoring (Heejin Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heejin Do, Yunsu Kim, Gary Geunbae Lee. (2024)<br><strong>Autoregressive Score Generation for Multi-trait Essay Scoring</strong><br><button class=copy-to-clipboard title="Autoregressive Score Generation for Multi-trait Essay Scoring" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: BERT, T5, Essay Scoring, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08332v1.pdf filename=2403.08332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, encoder-only pre-trained models such as <b>BERT</b> have been successfully applied in automated <b>essay</b> <b>scoring</b> (AES) to predict a single overall score. However, studies have yet to explore these models in multi-trait AES, possibly due to the inefficiency of replicating <b>BERT-based</b> models for each trait. Breaking away from the existing sole use of encoder, we propose an autoregressive prediction of multi-trait scores (ArTS), incorporating a decoding process by leveraging the pre-trained <b>T5.</b> Unlike prior regression or classification methods, we redefine AES as a score-generation task, allowing a single model to predict multiple scores. During decoding, the subsequent trait prediction can benefit by conditioning on the preceding trait scores. Experimental results proved the efficacy of ArTS, showing over 5% average improvements in both <b>prompts</b> and traits.</p></p class="citation"></blockquote><h3 id=2751--27310-embedded-translations-for-low-resource-automated-glossing-changbing-yang-et-al-2024>(27/51 | 27/310) Embedded Translations for Low-resource Automated Glossing (Changbing Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changbing Yang, Garrett Nicolai, Miikka Silfverberg. (2024)<br><strong>Embedded Translations for Low-resource Automated Glossing</strong><br><button class=copy-to-clipboard title="Embedded Translations for Low-resource Automated Glossing" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Low-Resource, BERT, T5, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08189v1.pdf filename=2403.08189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate automatic interlinear glossing in <b>low-resource</b> settings. We augment a hard-attentional neural model with embedded translation information extracted from interlinear glossed text. After encoding these translations using <b>large</b> <b>language</b> <b>models,</b> specifically <b>BERT</b> and <b>T5,</b> we introduce a character-level decoder for generating glossed output. Aided by these enhancements, our model demonstrates an average improvement of 3.97%-points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing. In a simulated ultra <b>low-resource</b> setting, trained on as few as 100 sentences, our system achieves an average 9.78%-point improvement over the plain hard-attentional baseline. These results highlight the critical role of translation information in boosting the system&rsquo;s performance, especially in processing and interpreting modest data sources. Our findings suggest a promising avenue for the documentation and preservation of languages, with our experiments on shared task datasets indicating significant advancements over the existing state of the art.</p></p class="citation"></blockquote><h3 id=2851--28310-automatic-speech-recognition-asr-for-the-diagnosis-of-pronunciation-of-speech-sound-disorders-in-korean-children-taekyung-ahn-et-al-2024>(28/51 | 28/310) Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of Speech Sound Disorders in Korean children (Taekyung Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taekyung Ahn, Yeonjung Hong, Younggon Im, Do Hyung Kim, Dayoung Kang, Joo Won Jeong, Jae Won Kim, Min Jung Kim, Ah-ra Cho, Dae-Hyun Jang, Hosung Nam. (2024)<br><strong>Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of Speech Sound Disorders in Korean children</strong><br><button class=copy-to-clipboard title="Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of Speech Sound Disorders in Korean children" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 40<br>Keywords: Fine-tuning, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08187v1.pdf filename=2403.08187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a model of <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> designed to diagnose pronunciation issues in children with <b>speech</b> <b>sound</b> disorders (SSDs) to replace manual transcriptions in clinical procedures. Since <b>ASR</b> models trained for general purposes primarily predict input <b>speech</b> <b>into</b> real words, employing a well-known high-performance <b>ASR</b> model for evaluating pronunciation in children with SSDs is impractical. We <b>fine-tuned</b> the wav2vec 2.0 XLS-R model to recognize <b>speech</b> <b>as</b> pronounced rather than as existing words. The model was <b>fine-tuned</b> with a <b>speech</b> <b>dataset</b> from 137 children with inadequate <b>speech</b> <b>production</b> pronouncing 73 Korean words selected for actual clinical diagnosis. The model&rsquo;s predictions of the pronunciations of the words matched the human annotations with about 90% accuracy. While the model still requires improvement in recognizing unclear pronunciation, this study demonstrates that <b>ASR</b> models can streamline complex pronunciation error diagnostic procedures in clinical fields.</p></p class="citation"></blockquote><h3 id=2951--29310-strengthening-multimodal-large-language-model-with-bootstrapped-preference-optimization-renjie-pi-et-al-2024>(29/51 | 29/310) Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization (Renjie Pi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, Tong Zhang. (2024)<br><strong>Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization</strong><br><button class=copy-to-clipboard title="Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Grounding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08730v1.pdf filename=2403.08730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) excel in generating responses based on visual inputs. However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information. We treat this bias as a &ldquo;preference&rdquo; for pretraining statistics, which hinders the model&rsquo;s <b>grounding</b> in visual input. To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based <b>LLM</b> to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the preference dataset, which is subsequently utilized to perform preference learning. Our approach effectively suppresses pretrained <b>LLM</b> bias, enabling enhanced <b>grounding</b> in visual inputs. Extensive experimentation demonstrates significant performance improvements across multiple <b>benchmarks,</b> advancing the state-of-the-art in <b>multimodal</b> conversational systems.</p></p class="citation"></blockquote><h3 id=3051--30310-evaluating-the-application-of-large-language-models-to-generate-feedback-in-programming-education-sven-jacobs-et-al-2024>(30/51 | 30/310) Evaluating the Application of Large Language Models to Generate Feedback in Programming Education (Sven Jacobs et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sven Jacobs, Steffen Jaschke. (2024)<br><strong>Evaluating the Application of Large Language Models to Generate Feedback in Programming Education</strong><br><button class=copy-to-clipboard title="Evaluating the Application of Large Language Models to Generate Feedback in Programming Education" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-HC, cs.CL<br>Keyword Score: 30<br>Keywords: GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09744v1.pdf filename=2403.09744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the application of <b>large</b> <b>language</b> <b>models,</b> specifically <b>GPT-4,</b> to enhance programming education. The research outlines the design of a web application that uses <b>GPT-4</b> to provide feedback on programming tasks, without giving away the solution. A web application for working on programming tasks was developed for the study and evaluated with 51 students over the course of one semester. The results show that most of the feedback generated by <b>GPT-4</b> effectively addressed code errors. However, challenges with incorrect suggestions and hallucinated issues indicate the need for further improvements.</p></p class="citation"></blockquote><h3 id=3151--31310-improving-acoustic-word-embeddings-through-correspondence-training-of-self-supervised-speech-representations-amit-meghanani-et-al-2024>(31/51 | 31/310) Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations (Amit Meghanani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Meghanani, Thomas Hain. (2024)<br><strong>Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations</strong><br><button class=copy-to-clipboard title="Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Self-supervised Learning, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08738v1.pdf filename=2403.08738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acoustic <b>word</b> <b>embeddings</b> (AWEs) are vector representations of spoken <b>words.</b> <b>An</b> effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE). In the past, the CAE method has been associated with traditional MFCC features. Representations obtained from <b>self-supervised</b> <b>learning</b> (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks. However, they have not been well studied in the context of learning AWEs. This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs. Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs. Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the best results for <b>word</b> <b>discrimination</b> in all languages, despite Hu-BERT being pre-trained on English only. Also, the HuBERT-based CAE model works well in cross-lingual settings. It outperforms MFCC-based CAE models trained on the target languages when trained on one source language and tested on target languages.</p></p class="citation"></blockquote><h3 id=3251--32310-token-alignment-via-character-matching-for-subword-completion-ben-athiwaratkun-et-al-2024>(32/51 | 32/310) Token Alignment via Character Matching for Subword Completion (Ben Athiwaratkun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Athiwaratkun, Shiqi Wang, Mingyue Shang, Yuchen Tian, Zijian Wang, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Rob Kwiatowski, Ramesh Nallapati, Bing Xiang. (2024)<br><strong>Token Alignment via Character Matching for Subword Completion</strong><br><button class=copy-to-clipboard title="Token Alignment via Character Matching for Subword Completion" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Tokenization, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08688v1.pdf filename=2403.08688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models, widely utilized in various applications, can often struggle with <b>prompts</b> corresponding to partial tokens. This struggle stems from <b>tokenization,</b> where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the <b>tokenization</b> artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model&rsquo;s generation aligns with the <b>prompt.</b> This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like code completion and text autocompletion.</p></p class="citation"></blockquote><h3 id=3351--33310-non-discrimination-criteria-for-generative-language-models-sara-sterlie-et-al-2024>(33/51 | 33/310) Non-discrimination Criteria for Generative Language Models (Sara Sterlie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Sterlie, Nina Weng, Aasa Feragen. (2024)<br><strong>Non-discrimination Criteria for Generative Language Models</strong><br><button class=copy-to-clipboard title="Non-discrimination Criteria for Generative Language Models" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08564v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08564v1.pdf filename=2403.08564v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Within recent years, <b>generative</b> <b>AI,</b> such as <b>large</b> <b>language</b> <b>models,</b> has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in <b>generative</b> <b>language</b> models. In particular, we derive <b>generative</b> <b>AI</b> analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design <b>prompts</b> for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the <b>generative</b> <b>AI</b> context. Our results address the presence of occupational gender bias within such conversational language models.</p></p class="citation"></blockquote><h3 id=3451--34310-do-large-language-models-solve-arc-visual-analogies-like-people-do-gustaw-opiełka-et-al-2024>(34/51 | 34/310) Do Large Language Models Solve ARC Visual Analogies Like People Do? (Gustaw Opiełka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gustaw Opiełka, Hannes Rosenbusch, Veerle Vijverberg, Claire E. Stevenson. (2024)<br><strong>Do Large Language Models Solve ARC Visual Analogies Like People Do?</strong><br><button class=copy-to-clipboard title="Do Large Language Models Solve ARC Visual Analogies Like People Do?" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09734v1.pdf filename=2403.09734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Abstraction <b>Reasoning</b> Corpus (ARC) is a visual analogical <b>reasoning</b> test designed for humans and machines (Chollet, 2019). We compared human and <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most <b>LLMs</b> on these tasks. Error analysis revealed a similar &ldquo;fallback&rdquo; solution strategy in <b>LLMs</b> and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, &ldquo;concept&rdquo; errors were more common in humans, and &ldquo;matrix&rdquo; errors were more common in <b>LLMs.</b> This study sheds new light on <b>LLM</b> <b>reasoning</b> ability and the extent to which we can use error analyses and comparisons with human development to understand how <b>LLMs</b> solve visual analogies.</p></p class="citation"></blockquote><h3 id=3551--35310-smart-submodular-data-mixture-strategy-for-instruction-tuning-h-s-v-n-s-kowndinya-renduchintala-et-al-2024>(35/51 | 35/310) SMART: Submodular Data Mixture Strategy for Instruction Tuning (H S V N S Kowndinya Renduchintala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H S V N S Kowndinya Renduchintala, Sumit Bhatia, Ganesh Ramakrishnan. (2024)<br><strong>SMART: Submodular Data Mixture Strategy for Instruction Tuning</strong><br><button class=copy-to-clipboard title="SMART: Submodular Data Mixture Strategy for Instruction Tuning" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Pruning, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08370v1.pdf filename=2403.08370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>Tuning</b> involves <b>finetuning</b> a language model on a collection of <b>instruction-formatted</b> <b>datasets</b> in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during <b>finetuning,</b> but finding the right balance remains challenging. Unfortunately, there&rsquo;s currently no systematic method beyond manual tuning or relying on practitioners&rsquo; intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for <b>instRuction</b> <b>Tuning)</b> - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a <b>fine-tuning</b> budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task <b>pruning</b> analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at <a href=https://github.com/kowndinya-renduchintala/SMART>https://github.com/kowndinya-renduchintala/SMART</a>.</p></p class="citation"></blockquote><h3 id=3651--36310-overleafcopilot-empowering-academic-writing-in-overleaf-with-large-language-models-haomin-wen-et-al-2024>(36/51 | 36/310) OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models (Haomin Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haomin Wen, Zhenjie Wei, Yan Lin, Jiyuan Wang, Yuxuan Liang, Huaiyu Wan. (2024)<br><strong>OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models</strong><br><button class=copy-to-clipboard title="OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09733v1.pdf filename=2403.09733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has facilitated a variety of applications from different domains. In this technical report, we explore the integration of <b>LLMs</b> and the popular academic writing tool, Overleaf, to enhance the efficiency and quality of academic writing. To achieve the above goal, there are three challenges: i) including seamless interaction between Overleaf and <b>LLMs,</b> ii) establishing reliable communication with the <b>LLM</b> provider, and iii) ensuring user privacy. To address these challenges, we present OverleafCopilot, the first-ever tool (i.e., a browser extension) that seamlessly integrates <b>LLMs</b> and Overleaf, enabling researchers to leverage the power of <b>LLMs</b> while writing papers. Specifically, we first propose an effective framework to bridge <b>LLMs</b> and Overleaf. Then, we developed PromptGenius, a website for researchers to easily find and share high-quality up-to-date <b>prompts.</b> Thirdly, we propose an agent command system to help researchers quickly build their customizable agents. OverleafCopilot (<a href=https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb>https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb</a> ) has been on the Chrome Extension Store, which now serves thousands of researchers. Additionally, the code of PromptGenius is released at <a href=https://github.com/wenhaomin/ChatGPT-PromptGenius>https://github.com/wenhaomin/ChatGPT-PromptGenius</a>. We believe our work has the potential to revolutionize academic writing practices, empowering researchers to produce higher-quality papers in less time.</p></p class="citation"></blockquote><h3 id=3751--37310-skipformer-a-skip-and-recover-strategy-for-efficient-speech-recognition-wenjing-zhu-et-al-2024>(37/51 | 37/310) Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition (Wenjing Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjing Zhu, Sining Sun, Changhao Shan, Peng Fan, Qing Yang. (2024)<br><strong>Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition</strong><br><button class=copy-to-clipboard title="Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Recurrent Neural Network, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08258v1.pdf filename=2403.08258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conformer-based attention models have become the de facto backbone model for <b>Automatic</b> <b>Speech</b> <b>Recognition</b> tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or <b>RNN-T</b> models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a &ldquo;Skip-and-Recover&rdquo; Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better recognition accuracy and faster inference speed than recent baseline models. Our code is open-sourced and available online.</p></p class="citation"></blockquote><h3 id=3851--38310-research-on-the-application-of-deep-learning-based-bert-model-in-sentiment-analysis-yichao-wu-et-al-2024>(38/51 | 38/310) Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis (Yichao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichao Wu, Zhengyu Jin, Chenxi Shi, Penghao Liang, Tong Zhan. (2024)<br><strong>Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis</strong><br><button class=copy-to-clipboard title="Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, BERT, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08217v1.pdf filename=2403.08217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the application of deep learning techniques, particularly focusing on <b>BERT</b> models, in <b>sentiment</b> <b>analysis.</b> It begins by introducing the fundamental concept of <b>sentiment</b> <b>analysis</b> and how deep learning methods are utilized in this domain. Subsequently, it delves into the architecture and characteristics of <b>BERT</b> models. Through detailed explanation, it elucidates the application effects and optimization strategies of <b>BERT</b> models in <b>sentiment</b> <b>analysis,</b> supported by experimental validation. The experimental findings indicate that <b>BERT</b> models exhibit robust performance in <b>sentiment</b> <b>analysis</b> tasks, with notable enhancements post <b>fine-tuning.</b> Lastly, the paper concludes by summarizing the potential applications of <b>BERT</b> models in <b>sentiment</b> <b>analysis</b> and suggests directions for future research and practical implementations.</p></p class="citation"></blockquote><h3 id=3951--39310-autoguide-automated-generation-and-selection-of-state-aware-guidelines-for-large-language-model-agents-yao-fu-et-al-2024>(39/51 | 39/310) AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents (Yao Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, Honglak Lee. (2024)<br><strong>AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents</strong><br><button class=copy-to-clipboard title="AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08978v1.pdf filename=2403.08978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The primary limitation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is their restricted understanding of the world. This poses significant difficulties for <b>LLM-based</b> agents, particularly in domains where pre-trained <b>LLMs</b> lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained <b>LLMs</b> by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent&rsquo;s current decision-making process. We show that our approach outperforms competitive <b>LLM-based</b> baselines by a <b>large</b> <b>margin</b> <b>in</b> sequential decision-making <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=4051--40310-the-garden-of-forking-paths-observing-dynamic-parameters-distribution-in-large-language-models-carlo-nicolini-et-al-2024>(40/51 | 40/310) The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models (Carlo Nicolini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlo Nicolini, Jacopo Staiano, Bruno Lepri, Raffaele Marino. (2024)<br><strong>The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models</strong><br><button class=copy-to-clipboard title="The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cond-mat-dis-nn, cond-mat-stat-mech, cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08739v1.pdf filename=2403.08739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A substantial gap persists in understanding the reasons behind the exceptional performance of the <b>Transformer</b> architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification.</p></p class="citation"></blockquote><h3 id=4151--41310-do-language-models-care-about-text-quality-evaluating-web-crawled-corpora-across-11-languages-rik-van-noord-et-al-2024>(41/51 | 41/310) Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora Across 11 Languages (Rik van Noord et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rik van Noord, Taja Kuzman, Peter Rupnik, Nikola Ljubešić, Miquel Esplà-Gomis, Gema Ramírez-Sánchez, Antonio Toral. (2024)<br><strong>Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora Across 11 Languages</strong><br><button class=copy-to-clipboard title="Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora Across 11 Languages" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: GPT, LLaMA<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08693v1.pdf filename=2403.08693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large, curated, web-crawled corpora play a vital role in training language models (LMs). They form the lion&rsquo;s share of the training data in virtually all recent LMs, such as the well-known <b>GPT,</b> <b>LLaMA</b> and XLM-RoBERTa models. However, despite this importance, relatively little attention has been given to the quality of these corpora. In this paper, we compare four of the currently most relevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across eleven lower-resourced European languages. Our approach is two-fold: first, we perform an intrinsic evaluation by performing a human evaluation of the quality of samples taken from different corpora; then, we assess the practical impact of the qualitative differences by training specific LMs on each of the corpora and evaluating their performance on downstream tasks. We find that there are clear differences in quality of the corpora, with MaCoCu and OSCAR obtaining the best results. However, during the extrinsic evaluation, we actually find that the CC100 corpus achieves the highest scores. We conclude that, in our experiments, the quality of the web-crawled corpora does not seem to play a significant role when training LMs.</p></p class="citation"></blockquote><h3 id=4251--42310-language-models-scale-reliably-with-over-training-and-on-downstream-tasks-samir-yitzhak-gadre-et-al-2024>(42/51 | 42/310) Language models scale reliably with over-training and on downstream tasks (Samir Yitzhak Gadre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, Ludwig Schmidt. (2024)<br><strong>Language models scale reliably with over-training and on downstream tasks</strong><br><button class=copy-to-clipboard title="Language models scale reliably with over-training and on downstream tasks" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Perplexity, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08540v1.pdf filename=2403.08540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Scaling</b> <b>laws</b> are useful guides for developing language models, but there are still gaps between current <b>scaling</b> <b>studies</b> and how language models are ultimately trained and evaluated. For instance, <b>scaling</b> <b>is</b> usually studied in the compute-optimal training regime (i.e., &ldquo;Chinchilla optimal&rdquo; regime); however, in practice, models are often over-trained to reduce inference costs. Moreover, <b>scaling</b> <b>laws</b> mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance. In this paper, we address both shortcomings. To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we investigate <b>scaling</b> <b>in</b> the over-trained regime. We fit <b>scaling</b> <b>laws</b> that extrapolate in both the number of model parameters and the ratio of training tokens to parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\times$ over-trained) and a 6.9B parameter, 138B token run$\unicode{x2014}$each from experiments that take 300$\times$ less compute. Second, we relate the <b>perplexity</b> of a language model to its downstream task performance via a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models using experiments that take 20$\times$ less compute. Our experiments are available at <a href=https://github.com/mlfoundations/scaling>https://github.com/mlfoundations/scaling</a>.</p></p class="citation"></blockquote><h3 id=4351--43310-authorship-verification-based-on-the-likelihood-ratio-of-grammar-models-andrea-nini-et-al-2024>(43/51 | 43/310) Authorship Verification based on the Likelihood Ratio of Grammar Models (Andrea Nini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Nini, Oren Halvani, Lukas Graner, Valerio Gherardi, Shunichi Ishihara. (2024)<br><strong>Authorship Verification based on the Likelihood Ratio of Grammar Models</strong><br><button class=copy-to-clipboard title="Authorship Verification based on the Likelihood Ratio of Grammar Models" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08462v1.pdf filename=2403.08462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Authorship Verification (AV) is the process of analyzing a set of documents to determine whether they were written by a specific author. This problem often arises in forensic scenarios, e.g., in cases where the documents in question constitute evidence for a crime. Existing state-of-the-art AV methods use computational solutions that are not supported by a plausible scientific explanation for their functioning and that are often difficult for analysts to interpret. To address this, we propose a method relying on calculating a quantity we call $\lambda_G$ (LambdaG): the ratio between the likelihood of a document given a model of the Grammar for the candidate author and the likelihood of the same document given a model of the Grammar for a reference population. These Grammar Models are estimated using $n$-gram language models that are trained solely on grammatical features. Despite not needing large amounts of data for training, LambdaG still outperforms other established AV methods with higher computational complexity, including a <b>fine-tuned</b> Siamese <b>Transformer</b> network. Our empirical evaluation based on four baseline methods applied to twelve datasets shows that LambdaG leads to better results in terms of both accuracy and AUC in eleven cases and in all twelve cases if considering only topic-agnostic methods. The algorithm is also highly robust to important variations in the genre of the reference population in many cross-genre comparisons. In addition to these properties, we demonstrate how LambdaG is easier to interpret than the current state-of-the-art. We argue that the advantage of LambdaG over other methods is due to fact that it is compatible with Cognitive Linguistic theories of language processing.</p></p class="citation"></blockquote><h3 id=4451--44310-learning-to-describe-for-predicting-zero-shot-drug-drug-interactions-fangqi-zhu-et-al-2024>(44/51 | 44/310) Learning to Describe for Predicting Zero-shot Drug-Drug Interactions (Fangqi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangqi Zhu, Yongqi Zhang, Lei Chen, Bing Qin, Ruifeng Xu. (2024)<br><strong>Learning to Describe for Predicting Zero-shot Drug-Drug Interactions</strong><br><button class=copy-to-clipboard title="Learning to Describe for Predicting Zero-shot Drug-Drug Interactions" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Few-shot, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08377v1.pdf filename=2403.08377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare. As the development of new drugs continues, the potential for unknown adverse effects resulting from DDIs becomes a growing concern. Traditional computational methods for DDI prediction may fail to capture interactions for new drugs due to the lack of knowledge. In this paper, we introduce a new problem setup as <b>zero-shot</b> DDI prediction that deals with the case of new drugs. Leveraging textual information from online databases like DrugBank and PubChem, we propose an innovative approach TextDDI with a language model-based DDI predictor and a reinforcement learning~(RL)-based information selector, enabling the selection of concise and pertinent text for accurate DDI prediction on new drugs. Empirical results show the benefits of the proposed approach on several settings including <b>zero-shot</b> and <b>few-shot</b> DDI prediction, and the selected texts are semantically relevant. Our code and data are available at \url{https://github.com/zhufq00/DDIs-Prediction}.</p></p class="citation"></blockquote><h3 id=4551--45310-knowledge-conflicts-for-llms-a-survey-rongwu-xu-et-al-2024>(45/51 | 45/310) Knowledge Conflicts for LLMs: A Survey (Rongwu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu. (2024)<br><strong>Knowledge Conflicts for LLMs: A Survey</strong><br><button class=copy-to-clipboard title="Knowledge Conflicts for LLMs: A Survey" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08319v1.pdf filename=2403.08319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This survey provides an in-depth analysis of knowledge conflicts for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of <b>LLMs,</b> especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of <b>LLMs</b> under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of <b>LLMs,</b> thereby serving as a valuable resource for advancing research in this evolving area.</p></p class="citation"></blockquote><h3 id=4651--46310-is-context-helpful-for-chat-translation-evaluation-sweta-agrawal-et-al-2024>(46/51 | 46/310) Is Context Helpful for Chat Translation Evaluation? (Sweta Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sweta Agrawal, Amin Farajian, Patrick Fernandes, Ricardo Rei, André F. T. Martins. (2024)<br><strong>Is Context Helpful for Chat Translation Evaluation?</strong><br><button class=copy-to-clipboard title="Is Context Helpful for Chat Translation Evaluation?" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08314v1.pdf filename=2403.08314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing sentence-level automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics affects their performance. Our findings show that augmenting neural learned metrics with contextual information helps improve correlation with human judgments in the reference-free scenario and when evaluating translations in out-of-English settings. Finally, we propose a new evaluation metric, Context-MQM, that utilizes bilingual context with a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> and further validate that adding context helps even for <b>LLM-based</b> evaluation metrics.</p></p class="citation"></blockquote><h3 id=4751--47310-streamingdialogue-prolonged-dialogue-learning-via-long-context-compression-with-minimal-losses-jia-nan-li-et-al-2024>(47/51 | 47/310) StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses (Jia-Nan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jia-Nan Li, Quan Tu, Cunli Mao, Zhengtao Yu, Ji-Rong Wen, Rui Yan. (2024)<br><strong>StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses</strong><br><button class=copy-to-clipboard title="StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08312v1.pdf filename=2403.08312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Standard <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks&rsquo;&rsquo; (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current <b>LLMs</b> already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize information losses from reconstruction after compression, we design two learning strategies of short-memory reconstruction (SMR) and long-memory reactivation (LMR). Our method outperforms strong baselines in dialogue tasks and achieves a 4 $\times$ speedup while reducing memory usage by 18 $\times$ compared to dense attention recomputation.</p></p class="citation"></blockquote><h3 id=4851--48310-from-um-to-yeah-producing-predicting-and-regulating-information-flow-in-human-conversation-claire-augusta-bergey-et-al-2024>(48/51 | 48/310) From &lsquo;um&rsquo; to &lsquo;yeah&rsquo;: Producing, predicting, and regulating information flow in human conversation (Claire Augusta Bergey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Claire Augusta Bergey, Simon DeDeo. (2024)<br><strong>From &lsquo;um&rsquo; to &lsquo;yeah&rsquo;: Producing, predicting, and regulating information flow in human conversation</strong><br><button class=copy-to-clipboard title="From 'um' to 'yeah': Producing, predicting, and regulating information flow in human conversation" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IT, cs.CL, math-IT, q-bio-NC<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08890v1.pdf filename=2403.08890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversation demands attention. Speakers must call words to mind, listeners must make sense of them, and both together must negotiate this flow of information, all in fractions of a second. We used <b>large</b> <b>language</b> <b>models</b> to study how this works in a <b>large-scale</b> <b>dataset</b> <b>of</b> English-language conversation, the CANDOR corpus. We provide a new estimate of the information density of unstructured conversation, of approximately 13 bits/second, and find significant effects associated with the cognitive load of both retrieving, and presenting, that information. We also reveal a role for backchannels &ndash; the brief yeahs, uh-huhs, and mhmms that listeners provide &ndash; in regulating the production of novelty: the lead-up to a backchannel is associated with declining information rate, while speech downstream rebounds to previous rates. Our results provide new insights into long-standing theories of how we respond to fluctuating demands on cognitive resources, and how we negotiate those demands in partnership with others.</p></p class="citation"></blockquote><h3 id=4951--49310-towards-personalized-evaluation-of-large-language-models-with-an-anonymous-crowd-sourcing-platform-mingyue-cheng-et-al-2024>(49/51 | 49/310) Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform (Mingyue Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li, Zhenya Huang, Enhong Chen. (2024)<br><strong>Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform</strong><br><button class=copy-to-clipboard title="Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08305v1.pdf filename=2403.08305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>model</b> evaluation plays a pivotal role in the enhancement of its capacity. Previously, numerous methods for evaluating <b>large</b> <b>language</b> <b>models</b> have been proposed in this area. Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for <b>large</b> <b>language</b> <b>models.</b> Additionally, these methods predominantly utilize centralized datasets for evaluation, with question banks concentrated within the evaluation platforms themselves. Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated. To address these limitations, we propose a novel anonymous crowd-sourcing evaluation platform, BingJian, for <b>large</b> <b>language</b> <b>models</b> that employs a competitive scoring mechanism where users participate in ranking models based on their performance. This platform stands out not only for its support of centralized evaluations to assess the general capabilities of models but also for offering an open evaluation gateway. Through this gateway, users have the opportunity to submit their questions, testing the models on a personalized and potentially broader range of capabilities. Furthermore, our platform introduces personalized evaluation scenarios, leveraging various forms of human-computer interaction to assess <b>large</b> <b>language</b> <b>models</b> in a manner that accounts for individual user preferences and contexts. The demonstration of BingJian can be accessed at <a href=https://github.com/Mingyue-Cheng/Bingjian>https://github.com/Mingyue-Cheng/Bingjian</a>.</p></p class="citation"></blockquote><h3 id=5051--50310-validating-and-exploring-large-geographic-corpora-jonathan-dunn-2024>(50/51 | 50/310) Validating and Exploring Large Geographic Corpora (Jonathan Dunn, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Dunn. (2024)<br><strong>Validating and Exploring Large Geographic Corpora</strong><br><button class=copy-to-clipboard title="Validating and Exploring Large Geographic Corpora" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Outlier Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08198v1.pdf filename=2403.08198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the impact of corpus creation decisions on large multi-lingual geographic web corpora. Beginning with a 427 billion word corpus derived from the Common Crawl, three methods are used to improve the quality of sub-corpora representing specific language-country pairs like New Zealand English: (i) the agreement of independent language identification systems, (ii) hash-based deduplication, and (iii) location-specific <b>outlier</b> <b>detection.</b> The impact of each of these steps is then evaluated at the language level and the country level by using corpus similarity measures to compare each resulting corpus with baseline data sets. The goal is to understand the impact of upstream data cleaning decisions on downstream corpora with a specific focus on under-represented languages and populations. The evaluation shows that the validity of sub-corpora is improved with each stage of cleaning but that this improvement is unevenly distributed across languages and populations. This result shows how standard corpus creation techniques can accidentally exclude under-represented populations.</p></p class="citation"></blockquote><h3 id=5151--51310-rethinking-loss-functions-for-fact-verification-yuta-mukobara-et-al-2024>(51/51 | 51/310) Rethinking Loss Functions for Fact Verification (Yuta Mukobara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuta Mukobara, Yutaro Shigeto, Masashi Shimbo. (2024)<br><strong>Rethinking Loss Functions for Fact Verification</strong><br><button class=copy-to-clipboard title="Rethinking Loss Functions for Fact Verification" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08174v1.pdf filename=2403.08174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore loss functions for <b>fact</b> <b>verification</b> in the FEVER shared task. While the cross-entropy loss is a standard objective for training verdict predictors, it fails to capture the heterogeneity among the FEVER verdict classes. In this paper, we develop two task-specific objectives tailored to FEVER. Experimental results confirm that the proposed objective functions outperform the standard cross-entropy. Performance is further improved when these objectives are combined with simple class weighting, which effectively overcomes the imbalance in the training data. The souce code is available at <a href=https://github.com/yuta-mukobara/RLF-KGAT>https://github.com/yuta-mukobara/RLF-KGAT</a></p></p class="citation"></blockquote><h2 id=cscv-85>cs.CV (85)</h2><h3 id=185--52310-tina-think-interaction-and-action-framework-for-zero-shot-vision-language-navigation-dingbang-li-et-al-2024>(1/85 | 52/310) TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation (Dingbang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingbang Li, Wenzhou Chen, Xin Lin. (2024)<br><strong>TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation</strong><br><button class=copy-to-clipboard title="TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 90<br>Keywords: Reinforcement Learning, Supervised Learning, Supervised Learning, Zero-shot, Question Answering, Reasoning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08833v1.pdf filename=2403.08833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> navigation is a critical challenge in <b>Vision-Language</b> Navigation (VLN) tasks, where the ability to adapt to unfamiliar instructions and to act in unknown environments is essential. Existing <b>supervised</b> <b>learning-based</b> models, trained using annotated data through <b>reinforcement</b> <b>learning,</b> exhibit limitations in generalization capabilities. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> with their extensive knowledge and emergent <b>reasoning</b> abilities, present a potential pathway for achieving <b>zero-shot</b> navigation. This paper presents a VLN agent based on <b>LLMs,</b> exploring approaches to the <b>zero-shot</b> navigation problem. To compensate for the shortcomings of <b>LLMs</b> in environmental perception, we propose the Thinking, Interacting, and Action (TINA) framework. TINA enables the agent to scrutinize perceptual information and autonomously query key clues within the environment through an introduced <b>question-answering</b> <b>module,</b> thereby aligning instructions with specific perceptual data. The navigation agent&rsquo;s perceptual abilities are enhanced through the TINA framework, while the explicit thought and query processes also improve the navigational procedure&rsquo;s explainability and transparency. We evaluate the performance of our method on the Room-to-Room dataset. The experiment results indicate that our approach improves the navigation performance of <b>LLM-based</b> agents. Our approach also outperformed some <b>supervised</b> <b>learning-based</b> methods, highlighting its efficacy in <b>zero-shot</b> navigation.</p></p class="citation"></blockquote><h3 id=285--53310-unleashing-the-power-of-meta-tuning-for-few-shot-generalization-through-sparse-interpolated-experts-shengzhuang-chen-et-al-2024>(2/85 | 53/310) Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts (Shengzhuang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan Richard Schwarz, Ying Wei. (2024)<br><strong>Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts</strong><br><button class=copy-to-clipboard title="Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, Foundation Model, Meta Learning, Out-of-domain, Transfer Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08477v1.pdf filename=2403.08477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional wisdom suggests parameter-efficient <b>fine-tuning</b> of <b>foundation</b> <b>models</b> as the state-of-the-art method for <b>transfer</b> <b>learning</b> in vision, replacing the rich literature of alternatives such as <b>meta-learning.</b> <b>In</b> trying to harness the best of both worlds, <b>meta-tuning</b> <b>introduces</b> a subsequent optimization stage of <b>foundation</b> <b>models</b> but has so far only shown limited success and crucially tends to underperform on <b>out-of-domain</b> (OOD) tasks. In this paper, we introduce Sparse <b>MetA-Tuning</b> <b>(SMAT),</b> a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for <b>meta-tuning</b> <b>on</b> each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the <b>transfer</b> <b>abilities</b> of vision <b>foundation</b> <b>models</b> beyond parameter-efficient <b>finetuning.</b> We establish new state-of-the-art results on a challenging combination of <b>Meta-Dataset</b> <b>augmented</b> with additional OOD tasks in both <b>zero-shot</b> and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-domain and <b>out-of-domain</b> generalization. Our code is publicly available.</p></p class="citation"></blockquote><h3 id=385--54310-efficient-prompt-tuning-of-large-vision-language-model-for-fine-grained-ship-classification-long-lan-et-al-2024>(3/85 | 54/310) Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification (Long Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Lan, Fengxiang Wang, Shuyan Li, Xiangtao Zheng, Zengmao Wang, Xinwang Liu. (2024)<br><strong>Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification</strong><br><button class=copy-to-clipboard title="Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, Supervised Learning, Zero-shot, Prompt, Vision-and-Language, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08271v1.pdf filename=2403.08271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional <b>supervised</b> classification methods. Recent advancements in large pre-trained <b>Vision-Language</b> Models (VLMs) have demonstrated impressive capabilities in <b>few-shot</b> or <b>zero-shot</b> <b>learning,</b> particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly <b>fine-tuning</b> VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To address these issues, we introduce a novel <b>prompt</b> tuning technique that employs a hierarchical, multi-granularity <b>prompt</b> design. Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network. This strategy enhances the model&rsquo;s generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features. Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes. Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques. The source code will be made publicly available.</p></p class="citation"></blockquote><h3 id=485--55310-coronetgan-controlled-pruning-of-gans-via-hypernetworks-aman-kumar-et-al-2024>(4/85 | 55/310) CoroNetGAN: Controlled Pruning of GANs via Hypernetworks (Aman Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aman Kumar, Khushboo Anand, Shubham Mandloi, Ashutosh Mishra, Avinash Thakur, Neeraj Kasera, Prathosh A P. (2024)<br><strong>CoroNetGAN: Controlled Pruning of GANs via Hypernetworks</strong><br><button class=copy-to-clipboard title="CoroNetGAN: Controlled Pruning of GANs via Hypernetworks" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, eess-IV<br>Keyword Score: 53<br>Keywords: Benchmarking, Generative Adversarial Network, Generative Adversarial Network, Knowledge Distillation, Knowledge Distillation, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08261v1.pdf filename=2403.08261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have proven to exhibit remarkable performance and are widely used across many <b>generative</b> <b>computer</b> <b>vision</b> applications. However, the unprecedented demand for the deployment of <b>GANs</b> on resource-constrained edge devices still poses a challenge due to huge number of parameters involved in the generation process. This has led to focused attention on the area of compressing <b>GANs.</b> Most of the existing works use <b>knowledge</b> <b>distillation</b> with the overhead of teacher dependency. Moreover, there is no ability to control the degree of compression in these methods. Hence, we propose CoroNet-GAN for compressing <b>GAN</b> using the combined strength of differentiable <b>pruning</b> method via hypernetworks. The proposed method provides the advantage of performing controllable compression while training along with reducing training time by a substantial factor. Experiments have been done on various conditional <b>GAN</b> architectures (Pix2Pix and CycleGAN) to signify the effectiveness of our approach on multiple <b>benchmark</b> datasets such as Edges-to-Shoes, Horse-to-Zebra and Summer-to-Winter. The results obtained illustrate that our approach succeeds to outperform the baselines on Zebra-to-Horse and Summer-to-Winter achieving the best FID score of 32.3 and 72.3 respectively, yielding high-fidelity images across all the datasets. Additionally, our approach also outperforms the state-of-the-art methods in achieving better inference time on various smart-phone chipsets and data-types making it a feasible solution for deployment on edge devices.</p></p class="citation"></blockquote><h3 id=585--56310-lafs-landmark-based-facial-self-supervised-learning-for-face-recognition-zhonglin-sun-et-al-2024>(5/85 | 56/310) LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition (Zhonglin Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhonglin Sun, Chen Feng, Ioannis Patras, Georgios Tzimiropoulos. (2024)<br><strong>LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition</strong><br><button class=copy-to-clipboard title="LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Face Recognition, Benchmarking, Few-shot, Self-supervised Learning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08161v1.pdf filename=2403.08161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we focus on learning facial representations that can be adapted to train effective <b>face</b> <b>recognition</b> models, particularly in the absence of labels. Firstly, compared with existing labelled <b>face</b> <b>datasets,</b> a vastly larger magnitude of unlabeled <b>faces</b> <b>exists</b> in the real world. We explore the learning strategy of these unlabeled facial images through <b>self-supervised</b> <b>pretraining</b> to transfer generalized <b>face</b> <b>recognition</b> performance. Moreover, motivated by one recent finding, that is, the <b>face</b> <b>saliency</b> area is critical for <b>face</b> <b>recognition,</b> in contrast to utilizing random cropped blocks of images for constructing augmentations in pretraining, we utilize patches localized by extracted facial landmarks. This enables our method - namely LAndmark-based Facial <b>Self-supervised</b> <b>learning</b> LAFS), to learn key representation that is more critical for <b>face</b> <b>recognition.</b> We also incorporate two landmark-specific augmentations which introduce more diversity of landmark information to further regularize the learning. With learned landmark-based facial representations, we further adapt the representation for <b>face</b> <b>recognition</b> with regularization mitigating variations in landmark positions. Our method achieves significant improvement over the state-of-the-art on multiple <b>face</b> <b>recognition</b> <b>benchmarks,</b> especially on more challenging <b>few-shot</b> scenarios.</p></p class="citation"></blockquote><h3 id=685--57310-language-driven-visual-consensus-for-zero-shot-semantic-segmentation-zicheng-zhang-et-al-2024>(6/85 | 57/310) Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation (Zicheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zicheng Zhang, Tong Zhang, Yi Zhu, Jianzhuang Liu, Xiaodan Liang, QiXiang Ye, Wei Ke. (2024)<br><strong>Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Zero-shot, Transformer, Prompt, Self-Attention, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08426v1.pdf filename=2403.08426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pre-trained <b>vision-language</b> model, exemplified by CLIP, advances <b>zero-shot</b> semantic segmentation by aligning visual features with class embeddings through a <b>transformer</b> decoder to generate semantic masks. Despite its effectiveness, prevailing methods within this paradigm encounter challenges, including overfitting on seen classes and small fragmentation in masks. To mitigate these issues, we propose a Language-Driven Visual Consensus (LDVC) approach, fostering improved alignment of semantic and visual information.Specifically, we leverage class embeddings as anchors due to their discrete and abstract nature, steering vision features toward class embeddings. Moreover, to circumvent noisy alignments from the vision part due to its redundant nature, we introduce route attention into <b>self-attention</b> for finding visual consensus, thereby enhancing semantic consistency within the same object. Equipped with a <b>vision-language</b> <b>prompting</b> strategy, our approach significantly boosts the generalization capacity of segmentation models for unseen classes. Experimental results underscore the effectiveness of our approach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the COCO-Stuff 164k for unseen classes compared with the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=785--58310-activating-wider-areas-in-image-super-resolution-cheng-cheng-et-al-2024>(7/85 | 58/310) Activating Wider Areas in Image Super-Resolution (Cheng Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Cheng, Hang Wang, Hongbin Sun. (2024)<br><strong>Activating Wider Areas in Image Super-Resolution</strong><br><button class=copy-to-clipboard title="Activating Wider Areas in Image Super-Resolution" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08330v1.pdf filename=2403.08330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prevalence of <b>convolution</b> neural networks <b>(CNNs)</b> and <b>vision</b> <b>transformers</b> (ViTs) has markedly revolutionized the area of single-image super-resolution (SISR). To further boost the SR performances, several techniques, such as residual learning and attention mechanism, are introduced, which can be largely attributed to a wider range of activated area, that is, the input pixels that strongly influence the SR results. However, the possibility of further improving SR performance through another versatile <b>vision</b> <b>backbone</b> remains an unresolved challenge. To address this issue, in this paper, we unleash the representation potential of the modern state space model, i.e., <b>Vision</b> <b>Mamba</b> (Vim), in the context of SISR. Specifically, we present three recipes for better utilization of Vim-based models: 1) Integration into a MetaFormer-style block; 2) Pre-training on a larger and broader dataset; 3) Employing complementary attention mechanism, upon which we introduce the MMA. The resulting network MMA is capable of finding the most relevant and representative input pixels to reconstruct the corresponding high-resolution images. Comprehensive experimental analysis reveals that MMA not only achieves competitive or even superior performance compared to state-of-the-art SISR methods but also maintains relatively low memory and computational overheads (e.g., +0.5 dB PSNR elevation on Manga109 dataset with 19.8 M parameters at the scale of 2). Furthermore, MMA proves its versatility in lightweight SR applications. Through this work, we aim to illuminate the potential applications of state space models in the broader realm of image processing rather than SISR, encouraging further exploration in this innovative direction.</p></p class="citation"></blockquote><h3 id=885--59310-attack-deterministic-conditional-image-generative-models-for-diverse-and-controllable-generation-tianyi-chu-et-al-2024>(8/85 | 59/310) Attack Deterministic Conditional Image Generative Models for Diverse and Controllable Generation (Tianyi Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Chu, Wei Xing, Jiafu Chen, Zhizhong Wang, Jiakai Sun, Lei Zhao, Haibo Chen, Huaizhong Lin. (2024)<br><strong>Attack Deterministic Conditional Image Generative Models for Diverse and Controllable Generation</strong><br><button class=copy-to-clipboard title="Attack Deterministic Conditional Image Generative Models for Diverse and Controllable Generation" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Generative Adversarial Network, Generative Adversarial Network, Style Transfer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08294v1.pdf filename=2403.08294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> based conditional image <b>generative</b> <b>models</b> <b>typically</b> produce fixed output for the same conditional input, which is unreasonable for highly subjective tasks, such as large-mask image inpainting or <b>style</b> <b>transfer.</b> On the other hand, <b>GAN-based</b> diverse image <b>generative</b> <b>methods</b> <b>require</b> retraining/fine-tuning the network or designing complex noise injection functions, which is computationally expensive, task-specific, or struggle to generate high-quality results. Given that many deterministic conditional image <b>generative</b> <b>models</b> <b>have</b> been able to produce high-quality yet fixed results, we raise an intriguing question: is it possible for pre-trained deterministic conditional image <b>generative</b> <b>models</b> <b>to</b> generate diverse results without changing network structures or parameters? To answer this question, we re-examine the conditional image generation tasks from the perspective of <b>adversarial</b> <b>attack</b> and propose a simple and efficient plug-in projected gradient descent (PGD) like method for diverse and controllable image generation. The key idea is attacking the pre-trained deterministic <b>generative</b> <b>models</b> <b>by</b> adding a micro perturbation to the input condition. In this way, diverse results can be generated without any adjustment of network structures or <b>fine-tuning</b> of the pre-trained models. In addition, we can also control the diverse results to be generated by specifying the attack direction according to a reference text or image. Our work opens the door to applying <b>adversarial</b> <b>attack</b> to low-level vision tasks, and experiments on various conditional image generation tasks demonstrate the effectiveness and superiority of the proposed method.</p></p class="citation"></blockquote><h3 id=985--60310-coin-a-benchmark-of-continual-instruction-tuning-for-multimodel-large-language-model-cheng-chen-et-al-2024>(9/85 | 60/310) CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model (Cheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Chen, Junchen Zhu, Xu Luo, Hengtao Shen, Lianli Gao, Jingkuan Song. (2024)<br><strong>CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model</strong><br><button class=copy-to-clipboard title="CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Instruction Following, Reasoning, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08350v1.pdf filename=2403.08350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> represents a prevalent strategy employed by <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) to align with human <b>instructions</b> <b>and</b> adapt to new tasks. Nevertheless, MLLMs encounter the challenge of adapting to users&rsquo; evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated. In this paper, we present a comprehensive <b>benchmark,</b> namely Continual <b>Instruction</b> <b>tuNing</b> (CoIN), to assess existing MLLMs in the sequential <b>instruction</b> <b>tuning</b> paradigm. CoIN comprises 10 commonly used datasets spanning 8 task categories, ensuring a diverse range of <b>instructions</b> <b>and</b> tasks. Besides, the trained model is evaluated from two aspects: <b>Instruction</b> <b>Following</b> and General Knowledge, which assess the alignment with human intention and knowledge preserved for <b>reasoning,</b> respectively. Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and the failure in intention alignment assumes the main responsibility, instead of the knowledge forgetting. To this end, we introduce MoELoRA to MLLMs which is effective to retain the previous <b>instruction</b> <b>alignment.</b> Experimental results consistently illustrate the forgetting decreased from this method on CoIN.</p></p class="citation"></blockquote><h3 id=1085--61310-dialoggen-multi-modal-interactive-dialogue-system-for-multi-turn-text-to-image-generation-minbin-huang-et-al-2024>(10/85 | 61/310) DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation (Minbin Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, Wei Liu. (2024)<br><strong>DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Benchmarking, Multi-modal, Dialogue System, Text2image, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08857v1.pdf filename=2403.08857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized <b>prompt</b> engineering knowledge and the inability to perform multi-turn image generation, hindering a dynamic and iterative creation process. Recent attempts have tried to equip <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) with T2I models to bring the user&rsquo;s natural language instructions into reality. Hence, the output modality of MLLMs is extended, and the multi-turn generation quality of T2I models is enhanced thanks to the strong <b>multi-modal</b> comprehension ability of MLLMs. However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper. Therefore, we propose DialogGen, an effective pipeline to align off-the-shelf MLLMs and T2I models to build a <b>Multi-modal</b> Interactive <b>Dialogue</b> <b>System</b> (MIDS) for multi-turn <b>Text-to-Image</b> generation. It is composed of drawing <b>prompt</b> alignment, careful training data curation, and error correction. Moreover, as the field of MIDS flourishes, comprehensive <b>benchmarks</b> are urgently needed to evaluate MIDS fairly in terms of output modality correctness and <b>multi-modal</b> output coherence. To address this issue, we introduce the <b>Multi-modal</b> <b>Dialogue</b> <b>Benchmark</b> (DialogBen), a comprehensive bilingual <b>benchmark</b> designed to assess the ability of MLLMs to generate accurate and coherent <b>multi-modal</b> content that supports image editing. It contains two evaluation metrics to measure the model&rsquo;s ability to switch modalities and the coherence of the output images. Our extensive experiments on DialogBen and user study demonstrate the effectiveness of DialogGen compared with other State-of-the-Art models.</p></p class="citation"></blockquote><h3 id=1185--62310-a-multimodal-fusion-network-for-student-emotion-recognition-based-on-transformer-and-tensor-product-ao-xiang-et-al-2024>(11/85 | 62/310) A Multimodal Fusion Network For Student Emotion Recognition Based on Transformer and Tensor Product (Ao Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ao Xiang, Zongqing Qi, Han Wang, Qin Yang, Danqing Ma. (2024)<br><strong>A Multimodal Fusion Network For Student Emotion Recognition Based on Transformer and Tensor Product</strong><br><button class=copy-to-clipboard title="A Multimodal Fusion Network For Student Emotion Recognition Based on Transformer and Tensor Product" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Yolo, Object Detection, Multi-modal, Multi-modal, Transformer, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08511v1.pdf filename=2403.08511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there have been frequent incidents of foreign <b>objects</b> <b>intruding</b> into railway and Airport runways. These <b>objects</b> <b>can</b> include pedestrians, vehicles, animals, and debris. This paper introduces an improved YOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance the detection of foreign <b>objects</b> <b>on</b> railways and Airport runways. This study proposes a new dataset, AARFOD (Aero and Rail Foreign <b>Object</b> <b>Detection),</b> which combines two public datasets for detecting foreign <b>objects</b> <b>in</b> aviation and railway systems. The dataset aims to improve the recognition capabilities of foreign <b>object</b> <b>targets.</b> Experimental results on this large dataset have demonstrated significant performance improvements of the proposed model over the baseline YOLOv5 model, reducing computational requirements. improved <b>YOLO</b> model shows a significant improvement in precision by 1.2%, recall rate by 1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters were reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%. In the ablation experiment, it is found that the FasterNet module can significantly reduce the number of parameters of the model, and the reference of the attention mechanism can slow down the performance loss caused by lightweight.</p></p class="citation"></blockquote><h3 id=1285--63310-meter-a-mobile-vision-transformer-architecture-for-monocular-depth-estimation-l-papa-et-al-2024>(12/85 | 63/310) METER: a mobile vision transformer architecture for monocular depth estimation (L. Papa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>L. Papa, P. Russo, I. Amerini. (2024)<br><strong>METER: a mobile vision transformer architecture for monocular depth estimation</strong><br><button class=copy-to-clipboard title="METER: a mobile vision transformer architecture for monocular depth estimation" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Vision Transformer, Benchmarking, Data Augmentation, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08368v1.pdf filename=2403.08368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Depth estimation is a fundamental knowledge for autonomous systems that need to assess their own state and perceive the surrounding environment. Deep learning algorithms for depth estimation have gained significant interest in recent years, owing to the potential benefits of this methodology in overcoming the limitations of active depth sensing systems. Moreover, due to the low cost and size of monocular cameras, researchers have focused their attention on monocular depth estimation (MDE), which consists in estimating a dense depth map from a single RGB video frame. State of the art MDE models typically rely on <b>vision</b> <b>transformers</b> (ViT) architectures that are highly deep and complex, making them unsuitable for fast inference on devices with hardware constraints. Purposely, in this paper, we address the problem of exploiting ViT in MDE on embedded devices. Those systems are usually characterized by limited memory capabilities and low-power CPU/GPU. We propose METER, a novel lightweight <b>vision</b> <b>transformer</b> architecture capable of achieving state of the art estimations and low latency inference performances on the considered embedded hardwares: NVIDIA Jetson TX1 and NVIDIA Jetson Nano. We provide a solution consisting of three alternative configurations of METER, a novel loss function to balance pixel estimation and reconstruction of image details, and a new <b>data</b> <b>augmentation</b> strategy to improve the overall final predictions. The proposed method outperforms previous lightweight works over the two <b>benchmark</b> datasets: the indoor NYU Depth v2 and the outdoor KITTI.</p></p class="citation"></blockquote><h3 id=1385--64310-fogguard-guarding-yolo-against-fog-using-perceptual-loss-soheil-gharatappeh-et-al-2024>(13/85 | 64/310) FogGuard: guarding YOLO against fog using perceptual loss (Soheil Gharatappeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soheil Gharatappeh, Sepideh Neshatfar, Salimeh Yasaei Sekeh, Vikas Dhiman. (2024)<br><strong>FogGuard: guarding YOLO against fog using perceptual loss</strong><br><button class=copy-to-clipboard title="FogGuard: guarding YOLO against fog using perceptual loss" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Yolo, Object Detection, Fine-tuning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08939v1.pdf filename=2403.08939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel fog-aware <b>object</b> <b>detection</b> network called FogGuard, designed to address the challenges posed by foggy weather conditions. Autonomous driving systems heavily rely on accurate <b>object</b> <b>detection</b> algorithms, but adverse weather conditions can significantly impact the reliability of deep neural networks (DNNs). Existing approaches fall into two main categories, 1) image enhancement such as IA-YOLO 2) <b>domain</b> <b>adaptation</b> based approaches. Image enhancement based techniques attempt to generate fog-free image. However, retrieving a fogless image from a foggy image is a much harder problem than detecting <b>objects</b> <b>in</b> a foggy image. <b>Domain-adaptation</b> <b>based</b> approaches, on the other hand, do not make use of labelled datasets in the target <b>domain.</b> <b>Both</b> categories of approaches are attempting to solve a harder version of the problem. Our approach builds over <b>fine-tuning</b> on the Our framework is specifically designed to compensate for foggy conditions present in the scene, ensuring robust performance even. We adopt YOLOv3 as the baseline <b>object</b> <b>detection</b> algorithm and introduce a novel Teacher-Student Perceptual loss, to high accuracy <b>object</b> <b>detection</b> in foggy images. Through extensive evaluations on common datasets such as PASCAL VOC and RTTS, we demonstrate the improvement in performance achieved by our network. We demonstrate that FogGuard achieves 69.43% mAP, as compared to 57.78% for YOLOv3 on the RTTS dataset. Furthermore, we show that while our training method increases time complexity, it does not introduce any additional overhead during inference compared to the regular <b>YOLO</b> network.</p></p class="citation"></blockquote><h3 id=1485--65310-holmes-holonym-meronym-based-semantic-inspection-for-convolutional-image-classifiers-francesco-dibitonto-et-al-2024>(14/85 | 65/310) HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers (Francesco Dibitonto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Dibitonto, Fabio Garcea, André Panisson, Alan Perotti, Lia Morra. (2024)<br><strong>HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers</strong><br><button class=copy-to-clipboard title="HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08536v1.pdf filename=2403.08536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks. However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users. In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model. Specifically, HOLMES leverages ontologies, web scraping and <b>transfer</b> <b>learning</b> to automatically construct meronym (parts)-based detectors for a given holonym (class). Then, it produces heatmaps at the meronym level and finally, by probing the holonym <b>CNN</b> with occluded images, it highlights the importance of each part on the classification output. Compared to state-of-the-art saliency methods, HOLMES takes a step further and provides information about both where and what the holonym <b>CNN</b> is looking at, without relying on densely annotated datasets and without forcing concepts to be associated to single computational units. Extensive experimental evaluation on different categories of objects (animals, tools and vehicles) shows the feasibility of our approach. On average, HOLMES explanations include at least two meronyms, and the ablation of a single meronym roughly halves the holonym model confidence. The resulting heatmaps were quantitatively evaluated using the deletion/insertion/preservation curves. All metrics were comparable to those achieved by GradCAM, while offering the advantage of further decomposing the heatmap in human-understandable concepts, thus highlighting both the relevance of meronyms to object classification, as well as HOLMES ability to capture it. The code is available at <a href=https://github.com/FrancesC0de/HOLMES>https://github.com/FrancesC0de/HOLMES</a>.</p></p class="citation"></blockquote><h3 id=1585--66310-multiscale-low-frequency-memory-network-for-improved-feature-extraction-in-convolutional-neural-networks-fuzhi-wu-et-al-2024>(15/85 | 66/310) Multiscale Low-Frequency Memory Network for Improved Feature Extraction in Convolutional Neural Networks (Fuzhi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fuzhi Wu, Jiasong Wu, Youyong Kong, Chunfeng Yang, Guanyu Yang, Huazhong Shu, Guy Carrault, Lotfi Senhadji. (2024)<br><strong>Multiscale Low-Frequency Memory Network for Improved Feature Extraction in Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Multiscale Low-Frequency Memory Network for Improved Feature Extraction in Convolutional Neural Networks" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08157v1.pdf filename=2403.08157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning and <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> have driven major transformations in diverse research areas. However, their limitations in handling low-frequency information present obstacles in certain tasks like interpreting global structures or managing smooth transition images. Despite the promising performance of <b>transformer</b> structures in numerous tasks, their intricate optimization complexities highlight the persistent need for refined <b>CNN</b> enhancements using limited resources. Responding to these complexities, we introduce a novel framework, the Multiscale Low-Frequency Memory (MLFM) Network, with the goal to harness the full potential of <b>CNNs</b> while keeping their complexity unchanged. The MLFM efficiently preserves low-frequency information, enhancing performance in targeted computer vision tasks. Central to our MLFM is the Low-Frequency Memory Unit (LFMU), which stores various low-frequency data and forms a parallel channel to the core network. A key advantage of MLFM is its seamless compatibility with various prevalent networks, requiring no alterations to their original core structure. Testing on ImageNet demonstrated substantial accuracy improvements in multiple 2D <b>CNNs,</b> including ResNet, MobileNet, EfficientNet, and ConvNeXt. Furthermore, we showcase MLFM&rsquo;s versatility beyond traditional image classification by successfully integrating it into image-to-image translation tasks, specifically in semantic segmentation networks like FCN and U-Net. In conclusion, our work signifies a pivotal stride in the journey of optimizing the efficacy and efficiency of <b>CNNs</b> with limited resources. This research builds upon the existing <b>CNN</b> foundations and paves the way for future advancements in computer vision. Our codes are available at <a href=https://github.com/AlphaWuSeu/>https://github.com/AlphaWuSeu/</a> MLFM.</p></p class="citation"></blockquote><h3 id=1685--67310-nerf-supervised-feature-point-detection-and-description-ali-youssef-et-al-2024>(16/85 | 67/310) NeRF-Supervised Feature Point Detection and Description (Ali Youssef et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Youssef, Francisco Vasconcelos. (2024)<br><strong>NeRF-Supervised Feature Point Detection and Description</strong><br><button class=copy-to-clipboard title="NeRF-Supervised Feature Point Detection and Description" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 38<br>Keywords: Benchmarking, Geometry, Simulation, Simulator, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08156v1.pdf filename=2403.08156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based <b>simulations</b> of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views <b>supervised</b> by perspective projective <b>geometry.</b> Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard <b>benchmarks</b> for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches.</p></p class="citation"></blockquote><h3 id=1785--68310-mim4d-masked-modeling-with-multi-view-video-for-autonomous-driving-representation-learning-jialv-zou-et-al-2024>(17/85 | 68/310) MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving Representation Learning (Jialv Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialv Zou, Bencheng Liao, Qian Zhang, Wenyu Liu, Xinggang Wang. (2024)<br><strong>MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving Representation Learning</strong><br><button class=copy-to-clipboard title="MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving Representation Learning" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Object Detection, Representation Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08760v1.pdf filename=2403.08760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning robust and scalable visual <b>representations</b> <b>from</b> massive multi-view video data remains a challenge in computer vision and autonomous driving. Existing pre-training methods either rely on expensive <b>supervised</b> <b>learning</b> with 3D annotations, limiting the scalability, or focus on single-frame or monocular inputs, neglecting the temporal information. We propose MIM4D, a novel pre-training paradigm based on dual masked image modeling (MIM). MIM4D leverages both spatial and temporal relations by training on masked multi-view video inputs. It constructs pseudo-3D features using continuous scene flow and projects them onto 2D plane for supervision. To address the lack of dense 3D supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable rendering to learn geometric <b>representations.</b> <b>We</b> demonstrate that MIM4D achieves state-of-the-art performance on the nuScenes dataset for visual <b>representation</b> <b>learning</b> in autonomous driving. It significantly improves existing methods on multiple downstream tasks, including BEV segmentation (8.7% IoU), 3D <b>object</b> <b>detection</b> (3.5% mAP), and HD map construction (1.4% mAP). Our work offers a new choice for learning <b>representation</b> <b>at</b> scale in autonomous driving. Code and models are released at <a href=https://github.com/hustvl/MIM4D>https://github.com/hustvl/MIM4D</a></p></p class="citation"></blockquote><h3 id=1885--69310-masked-generative-story-transformer-with-character-guidance-and-caption-augmentation-christos-papadimitriou-et-al-2024>(18/85 | 69/310) Masked Generative Story Transformer with Character Guidance and Caption Augmentation (Christos Papadimitriou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christos Papadimitriou, Giorgos Filandrianos, Maria Lymperaiou, Giorgos Stamou. (2024)<br><strong>Masked Generative Story Transformer with Character Guidance and Caption Augmentation</strong><br><button class=copy-to-clipboard title="Masked Generative Story Transformer with Character Guidance and Caption Augmentation" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08502v1.pdf filename=2403.08502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences. Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters. On the contrary, we embrace a completely parallel <b>transformer-based</b> approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency. Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space. We also employ a caption-augmentation technique, carried out by a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> to enhance the robustness of our approach. The combination of these methods culminates into state-of-the-art (SOTA) results over various metrics in the most prominent SV <b>benchmark</b> (Pororo-SV), attained with constraint resources while achieving superior computational complexity compared to previous arts. The validity of our quantitative results is supported by a human survey.</p></p class="citation"></blockquote><h3 id=1985--70310-dam-dynamic-adapter-merging-for-continual-video-qa-learning-feng-cheng-et-al-2024>(19/85 | 70/310) DAM: Dynamic Adapter Merging for Continual Video QA Learning (Feng Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Cheng, Ziyang Wang, Yi-Lin Sung, Yan-Bo Lin, Mohit Bansal, Gedas Bertasius. (2024)<br><strong>DAM: Dynamic Adapter Merging for Continual Video QA Learning</strong><br><button class=copy-to-clipboard title="DAM: Dynamic Adapter Merging for Continual Video QA Learning" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Continual Learning, Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08755v1.pdf filename=2403.08755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a parameter-efficient method for <b>continual</b> <b>video</b> <b>question-answering</b> <b>(VidQA)</b> learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weights into a new adapter instance tailored for that particular test sample to compute the final VidQA prediction, mitigating the impact of inaccurate router predictions and facilitating knowledge sharing across domains. Our DAM model outperforms prior state-of-the-art <b>continual</b> <b>learning</b> approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA datasets spanning various domains. We further extend DAM to <b>continual</b> <b>image</b> classification and image <b>QA</b> and outperform prior methods by a large margin. The code is publicly available at: <a href=https://github.com/klauscc/DAM>https://github.com/klauscc/DAM</a></p></p class="citation"></blockquote><h3 id=2085--71310-gaussctrl-multi-view-consistent-text-driven-3d-gaussian-splatting-editing-jing-wu-et-al-2024>(20/85 | 71/310) GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing (Jing Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu. (2024)<br><strong>GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing</strong><br><button class=copy-to-clipboard title="GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: ControlNet, Diffusion Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08733v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08733v2.pdf filename=2403.08733v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS). Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D <b>diffusion</b> <b>model</b> <b>(ControlNet)</b> based on the input <b>prompt,</b> which is then used to optimise the 3D model. Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works. It leads to faster editing as well as higher visual quality. This is achieved by the two terms: (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps. (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images&rsquo; latent representations. Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2185--72310-deep-learning-for-in-orbit-cloud-segmentation-and-classification-in-hyperspectral-satellite-data-daniel-kovac-et-al-2024>(21/85 | 72/310) Deep Learning for In-Orbit Cloud Segmentation and Classification in Hyperspectral Satellite Data (Daniel Kovac et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Kovac, Jan Mucha, Jon Alvarez Justo, Jiri Mekyska, Zoltan Galaz, Krystof Novotny, Radoslav Pitonak, Jan Knezik, Jonas Herec, Tor Arne Johansen. (2024)<br><strong>Deep Learning for In-Orbit Cloud Segmentation and Classification in Hyperspectral Satellite Data</strong><br><button class=copy-to-clipboard title="Deep Learning for In-Orbit Cloud Segmentation and Classification in Hyperspectral Satellite Data" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08695v1.pdf filename=2403.08695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article explores the latest <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> for cloud detection aboard hyperspectral satellites. The performance of the latest 1D <b>CNN</b> (1D-Justo-LiuNet) and two recent 2D <b>CNNs</b> (nnU-net and 2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed. Evaluation criteria include precision and computational efficiency for in-orbit deployment. Experiments utilize NASA&rsquo;s EO-1 Hyperion data, with varying spectral channel numbers after Principal Component Analysis. Results indicate that 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D <b>CNNs,</b> while maintaining compactness with larger spectral channel sets, albeit with increased inference times. However, the performance of 1D <b>CNN</b> degrades with significant channel reduction. In this context, the 2D-Justo-UNet-Simple offers the best balance for in-orbit deployment, considering precision, memory, and time costs. While nnU-net is suitable for on-ground processing, deployment of lightweight 1D-Justo-LiuNet is recommended for high-precision applications. Alternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced costs between timing and precision in orbit.</p></p class="citation"></blockquote><h3 id=2285--73310-onevos-unifying-video-object-segmentation-with-all-in-one-transformer-framework-wanyun-li-et-al-2024>(22/85 | 73/310) OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework (Wanyun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanyun Li, Pinxue Guo, Xinyu Zhou, Lingyi Hong, Yangji He, Xiangyu Zheng, Wei Zhang, Wenqiang Zhang. (2024)<br><strong>OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework</strong><br><button class=copy-to-clipboard title="OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08682v1.pdf filename=2403.08682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contemporary Video Object Segmentation (VOS) approaches typically consist stages of feature extraction, matching, memory management, and multiple objects aggregation. Recent advanced models either employ a discrete modeling for these components in a sequential manner, or optimize a combined pipeline through substructure aggregation. However, these existing explicit staged approaches prevent the VOS framework from being optimized as a unified whole, leading to the limited capacity and suboptimal performance in tackling complex videos. In this paper, we propose OneVOS, a novel framework that unifies the core components of VOS with All-in-One <b>Transformer.</b> Specifically, to unify all aforementioned modules into a <b>vision</b> <b>transformer,</b> we model all the features of frames, masks and memory for multiple objects as <b>transformer</b> tokens, and integrally accomplish feature extraction, matching and memory management of multiple objects through the flexible attention mechanism. Furthermore, a Unidirectional Hybrid Attention is proposed through a double decoupling of the original attention operation, to rectify semantic errors and ambiguities of stored tokens in OneVOS framework. Finally, to alleviate the storage burden and expedite inference, we propose the Dynamic Token Selector, which unveils the working mechanism of OneVOS and naturally leads to a more efficient version of OneVOS. Extensive experiments demonstrate the superiority of OneVOS, achieving state-of-the-art performance across 7 datasets, particularly excelling in complex LVOS and MOSE datasets with 70.1% and 66.4% $J & F$ scores, surpassing previous state-of-the-art methods by 4.2% and 7.0%, respectively. And our code will be available for reproducibility and further research.</p></p class="citation"></blockquote><h3 id=2385--74310-pig-aggression-classification-using-cnn-transformers-and-recurrent-networks-junior-silva-souza-et-al-2024>(23/85 | 74/310) Pig aggression classification using CNN, Transformers and Recurrent Networks (Junior Silva Souza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junior Silva Souza, Eduardo Bedin, Gabriel Toshio Hirokawa Higa, Newton Loebens, Hemerson Pistori. (2024)<br><strong>Pig aggression classification using CNN, Transformers and Recurrent Networks</strong><br><button class=copy-to-clipboard title="Pig aggression classification using CNN, Transformers and Recurrent Networks" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08528v1.pdf filename=2403.08528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of techniques that can be used to analyze and detect animal behavior is a crucial activity for the livestock sector, as it is possible to monitor the stress and animal welfare and contributes to decision making in the farm. Thus, the development of applications can assist breeders in making decisions to improve production performance and reduce costs, once the animal behavior is analyzed by humans and this can lead to susceptible errors and time consumption. Aggressiveness in pigs is an example of behavior that is studied to reduce its impact through animal classification and identification. However, this process is laborious and susceptible to errors, which can be reduced through automation by visually classifying videos captured in controlled environment. The captured videos can be used for training and, as a result, for classification through computer vision and artificial intelligence, employing neural network techniques. The main techniques utilized in this study are variants of <b>transformers:</b> STAM, TimeSformer, and ViViT, as well as techniques using <b>convolutions,</b> such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These techniques were employed for pig video classification with the objective of identifying aggressive and non-aggressive behaviors. In this work, various techniques were compared to analyze the contribution of using <b>transformers,</b> in addition to the effectiveness of the <b>convolution</b> technique in video classification. The performance was evaluated using accuracy, precision, and recall. The TimerSformer technique showed the best results in video classification, with median accuracy of 0.729.</p></p class="citation"></blockquote><h3 id=2485--75310-optimized-detection-and-classification-on-gtrsb-advancing-traffic-sign-recognition-with-convolutional-neural-networks-dhruv-toshniwal-et-al-2024>(24/85 | 75/310) Optimized Detection and Classification on GTRSB: Advancing Traffic Sign Recognition with Convolutional Neural Networks (Dhruv Toshniwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhruv Toshniwal, Saurabh Loya, Anuj Khot, Yash Marda. (2024)<br><strong>Optimized Detection and Classification on GTRSB: Advancing Traffic Sign Recognition with Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Optimized Detection and Classification on GTRSB: Advancing Traffic Sign Recognition with Convolutional Neural Networks" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08283v1.pdf filename=2403.08283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving landscape of transportation, the proliferation of automobiles has made road traffic more complex, necessitating advanced vision-assisted technologies for enhanced safety and navigation. These technologies are imperative for providing critical traffic sign information, influencing driver behavior, and supporting vehicle control, especially for drivers with disabilities and in the burgeoning field of autonomous vehicles. Traffic sign detection and recognition have emerged as key areas of research due to their essential roles in ensuring road safety and compliance with traffic regulations. Traditional computer vision methods have faced challenges in achieving optimal accuracy and speed due to real-world variabilities. However, the advent of deep learning and <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> has revolutionized this domain, offering solutions that significantly surpass previous capabilities in terms of speed and reliability. This paper presents an innovative approach leveraging <b>CNNs</b> that achieves an accuracy of nearly 96%, highlighting the potential for even greater precision through advanced localization techniques. Our findings not only contribute to the ongoing advancement of traffic sign recognition technology but also underscore the critical impact of these developments on road safety and the future of autonomous driving.</p></p class="citation"></blockquote><h3 id=2585--76310-vlogger-multimodal-diffusion-for-embodied-avatar-synthesis-enric-corona-et-al-2024>(25/85 | 76/310) VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis (Enric Corona et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, Cristian Sminchisescu. (2024)<br><strong>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</strong><br><button class=copy-to-clipboard title="VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Diffusion Model, Benchmarking, Multi-modal, Multi-modal, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08764v1.pdf filename=2403.08764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative <b>diffusion</b> <b>models.</b> Our method consists of 1) a stochastic human-to-3d-motion <b>diffusion</b> <b>model,</b> and 2) a novel <b>diffusion-based</b> <b>architecture</b> that augments <b>text-to-image</b> models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions. VLOGGER outperforms state-of-the-art methods in three public <b>benchmarks,</b> considering image quality, identity preservation and temporal consistency while also generating upper-body gestures. We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization.</p></p class="citation"></blockquote><h3 id=2685--77310-pathm3-a-multimodal-multi-task-multiple-instance-learning-framework-for-whole-slide-image-classification-and-captioning-qifeng-zhou-et-al-2024>(26/85 | 77/310) PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning (Qifeng Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qifeng Zhou, Wenliang Zhong, Yuzhi Guo, Michael Xiao, Hehuan Ma, Junzhou Huang. (2024)<br><strong>PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning</strong><br><button class=copy-to-clipboard title="PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Multiple Instance Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08967v1.pdf filename=2403.08967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of computational histopathology, both whole slide images (WSIs) and diagnostic captions provide valuable insights for making diagnostic decisions. However, aligning WSIs with diagnostic captions presents a significant challenge. This difficulty arises from two main factors: 1) Gigapixel WSIs are unsuitable for direct input into deep learning models, and the redundancy and correlation among the patches demand more attention; and 2) Authentic WSI diagnostic captions are extremely limited, making it difficult to train an effective model. To overcome these obstacles, we present PathM3, a <b>multimodal,</b> multi-task, <b>multiple</b> <b>instance</b> <b>learning</b> (MIL) framework for WSI classification and captioning. PathM3 adapts a query-based <b>transformer</b> to effectively align WSIs with diagnostic captions. Given that histopathology visual patterns are redundantly distributed across WSIs, we aggregate each patch feature with MIL method that considers the correlations among instances. Furthermore, our PathM3 overcomes data scarcity in WSI-level captions by leveraging limited WSI diagnostic caption data in the manner of multi-task joint learning. Extensive experiments with improved classification accuracy and caption generation demonstrate the effectiveness of our method on both WSI classification and captioning task.</p></p class="citation"></blockquote><h3 id=2785--78310-envision3d-one-image-to-3d-with-anchor-views-interpolation-yatian-pang-et-al-2024>(27/85 | 78/310) Envision3D: One Image to 3D with Anchor Views Interpolation (Yatian Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yatian Pang, Tanghui Jia, Yujun Shi, Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Xing Zhou, Francis E. H. Tay, Li Yuan. (2024)<br><strong>Envision3D: One Image to 3D with Anchor Views Interpolation</strong><br><button class=copy-to-clipboard title="Envision3D: One Image to 3D with Anchor Views Interpolation" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Fine-tuning, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08902v1.pdf filename=2403.08902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Envision3D, a novel method for efficiently generating high-quality 3D content from a single image. Recent methods that extract 3D content from multi-view images generated by <b>diffusion</b> <b>models</b> show great potential. However, it is still challenging for <b>diffusion</b> <b>models</b> to generate dense multi-view consistent images, which is crucial for the quality of 3D content extraction. To address this issue, we propose a novel cascade <b>diffusion</b> <b>framework,</b> which decomposes the challenging dense views generation task into two tractable stages, namely anchor views generation and anchor views interpolation. In the first stage, we train the image <b>diffusion</b> <b>model</b> to generate global consistent anchor views conditioning on image-normal pairs. Subsequently, leveraging our video <b>diffusion</b> <b>model</b> <b>fine-tuned</b> on consecutive multi-view images, we conduct interpolation on the previous anchor views to generate extra dense views. This framework yields dense, multi-view consistent images, providing comprehensive 3D information. To further enhance the overall generation quality, we introduce a coarse-to-fine sampling strategy for the reconstruction algorithm to robustly extract textured meshes from the generated dense images. Extensive experiments demonstrate that our method is capable of generating high-quality 3D content in terms of texture and <b>geometry,</b> surpassing previous image-to-3D baseline methods.</p></p class="citation"></blockquote><h3 id=2885--79310-refractive-colmap-refractive-structure-from-motion-revisited-mengkun-she-et-al-2024>(28/85 | 79/310) Refractive COLMAP: Refractive Structure-from-Motion Revisited (Mengkun She et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengkun She, Felix Seegräber, David Nakath, Kevin Köser. (2024)<br><strong>Refractive COLMAP: Refractive Structure-from-Motion Revisited</strong><br><button class=copy-to-clipboard title="Refractive COLMAP: Refractive Structure-from-Motion Revisited" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08640v1.pdf filename=2403.08640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a complete refractive Structure-from-Motion (RSfM) framework for underwater 3D reconstruction using refractive camera setups (for both, flat- and dome-port underwater housings). Despite notable achievements in refractive multi-view <b>geometry</b> over the past decade, a robust, complete and publicly available solution for such tasks is not available at present, and often practical applications have to resort to approximating refraction effects by the intrinsic (distortion) parameters of a pinhole camera model. To fill this gap, we have integrated refraction considerations throughout the entire SfM process within the state-of-the-art, open-source SfM framework COLMAP. Numerical <b>simulations</b> and reconstruction results on synthetically generated but photo-realistic images with ground truth validate that enabling refraction does not compromise accuracy or robustness as compared to in-air reconstructions. Finally, we demonstrate the capability of our approach for large-scale refractive scenarios using a dataset consisting of nearly 6000 images. The implementation is released as open-source at: <a href=https://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater>https://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater</a>.</p></p class="citation"></blockquote><h3 id=2985--80310-consistent-prompting-for-rehearsal-free-continual-learning-zhanxin-gao-et-al-2024>(29/85 | 80/310) Consistent Prompting for Rehearsal-Free Continual Learning (Zhanxin Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanxin Gao, Jun Cen, Xiaobin Chang. (2024)<br><strong>Consistent Prompting for Rehearsal-Free Continual Learning</strong><br><button class=copy-to-clipboard title="Consistent Prompting for Rehearsal-Free Continual Learning" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Continual Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08568v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08568v2.pdf filename=2403.08568v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. <b>Prompt-based</b> approaches are built on frozen pre-trained models to learn the task-specific <b>prompts</b> and classifiers efficiently. Existing <b>prompt-based</b> methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. <b>Prompt</b> inconsistency indicates that the <b>prompt</b> selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel <b>prompt-based</b> method, Consistent <b>Prompting</b> (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to <b>prompt</b> training, resulting in classifier consistency learning. In addition, <b>prompt</b> consistency learning is proposed to enhance prediction robustness and boost <b>prompt</b> selection accuracy. Our Consistent <b>Prompting</b> surpasses its <b>prompt-based</b> counterparts and achieves state-of-the-art performance on multiple <b>continual</b> <b>learning</b> <b>benchmarks.</b> Detailed analysis shows that improvements come from more consistent training and testing.</p></p class="citation"></blockquote><h3 id=3085--81310-using-deep-learning-for-morphological-classification-in-pigs-with-a-focus-on-sanitary-monitoring-eduardo-bedin-et-al-2024>(30/85 | 81/310) Using Deep Learning for Morphological Classification in Pigs with a Focus on Sanitary Monitoring (Eduardo Bedin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eduardo Bedin, Junior Silva Souza, Gabriel Toshio Hirokawa Higa, Alexandre Pereira, Charles Kiefer, Newton Loebens, Hemerson Pistori. (2024)<br><strong>Using Deep Learning for Morphological Classification in Pigs with a Focus on Sanitary Monitoring</strong><br><button class=copy-to-clipboard title="Using Deep Learning for Morphological Classification in Pigs with a Focus on Sanitary Monitoring" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08962v1.pdf filename=2403.08962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The aim of this paper is to evaluate the use of D-CNN (Deep <b>Convolutional</b> <b>Neural</b> <b>Networks)</b> algorithms to classify pig body conditions in normal or not normal conditions, with a focus on characteristics that are observed in sanitary monitoring, and were used six different algorithms to do this task. The study focused on five pig characteristics, being these caudophagy, ear hematoma, scratches on the body, redness, and natural stains (brown or black). The results of the study showed that D-CNN was effective in classifying deviations in pig body morphologies related to skin characteristics. The evaluation was conducted by analyzing the performance metrics Precision, Recall, and F-score, as well as the statistical analyses ANOVA and the Scott-Knott test. The contribution of this article is characterized by the proposal of using D-CNN networks for morphological classification in pigs, with a focus on characteristics identified in sanitary monitoring. Among the best results, the average Precision metric of 80.6% to classify caudophagy was achieved for the InceptionResNetV2 network, indicating the potential use of this technology for the proposed task. Additionally, a new image database was created, containing various pig&rsquo;s distinct body characteristics, which can serve as data for future research.</p></p class="citation"></blockquote><h3 id=3185--82310-clip-bevformer-enhancing-multi-view-image-based-bev-detector-with-ground-truth-flow-chenbin-pan-et-al-2024>(31/85 | 82/310) CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow (Chenbin Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenbin Pan, Burhaneddin Yaman, Senem Velipasalar, Liu Ren. (2024)<br><strong>CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow</strong><br><button class=copy-to-clipboard title="CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08919v1.pdf filename=2403.08919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving stands as a pivotal domain in computer vision, shaping the future of transportation. Within this paradigm, the backbone of the system plays a crucial role in interpreting the complex environment. However, a notable challenge has been the loss of clear supervision when it comes to Bird&rsquo;s Eye View elements. To address this limitation, we introduce CLIP-BEVFormer, a novel approach that leverages the power of <b>contrastive</b> <b>learning</b> techniques to enhance the multi-view image-derived BEV backbones with ground truth information flow. We conduct extensive experiments on the challenging nuScenes dataset and showcase significant and consistent improvements over the SOTA. Specifically, CLIP-BEVFormer achieves an impressive 8.5% and 9.2% enhancement in terms of NDS and mAP, respectively, over the previous best BEV model on the 3D <b>object</b> <b>detection</b> task.</p></p class="citation"></blockquote><h3 id=3285--83310-icontra-toward-thematic-collection-design-via-interactive-concept-transfer-dinh-khoi-vo-et-al-2024>(32/85 | 83/310) iCONTRA: Toward Thematic Collection Design Via Interactive Concept Transfer (Dinh-Khoi Vo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dinh-Khoi Vo, Duy-Nam Ly, Khanh-Duy Le, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le. (2024)<br><strong>iCONTRA: Toward Thematic Collection Design Via Interactive Concept Transfer</strong><br><button class=copy-to-clipboard title="iCONTRA: Toward Thematic Collection Design Via Interactive Concept Transfer" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08746v1.pdf filename=2403.08746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating thematic collections in industries demands innovative designs and cohesive concepts. Designers may face challenges in maintaining thematic consistency when drawing inspiration from existing objects, landscapes, or artifacts. While AI-powered graphic design tools offer help, they often fail to generate cohesive sets based on specific thematic concepts. In response, we introduce iCONTRA, an interactive CONcept TRAnsfer system. With a user-friendly interface, iCONTRA enables both experienced designers and novices to effortlessly explore creative design concepts and efficiently generate thematic collections. We also propose a <b>zero-shot</b> image editing algorithm, eliminating the need for <b>fine-tuning</b> models, which gradually integrates information from initial objects, ensuring consistency in the generation process without influencing the background. A pilot study suggests iCONTRA&rsquo;s potential to reduce designers&rsquo; efforts. Experimental results demonstrate its effectiveness in producing consistent and high-quality object concept transfers. iCONTRA stands as a promising tool for innovation and creative exploration in thematic collection design. The source code will be available at: <a href=https://github.com/vdkhoi20/iCONTRA>https://github.com/vdkhoi20/iCONTRA</a>.</p></p class="citation"></blockquote><h3 id=3385--84310-data-augmentation-in-human-centric-vision-wentao-jiang-et-al-2024>(33/85 | 84/310) Data Augmentation in Human-Centric Vision (Wentao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Jiang, Yige Zhang, Shaozhong Zheng, Si Liu, Shuicheng Yan. (2024)<br><strong>Data Augmentation in Human-Centric Vision</strong><br><button class=copy-to-clipboard title="Data Augmentation in Human-Centric Vision" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08650v1.pdf filename=2403.08650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This survey presents a comprehensive analysis of <b>data</b> <b>augmentation</b> techniques in human-centric vision tasks, a first of its kind in the field. It delves into a wide range of research areas including person ReID, human parsing, human pose estimation, and pedestrian detection, addressing the significant challenges posed by overfitting and limited training <b>data</b> <b>in</b> these domains. Our work categorizes <b>data</b> <b>augmentation</b> methods into two main types: <b>data</b> <b>generation</b> and <b>data</b> <b>perturbation.</b> <b>Data</b> <b>generation</b> covers techniques like graphic engine-based generation, generative model-based generation, and <b>data</b> <b>recombination,</b> while <b>data</b> <b>perturbation</b> is divided into image-level and human-level perturbations. Each method is tailored to the unique requirements of human-centric tasks, with some applicable across multiple areas. Our contributions include an extensive literature review, providing deep insights into the influence of these augmentation techniques in human-centric vision and highlighting the nuances of each method. We also discuss open issues and future directions, such as the integration of advanced generative models like Latent <b>Diffusion</b> <b>Models,</b> for creating more realistic and diverse training <b>data.</b> <b>This</b> survey not only encapsulates the current state of <b>data</b> <b>augmentation</b> in human-centric vision but also charts a course for future research, aiming to develop more robust, accurate, and efficient human-centric vision systems.</p></p class="citation"></blockquote><h3 id=3485--85310-improved-yolov5-based-on-attention-mechanism-and-fasternet-for-foreign-object-detection-on-railway-and-airway-tracks-zongqing-qi-et-al-2024>(34/85 | 85/310) Improved YOLOv5 Based on Attention Mechanism and FasterNet for Foreign Object Detection on Railway and Airway tracks (Zongqing Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zongqing Qi, Danqing Ma, Jingyu Xu, Ao Xiang, Hedi Qu. (2024)<br><strong>Improved YOLOv5 Based on Attention Mechanism and FasterNet for Foreign Object Detection on Railway and Airway tracks</strong><br><button class=copy-to-clipboard title="Improved YOLOv5 Based on Attention Mechanism and FasterNet for Foreign Object Detection on Railway and Airway tracks" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Yolo, Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08499v1.pdf filename=2403.08499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there have been frequent incidents of foreign <b>objects</b> <b>intruding</b> into railway and Airport runways. These <b>objects</b> <b>can</b> include pedestrians, vehicles, animals, and debris. This paper introduces an improved YOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance the detection of foreign <b>objects</b> <b>on</b> railways and Airport runways. This study proposes a new dataset, AARFOD (Aero and Rail Foreign <b>Object</b> <b>Detection),</b> which combines two public datasets for detecting foreign <b>objects</b> <b>in</b> aviation and railway systems.The dataset aims to improve the recognition capabilities of foreign <b>object</b> <b>targets.</b> Experimental results on this large dataset have demonstrated significant performance improvements of the proposed model over the baseline YOLOv5 model, reducing computational requirements.Improved <b>YOLO</b> model shows a significant improvement in precision by 1.2%, recall rate by 1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters were reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%. In the ablation experiment, it is found that the FasterNet module can significantly reduce the number of parameters of the model, and the reference of the attention mechanism can slow down the performance loss caused by lightweight.</p></p class="citation"></blockquote><h3 id=3585--86310-model-will-tell-training-membership-inference-for-diffusion-models-xiaomeng-fu-et-al-2024>(35/85 | 86/310) Model Will Tell: Training Membership Inference for Diffusion Models (Xiaomeng Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaomeng Fu, Xi Wang, Qiao Li, Jin Liu, Jiao Dai, Jizhong Han. (2024)<br><strong>Model Will Tell: Training Membership Inference for Diffusion Models</strong><br><button class=copy-to-clipboard title="Model Will Tell: Training Membership Inference for Diffusion Models" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08487v1.pdf filename=2403.08487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> pose risks of privacy breaches and copyright disputes, primarily <b>stemming</b> from the potential utilization of unauthorized data during the training phase. The Training Membership Inference (TMI) task aims to determine whether a specific sample has been used in the training process of a target model, representing a critical tool for privacy violation verification. However, the increased stochasticity inherent in <b>diffusion</b> <b>renders</b> traditional shadow-model-based or metric-based methods ineffective when applied to <b>diffusion</b> <b>models.</b> Moreover, existing methods only yield binary classification labels which lack necessary comprehensibility in practical applications. In this paper, we explore a novel perspective for the TMI task by leveraging the intrinsic generative priors within the <b>diffusion</b> <b>model.</b> Compared with unseen samples, training samples exhibit stronger generative priors within the <b>diffusion</b> <b>model,</b> enabling the successful reconstruction of substantially degraded training images. Consequently, we propose the Degrade Restore Compare (DRC) framework. In this framework, an image undergoes sequential degradation and restoration, and its membership is determined by comparing it with the restored counterpart. Experimental results verify that our approach not only significantly outperforms existing methods in terms of accuracy but also provides comprehensible decision criteria, offering evidence for potential privacy violations.</p></p class="citation"></blockquote><h3 id=3685--87310-an-empirical-study-of-parameter-efficient-fine-tuning-on-vision-language-pre-train-model-yuxin-tian-et-al-2024>(36/85 | 87/310) An Empirical Study of Parameter Efficient Fine-tuning on Vision-Language Pre-train Model (Yuxin Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Tian, Mouxing Yang, Yunfan Li, Dayiheng Liu, Xingzhang Ren, Xi Peng, Jiancheng Lv. (2024)<br><strong>An Empirical Study of Parameter Efficient Fine-tuning on Vision-Language Pre-train Model</strong><br><button class=copy-to-clipboard title="An Empirical Study of Parameter Efficient Fine-tuning on Vision-Language Pre-train Model" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08433v1.pdf filename=2403.08433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies applied Parameter Efficient <b>Fine-Tuning</b> techniques (PEFTs) to efficiently narrow the performance gap between pre-training and downstream. There are two important factors for various PEFTs, namely, the accessible data size and fine-tunable parameter size. A natural expectation for PEFTs is that the performance of various PEFTs is positively related to the data size and fine-tunable parameter size. However, according to the evaluation of five PEFTs on two downstream <b>vision-language</b> (VL) tasks, we find that such an intuition holds only if the downstream data and task are not consistent with pre-training. For downstream <b>fine-tuning</b> consistent with pre-training, data size no longer affects the performance, while the influence of fine-tunable parameter size is not monotonous. We believe such an observation could guide the choice of training strategy for various PEFTs.</p></p class="citation"></blockquote><h3 id=3785--88310-raf-gi-towards-robust-accurate-and-fast-convergent-gradient-inversion-attack-in-federated-learning-can-liu-et-al-2024>(37/85 | 88/310) RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion Attack in Federated Learning (Can Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Can Liu, Jin Wang, Dongyang Yu. (2024)<br><strong>RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion Attack in Federated Learning</strong><br><button class=copy-to-clipboard title="RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion Attack in Federated Learning" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08383v1.pdf filename=2403.08383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) empowers privacy-preservation in model training by only exposing users&rsquo; model gradients. Yet, FL users are susceptible to the gradient inversion (GI) attack which can reconstruct ground-truth training data such as images based on model gradients. However, reconstructing high-resolution images by existing GI attack works faces two challenges: inferior accuracy and slow-convergence, especially when the context is complicated, e.g., the training batch size is much greater than 1 on each FL user. To address these challenges, we present a Robust, Accurate and Fast-convergent GI attack algorithm, called RAF-GI, with two components: 1) Additional <b>Convolution</b> Block (ACB) which can restore labels with up to 20% improvement compared with existing works; 2) Total variance, three-channel mEan and cAnny edge detection regularization term (TEA), which is a white-box attack strategy to reconstruct images based on labels inferred by ACB. Moreover, RAF-GI is robust that can still accurately reconstruct ground-truth data when the users&rsquo; training batch size is no more than 48. Our experimental results manifest that RAF-GI can diminish 94% time costs while achieving superb inversion quality in ImageNet dataset. Notably, with a batch size of 1, RAF-GI exhibits a 7.89 higher Peak Signal-to-Noise Ratio (PSNR) compared to the state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=3885--89310-stmpl-human-soft-tissue-simulation-anton-agafonov-et-al-2024>(38/85 | 89/310) STMPL: Human Soft-Tissue Simulation (Anton Agafonov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anton Agafonov, Lihi Zelnik-Manor. (2024)<br><strong>STMPL: Human Soft-Tissue Simulation</strong><br><button class=copy-to-clipboard title="STMPL: Human Soft-Tissue Simulation" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08344v1.pdf filename=2403.08344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In various applications, such as virtual reality and gaming, simulating the deformation of soft tissues in the human body during interactions with external objects is essential. Traditionally, Finite Element Methods (FEM) have been employed for this purpose, but they tend to be slow and resource-intensive. In this paper, we propose a unified representation of human body shape and soft tissue with a data-driven simulator of non-rigid deformations. This approach enables rapid <b>simulation</b> of realistic interactions. Our method builds upon the SMPL model, which generates human body shapes considering rigid transformations. We extend SMPL by incorporating a soft tissue layer and an intuitive representation of external forces applied to the body during object interactions. Specifically, we mapped the 3D body shape and soft tissue and applied external forces to 2D UV maps. Leveraging a UNET architecture designed for 2D data, our approach achieves high-accuracy inference in real time. Our experiment shows that our method achieves plausible deformation of the soft tissue layer, even for unseen scenarios.</p></p class="citation"></blockquote><h3 id=3985--90310-styledyrf-zero-shot-4d-style-transfer-for-dynamic-neural-radiance-fields-hongbin-xu-et-al-2024>(39/85 | 90/310) StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance Fields (Hongbin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongbin Xu, Weitao Chen, Feng Xiao, Baigui Sun, Wenxiong Kang. (2024)<br><strong>StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance Fields</strong><br><button class=copy-to-clipboard title="StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance Fields" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08310v1.pdf filename=2403.08310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>4D <b>style</b> <b>transfer</b> aims at transferring arbitrary visual <b>style</b> <b>to</b> the synthesized novel views of a dynamic 4D scene with varying viewpoints and times. Existing efforts on 3D <b>style</b> <b>transfer</b> can effectively combine the visual features of <b>style</b> <b>images</b> and neural radiance fields (NeRF) but fail to handle the 4D dynamic scenes limited by the static scene assumption. Consequently, we aim to handle the novel challenging problem of 4D <b>style</b> <b>transfer</b> for the first time, which further requires the consistency of stylized results on dynamic objects. In this paper, we introduce StyleDyRF, a method that represents the 4D feature space by deforming a canonical feature volume and learns a linear <b>style</b> <b>transformation</b> matrix on the feature volume in a data-driven fashion. To obtain the canonical feature volume, the rays at each time step are deformed with the geometric prior of a pre-trained dynamic NeRF to render the feature map under the supervision of pre-trained visual encoders. With the content and <b>style</b> <b>cues</b> in the canonical feature volume and the <b>style</b> <b>image,</b> we can learn the <b>style</b> <b>transformation</b> matrix from their covariance matrices with lightweight neural networks. The learned <b>style</b> <b>transformation</b> matrix can reflect a direct matching of feature covariance from the content volume to the given <b>style</b> <b>pattern,</b> in analogy with the optimization of the Gram matrix in traditional 2D neural <b>style</b> <b>transfer.</b> The experimental results show that our method not only renders 4D photorealistic <b>style</b> <b>transfer</b> results in a <b>zero-shot</b> manner but also outperforms existing methods in terms of visual quality and consistency.</p></p class="citation"></blockquote><h3 id=4085--91310-mgic-a-multi-label-gradient-inversion-attack-based-on-canny-edge-detection-on-federated-learning-can-liu-et-al-2024>(40/85 | 91/310) MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge Detection on Federated Learning (Can Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Can Liu, Jin Wang. (2024)<br><strong>MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge Detection on Federated Learning</strong><br><button class=copy-to-clipboard title="MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge Detection on Federated Learning" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08284v1.pdf filename=2403.08284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a new distributed computing framework that can protect data privacy, <b>federated</b> <b>learning</b> (FL) has attracted more and more attention in recent years. It receives gradients from users to train the global model and releases the trained global model to working users. Nonetheless, the gradient inversion (GI) attack reflects the risk of privacy leakage in <b>federated</b> <b>learning.</b> Attackers only need to use gradients through hundreds of thousands of simple iterations to obtain relatively accurate private data stored on users&rsquo; local devices. For this, some works propose simple but effective strategies to obtain user data under a single-label dataset. However, these strategies induce a satisfactory visual effect of the inversion image at the expense of higher time costs. Due to the semantic limitation of a single label, the image obtained by gradient inversion may have semantic errors. We present a novel gradient inversion strategy based on canny edge detection (MGIC) in both the multi-label and single-label datasets. To reduce semantic errors caused by a single label, we add new <b>convolution</b> layers&rsquo; blocks in the trained model to obtain the image&rsquo;s multi-label. Through multi-label representation, serious semantic errors in inversion images are reduced. Then, we analyze the impact of parameters on the difficulty of input image reconstruction and discuss how image multi-subjects affect the inversion performance. Our proposed strategy has better visual inversion image results than the most widely used ones, saving more than 78% of time costs in the ImageNet dataset.</p></p class="citation"></blockquote><h3 id=4185--92310-vigface-virtual-identity-generation-model-for-face-image-synthesis-minsoo-kim-et-al-2024>(41/85 | 92/310) VIGFace: Virtual Identity Generation Model for Face Image Synthesis (Minsoo Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsoo Kim, Min-Cheol Sagong, Gi Pyo Nam, Junghyun Cho, Ig-Jae Kim. (2024)<br><strong>VIGFace: Virtual Identity Generation Model for Face Image Synthesis</strong><br><button class=copy-to-clipboard title="VIGFace: Virtual Identity Generation Model for Face Image Synthesis" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08277v1.pdf filename=2403.08277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based <b>face</b> <b>recognition</b> continues to <b>face</b> <b>challenges</b> due to its reliance on huge datasets obtained from web crawling, which can be costly to gather and raise significant real-world privacy concerns. To address this issue, we propose VIGFace, a novel framework capable of generating synthetic facial images. Initially, we train the <b>face</b> <b>recognition</b> model using a real <b>face</b> <b>dataset</b> and create a feature space for both real and virtual IDs where virtual prototypes are orthogonal to other prototypes. Subsequently, we generate synthetic images by using the <b>diffusion</b> <b>model</b> based on the feature space. Our proposed framework provides two significant benefits. Firstly, it allows for creating virtual facial images without concerns about portrait rights, guaranteeing that the generated virtual <b>face</b> <b>images</b> are clearly differentiated from existing individuals. Secondly, it serves as an effective augmentation method by incorporating real existing images. Further experiments demonstrate the efficacy of our framework, achieving state-of-the-art results from both perspectives without any external data.</p></p class="citation"></blockquote><h3 id=4285--93310-point-cloud-compression-via-constrained-optimal-transport-zezeng-li-et-al-2024>(42/85 | 93/310) Point Cloud Compression via Constrained Optimal Transport (Zezeng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zezeng Li, Weimin Wang, Ziliang Wang, Na Lei. (2024)<br><strong>Point Cloud Compression via Constrained Optimal Transport</strong><br><button class=copy-to-clipboard title="Point Cloud Compression via Constrained Optimal Transport" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08236v1.pdf filename=2403.08236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel point cloud compression method COT-PCC by formulating the task as a constrained optimal transport (COT) problem. COT-PCC takes the bitrate of compressed features as an extra constraint of optimal transport (OT) which learns the distribution transformation between original and reconstructed points. Specifically, the formulated COT is implemented with a <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> and a bitrate loss for training. The discriminator measures the Wasserstein distance between input and reconstructed points, and a generator calculates the optimal mapping between distributions of input and reconstructed point cloud. Moreover, we introduce a learnable sampling module for downsampling in the compression procedure. Extensive results on both sparse and dense point cloud datasets demonstrate that COT-PCC outperforms state-of-the-art methods in terms of both CD and PSNR metrics. Source codes are available at \url{https://github.com/cognaclee/PCC-COT}.</p></p class="citation"></blockquote><h3 id=4385--94310-lix-implicitly-infusing-spatial-geometric-prior-knowledge-into-visual-semantic-segmentation-for-autonomous-driving-sicen-guo-et-al-2024>(43/85 | 94/310) LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving (Sicen Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sicen Guo, Zhiyuan Wu, Qijun Chen, Ioannis Pitas, Rui Fan. (2024)<br><strong>LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving</strong><br><button class=copy-to-clipboard title="LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08215v1.pdf filename=2403.08215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior <b>knowledge</b> <b>acquired</b> by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to <b>knowledge</b> <b>distillation</b> approaches to address this problem. We introduce the Learning to Infuse &ldquo;X&rdquo; (LIX) framework, with novel contributions in both logit <b>distillation</b> and feature <b>distillation</b> aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled <b>knowledge</b> <b>distillation</b> and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature <b>distillation</b> algorithm, including two technical novelties: feature recalibration via kernel regression and in-depth feature consistency quantification via centered kernel alignment. Extensive experiments conducted with intermediate-fusion and late-fusion networks across various public datasets provide both quantitative and qualitative evaluations, demonstrating the superior performance of our LIX framework when compared to other state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=4485--95310-shadowremovalnet-efficient-real-time-shadow-removal-alzayat-saleh-et-al-2024>(44/85 | 95/310) ShadowRemovalNet: Efficient Real-Time Shadow Removal (Alzayat Saleh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alzayat Saleh, Alex Olsen, Jake Wood, Bronson Philippa, Mostafa Rahimi Azghadi. (2024)<br><strong>ShadowRemovalNet: Efficient Real-Time Shadow Removal</strong><br><button class=copy-to-clipboard title="ShadowRemovalNet: Efficient Real-Time Shadow Removal" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08142v1.pdf filename=2403.08142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shadows significantly impact computer vision tasks, particularly in outdoor environments. State-of-the-art shadow removal methods are typically too computationally intensive for real-time image processing on edge hardware. We propose ShadowRemovalNet, a novel method designed for real-time image processing on resource-constrained hardware. ShadowRemovalNet achieves significantly higher frame rates compared to existing methods, making it suitable for real-time computer vision pipelines like those used in field robotics. Beyond speed, ShadowRemovalNet offers advantages in efficiency and simplicity, as it does not require a separate shadow mask during inference. ShadowRemovalNet also addresses challenges associated with <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> for shadow removal, including artefacts, inaccurate mask estimations, and inconsistent supervision between shadow and boundary pixels. To address these limitations, we introduce a novel loss function that substantially reduces shadow removal errors. ShadowRemovalNet&rsquo;s efficiency and straightforwardness make it a robust and effective solution for real-time shadow removal in outdoor robotics and edge computing applications.</p></p class="citation"></blockquote><h3 id=4585--96310-monoocc-digging-into-monocular-semantic-occupancy-prediction-yupeng-zheng-et-al-2024>(45/85 | 96/310) MonoOcc: Digging into Monocular Semantic Occupancy Prediction (Yupeng Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yupeng Zheng, Xiang Li, Pengfei Li, Yuhang Zheng, Bu Jin, Chengliang Zhong, Xiaoxiao Long, Hao Zhao, Qichao Zhang. (2024)<br><strong>MonoOcc: Digging into Monocular Semantic Occupancy Prediction</strong><br><button class=copy-to-clipboard title="MonoOcc: Digging into Monocular Semantic Occupancy Prediction" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Geometry, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08766v1.pdf filename=2403.08766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular Semantic Occupancy Prediction aims to infer the complete 3D <b>geometry</b> and semantic information of scenes from only 2D images. It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles. However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network&rsquo;s output, single-frame input, and the utilization of a small backbone. These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects. To address these issues, we propose MonoOcc. In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a <b>distillation</b> module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware. With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion <b>benchmark.</b> Codes and models can be accessed at <a href=https://github.com/ucaszyp/MonoOcc>https://github.com/ucaszyp/MonoOcc</a></p></p class="citation"></blockquote><h3 id=4685--97310-repair-rank-correlation-and-noisy-pair-half-replacing-with-memory-for-noisy-correspondence-ruochen-zheng-et-al-2024>(46/85 | 97/310) REPAIR: Rank Correlation and Noisy Pair Half-replacing with Memory for Noisy Correspondence (Ruochen Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruochen Zheng, Jiahao Hong, Changxin Gao, Nong Sang. (2024)<br><strong>REPAIR: Rank Correlation and Noisy Pair Half-replacing with Memory for Noisy Correspondence</strong><br><button class=copy-to-clipboard title="REPAIR: Rank Correlation and Noisy Pair Half-replacing with Memory for Noisy Correspondence" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08224v1.pdf filename=2403.08224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The presence of noise in acquired data invariably leads to performance degradation in cross-modal matching. Unfortunately, obtaining precise annotations in the <b>multimodal</b> field is expensive, which has <b>prompted</b> some methods to tackle the mismatched data pair issue in cross-modal matching contexts, termed as noisy correspondence. However, most of these existing noisy correspondence methods exhibit the following limitations: a) the problem of self-reinforcing error accumulation, and b) improper handling of noisy data pair. To tackle the two problems, we propose a generalized framework termed as Rank corrElation and noisy Pair hAlf-replacing wIth memoRy (REPAIR), which benefits from maintaining a memory bank for features of matched pairs. Specifically, we calculate the distances between the features in the memory bank and those of the target pair for each respective modality, and use the rank correlation of these two sets of distances to estimate the soft correspondence label of the target pair. Estimating soft correspondence based on memory bank features rather than using a similarity network can avoid the accumulation of errors due to incorrect network identifications. For pairs that are completely mismatched, REPAIR searches the memory bank for the most matching feature to replace one feature of one modality, instead of using the original pair directly or merely discarding the mismatched pair. We conduct experiments on three cross-modal datasets, i.e., Flickr30K, MSCOCO, and CC152K, proving the effectiveness and robustness of our REPAIR on synthetic and real-world noise.</p></p class="citation"></blockquote><h3 id=4785--98310-secg-semantic-enhanced-3d-visual-grounding-via-cross-modal-graph-attention-feng-xiao-et-al-2024>(47/85 | 98/310) SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention (Feng Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Xiao, Hongbin Xu, Qiuxia Wu, Wenxiong Kang. (2024)<br><strong>SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention</strong><br><button class=copy-to-clipboard title="SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08182v1.pdf filename=2403.08182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D visual <b>grounding</b> aims to automatically locate the 3D region of the specified object given the corresponding textual description. Existing works fail to distinguish similar objects especially when multiple referred objects are involved in the description. Experiments show that direct matching of language and visual modal has limited capacity to comprehend complex referential relationships in utterances. It is mainly due to the interference caused by redundant visual information in cross-modal alignment. To strengthen relation-orientated mapping between different modalities, we propose SeCG, a semantic-enhanced relational learning model based on a <b>graph</b> network with our designed memory <b>graph</b> attention layer. Our method replaces original language-independent encoding with cross-modal encoding in visual analysis. More text-related feature expressions are obtained through the guidance of global semantics and implicit relationships. Experimental results on ReferIt3D and ScanRefer <b>benchmarks</b> show that the proposed method outperforms the existing state-of-the-art methods, particularly improving the localization performance for the multi-relation challenges.</p></p class="citation"></blockquote><h3 id=4885--99310-pnesm-arbitrary-3d-scene-stylization-via-prompt-based-neural-style-mapping-jiafu-chen-et-al-2024>(48/85 | 99/310) PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping (Jiafu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiafu Chen, Wei Xing, Jiakai Sun, Tianyi Chu, Yiling Huang, Boyan Ji, Lei Zhao, Huaizhong Lin, Haibo Chen, Zhizhong Wang. (2024)<br><strong>PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping</strong><br><button class=copy-to-clipboard title="PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08252v1.pdf filename=2403.08252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D scene stylization refers to transform the appearance of a 3D scene to match a given style image, ensuring that images rendered from different viewpoints exhibit the same style as the given style image, while maintaining the 3D consistency of the stylized scene. Several existing methods have obtained impressive results in stylizing 3D scenes. However, the models proposed by these methods need to be re-trained when applied to a new scene. In other words, their models are coupled with a specific scene and cannot adapt to arbitrary other scenes. To address this issue, we propose a novel 3D scene stylization framework to transfer an arbitrary style to an arbitrary scene, without any style-related or scene-related re-training. Concretely, we first map the appearance of the 3D scene into a 2D style pattern space, which realizes complete disentanglement of the <b>geometry</b> and appearance of the 3D scene and makes our model be generalized to arbitrary 3D scenes. Then we stylize the appearance of the 3D scene in the 2D style pattern space via a <b>prompt-based</b> 2D stylization algorithm. Experimental results demonstrate that our proposed framework is superior to SOTA methods in both visual quality and generalization.</p></p class="citation"></blockquote><h3 id=4985--100310-cart-caltech-aerial-rgb-thermal-dataset-in-the-wild-connor-lee-et-al-2024>(49/85 | 100/310) CART: Caltech Aerial RGB-Thermal Dataset in the Wild (Connor Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Connor Lee, Matthew Anderson, Nikhil Raganathan, Xingxing Zuo, Kevin Do, Georgia Gkioxari, Soon-Jo Chung. (2024)<br><strong>CART: Caltech Aerial RGB-Thermal Dataset in the Wild</strong><br><button class=copy-to-clipboard title="CART: Caltech Aerial RGB-Thermal Dataset in the Wild" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08997v1.pdf filename=2403.08997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the first publicly available RGB-thermal dataset designed for aerial robotics operating in natural environments. Our dataset captures a variety of terrains across the continental United States, including rivers, lakes, coastlines, deserts, and forests, and consists of synchronized RGB, long-wave thermal, global positioning, and inertial data. Furthermore, we provide semantic segmentation annotations for 10 classes commonly encountered in natural settings in order to facilitate the development of perception algorithms robust to adverse weather and nighttime conditions. Using this dataset, we propose new and challenging <b>benchmarks</b> for thermal and RGB-thermal semantic segmentation, RGB-to-thermal <b>image</b> <b>translation,</b> and visual-inertial odometry. We present extensive results using state-of-the-art methods and highlight the challenges posed by temporal and geographical domain shifts in our data. Dataset and accompanying code will be provided at <a href=https://github.com/aerorobotics/caltech-aerial-rgbt-dataset>https://github.com/aerorobotics/caltech-aerial-rgbt-dataset</a></p></p class="citation"></blockquote><h3 id=5085--101310-actiondiffusion-an-action-aware-diffusion-model-for-procedure-planning-in-instructional-videos-lei-shi-et-al-2024>(50/85 | 101/310) ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning in Instructional Videos (Lei Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Shi, Paul Bürkner, Andreas Bulling. (2024)<br><strong>ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning in Instructional Videos</strong><br><button class=copy-to-clipboard title="ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning in Instructional Videos" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08591v1.pdf filename=2403.08591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present ActionDiffusion &ndash; a novel <b>diffusion</b> <b>model</b> for procedure planning in instructional videos that is the first to take temporal inter-dependencies between actions into account in a <b>diffusion</b> <b>model</b> for procedure planning. This approach is in stark contrast to existing methods that fail to exploit the rich information content available in the particular order in which actions are performed. Our method unifies the learning of temporal dependencies between actions and denoising of the action plan in the <b>diffusion</b> <b>process</b> by projecting the action information into the noise space. This is achieved 1) by adding action embeddings in the noise masks in the noise-adding phase and 2) by introducing an attention mechanism in the noise prediction network to learn the correlations between different action steps. We report extensive experiments on three instructional video <b>benchmark</b> datasets (CrossTask, Coin, and NIV) and show that our method outperforms previous state-of-the-art methods on all metrics on CrossTask and NIV and all metrics except accuracy on Coin dataset. We show that by adding action embeddings into the noise mask the <b>diffusion</b> <b>model</b> can better learn action temporal dependencies and increase the performances on procedure planning.</p></p class="citation"></blockquote><h3 id=5185--102310-towards-dense-and-accurate-radar-perception-via-efficient-cross-modal-diffusion-model-ruibin-zhang-et-al-2024>(51/85 | 102/310) Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal Diffusion Model (Ruibin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruibin Zhang, Donglai Xue, Yuhan Wang, Ruixu Geng, Fei Gao. (2024)<br><strong>Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal Diffusion Model</strong><br><button class=copy-to-clipboard title="Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal Diffusion Model" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08460v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08460v2.pdf filename=2403.08460v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Millimeter wave (mmWave) radars have attracted significant attention from both academia and industry due to their capability to operate in extreme weather conditions. However, they face challenges in terms of sparsity and noise interference, which hinder their application in the field of micro aerial vehicle (MAV) autonomous navigation. To this end, this paper proposes a novel approach to dense and accurate mmWave radar point cloud construction via cross-modal learning. Specifically, we introduce <b>diffusion</b> <b>models,</b> which possess state-of-the-art performance in generative modeling, to predict LiDAR-like point clouds from paired raw radar data. We also incorporate the most recent <b>diffusion</b> <b>model</b> inference accelerating techniques to ensure that the proposed method can be implemented on MAVs with limited computing resources.We validate the proposed method through extensive <b>benchmark</b> comparisons and real-world experiments, demonstrating its superior performance and generalization ability. Code and pretrained models will be available at <a href=https://github.com/ZJU-FAST-Lab/Radar-Diffusion>https://github.com/ZJU-FAST-Lab/Radar-Diffusion</a>.</p></p class="citation"></blockquote><h3 id=5285--103310-iterative-online-image-synthesis-via-diffusion-model-for-imbalanced-classification-shuhan-li-et-al-2024>(52/85 | 103/310) Iterative Online Image Synthesis via Diffusion Model for Imbalanced Classification (Shuhan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuhan Li, Yi Lin, Hao Chen, Kwang-Ting Cheng. (2024)<br><strong>Iterative Online Image Synthesis via Diffusion Model for Imbalanced Classification</strong><br><button class=copy-to-clipboard title="Iterative Online Image Synthesis via Diffusion Model for Imbalanced Classification" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08407v1.pdf filename=2403.08407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate and robust classification of diseases is important for proper diagnosis and treatment. However, medical datasets often face challenges related to limited <b>sample</b> <b>sizes</b> and inherent imbalanced distributions, due to difficulties in data collection and variations in disease prevalence across different types. In this paper, we introduce an Iterative Online Image Synthesis (IOIS) framework to address the class imbalance problem in medical image classification. Our framework incorporates two key modules, namely Online Image Synthesis (OIS) and Accuracy Adaptive Sampling (AAS), which collectively target the imbalance classification issue at both the instance level and the class level. The OIS module alleviates the data insufficiency problem by generating representative <b>samples</b> <b>tailored</b> for online training of the classifier. On the other hand, the AAS module dynamically balances the synthesized <b>samples</b> <b>among</b> various classes, targeting those with low training accuracy. To evaluate the effectiveness of our proposed method in addressing imbalanced classification, we conduct experiments on the HAM10000 and APTOS datasets. The results obtained demonstrate the superiority of our approach over state-of-the-art methods as well as the effectiveness of each component. The source code will be released upon acceptance.</p></p class="citation"></blockquote><h3 id=5385--104310-hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation-zhonghan-zhao-et-al-2024>(53/85 | 104/310) Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation (Zhonghan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhonghan Zhao, Kewei Chen, Dongxu Guo, Wenhao Chai, Tian Ye, Yanting Zhang, Gaoang Wang. (2024)<br><strong>Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation</strong><br><button class=copy-to-clipboard title="Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08282v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08282v2.pdf filename=2403.08282v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the dynamic and unpredictable open-world setting, navigating complex environments in Minecraft poses significant challenges for multi-agent systems. Agents must interact with the environment and coordinate their actions with other agents to achieve common objectives. However, traditional approaches often struggle to efficiently manage inter-agent communication and task distribution, crucial for effective multi-agent navigation. Furthermore, processing and integrating <b>multi-modal</b> information (such as visual, textual, and auditory data) is essential for agents to comprehend their goals and navigate the environment successfully and fully. To address this issue, we design the HAS framework to auto-organize groups of <b>LLM-based</b> agents to complete navigation tasks. In our approach, we devise a hierarchical auto-organizing navigation system, which is characterized by 1) a hierarchical system for multi-agent organization, ensuring centralized planning and decentralized execution; 2) an auto-organizing and intra-communication mechanism, enabling dynamic group adjustment under subtasks; 3) a <b>multi-modal</b> information platform, facilitating <b>multi-modal</b> perception to perform the three navigation tasks with one system. To assess organizational behavior, we design a series of navigation tasks in the Minecraft environment, which includes searching and exploring. We aim to develop embodied organizations that push the boundaries of embodied AI, moving it towards a more human-like organizational structure.</p></p class="citation"></blockquote><h3 id=5485--105310-ig-fiqa-improving-face-image-quality-assessment-through-intra-class-variance-guidance-robust-to-inaccurate-pseudo-labels-minsoo-kim-et-al-2024>(54/85 | 105/310) IG-FIQA: Improving Face Image Quality Assessment through Intra-class Variance Guidance robust to Inaccurate Pseudo-Labels (Minsoo Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsoo Kim, Gi Pyo Nam, Haksub Kim, Haesol Park, Ig-Jae Kim. (2024)<br><strong>IG-FIQA: Improving Face Image Quality Assessment through Intra-class Variance Guidance robust to Inaccurate Pseudo-Labels</strong><br><button class=copy-to-clipboard title="IG-FIQA: Improving Face Image Quality Assessment through Intra-class Variance Guidance robust to Inaccurate Pseudo-Labels" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08256v1.pdf filename=2403.08256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of face image quality assesment (FIQA), method based on sample relative classification have shown impressive performance. However, the quality scores used as pseudo-labels assigned from images of classes with low intra-class variance could be unrelated to the actual quality in this method. To address this issue, we present IG-FIQA, a novel approach to guide FIQA training, introducing a weight parameter to alleviate the adverse impact of these classes. This method involves estimating sample intra-class variance at each iteration during training, ensuring minimal computational overhead and straightforward implementation. Furthermore, this paper proposes an on-the-fly <b>data</b> <b>augmentation</b> methodology for improved generalization performance in FIQA. On various <b>benchmark</b> datasets, our proposed method, IG-FIQA, achieved novel state-of-the-art (SOTA) performance.</p></p class="citation"></blockquote><h3 id=5585--106310-make-me-happier-evoking-emotions-through-image-diffusion-models-qing-lin-et-al-2024>(55/85 | 106/310) Make Me Happier: Evoking Emotions Through Image Diffusion Models (Qing Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qing Lin, Jingfeng Zhang, Yew Soon Ong, Mengmi Zhang. (2024)<br><strong>Make Me Happier: Evoking Emotions Through Image Diffusion Models</strong><br><button class=copy-to-clipboard title="Make Me Happier: Evoking Emotions Through Image Diffusion Models" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08255v1.pdf filename=2403.08255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the rapid progress in image generation, emotional image editing remains under-explored. The semantics, context, and structure of an image can evoke emotional responses, making emotional image editing techniques valuable for various real-world applications, including treatment of psychological disorders, commercialization of products, and artistic design. For the first time, we present a novel challenge of emotion-evoked image generation, aiming to synthesize images that evoke target emotions while retaining the semantics and structures of the original scenes. To address this challenge, we propose a <b>diffusion</b> <b>model</b> capable of effectively understanding and editing source images to convey desired emotions and sentiments. Moreover, due to the lack of emotion editing datasets, we provide a unique dataset consisting of 340,000 pairs of images and their emotion annotations. Furthermore, we conduct human psychophysics experiments and introduce four new evaluation metrics to systematically <b>benchmark</b> all the methods. Experimental results demonstrate that our method surpasses all competitive baselines. Our <b>diffusion</b> <b>model</b> is capable of identifying emotional cues from original images, editing images that elicit desired emotions, and meanwhile, preserving the semantic structure of the original images. All code, model, and data will be made public.</p></p class="citation"></blockquote><h3 id=5685--107310-ntire-2023-image-shadow-removal-challenge-technical-report-team-iim_tti-yuki-kondo-et-al-2024>(56/85 | 107/310) NTIRE 2023 Image Shadow Removal Challenge Technical Report: Team IIM_TTI (Yuki Kondo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuki Kondo, Riku Miyata, Fuma Yasue, Taito Naruki, Norimichi Ukita. (2024)<br><strong>NTIRE 2023 Image Shadow Removal Challenge Technical Report: Team IIM_TTI</strong><br><button class=copy-to-clipboard title="NTIRE 2023 Image Shadow Removal Challenge Technical Report: Team IIM_TTI" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08995v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08995v2.pdf filename=2403.08995v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we analyze and discuss ShadowFormer in preparation for the NTIRE2023 Shadow Removal Challenge [1], implementing five key improvements: image alignment, the introduction of a perceptual quality loss function, the semi-automatic annotation for shadow detection, joint learning of shadow detection and removal, and the introduction of new <b>data</b> <b>augmentation</b> technique &ldquo;CutShadow&rdquo; for shadow removal. Our method achieved scores of 0.196 (3rd out of 19) in LPIPS and 7.44 (4th out of 19) in the Mean Opinion Score (MOS).</p></p class="citation"></blockquote><h3 id=5785--108310-unveiling-the-truth-exploring-human-gaze-patterns-in-fake-images-giuseppe-cartella-et-al-2024>(57/85 | 108/310) Unveiling the Truth: Exploring Human Gaze Patterns in Fake Images (Giuseppe Cartella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giuseppe Cartella, Vittorio Cuculo, Marcella Cornia, Rita Cucchiara. (2024)<br><strong>Unveiling the Truth: Exploring Human Gaze Patterns in Fake Images</strong><br><button class=copy-to-clipboard title="Unveiling the Truth: Exploring Human Gaze Patterns in Fake Images" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08933v1.pdf filename=2403.08933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating high-quality and realistic images is now possible thanks to the impressive advancements in image generation. A description in natural language of your desired output is all you need to obtain breathtaking results. However, as the use of generative models grows, so do concerns about the propagation of malicious content and misinformation. Consequently, the research community is actively working on the development of novel fake detection techniques, primarily focusing on low-level features and possible fingerprints left by generative models during the image generation process. In a different vein, in our work, we leverage human semantic knowledge to investigate the possibility of being included in frameworks of fake image detection. To achieve this, we collect a novel dataset of partially manipulated images using <b>diffusion</b> <b>models</b> and conduct an eye-tracking experiment to record the eye movements of different observers while viewing real and fake stimuli. A preliminary statistical analysis is conducted to explore the distinctive patterns in how humans perceive genuine and altered images. Statistical findings reveal that, when perceiving counterfeit samples, humans tend to focus on more confined regions of the image, in contrast to the more dispersed observational pattern observed when viewing genuine images. Our dataset is publicly available at: <a href=https://github.com/aimagelab/unveiling-the-truth>https://github.com/aimagelab/unveiling-the-truth</a>.</p></p class="citation"></blockquote><h3 id=5885--109310-federated-data-model-xiao-chen-et-al-2024>(58/85 | 109/310) Federated Data Model (Xiao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Chen, Shunan Zhang, Eric Z. Chen, Yikang Liu, Lin Zhao, Terrence Chen, Shanhui Sun. (2024)<br><strong>Federated Data Model</strong><br><button class=copy-to-clipboard title="Federated Data Model" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08887v1.pdf filename=2403.08887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In artificial intelligence (AI), especially deep learning, data diversity and volume play a pivotal role in model development. However, training a robust deep learning model often faces challenges due to data privacy, regulations, and the difficulty of sharing data between different locations, especially for medical applications. To address this, we developed a method called the Federated Data Model (FDM). This method uses <b>diffusion</b> <b>models</b> to learn the characteristics of data at one site and then creates synthetic data that can be used at another site without sharing the actual data. We tested this approach with a medical image segmentation task, focusing on cardiac magnetic resonance images from different hospitals. Our results show that models trained with this method perform well both on the data they were originally trained on and on data from other sites. This approach offers a promising way to train accurate and privacy-respecting AI models across different locations.</p></p class="citation"></blockquote><h3 id=5985--110310-artvista-gateway-to-empower-anyone-into-artist-trong-vu-hoang-et-al-2024>(59/85 | 110/310) ARtVista: Gateway To Empower Anyone Into Artist (Trong-Vu Hoang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Trong-Vu Hoang, Quang-Binh Nguyen, Duy-Nam Ly, Khanh-Duy Le, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le. (2024)<br><strong>ARtVista: Gateway To Empower Anyone Into Artist</strong><br><button class=copy-to-clipboard title="ARtVista: Gateway To Empower Anyone Into Artist" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08876v1.pdf filename=2403.08876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drawing is an art that enables people to express their imagination and emotions. However, individuals usually face challenges in drawing, especially when translating conceptual ideas into visually coherent representations and bridging the gap between mental visualization and practical execution. In response, we propose ARtVista - a novel system integrating AR and <b>generative</b> <b>AI</b> technologies. ARtVista not only recommends reference images aligned with users&rsquo; abstract ideas and generates sketches for users to draw but also goes beyond, crafting vibrant paintings in various painting styles. ARtVista also offers users an alternative approach to create striking paintings by simulating the paint-by-number concept on reference images, empowering users to create visually stunning artwork devoid of the necessity for advanced drawing skills. We perform a pilot study and reveal positive feedback on its usability, emphasizing its effectiveness in visualizing user ideas and aiding the painting process to achieve stunning pictures without requiring advanced drawing skills. The source code will be available at <a href=https://github.com/htrvu/ARtVista>https://github.com/htrvu/ARtVista</a>.</p></p class="citation"></blockquote><h3 id=6085--111310-ambient-diffusion-posterior-sampling-solving-inverse-problems-with-diffusion-models-trained-on-corrupted-data-asad-aali-et-al-2024>(60/85 | 111/310) Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data (Asad Aali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asad Aali, Giannis Daras, Brett Levac, Sidharth Kumar, Alexandros G. Dimakis, Jonathan I. Tamir. (2024)<br><strong>Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data</strong><br><button class=copy-to-clipboard title="Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08728v1.pdf filename=2403.08728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We provide a framework for solving inverse problems with <b>diffusion</b> <b>models</b> learned from linearly corrupted data. Our method, Ambient <b>Diffusion</b> <b>Posterior</b> Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring). We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. We further extend the Ambient <b>Diffusion</b> <b>framework</b> to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8). We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration regime than models trained on fully sampled data. We open-source our code and the trained Ambient <b>Diffusion</b> <b>MRI</b> models: <a href=https://github.com/utcsilab/ambient-diffusion-mri>https://github.com/utcsilab/ambient-diffusion-mri</a> .</p></p class="citation"></blockquote><h3 id=6185--112310-haifit-human-centered-ai-for-fashion-image-translation-jianan-jiang-et-al-2024>(61/85 | 112/310) HAIFIT: Human-Centered AI for Fashion Image Translation (Jianan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianan Jiang, Xinglin Li, Weiren Yu, Di Wu. (2024)<br><strong>HAIFIT: Human-Centered AI for Fashion Image Translation</strong><br><button class=copy-to-clipboard title="HAIFIT: Human-Centered AI for Fashion Image Translation" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08651v1.pdf filename=2403.08651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of fashion design, sketches serve as the canvas for expressing an artist&rsquo;s distinctive drawing style and creative vision, capturing intricate details like stroke variations and texture nuances. The advent of sketch-to-image cross-modal translation technology has notably aided designers. However, existing methods often compromise these sketch details during <b>image</b> <b>generation,</b> resulting in <b>images</b> <b>that</b> deviate from the designer&rsquo;s intended concept. This limitation hampers the ability to offer designers a precise preview of the final output. To overcome this challenge, we introduce HAIFIT, a novel approach that transforms sketches into high-fidelity, lifelike clothing <b>images</b> <b>by</b> integrating multi-scale features and capturing extensive feature map dependencies from diverse perspectives. Through extensive qualitative and quantitative evaluations conducted on our self-collected dataset, our method demonstrates superior performance compared to existing methods in generating photorealistic clothing <b>images.</b> <b>Our</b> method excels in preserving the distinctive style and intricate details essential for fashion design applications.</p></p class="citation"></blockquote><h3 id=6285--113310-scaling-up-dynamic-human-scene-interaction-modeling-nan-jiang-et-al-2024>(62/85 | 113/310) Scaling Up Dynamic Human-Scene Interaction Modeling (Nan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang. (2024)<br><strong>Scaling Up Dynamic Human-Scene Interaction Modeling</strong><br><button class=copy-to-clipboard title="Scaling Up Dynamic Human-Scene Interaction Modeling" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08629v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08629v1.pdf filename=2403.08629v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Confronting the challenges of data scarcity and advanced motion synthesis in human-scene interaction modeling, we introduce the TRUMANS dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates HSI sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable <b>zero-shot</b> generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies.</p></p class="citation"></blockquote><h3 id=6385--114310-aigcs-confuse-ai-too-investigating-and-explaining-synthetic-image-induced-hallucinations-in-large-vision-language-models-yifei-gao-et-al-2024>(63/85 | 114/310) AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models (Yifei Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Gao, Jiaqi Wang, Zhiyu Lin, Jitao Sang. (2024)<br><strong>AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08542v1.pdf filename=2403.08542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of Artificial Intelligence Generated Contents (AIGCs) is advancing towards higher quality. The growing interactions with AIGCs present a new challenge to the data-driven AI community: While AI-generated contents have played a crucial role in a wide range of AI models, the potential hidden risks they introduce have not been thoroughly examined. Beyond human-oriented forgery detection, AI-generated content poses potential issues for AI models originally designed to process natural data. In this study, we underscore the exacerbated hallucination phenomena in Large <b>Vision-Language</b> Models (LVLMs) caused by AI-synthetic images. Remarkably, our findings shed light on a consistent AIGC \textbf{hallucination bias}: the object hallucinations induced by synthetic images are characterized by a greater quantity and a more uniform position distribution, even these synthetic images do not manifest unrealistic or additional relevant visual features compared to natural images. Moreover, our investigations on Q-former and Linear projector reveal that synthetic images may present token deviations after visual projection, thereby amplifying the hallucination bias.</p></p class="citation"></blockquote><h3 id=6485--115310-unilidar-bridge-the-domain-gap-among-different-lidars-for-continual-learning-zikun-xu-et-al-2024>(64/85 | 115/310) UniLiDAR: Bridge the domain gap among different LiDARs for continual learning (Zikun Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zikun Xu, Jianqiang Wang, Shaobing Xu. (2024)<br><strong>UniLiDAR: Bridge the domain gap among different LiDARs for continual learning</strong><br><button class=copy-to-clipboard title="UniLiDAR: Bridge the domain gap among different LiDARs for continual learning" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08512v1.pdf filename=2403.08512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LiDAR-based 3D perception algorithms have evolved rapidly alongside the emergence of large datasets. Nonetheless, considerable performance degradation often ensues when models trained on a specific dataset are applied to other datasets or real-world scenarios with different LiDAR. This paper aims to develop a unified model capable of handling different LiDARs, enabling <b>continual</b> <b>learning</b> across diverse LiDAR datasets and seamless deployment across heterogeneous platforms. We observe that the gaps among datasets primarily manifest in geometric disparities (such as variations in beams and point counts) and semantic inconsistencies (taxonomy conflicts). To this end, this paper proposes UniLiDAR, an occupancy prediction pipeline that leverages geometric realignment and semantic label mapping to facilitate multiple datasets training and mitigate performance degradation during deployment on heterogeneous platforms. Moreover, our method can be easily combined with existing 3D perception models. The efficacy of the proposed approach in bridging LiDAR domain gaps is verified by comprehensive experiments on two prominent datasets: OpenOccupancy-nuScenes and SemanticKITTI. UniLiDAR elevates the mIoU of occupancy prediction by 15.7% and 12.5%, respectively, compared to the model trained on the directly merged dataset. Moreover, it outperforms several SOTA methods trained on individual datasets. We expect our research to facilitate further study of 3D generalization, the code will be available soon.</p></p class="citation"></blockquote><h3 id=6585--116310-gaussian-splatting-in-style-abhishek-saroha-et-al-2024>(65/85 | 116/310) Gaussian Splatting in Style (Abhishek Saroha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers. (2024)<br><strong>Gaussian Splatting in Style</strong><br><button class=copy-to-clipboard title="Gaussian Splatting in Style" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08498v1.pdf filename=2403.08498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene stylization extends the work of neural <b>style</b> <b>transfer</b> to three spatial dimensions. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting. A vast majority of the previous works achieve this by optimizing the scene with a specific <b>style</b> <b>image.</b> In contrast, we propose a novel architecture trained on a collection of <b>style</b> <b>images,</b> that at test time produces high quality stylized novel views. Our work builds up on the framework of 3D Gaussian splatting. For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views. The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime. This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications. Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data.</p></p class="citation"></blockquote><h3 id=6685--117310-noisediffusion-correcting-noise-for-image-interpolation-with-diffusion-models-beyond-spherical-linear-interpolation-pengfei-zheng-et-al-2024>(66/85 | 117/310) NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation (PengFei Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>PengFei Zheng, Yonggang Zhang, Zhen Fang, Tongliang Liu, Defu Lian, Bo Han. (2024)<br><strong>NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation</strong><br><button class=copy-to-clipboard title="NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08840v1.pdf filename=2403.08840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image interpolation based on <b>diffusion</b> <b>models</b> is promising in creating fresh and interesting images. Advanced interpolation methods mainly focus on spherical linear interpolation, where images are encoded into the noise space and then interpolated for denoising to images. However, existing methods face challenges in effectively interpolating natural images (not generated by <b>diffusion</b> <b>models),</b> thereby restricting their practical applicability. Our experimental investigations reveal that these challenges stem from the invalidity of the encoding noise, which may no longer obey the expected noise distribution, e.g., a normal distribution. To address these challenges, we propose a novel approach to correct noise for image interpolation, NoiseDiffusion. Specifically, NoiseDiffusion approaches the invalid noise to the expected distribution by introducing subtle Gaussian noise and introduces a constraint to suppress noise with extreme values. In this context, promoting noise validity contributes to mitigating image artifacts, but the constraint and introduced exogenous noise typically lead to a reduction in signal-to-noise ratio, i.e., loss of original image information. Hence, NoiseDiffusion performs interpolation within the noisy image space and injects raw images into these noisy counterparts to address the challenge of information loss. Consequently, NoiseDiffusion enables us to interpolate natural images without causing artifacts or information loss, thus achieving the best interpolation results.</p></p class="citation"></blockquote><h3 id=6785--118310-pfstorer-personalized-face-restoration-and-super-resolution-tuomas-varanka-et-al-2024>(67/85 | 118/310) PFStorer: Personalized Face Restoration and Super-Resolution (Tuomas Varanka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuomas Varanka, Tapani Toivonen, Soumya Tripathy, Guoying Zhao, Erman Acar. (2024)<br><strong>PFStorer: Personalized Face Restoration and Super-Resolution</strong><br><button class=copy-to-clipboard title="PFStorer: Personalized Face Restoration and Super-Resolution" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08436v1.pdf filename=2403.08436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in face restoration have achieved remarkable results in producing high-quality and lifelike outputs. The stunning results however often fail to be faithful with respect to the identity of the person as the models lack necessary context. In this paper, we explore the potential of personalized face restoration with <b>diffusion</b> <b>models.</b> In our approach a restoration model is personalized using a few images of the identity, leading to tailored restoration with respect to the identity while retaining fine-grained details. By using independent trainable blocks for personalization, the rich prior of a base restoration model can be exploited to its fullest. To avoid the model relying on parts of identity left in the conditioning low-quality images, a generative regularizer is employed. With a learnable parameter, the model learns to balance between the details generated based on the input image and the degree of personalization. Moreover, we improve the training pipeline of face restoration models to enable an alignment-free approach. We showcase the robust capabilities of our approach in several real-world scenarios with multiple identities, demonstrating our method&rsquo;s ability to generate fine-grained details with faithful restoration. In the user study we evaluate the perceptual quality and faithfulness of the genereated details, with our method being voted best 61% of the time compared to the second best with 25% of the votes.</p></p class="citation"></blockquote><h3 id=6885--119310-low-cost-and-real-time-industrial-human-action-recognitions-based-on-large-scale-foundation-models-wensheng-liang-et-al-2024>(68/85 | 119/310) Low-Cost and Real-Time Industrial Human Action Recognitions Based on Large-Scale Foundation Models (Wensheng Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wensheng Liang, Ruiyan Zhuang, Xianwei Shi, Shuai Li, Zhicheng Wang, Xiaoguang Ma. (2024)<br><strong>Low-Cost and Real-Time Industrial Human Action Recognitions Based on Large-Scale Foundation Models</strong><br><button class=copy-to-clipboard title="Low-Cost and Real-Time Industrial Human Action Recognitions Based on Large-Scale Foundation Models" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08420v1.pdf filename=2403.08420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industrial managements, including quality control, cost and safety optimization, etc., heavily rely on high quality industrial human action recognitions (IHARs) which were hard to be implemented in large-scale industrial scenes due to their high costs and poor real-time performance. In this paper, we proposed a large-scale <b>foundation</b> <b>model(LSFM)-based</b> IHAR method, wherein various LSFMs and lightweight methods were jointly used, for the first time, to fulfill low-cost dataset establishment and real-time IHARs. Comprehensive tests on in-situ large-scale industrial manufacturing lines elucidated that the proposed method realized great reduction on employment costs, superior real-time performance, and satisfactory accuracy and generalization capabilities, indicating its great potential as a backbone IHAR method, especially for large-scale industrial applications.</p></p class="citation"></blockquote><h3 id=6985--120310-tackling-the-singularities-at-the-endpoints-of-time-intervals-in-diffusion-models-pengze-zhang-et-al-2024>(69/85 | 120/310) Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models (Pengze Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengze Zhang, Hubery Yin, Chen Li, Xiaohua Xie. (2024)<br><strong>Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models</strong><br><button class=copy-to-clipboard title="Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08381v1.pdf filename=2403.08381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most <b>diffusion</b> <b>models</b> assume that the reverse process adheres to a Gaussian distribution. However, this approximation has not been rigorously validated, especially at singularities, where t=0 and t=1. Improperly dealing with such singularities leads to an average brightness issue in applications, and limits the generation of images with extreme brightness or darkness. We primarily focus on tackling singularities from both theoretical and practical perspectives. Initially, we establish the error bounds for the reverse process approximation, and showcase its Gaussian characteristics at singularity time steps. Based on this theoretical insight, we confirm the singularity at t=1 is conditionally removable while it at t=0 is an inherent property. Upon these significant conclusions, we propose a novel plug-and-play method SingDiffusion to address the initial singular time step sampling, which not only effectively resolves the average brightness issue for a wide range of <b>diffusion</b> <b>models</b> without extra training efforts, but also enhances their generation capability in achieving notable lower FID scores. Code and models are released at <a href=https://github.com/PangzeCheung/SingDiffusion>https://github.com/PangzeCheung/SingDiffusion</a>.</p></p class="citation"></blockquote><h3 id=7085--121310-mitigate-target-level-insensitivity-of-infrared-small-target-detection-via-posterior-distribution-modeling-haoqing-li-et-al-2024>(70/85 | 121/310) Mitigate Target-level Insensitivity of Infrared Small Target Detection via Posterior Distribution Modeling (Haoqing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoqing Li, Jinfu Yang, Yifei Xu, Runshi Wang. (2024)<br><strong>Mitigate Target-level Insensitivity of Infrared Small Target Detection via Posterior Distribution Modeling</strong><br><button class=copy-to-clipboard title="Mitigate Target-level Insensitivity of Infrared Small Target Detection via Posterior Distribution Modeling" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08380v1.pdf filename=2403.08380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Infrared Small Target Detection (IRSTD) aims to segment small targets from infrared clutter background. Existing methods mainly focus on discriminative approaches, i.e., a pixel-level front-background binary segmentation. Since infrared small targets are small and low signal-to-clutter ratio, empirical risk has few disturbances when a certain false alarm and missed detection exist, which seriously affect the further improvement of such methods. Motivated by the dense prediction generative methods, in this paper, we propose a <b>diffusion</b> <b>model</b> framework for Infrared Small Target Detection which compensates pixel-level discriminant with mask posterior distribution modeling. Furthermore, we design a Low-frequency Isolation in the wavelet domain to suppress the interference of intrinsic infrared noise on the <b>diffusion</b> <b>noise</b> estimation. This transition from the discriminative paradigm to generative one enables us to bypass the target-level insensitivity. Experiments show that the proposed method achieves competitive performance gains over state-of-the-art methods on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets. Code are available at <a href=https://github.com/Li-Haoqing/IRSTD-Diff>https://github.com/Li-Haoqing/IRSTD-Diff</a>.</p></p class="citation"></blockquote><h3 id=7185--122310-improved-image-based-pose-regressor-models-for-underwater-environments-luyuan-peng-et-al-2024>(71/85 | 122/310) Improved Image-based Pose Regressor Models for Underwater Environments (Luyuan Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luyuan Peng, Hari Vishnu, Mandar Chitre, Yuen Min Too, Bharath Kalyan, Rajat Mishra. (2024)<br><strong>Improved Image-based Pose Regressor Models for Underwater Environments</strong><br><button class=copy-to-clipboard title="Improved Image-based Pose Regressor Models for Underwater Environments" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08360v1.pdf filename=2403.08360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the performance of image-based pose regressor models in underwater environments for relocalization. Leveraging PoseNet and PoseLSTM, we regress a 6-degree-of-freedom pose from single RGB images with high accuracy. Additionally, we explore <b>data</b> <b>augmentation</b> with stereo camera images to improve model accuracy. Experimental results demonstrate that the models achieve high accuracy in both simulated and clear waters, promising effective real-world underwater navigation and inspection applications.</p></p class="citation"></blockquote><h3 id=7285--123310-identity-aware-dual-constraint-network-for-cloth-changing-person-re-identification-peini-guo-et-al-2024>(72/85 | 123/310) Identity-aware Dual-constraint Network for Cloth-Changing Person Re-identification (Peini Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peini Guo, Mengyuan Liu, Hong Liu, Ruijia Fan, Guoquan Wang, Bin He. (2024)<br><strong>Identity-aware Dual-constraint Network for Cloth-Changing Person Re-identification</strong><br><button class=copy-to-clipboard title="Identity-aware Dual-constraint Network for Cloth-Changing Person Re-identification" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08270v1.pdf filename=2403.08270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify the target person in more realistic surveillance scenarios, where pedestrians usually change their clothing. Despite great progress, limited cloth-changing training samples in existing CC-ReID datasets still prevent the model from adequately learning cloth-irrelevant features. In addition, due to the absence of explicit supervision to keep the model constantly focused on cloth-irrelevant areas, existing methods are still hampered by the disruption of clothing variations. To solve the above issues, we propose an Identity-aware Dual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the model extract cloth-irrelevant clues, we propose a Clothes Diversity Augmentation (CDA), which generates more realistic cloth-changing samples by enriching the clothing color while preserving the texture. In addition, a Multi-scale Constraint Block (MCB) is designed, which extracts fine-grained identity-related features and effectively transfers cloth-irrelevant knowledge. Moreover, a <b>Counterfactual-guided</b> Attention Module (CAM) is presented, which learns cloth-irrelevant features from channel and space dimensions and utilizes the <b>counterfactual</b> intervention for supervising the attention map to highlight identity-related regions. Finally, a Semantic Alignment Constraint (SAC) is designed to facilitate high-level semantic feature interaction. Comprehensive experiments on four CC-ReID datasets indicate that our method outperforms prior state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=7385--124310-follow-your-click-open-domain-regional-image-animation-via-short-prompts-yue-ma-et-al-2024>(73/85 | 124/310) Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts (Yue Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, Qifeng Chen. (2024)<br><strong>Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts</strong><br><button class=copy-to-clipboard title="Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08268v1.pdf filename=2403.08268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent advances in image-to-video generation, better controllability and local animation are less explored. Most existing image-to-video methods are not locally aware and tend to move the entire scene. However, human artists may need to control the movement of different objects or regions. Additionally, current I2V methods require users not only to describe the target motion but also to provide redundant detailed descriptions of frame contents. These two issues hinder the practical utilization of current I2V tools. In this paper, we propose a practical framework, named Follow-Your-Click, to achieve image animation with a simple user click (for specifying what to move) and a short motion <b>prompt</b> (for specifying how to move). Technically, we propose the first-frame masking strategy, which significantly improves the video generation quality, and a motion-augmented module equipped with a short motion <b>prompt</b> dataset to improve the short <b>prompt</b> following abilities of our model. To further control the motion speed, we propose flow-based motion magnitude control to control the speed of target movement more precisely. Our framework has simpler yet precise user control and better generation performance than previous methods. Extensive experiments compared with 7 baselines, including both commercial tools and research methods on 8 metrics, suggest the superiority of our approach. Project Page: <a href=https://follow-your-click.github.io/>https://follow-your-click.github.io/</a></p></p class="citation"></blockquote><h3 id=7485--125310-sketch2manga-shaded-manga-screening-from-sketch-with-diffusion-models-jian-lin-et-al-2024>(74/85 | 125/310) Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models (Jian Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Lin, Xueting Liu, Chengze Li, Minshan Xie, Tien-Tsin Wong. (2024)<br><strong>Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models</strong><br><button class=copy-to-clipboard title="Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-6; I-3-3; I-3-8, cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08266v1.pdf filename=2403.08266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While manga is a popular entertainment form, creating manga is tedious, especially adding screentones to the created sketch, namely manga screening. Unfortunately, there is no existing method that tailors for automatic manga screening, probably due to the difficulty of generating high-quality shaded high-frequency screentones. The classic manga screening approaches generally require user input to provide screentone exemplars or a reference manga image. The recent deep learning models enables the automatic generation by learning from a large-scale dataset. However, the state-of-the-art models still fail to generate high-quality shaded screentones due to the lack of a tailored model and high-quality manga training data. In this paper, we propose a novel sketch-to-manga framework that first generates a color illustration from the sketch and then generates a screentoned manga based on the intensity guidance. Our method significantly outperforms existing methods in generating high-quality manga with shaded high-frequency screentones.</p></p class="citation"></blockquote><h3 id=7585--126310-p2lhapwearable-sensor-based-human-activity-recognition-segmentation-and-forecast-through-patch-to-label-seq2seq-transformer-shuangjian-li-et-al-2024>(75/85 | 126/310) P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer (Shuangjian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuangjian Li, Tao Zhu, Mingxing Nie, Huansheng Ning, Zhenyu Liu, Liming Chen. (2024)<br><strong>P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer</strong><br><button class=copy-to-clipboard title="P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08214v1.pdf filename=2403.08214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of &ldquo;patches&rdquo;, served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent <b>Transformer</b> encoders and decoders. All channels share embedding and <b>Transformer</b> weights across all sequences. Evaluated on three public datasets, P2LHAP significantly outperforms the state-of-the-art in all three tasks, demonstrating its effectiveness and potential for real-world applications.</p></p class="citation"></blockquote><h3 id=7685--127310-versatile-defense-against-adversarial-attacks-on-image-recognition-haibo-zhang-et-al-2024>(76/85 | 127/310) Versatile Defense Against Adversarial Attacks on Image Recognition (Haibo Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haibo Zhang, Zhihua Yao, Kouichi Sakurai. (2024)<br><strong>Versatile Defense Against Adversarial Attacks on Image Recognition</strong><br><button class=copy-to-clipboard title="Versatile Defense Against Adversarial Attacks on Image Recognition" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08170v1.pdf filename=2403.08170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>attacks</b> present a significant security risk to image recognition tasks. Defending against these attacks in a real-life setting can be compared to the way antivirus software works, with a key consideration being how well the defense can adapt to new and evolving attacks. Another important factor is the resources involved in terms of time and cost for training defense models and updating the model database. Training many models that are specific to each type of attack can be time-consuming and expensive. Ideally, we should be able to train one single model that can handle a wide range of attacks. It appears that a defense method based on image-to-image translation may be capable of this. The proposed versatile defense approach in this paper only requires training one model to effectively resist various unknown <b>adversarial</b> <b>attacks.</b> The trained model has successfully improved the classification accuracy from nearly zero to an average of 86%, performing better than other defense methods proposed in prior studies. When facing the PGD attack and the MI-FGSM attack, versatile defense model even outperforms the attack-specific models trained based on these two attacks. The robustness check also shows that our versatile defense model performs stably regardless with the attack strength.</p></p class="citation"></blockquote><h3 id=7785--128310-fastmac-stochastic-spectral-sampling-of-correspondence-graph-yifei-zhang-et-al-2024>(77/85 | 128/310) FastMAC: Stochastic Spectral Sampling of Correspondence Graph (Yifei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Zhang, Hao Zhao, Hongyang Li, Siheng Chen. (2024)<br><strong>FastMAC: Stochastic Spectral Sampling of Correspondence Graph</strong><br><button class=copy-to-clipboard title="FastMAC: Stochastic Spectral Sampling of Correspondence Graph" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08770v1.pdf filename=2403.08770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision. A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence <b>graph.</b> This <b>graph</b> is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC). However, its properties have not been well understood. So we present the first study that introduces <b>graph</b> signal processing into the domain of correspondence <b>graph.</b> We exploit the generalized degree signal on correspondence <b>graph</b> and pursue sampling strategies that preserve high-frequency components of this signal. To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy. As such, the core of our method is the stochastic spectral sampling of correspondence <b>graph.</b> As an application, we build a complete 3D registration algorithm termed as FastMAC, that reaches real-time speed while leading to little to none performance drop. Through extensive experiments, we validate that FastMAC works for both indoor and outdoor <b>benchmarks.</b> For example, FastMAC can accelerate MAC by 80 times while maintaining high registration success rate on KITTI. Codes are publicly available at <a href=https://github.com/Forrest-110/FastMAC>https://github.com/Forrest-110/FastMAC</a>.</p></p class="citation"></blockquote><h3 id=7885--129310-representing-anatomical-trees-by-denoising-diffusion-of-implicit-neural-fields-ashish-sinha-et-al-2024>(78/85 | 129/310) Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields (Ashish Sinha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashish Sinha, Ghassan Hamarneh. (2024)<br><strong>Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields</strong><br><button class=copy-to-clipboard title="Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08974v1.pdf filename=2403.08974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anatomical trees play a central role in clinical diagnosis and treatment planning. However, accurately representing anatomical trees is challenging due to their varying and complex topology and <b>geometry.</b> Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency. Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently. We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs. We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution. Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reconstruction with arbitrary resolution yet compact storage, and versatility across anatomical sites and tree complexities.</p></p class="citation"></blockquote><h3 id=7985--130310-slcf-net-sequential-lidar-camera-fusion-for-semantic-scene-completion-using-a-3d-recurrent-u-net-helin-cao-et-al-2024>(79/85 | 130/310) SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net (Helin Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Helin Cao, Sven Behnke. (2024)<br><strong>SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net</strong><br><button class=copy-to-clipboard title="SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08885v1.pdf filename=2403.08885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates missing <b>geometry</b> and semantics in a scene from sequences of RGB images and sparse LiDAR measurements. The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP). This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency. We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics and shows great temporal consistency.</p></p class="citation"></blockquote><h3 id=8085--131310-3dfires-few-image-3d-reconstruction-for-scenes-with-hidden-surface-linyi-jin-et-al-2024>(80/85 | 131/310) 3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface (Linyi Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linyi Jin, Nilesh Kulkarni, David Fouhey. (2024)<br><strong>3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface</strong><br><button class=copy-to-clipboard title="3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08768v1.pdf filename=2403.08768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces 3DFIRES, a novel system for scene-level 3D reconstruction from posed images. Designed to work with as few as one view, 3DFIRES reconstructs the complete <b>geometry</b> of unseen scenes, including hidden surfaces. With multiple view inputs, our method produces full reconstruction within all camera frustums. A key feature of our approach is the fusion of multi-view information at the feature level, enabling the production of coherent and comprehensive 3D reconstruction. We train our system on non-watertight scans from large-scale real scene dataset. We show it matches the efficacy of single-view reconstruction methods with only one input and surpasses existing techniques in both quantitative and qualitative measures for sparse-view 3D reconstruction.</p></p class="citation"></blockquote><h3 id=8185--132310-himap-hybrid-representation-learning-for-end-to-end-vectorized-hd-map-construction-yi-zhou-et-al-2024>(81/85 | 132/310) HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction (Yi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Zhou, Hui Zhang, Jiaqian Yu, Yifan Yang, Sangil Jung, Seung-In Park, ByungIn Yoo. (2024)<br><strong>HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction</strong><br><button class=copy-to-clipboard title="HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08639v1.pdf filename=2403.08639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vectorized High-Definition (HD) map construction requires predictions of the category and point coordinates of map elements (e.g. road boundary, lane divider, pedestrian crossing, etc.). State-of-the-art methods are mainly based on point-level <b>representation</b> <b>learning</b> for regressing accurate point coordinates. However, this pipeline has limitations in obtaining element-level information and handling element-level failures, e.g. erroneous element shape or entanglement between elements. To tackle the above issues, we propose a simple yet effective HybrId framework named HIMap to sufficiently learn and interact both point-level and element-level information. Concretely, we introduce a hybrid <b>representation</b> <b>called</b> HIQuery to represent all map elements, and propose a point-element interactor to interactively extract and encode the hybrid information of elements, e.g. point position and element shape, into the HIQuery. Additionally, we present a point-element consistency constraint to enhance the consistency between the point-level and element-level information. Finally, the output point-element integrated HIQuery can be directly converted into map elements&rsquo; class, point coordinates, and mask. We conduct extensive experiments and consistently outperform previous methods on both nuScenes and Argoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes dataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.</p></p class="citation"></blockquote><h3 id=8285--133310-occfiner-offboard-occupancy-refinement-with-hybrid-propagation-hao-shi-et-al-2024>(82/85 | 133/310) OccFiner: Offboard Occupancy Refinement with Hybrid Propagation (Hao Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Shi, Song Wang, Jiaming Zhang, Xiaoting Yin, Zhongdao Wang, Zhijian Zhao, Guangming Wang, Jianke Zhu, Kailun Yang, Kaiwei Wang. (2024)<br><strong>OccFiner: Offboard Occupancy Refinement with Hybrid Propagation</strong><br><button class=copy-to-clipboard title="OccFiner: Offboard Occupancy Refinement with Hybrid Propagation" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV, eess-IV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08504v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08504v2.pdf filename=2403.08504v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision-based occupancy prediction, also known as 3D Semantic Scene Completion (SSC), presents a significant challenge in computer vision. Previous methods, confined to onboard processing, struggle with simultaneous geometric and semantic estimation, continuity across varying viewpoints, and single-view occlusion. Our paper introduces OccFiner, a novel offboard framework designed to enhance the accuracy of vision-based occupancy predictions. OccFiner operates in two hybrid phases: 1) a multi-to-multi local propagation network that implicitly aligns and processes multiple local frames for correcting onboard model errors and consistently enhancing occupancy accuracy across all distances. 2) the region-centric global propagation, focuses on refining labels using explicit multi-view <b>geometry</b> and integrating sensor bias, especially to increase the accuracy of distant occupied voxels. Extensive experiments demonstrate that OccFiner improves both geometric and semantic accuracy across various types of coarse occupancy, setting a new state-of-the-art performance on the SemanticKITTI dataset. Notably, OccFiner elevates vision-based SSC models to a level even surpassing that of LiDAR-based onboard SSC models.</p></p class="citation"></blockquote><h3 id=8385--134310-drfer-learning-disentangled-representations-for-3d-facial-expression-recognition-hebeizi-li-et-al-2024>(83/85 | 134/310) DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition (Hebeizi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hebeizi Li, Hongyu Yang, Di Huang. (2024)<br><strong>DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition</strong><br><button class=copy-to-clipboard title="DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08318v1.pdf filename=2403.08318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial Expression Recognition (FER) has consistently been a focal point in the field of facial analysis. In the context of existing methodologies for 3D FER or 2D+3D FER, the extraction of expression features often gets entangled with identity information, compromising the distinctiveness of these features. To tackle this challenge, we introduce the innovative DrFER method, which brings the concept of disentangled <b>representation</b> <b>learning</b> to the field of 3D FER. DrFER employs a dual-branch framework to effectively disentangle expression information from identity information. Diverging from prior disentanglement endeavors in the 3D facial domain, we have carefully reconfigured both the loss functions and network structure to make the overall framework adaptable to point cloud data. This adaptation enhances the capability of the framework in recognizing facial expressions, even in cases involving varying head poses. Extensive evaluations conducted on the BU-3DFE and Bosphorus datasets substantiate that DrFER surpasses the performance of other 3D FER methods.</p></p class="citation"></blockquote><h3 id=8485--135310-prago-differentiable-multi-view-pose-optimization-from-objectness-detections-matteo-taiana-et-al-2024>(84/85 | 135/310) PRAGO: Differentiable Multi-View Pose Optimization From Objectness Detections (Matteo Taiana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Taiana, Matteo Toso, Stuart James, Alessio Del Bue. (2024)<br><strong>PRAGO: Differentiable Multi-View Pose Optimization From Objectness Detections</strong><br><button class=copy-to-clipboard title="PRAGO: Differentiable Multi-View Pose Optimization From Objectness Detections" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08586v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08586v2.pdf filename=2403.08586v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robustly estimating camera poses from a set of images is a fundamental task which remains challenging for differentiable methods, especially in the case of small and sparse camera pose <b>graphs.</b> To overcome this challenge, we propose Pose-refined Rotation Averaging <b>Graph</b> Optimization (PRAGO). From a set of objectness detections on unordered images, our method reconstructs the rotational pose, and in turn, the absolute pose, in a differentiable manner benefiting from the optimization of a sequence of geometrical tasks. We show how our objectness pose-refinement module in PRAGO is able to refine the inherent ambiguities in pairwise relative pose estimation without removing edges and avoiding making early decisions on the viability of <b>graph</b> edges. PRAGO then refines the absolute rotations through iterative <b>graph</b> construction, reweighting the <b>graph</b> edges to compute the final rotational pose, which can be converted into absolute poses using translation averaging. We show that PRAGO is able to outperform non-differentiable solvers on small and sparse scenes extracted from 7-Scenes achieving a relative improvement of 21% for rotations while achieving similar translation estimates.</p></p class="citation"></blockquote><h3 id=8585--136310-better-fit-accommodate-variations-in-clothing-types-for-virtual-try-on-xuanpu-zhang-et-al-2024>(85/85 | 136/310) Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on (Xuanpu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanpu Zhang, Dan Song, Pengxin Zhan, Qingguo Chen, Kuilong Liu, Anan Liu. (2024)<br><strong>Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on</strong><br><button class=copy-to-clipboard title="Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08453v1.pdf filename=2403.08453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image-based virtual try-on aims to transfer target in-shop clothing to a dressed model image, the objectives of which are totally taking off original clothing while preserving the contents outside of the try-on area, naturally wearing target clothing and correctly inpainting the gap between target clothing and original clothing. Tremendous efforts have been made to facilitate this popular research area, but cannot keep the type of target clothing with the try-on area affected by original clothing. In this paper, we focus on the unpaired virtual try-on situation where target clothing and original clothing on the model are different, i.e., the practical scenario. To break the correlation between the try-on area and the original clothing and make the model learn the correct information to inpaint, we propose an adaptive mask training paradigm that dynamically adjusts training masks. It not only improves the alignment and fit of clothing but also significantly enhances the fidelity of virtual try-on experience. Furthermore, we for the first time propose two metrics for unpaired try-on evaluation, the Semantic-Densepose-Ratio (SDR) and Skeleton-LPIPS (S-LPIPS), to evaluate the correctness of clothing type and the accuracy of clothing texture. For unpaired try-on validation, we construct a comprehensive cross-try-on <b>benchmark</b> (Cross-27) with distinctive clothing items and model physiques, covering a broad try-on scenarios. Experiments demonstrate the effectiveness of the proposed methods, contributing to the advancement of virtual try-on technology and offering new insights and tools for future research in the field. The code, model and <b>benchmark</b> will be publicly released.</p></p class="citation"></blockquote><h2 id=cssd-3>cs.SD (3)</h2><h3 id=13--137310-em-tts-efficiently-trained-low-resource-mongolian-lightweight-text-to-speech-ziqi-liang-et-al-2024>(1/3 | 137/310) EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech (Ziqi Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqi Liang, Haoxiang Shi, Jiawei Wang, Keda Lu. (2024)<br><strong>EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech</strong><br><button class=copy-to-clipboard title="EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 90<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Low-Resource, Recurrent Neural Network, Recurrent Neural Network, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08164v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08164v2.pdf filename=2403.08164v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, deep learning-based <b>Text-to-Speech</b> <b>(TTS)</b> systems have achieved high-quality speech synthesis results. <b>Recurrent</b> <b>neural</b> <b>networks</b> have become a standard modeling technique for sequential <b>data</b> <b>in</b> <b>TTS</b> systems and are widely used. However, training a <b>TTS</b> model which includes <b>RNN</b> components requires powerful GPU performance and takes a long time. In contrast, <b>CNN-based</b> sequence synthesis techniques can significantly reduce the parameters and training time of a <b>TTS</b> model while guaranteeing a certain performance due to their high parallelism, which alleviate these economic costs of training. In this paper, we propose a lightweight <b>TTS</b> system based on deep <b>convolutional</b> <b>neural</b> <b>networks,</b> which is a two-stage training end-to-end <b>TTS</b> model and does not employ any <b>recurrent</b> <b>units.</b> <b>Our</b> model consists of two stages: Text2Spectrum and SSRN. The former is used to encode phonemes into a coarse mel spectrogram and the latter is used to synthesize the complete spectrum from the coarse mel spectrogram. Meanwhile, we improve the robustness of our model by a series of <b>data</b> <b>augmentations,</b> such as noise suppression, time warping, frequency masking and time masking, for solving the low resource mongolian problem. Experiments show that our model can reduce the training time and parameters while ensuring the quality and naturalness of the synthesized speech compared to using mainstream <b>TTS</b> models. Our method uses NCMMSC2022-MTTSC Challenge dataset for validation, which significantly reduces training time while maintaining a certain accuracy.</p></p class="citation"></blockquote><h3 id=23--138310-end-to-end-amp-modeling-from-data-to-controllable-guitar-amplifier-models-lauri-juvela-et-al-2024>(2/3 | 138/310) End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier Models (Lauri Juvela et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lauri Juvela, Eero-Pekka Damskägg, Aleksi Peussa, Jaakko Mäkinen, Thomas Sherson, Stylianos I. Mimilakis, Athanasios Gotsopoulos. (2024)<br><strong>End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier Models</strong><br><button class=copy-to-clipboard title="End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier Models" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 35<br>Keywords: Black Box, Simulation, Simulator, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08559v1.pdf filename=2403.08559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes a data-driven approach to creating real-time neural network models of guitar amplifiers, recreating the amplifiers&rsquo; sonic response to arbitrary inputs at the full range of controls present on the physical device. While the focus on the paper is on the data collection pipeline, we demonstrate the effectiveness of this conditioned <b>black-box</b> <b>approach</b> by training an <b>LSTM</b> model to the task, and comparing its performance to an offline white-box SPICE circuit <b>simulation.</b> Our listening test results demonstrate that the neural amplifier modeling approach can match the subjective performance of a high-quality SPICE model, all while using an automated, non-intrusive data collection process, and an end-to-end trainable, real-time feasible neural network model.</p></p class="citation"></blockquote><h3 id=33--139310-from-weak-to-strong-sound-event-labels-using-adaptive-change-point-detection-and-active-learning-john-martinsson-et-al-2024>(3/3 | 139/310) From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning (John Martinsson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John Martinsson, Olof Mogren, Maria Sandsten, Tuomas Virtanen. (2024)<br><strong>From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning</strong><br><button class=copy-to-clipboard title="From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08525v1.pdf filename=2403.08525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. The goal is to maximize the amount of information gained about the temporal activation&rsquo;s of the target sounds. For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation. The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset. The prediction model then gradually adapts to the annotations provided by the annotator in an <b>active</b> <b>learning</b> loop. The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favorable results for A-CPD when compared to two baseline query strategies.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=11--140310-scvgae-a-novel-approach-using-zinb-based-variational-graph-autoencoder-for-single-cell-rna-seq-imputation-yoshitaka-inoue-2024>(1/1 | 140/310) scVGAE: A Novel Approach using ZINB-Based Variational Graph Autoencoder for Single-Cell RNA-Seq Imputation (Yoshitaka Inoue, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoshitaka Inoue. (2024)<br><strong>scVGAE: A Novel Approach using ZINB-Based Variational Graph Autoencoder for Single-Cell RNA-Seq Imputation</strong><br><button class=copy-to-clipboard title="scVGAE: A Novel Approach using ZINB-Based Variational Graph Autoencoder for Single-Cell RNA-Seq Imputation" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-CE, q-bio-GN, q-bio.GN<br>Keyword Score: 86<br>Keywords: Graph Attention Networks, Graph Attention Networks, Graph Convolutional Network, Graph Convolutional Network, Graph, Autoencoder, Clustering, Convolution, Convolutional Neural Network, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08959v1.pdf filename=2403.08959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to study individual cellular distinctions and uncover unique cell characteristics. However, a significant technical challenge in scRNA-seq analysis is the occurrence of &ldquo;dropout&rdquo; events, where certain gene expressions cannot be detected. This issue is particularly pronounced in genes with low or sparse expression levels, impacting the precision and interpretability of the obtained data. To address this challenge, various imputation methods have been implemented to predict such missing values, aiming to enhance the analysis&rsquo;s accuracy and usefulness. A prevailing hypothesis posits that scRNA-seq data conforms to a zero-inflated negative binomial (ZINB) distribution. Consequently, methods have been developed to model the data according to this distribution. Recent trends in scRNA-seq analysis have seen the emergence of deep learning approaches. Some techniques, such as the <b>variational</b> <b>autoencoder,</b> incorporate the ZINB distribution as a model loss function. <b>Graph-based</b> <b>methods</b> <b>like</b> <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCN)</b> and <b>Graph</b> <b>Attention</b> <b>Networks</b> <b>(GAT)</b> have also gained attention as deep learning methodologies for scRNA-seq analysis. This study introduces scVGAE, an innovative approach integrating <b>GCN</b> into a <b>variational</b> <b>autoencoder</b> framework while utilizing a ZINB loss function. This integration presents a promising avenue for effectively addressing dropout events in scRNA-seq data, thereby enhancing the accuracy and reliability of downstream analyses. scVGAE outperforms other methods in cell <b>clustering,</b> with the best performance in 11 out of 14 datasets. Ablation study shows all components of scVGAE are necessary. scVGAE is implemented in Python and downloadable at <a href=https://github.com/inoue0426/scVGAE>https://github.com/inoue0426/scVGAE</a>.</p></p class="citation"></blockquote><h2 id=eessiv-13>eess.IV (13)</h2><h3 id=113--141310-segmentation-of-knee-bones-for-osteoarthritis-assessment-a-comparative-analysis-of-supervised-few-shot-and-zero-shot-learning-approaches-yun-xin-teoh-et-al-2024>(1/13 | 141/310) Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches (Yun Xin Teoh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai. (2024)<br><strong>Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches</strong><br><button class=copy-to-clipboard title="Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, Supervised Learning, Zero-shot, Morphological Analysis, Prompt, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08761v1.pdf filename=2403.08761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knee osteoarthritis is a degenerative joint disease that induces chronic pain and disability. Bone <b>morphological</b> <b>analysis</b> is a promising tool to understand the mechanical aspect of this disorder. This study proposes a 2D bone <b>morphological</b> <b>analysis</b> using manually segmented bones to explore <b>morphological</b> <b>features</b> related to distinct pain conditions. Furthermore, six semantic segmentation algorithms are assessed for extracting femur and tibia bones from X-ray images. Our analysis reveals that the morphology of the femur undergoes significant changes in instances where pain worsens. Conversely, improvements in pain may not manifest pronounced alterations in bone shape. The <b>few-shot-learning-based</b> <b>algorithm,</b> UniverSeg, demonstrated superior segmentation results with Dice scores of 99.69% for femur and 99.60% for tibia. Regarding pain condition classification, the <b>zero-shot-learning-based</b> <b>algorithm,</b> CP-SAM, achieved the highest accuracy at 66% among all models. UniverSeg is recommended for automatic knee bone segmentation, while SAM models show potential with <b>prompt</b> encoder modifications for optimized outcomes. These findings highlight the effectiveness of <b>few-shot</b> <b>learning</b> for semantic segmentation and the potential of <b>zero-shot</b> <b>learning</b> in enhancing classification models for knee osteoarthritis diagnosis.</p></p class="citation"></blockquote><h3 id=213--142310-7t-mri-synthesization-from-3t-acquisitions-qiming-cui-et-al-2024>(2/13 | 142/310) 7T MRI Synthesization from 3T Acquisitions (Qiming Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiming Cui, Duygu Tosun, Reza Abbasi-Asl. (2024)<br><strong>7T MRI Synthesization from 3T Acquisitions</strong><br><button class=copy-to-clipboard title="7T MRI Synthesization from 3T Acquisitions" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 43<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Data Augmentation, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08979v1.pdf filename=2403.08979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> deep learning techniques can be used to generate synthetic 7T MRIs from 3T MRI inputs. This image enhancement process leverages the advantages of ultra-high-field MRI to improve the signal-to-noise and contrast-to-noise ratios of 3T acquisitions. In this paper, we introduce multiple novel 7T synthesization algorithms based on custom-designed variants of the V-Net <b>convolutional</b> <b>neural</b> <b>network.</b> We demonstrate that the V-Net based model has superior performance in enhancing both single-site and multi-site MRI datasets compared to the existing <b>benchmark</b> model. When trained on 3T-7T MRI pairs from 8 subjects with mild Traumatic Brain Injury (TBI), our model achieves state-of-the-art 7T synthesization performance. Compared to previous works, synthetic 7T images generated from our pipeline also display superior enhancement of pathological tissue. Additionally, we implement and test a <b>data</b> <b>augmentation</b> scheme for training models that are robust to variations in the input distribution. This allows synthetic 7T models to accommodate intra-scanner and inter-scanner variability in multisite datasets. On a harmonized dataset consisting of 18 3T-7T MRI pairs from two institutions, including both healthy subjects and those with mild TBI, our model maintains its performance and can generalize to 3T MRI inputs with lower resolution. Our findings demonstrate the promise of V-Net based models for MRI enhancement and offer a preliminary probe into improving the generalizability of synthetic 7T models with <b>data</b> <b>augmentation.</b></p></p class="citation"></blockquote><h3 id=313--143310-md-dose-a-diffusion-model-based-on-the-mamba-for-radiotherapy-dose-prediction-linjie-fu-et-al-2024>(3/13 | 143/310) MD-Dose: A Diffusion Model based on the Mamba for Radiotherapy Dose Prediction (Linjie Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linjie Fu, Xia Li, Xiuding Cai, Yingkai Wang, Xueyao Wang, Yali Shen, Yu Yao. (2024)<br><strong>MD-Dose: A Diffusion Model based on the Mamba for Radiotherapy Dose Prediction</strong><br><button class=copy-to-clipboard title="MD-Dose: A Diffusion Model based on the Mamba for Radiotherapy Dose Prediction" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, physics-med-ph<br>Keyword Score: 30<br>Keywords: Diffusion Model, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08479v1.pdf filename=2403.08479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radiation therapy is crucial in cancer treatment. Experienced experts typically iteratively generate high-quality dose distribution maps, forming the basis for excellent radiation therapy plans. Therefore, automated prediction of dose distribution maps is significant in expediting the treatment process and providing a better starting point for developing radiation therapy plans. With the remarkable results of <b>diffusion</b> <b>models</b> in predicting high-frequency regions of dose distribution maps, dose prediction methods based on <b>diffusion</b> <b>models</b> have been extensively studied. However, existing methods mainly utilize <b>CNNs</b> or <b>Transformers</b> as denoising networks. <b>CNNs</b> lack the capture of global receptive fields, resulting in suboptimal prediction performance. <b>Transformers</b> excel in global modeling but face quadratic complexity with image size, resulting in significant computational overhead. To tackle these challenges, we introduce a novel <b>diffusion</b> <b>model,</b> MD-Dose, based on the Mamba architecture for predicting radiation therapy dose distribution in thoracic cancer patients. In the forward process, MD-Dose adds Gaussian noise to dose distribution maps to obtain pure noise images. In the backward process, MD-Dose utilizes a noise predictor based on the Mamba to predict the noise, ultimately outputting the dose distribution maps. Furthermore, We develop a Mamba encoder to extract structural information and integrate it into the noise predictor for localizing dose regions in the planning target volume (PTV) and organs at risk (OARs). Through extensive experiments on a dataset of 300 thoracic tumor patients, we showcase the superiority of MD-Dose in various metrics and time consumption.</p></p class="citation"></blockquote><h3 id=413--144310-diffusion-models-with-implicit-guidance-for-medical-anomaly-detection-cosmin-i-bercea-et-al-2024>(4/13 | 144/310) Diffusion Models with Implicit Guidance for Medical Anomaly Detection (Cosmin I. Bercea et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cosmin I. Bercea, Benedikt Wiestler, Daniel Rueckert, Julia A. Schnabel. (2024)<br><strong>Diffusion Models with Implicit Guidance for Medical Anomaly Detection</strong><br><button class=copy-to-clipboard title="Diffusion Models with Implicit Guidance for Medical Anomaly Detection" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Anomaly Detection, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08464v1.pdf filename=2403.08464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have advanced <b>unsupervised</b> <b>anomaly</b> <b>detection</b> by improving the transformation of pathological images into pseudo-healthy equivalents. Nonetheless, standard approaches may compromise critical information during pathology removal, leading to restorations that do not align with unaffected regions in the original scans. Such discrepancies can inadvertently increase false positive rates and reduce specificity, complicating radiological evaluations. This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which refines the de-noising process by integrating implicit guidance through temporal <b>anomaly</b> <b>maps.</b> THOR aims to preserve the integrity of healthy tissue in areas unaffected by pathology. Comparative evaluations show that THOR surpasses existing <b>diffusion-based</b> <b>methods</b> in detecting and segmenting anomalies in brain MRIs and wrist X-rays. Code: <a href=https://github.com/ci-ber/THOR_DDPM>https://github.com/ci-ber/THOR_DDPM</a>.</p></p class="citation"></blockquote><h3 id=513--145310-exploiting-structural-consistency-of-chest-anatomy-for-unsupervised-anomaly-detection-in-radiography-images-tiange-xiang-et-al-2024>(5/13 | 145/310) Exploiting Structural Consistency of Chest Anatomy for Unsupervised Anomaly Detection in Radiography Images (Tiange Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiange Xiang, Yixiao Zhang, Yongyi Lu, Alan Yuille, Chaoyi Zhang, Weidong Cai, Zongwei Zhou. (2024)<br><strong>Exploiting Structural Consistency of Chest Anatomy for Unsupervised Anomaly Detection in Radiography Images</strong><br><button class=copy-to-clipboard title="Exploiting Structural Consistency of Chest Anatomy for Unsupervised Anomaly Detection in Radiography Images" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08689v1.pdf filename=2403.08689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radiography imaging protocols focus on particular body regions, therefore producing images of great similarity and yielding recurrent anatomical structures across patients. Exploiting this structured information could potentially ease the detection of anomalies from radiography images. To this end, we propose a Simple Space-Aware Memory Matrix for In-painting and Detecting anomalies from radiography images (abbreviated as SimSID). We formulate <b>anomaly</b> <b>detection</b> as an image reconstruction task, consisting of a space-aware memory matrix and an in-painting block in the feature space. During the training, SimSID can taxonomize the ingrained anatomical structures into recurrent visual patterns, and in the inference, it can identify anomalies (unseen/modified visual patterns) from the test image. Our SimSID surpasses the state of the arts in <b>unsupervised</b> <b>anomaly</b> <b>detection</b> by +8.0%, +5.0%, and +9.9% AUC scores on ZhangLab, COVIDx, and CheXpert <b>benchmark</b> datasets, respectively. Code: <a href=https://github.com/MrGiovanni/SimSID>https://github.com/MrGiovanni/SimSID</a></p></p class="citation"></blockquote><h3 id=613--146310-robust-covid-19-detection-in-ct-images-with-clip-li-lin-et-al-2024>(6/13 | 146/310) Robust COVID-19 Detection in CT Images with CLIP (Li Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Lin, Yamini Sri Krubha, Zhenhuan Yang, Cheng Ren, Thuc Duy Le, Irene Amerini, Xin Wang, Shu Hu. (2024)<br><strong>Robust COVID-19 Detection in CT Images with CLIP</strong><br><button class=copy-to-clipboard title="Robust COVID-19 Detection in CT Images with CLIP" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08947v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08947v2.pdf filename=2403.08947v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of medical imaging, particularly for COVID-19 detection, deep learning models face substantial challenges such as the necessity for extensive computational resources, the paucity of well-annotated datasets, and a significant amount of unlabeled data. In this work, we introduce the first lightweight detector designed to overcome these obstacles, leveraging a frozen CLIP image encoder and a trainable multilayer perception (MLP). Enhanced with Conditional Value at Risk (CVaR) for robustness and a loss landscape flattening strategy for improved generalization, our model is tailored for high efficacy in COVID-19 detection. Furthermore, we integrate a teacher-student framework to capitalize on the vast amounts of unlabeled data, enabling our model to achieve superior performance despite the inherent data limitations. Experimental results on the COV19-CT-DB dataset demonstrate the effectiveness of our approach, surpassing baseline by up to 10.6% in `macro&rsquo; F1 score in <b>supervised</b> <b>learning.</b> The code is available at <a href=https://github.com/Purdue-M2/COVID-19_Detection_M2_PURDUE>https://github.com/Purdue-M2/COVID-19_Detection_M2_PURDUE</a>.</p></p class="citation"></blockquote><h3 id=713--147310-diffusion-based-iterative-counterfactual-explanations-for-fetal-ultrasound-image-quality-assessment-paraskevas-pegios-et-al-2024>(7/13 | 147/310) Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment (Paraskevas Pegios et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paraskevas Pegios, Manxi Lin, Nina Weng, Morten Bo Søndergaard Svendsen, Zahra Bashir, Siavash Bigdeli, Anders Nymark Christensen, Martin Tolsgaard, Aasa Feragen. (2024)<br><strong>Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment</strong><br><button class=copy-to-clipboard title="Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-HC, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Counter-factual, Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08700v1.pdf filename=2403.08700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Obstetric ultrasound image quality is crucial for accurate diagnosis and monitoring of fetal health. However, producing high-quality standard planes is difficult, influenced by the sonographer&rsquo;s expertise and factors like the maternal BMI or the fetus dynamics. In this work, we propose using diffusion-based <b>counterfactual</b> <b>explainable</b> <b>AI</b> to generate realistic high-quality standard planes from low-quality non-standard ones. Through quantitative and qualitative evaluation, we demonstrate the effectiveness of our method in producing plausible <b>counterfactuals</b> of increased quality. This shows future promise both for enhancing training of clinicians by providing visual feedback, as well as for improving image quality and, consequently, downstream diagnosis and monitoring.</p></p class="citation"></blockquote><h3 id=813--148310-focusmae-gallbladder-cancer-detection-from-ultrasound-videos-with-focused-masked-autoencoders-soumen-basu-et-al-2024>(8/13 | 148/310) FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders (Soumen Basu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soumen Basu, Mayuna Gupta, Chetan Madan, Pankaj Gupta, Chetan Arora. (2024)<br><strong>FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders</strong><br><button class=copy-to-clipboard title="FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 15<br>Keywords: Autoencoder, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08848v1.pdf filename=2403.08848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, automated Gallbladder Cancer (GBC) detection has gained the attention of researchers. Current state-of-the-art (SOTA) methodologies relying on ultrasound sonography (US) images exhibit limited generalization, emphasizing the need for transformative approaches. We observe that individual US frames may lack sufficient information to capture disease manifestation. This study advocates for a paradigm shift towards video-based GBC detection, leveraging the inherent advantages of spatiotemporal <b>representations.</b> <b>Employing</b> the Masked <b>Autoencoder</b> (MAE) for <b>representation</b> <b>learning,</b> we address shortcomings in conventional image-based methods. We propose a novel design called FocusMAE to systematically bias the selection of masking tokens from high-information regions, fostering a more refined <b>representation</b> <b>of</b> malignancy. Additionally, we contribute the most extensive US video dataset for GBC detection. We also note that, this is the first study on US video-based GBC detection. We validate the proposed methods on the curated dataset, and report a new state-of-the-art (SOTA) accuracy of 96.4% for the GBC detection problem, against an accuracy of 84% by current Image-based SOTA - GBCNet, and RadFormer, and 94.7% by Video-based SOTA - AdaMAE. We further demonstrate the generality of the proposed FocusMAE on a public CT-based Covid detection dataset, reporting an improvement in accuracy by 3.3% over current baselines. The source code and pretrained models are available at: <a href=https://github.com/sbasu276/FocusMAE>https://github.com/sbasu276/FocusMAE</a>.</p></p class="citation"></blockquote><h3 id=913--149310-spatiotemporal-diffusion-model-with-paired-sampling-for-accelerated-cardiac-cine-mri-shihan-qiu-et-al-2024>(9/13 | 149/310) Spatiotemporal Diffusion Model with Paired Sampling for Accelerated Cardiac Cine MRI (Shihan Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shihan Qiu, Shaoyan Pan, Yikang Liu, Lin Zhao, Jian Xu, Qi Liu, Terrence Chen, Eric Z. Chen, Xiao Chen, Shanhui Sun. (2024)<br><strong>Spatiotemporal Diffusion Model with Paired Sampling for Accelerated Cardiac Cine MRI</strong><br><button class=copy-to-clipboard title="Spatiotemporal Diffusion Model with Paired Sampling for Accelerated Cardiac Cine MRI" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08758v1.pdf filename=2403.08758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current deep learning reconstruction for accelerated cardiac cine MRI suffers from spatial and temporal blurring. We aim to improve image sharpness and motion delineation for cine MRI under high undersampling rates. A spatiotemporal <b>diffusion</b> <b>enhancement</b> model conditional on an existing deep learning reconstruction along with a novel paired sampling strategy was developed. The <b>diffusion</b> <b>model</b> provided sharper tissue boundaries and clearer motion than the original reconstruction in experts evaluation on clinical data. The innovative paired sampling strategy substantially reduced artificial noises in the generative results.</p></p class="citation"></blockquote><h3 id=1013--150310-clinically-feasible-diffusion-reconstruction-for-highly-accelerated-cardiac-cine-mri-shihan-qiu-et-al-2024>(10/13 | 150/310) Clinically Feasible Diffusion Reconstruction for Highly-Accelerated Cardiac Cine MRI (Shihan Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shihan Qiu, Shaoyan Pan, Yikang Liu, Lin Zhao, Jian Xu, Qi Liu, Terrence Chen, Eric Z. Chen, Xiao Chen, Shanhui Sun. (2024)<br><strong>Clinically Feasible Diffusion Reconstruction for Highly-Accelerated Cardiac Cine MRI</strong><br><button class=copy-to-clipboard title="Clinically Feasible Diffusion Reconstruction for Highly-Accelerated Cardiac Cine MRI" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08749v1.pdf filename=2403.08749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The currently limited quality of accelerated cardiac cine reconstruction may potentially be improved by the emerging <b>diffusion</b> <b>models,</b> but the clinically unacceptable long processing time poses a challenge. We aim to develop a clinically feasible <b>diffusion-model-based</b> <b>reconstruction</b> pipeline to improve the image quality of cine MRI. A multi-in multi-out <b>diffusion</b> <b>enhancement</b> model together with fast inference strategies were developed to be used in conjunction with a reconstruction model. The <b>diffusion</b> <b>reconstruction</b> reduced spatial and temporal blurring in prospectively undersampled clinical data, as validated by experts inspection. The 1.5s per video processing time enabled the approach to be applied in clinical scenarios.</p></p class="citation"></blockquote><h3 id=1113--151310-gaussianimage-1000-fps-image-representation-and-compression-by-2d-gaussian-splatting-xinjie-zhang-et-al-2024>(11/13 | 151/310) GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting (Xinjie Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, Jun Zhang. (2024)<br><strong>GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting</strong><br><button class=copy-to-clipboard title="GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-MM, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08551v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08551v2.pdf filename=2403.08551v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing vector <b>quantization</b> technique to build an image codec. Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 1000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding.</p></p class="citation"></blockquote><h3 id=1213--152310-content-aware-masked-image-modeling-transformer-for-stereo-image-compression-xinjie-zhang-et-al-2024>(12/13 | 152/310) Content-aware Masked Image Modeling Transformer for Stereo Image Compression (Xinjie Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinjie Zhang, Shenyuan Gao, Zhening Liu, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Jun Zhang. (2024)<br><strong>Content-aware Masked Image Modeling Transformer for Stereo Image Compression</strong><br><button class=copy-to-clipboard title="Content-aware Masked Image Modeling Transformer for Stereo Image Compression" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-MM, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08505v1.pdf filename=2403.08505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free <b>Transformer</b> entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra <b>Transformer</b> decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-distortion performance on two stereo image datasets Cityscapes and InStereo2K with fast encoding and decoding speed.</p></p class="citation"></blockquote><h3 id=1313--153310-pre-examinations-improve-automated-metastases-detection-on-cranial-mri-katerina-deike-hofmann-et-al-2024>(13/13 | 153/310) Pre-examinations Improve Automated Metastases Detection on Cranial MRI (Katerina Deike-Hofmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katerina Deike-Hofmann, Dorottya Dancs, Daniel Paech, Heinz-Peter Schlemmer, Klaus Maier-Hein, Philipp Bäumer, Alexander Radbruch, Michael Götz. (2024)<br><strong>Pre-examinations Improve Automated Metastases Detection on Cranial MRI</strong><br><button class=copy-to-clipboard title="Pre-examinations Improve Automated Metastases Detection on Cranial MRI" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, physics-med-ph, q-bio-QM<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08280v1.pdf filename=2403.08280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Materials and methods: First, a dual-time approach was assessed, for which the <b>CNN</b> was provided sequences of the MRI that initially depicted new MM (diagnosis MRI) as well as of a prediagnosis MRI: inclusion of only contrast-enhanced T1-weighted images (CNNdual_ce) was compared with inclusion of also the native T1-weighted images, T2-weighted images, and FLAIR sequences of both time points (CNNdual_all).Second, results were compared with the corresponding single time approaches, in which the <b>CNN</b> was provided exclusively the respective sequences of the diagnosis MRI.Casewise diagnostic performance parameters were calculated from 5-fold cross-validation. Results: In total, 94 cases with 494 MMs were included. Overall, the highest diagnostic performance was achieved by inclusion of only the contrast-enhanced T1-weighted images of the diagnosis and of a prediagnosis MRI (CNNdual_ce, sensitivity = 73%, PPV = 25%, F1-score = 36%). Using exclusively contrast-enhanced T1-weighted images as input resulted in significantly less false-positives (FPs) compared with inclusion of further sequences beyond contrast-enhanced T1-weighted images (FPs = 5/7 for CNNdual_ce/CNNdual_all, P &lt; 1e-5). Comparison of contrast-enhanced dual and mono time approaches revealed that exclusion of prediagnosis MRI significantly increased FPs (FPs = 5/10 for CNNdual_ce/CNNce, P &lt; 1e-9).Approaches with only native sequences were clearly inferior to <b>CNNs</b> that were provided contrast-enhanced sequences. Conclusions: Automated MM detection on contrast-enhanced T1-weighted images performed with high sensitivity. Frequent FPs due to artifacts and vessels were significantly reduced by additional inclusion of prediagnosis MRI, but not by inclusion of further sequences beyond contrast-enhanced T1-weighted images. Future studies might investigate different change detection architectures for computer-aided detection.</p></p class="citation"></blockquote><h2 id=cscr-3>cs.CR (3)</h2><h3 id=13--154310-review-of-generative-ai-methods-in-cybersecurity-yagmur-yigit-et-al-2024>(1/3 | 154/310) Review of Generative AI Methods in Cybersecurity (Yagmur Yigit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yagmur Yigit, William J Buchanan, Madjid G Tehrani, Leandros Maglaras. (2024)<br><strong>Review of Generative AI Methods in Cybersecurity</strong><br><button class=copy-to-clipboard title="Review of Generative AI Methods in Cybersecurity" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 70<br>Keywords: Generative AI, ChatGPT, Gemini, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08701v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08701v2.pdf filename=2403.08701v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the last decade, Artificial Intelligence (AI) has become increasingly popular, especially with the use of <b>chatbots</b> such as <b>ChatGPT,</b> <b>Gemini,</b> and DALL-E. With this rise, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>Generative</b> <b>AI</b> (GenAI) have also become more prevalent in everyday use. These advancements strengthen cybersecurity&rsquo;s defensive posture and open up new attack avenues for adversaries as well. This paper provides a comprehensive overview of the current state-of-the-art deployments of GenAI, covering assaults, jailbreaking, and applications of <b>prompt</b> injection and reverse psychology. This paper also provides the various applications of GenAI in cybercrimes, such as automated hacking, phishing emails, social engineering, reverse cryptography, creating attack payloads, and creating malware. GenAI can significantly improve the automation of defensive cyber security processes through strategies such as dataset construction, safe code development, threat intelligence, defensive measures, reporting, and cyberattack detection. In this study, we suggest that future research should focus on developing robust ethical norms and innovative defense mechanisms to address the current issues that GenAI creates and to also further encourage an impartial approach to its future application in cybersecurity. Moreover, we underscore the importance of interdisciplinary approaches further to bridge the gap between scientific developments and ethical considerations.</p></p class="citation"></blockquote><h3 id=23--155310-tastle-distract-large-language-models-for-automatic-jailbreak-attack-zeguan-xiao-et-al-2024>(2/3 | 155/310) Tastle: Distract Large Language Models for Automatic Jailbreak Attack (Zeguan Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen. (2024)<br><strong>Tastle: Distract Large Language Models for Automatic Jailbreak Attack</strong><br><button class=copy-to-clipboard title="Tastle: Distract Large Language Models for Automatic Jailbreak Attack" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 35<br>Keywords: Black Box, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08424v1.pdf filename=2403.08424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved significant advances in recent days. Extensive efforts have been made before the public release of <b>LLMs</b> to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned <b>LLMs</b> remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious <b>prompt</b> that escapes from the <b>LLM</b> security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming <b>LLMs,</b> yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel <b>black-box</b> <b>jailbreak</b> framework for automated red teaming of <b>LLMs.</b> We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak <b>LLMs,</b> motivated by the research about the distractibility and over-confidence phenomenon of <b>LLMs.</b> Extensive experiments of jailbreaking both open-source and proprietary <b>LLMs</b> demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability. We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies.</p></p class="citation"></blockquote><h3 id=33--156310-advancing-security-in-ai-systems-a-novel-approach-to-detecting-backdoors-in-deep-neural-networks-khondoker-murad-hossain-et-al-2024>(3/3 | 156/310) Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks (Khondoker Murad Hossain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khondoker Murad Hossain, Tim Oates. (2024)<br><strong>Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks</strong><br><button class=copy-to-clipboard title="Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs.CR<br>Keyword Score: 20<br>Keywords: Object Detection, Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08208v1.pdf filename=2403.08208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving landscape of communication and network security, the increasing reliance on deep neural networks (DNNs) and cloud services for data processing presents a significant vulnerability: the potential for backdoors that can be exploited by malicious actors. Our approach leverages advanced <b>tensor</b> <b>decomposition</b> algorithms Independent Vector Analysis (IVA), Multiset Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2) to meticulously analyze the weights of pre-trained DNNs and distinguish between backdoored and clean models effectively. The key strengths of our method lie in its domain independence, adaptability to various network architectures, and ability to operate without access to the training data of the scrutinized models. This not only ensures versatility across different application scenarios but also addresses the challenge of identifying backdoors without prior knowledge of the specific triggers employed to alter network behavior. We have applied our detection pipeline to three distinct computer vision datasets, encompassing both image classification and <b>object</b> <b>detection</b> tasks. The results demonstrate a marked improvement in both accuracy and efficiency over existing backdoor detection methods. This advancement enhances the security of deep learning and AI in networked systems, providing essential cybersecurity against evolving threats in emerging technologies.</p></p class="citation"></blockquote><h2 id=csse-9>cs.SE (9)</h2><h3 id=19--157310-software-vulnerability-and-functionality-assessment-using-llms-rasmus-ingemann-tuffveson-jensen-et-al-2024>(1/9 | 157/310) Software Vulnerability and Functionality Assessment using LLMs (Rasmus Ingemann Tuffveson Jensen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rasmus Ingemann Tuffveson Jensen, Vali Tawosi, Salwa Alamir. (2024)<br><strong>Software Vulnerability and Functionality Assessment using LLMs</strong><br><button class=copy-to-clipboard title="Software Vulnerability and Functionality Assessment using LLMs" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 70<br>Keywords: Recommendation, Zero-shot, Code Generation, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08429v1.pdf filename=2403.08429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>code</b> <b>review</b> is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can aid with <b>code</b> <b>reviews.</b> Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging <b>code</b> <b>with</b> security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that <b>code</b> <b>meets</b> its intended functionality. To test performance on both tasks, we use <b>zero-shot</b> and <b>chain-of-thought</b> <b>prompting</b> to obtain final ``approve or reject&rsquo;&rsquo; <b>recommendations.</b> As data, we employ seminal <b>code</b> <b>generation</b> datasets (HumanEval and MBPP) along with expert-written <b>code</b> <b>snippets</b> with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source <b>LLMs.</b> We find that the former outperforms the latter by a <b>large</b> <b>margin.</b> <b>Motivated</b> by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7% of <b>LLM-generated</b> descriptions can be associated with true CWE vulnerabilities.</p></p class="citation"></blockquote><h3 id=29--158310-search-based-optimisation-of-llm-learning-shots-for-story-point-estimation-vali-tawosi-et-al-2024>(2/9 | 158/310) Search-based Optimisation of LLM Learning Shots for Story Point Estimation (Vali Tawosi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vali Tawosi, Salwa Alamir, Xiaomo Liu. (2024)<br><strong>Search-based Optimisation of LLM Learning Shots for Story Point Estimation</strong><br><button class=copy-to-clipboard title="Search-based Optimisation of LLM Learning Shots for Story Point Estimation" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 60<br>Keywords: Few-shot, Few-shot Learning, Meta Learning, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08430v1.pdf filename=2403.08430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the ways <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are used to perform machine learning tasks is to provide them with a few examples before asking them to produce a prediction. This is a <b>meta-learning</b> <b>process</b> known as <b>few-shot</b> <b>learning.</b> In this paper, we use available Search-Based methods to optimise the number and combination of examples that can improve an <b>LLM&rsquo;s</b> estimation performance, when it is used to estimate story points for new agile tasks. Our preliminary results show that our SBSE technique improves the estimation performance of the <b>LLM</b> by 59.34% on average (in terms of mean absolute error of the estimation) over three datasets against a <b>zero-shot</b> setting.</p></p class="citation"></blockquote><h3 id=39--159310-a-picture-is-worth-a-thousand-words-exploring-diagram-and-video-based-oop-exercises-to-counter-llm-over-reliance-bruno-pereira-cipriano-et-al-2024>(3/9 | 159/310) A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance (Bruno Pereira Cipriano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bruno Pereira Cipriano, Pedro Alves, Paul Denny. (2024)<br><strong>A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance</strong><br><button class=copy-to-clipboard title="A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-HC, cs-SE, cs.SE<br>Keyword Score: 60<br>Keywords: Bard, GPT, GPT-4, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08396v1.pdf filename=2403.08396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Much research has highlighted the impressive capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> like <b>GPT</b> and <b>Bard,</b> for solving introductory programming exercises. Recent work has shown that <b>LLMs</b> can effectively solve a range of more complex object-oriented programming (OOP) exercises with text-based specifications. This raises concerns about academic integrity, as students might use these models to complete assignments unethically, neglecting the development of important skills such as program design, problem-solving, and computational thinking. To address this, we propose an innovative approach to formulating OOP tasks using diagrams and videos, as a way to foster problem-solving and deter students from a copy-and-prompt approach in OOP courses. We introduce a novel notation system for specifying OOP assignments, encompassing structural and behavioral requirements, and assess its use in a classroom setting over a semester. Student perceptions of this approach are explored through a survey (n=56). Generally, students responded positively to diagrams and videos, with video-based projects being better received than diagram-based exercises. This notation appears to have several benefits, with students investing more effort in understanding the diagrams and feeling more motivated to engage with the video-based projects. Furthermore, students reported being less inclined to rely on <b>LLM-based</b> <b>code</b> <b>generation</b> tools for these diagram and video-based exercises. Experiments with <b>GPT-4</b> and <b>Bard&rsquo;s</b> vision abilities revealed that they currently fall short in interpreting these diagrams to generate accurate <b>code</b> <b>solutions.</b></p></p class="citation"></blockquote><h3 id=49--160310-bugs-in-large-language-models-generated-code-an-empirical-study-florian-tambon-et-al-2024>(4/9 | 160/310) Bugs in Large Language Models Generated Code: An Empirical Study (Florian Tambon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Tambon, Arghavan Moradi Dakhel, Amin Nikanjam, Foutse Khomh, Michel C. Desmarais, Giuliano Antoniol. (2024)<br><strong>Bugs in Large Language Models Generated Code: An Empirical Study</strong><br><button class=copy-to-clipboard title="Bugs in Large Language Models Generated Code: An Empirical Study" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: CodeGen, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08937v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08937v2.pdf filename=2403.08937v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for <b>code</b> <b>have</b> gained significant attention recently. They can generate <b>code</b> <b>in</b> different programming languages based on provided <b>prompts,</b> fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic <b>code</b> <b>generation.</b> Similar to human-written <b>code,</b> <b>LLM-generated</b> <b>code</b> <b>is</b> prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of <b>LLM-based</b> <b>code</b> <b>generation</b> tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in <b>code</b> <b>generated</b> by <b>LLMs.</b> This paper examines a sample of 333 bugs collected from <b>code</b> <b>generated</b> using three leading <b>LLMs</b> (i.e., <b>CodeGen,</b> PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, <b>Prompt-biased</b> <b>code,</b> <b>Missing</b> Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using an online survey with 34 <b>LLM</b> practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for <b>LLM-generated</b> <b>code.</b> <b>This</b> study sheds light on the distinctive characteristics of <b>LLM-generated</b> code.</p></p class="citation"></blockquote><h3 id=59--161310-teaching-machines-to-code-smart-contract-translation-with-llms-rabimba-karanjai-et-al-2024>(5/9 | 161/310) Teaching Machines to Code: Smart Contract Translation with LLMs (Rabimba Karanjai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rabimba Karanjai, Lei Xu, Weidong Shi. (2024)<br><strong>Teaching Machines to Code: Smart Contract Translation with LLMs</strong><br><button class=copy-to-clipboard title="Teaching Machines to Code: Smart Contract Translation with LLMs" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09740v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09740v1.pdf filename=2403.09740v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has marked a significant milestone in the realm of artificial intelligence, with their capabilities often matching or surpassing human expertise in various domains. Among these achievements, their adeptness in translation tasks stands out, closely mimicking the intricate and preliminary processes undertaken by human translators to ensure the fidelity and quality of the translated content. Despite the advancements in utilizing <b>LLMs</b> for translating programming code across different languages, the domain of smart contract translation, particularly into languages not previously encountered by the <b>LLM,</b> remains largely unexplored. In our research, we present a pioneering approach, SolMover, which harnesses the synergy of two distinct <b>LLMs</b> within a unified framework. This framework is designed to grasp coding principles and apply this understanding to the translation of code into an unfamiliar language. Our study delves into the capacity of <b>LLMs</b> to mimic human learning processes, offering an in-depth evaluation of our methodology for converting smart contracts written in Solidity to Move, a language with limited resources. The framework employs one <b>LLM</b> to decipher coding conventions for the new language, creating a blueprint for the second <b>LLM,</b> which, lacking planning abilities, possesses coding expertise. The empirical evidence from our experiments suggests that SolMover substantially enhances performance compared to <b>gpt-3.5-turbo-1106,</b> and achieves superior results over competitors such as Palm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the efficacy of our bug mitigation strategy in elevating code quality across all models, even outside the SolMover framework.</p></p class="citation"></blockquote><h3 id=69--162310-system-for-systematic-literature-review-using-multiple-ai-agents-concept-and-an-empirical-evaluation-abdul-malik-sami-et-al-2024>(6/9 | 162/310) System for systematic literature review using multiple AI agents: Concept and an empirical evaluation (Abdul Malik Sami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdul Malik Sami, Zeeshan Rasheed, Kai-Kristian Kemell, Muhammad Waseem, Terhi Kilamo, Mika Saari, Anh Nguyen Duc, Kari Systä, Pekka Abrahamsson. (2024)<br><strong>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation</strong><br><button class=copy-to-clipboard title="System for systematic literature review using multiple AI agents: Concept and an empirical evaluation" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08399v1.pdf filename=2403.08399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Systematic Literature Reviews (SLRs) have become the foundation of evidence-based studies, enabling researchers to identify, classify, and combine existing studies based on specific research questions. Conducting an SLR is largely a manual process. Over the previous years, researchers have made significant progress in automating certain phases of the SLR process, aiming to reduce the effort and time needed to carry out high-quality SLRs. However, there is still a lack of AI agent-based models that automate the entire SLR process. To this end, we introduce a novel multi-AI agent model designed to fully automate the process of conducting an SLR. By utilizing the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> our proposed model streamlines the review process, enhancing efficiency and accuracy. The model operates through a user-friendly interface where researchers input their topic, and in response, the model generates a search string used to retrieve relevant academic papers. Subsequently, an inclusive and exclusive filtering process is applied, focusing on titles relevant to the specific research area. The model then autonomously <b>summarizes</b> the abstracts of these papers, retaining only those directly related to the field of study. In the final phase, the model conducts a thorough analysis of the selected papers concerning predefined research questions. We also evaluated the proposed model by sharing it with ten competent software engineering researchers for testing and analysis. The researchers expressed strong satisfaction with the proposed model and provided feedback for further improvement. The code for this project can be found on the GitHub repository at <a href=https://github.com/GPT-Laboratory/SLR-automation>https://github.com/GPT-Laboratory/SLR-automation</a>.</p></p class="citation"></blockquote><h3 id=79--163310-understanding-and-evaluating-developer-behaviour-in-programming-tasks-martin-schröer-et-al-2024>(7/9 | 163/310) Understanding and Evaluating Developer Behaviour in Programming Tasks (Martin Schröer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Schröer, Rainer Koschke. (2024)<br><strong>Understanding and Evaluating Developer Behaviour in Programming Tasks</strong><br><button class=copy-to-clipboard title="Understanding and Evaluating Developer Behaviour in Programming Tasks" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Automatic Evaluation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08480v1.pdf filename=2403.08480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To evaluate how developers perform differently in solving programming tasks, i.e., which actions and behaviours are more beneficial to them than others and if there are any specific strategies and behaviours that may indicate good versus poor understanding of the task and program given to them, we used the MIMESIS plug-in to record developers&rsquo; interactions with the IDE. In a series of three studies we investigated the specific behaviour of developers solving a specific programming task. We focused on which source code files they visited, how they related pieces of code and knowledge to others and when and how successful they performed code edits. To cope with the variety of behaviours due to interpersonal differences such as different level of knowledge, development style or problem solving stratiegies, we used an abstraction of the observed behaviour, which enables for a better comparison between different individual attributes such as skill, speed and used stratiegies and also facilitates later <b>automatic</b> <b>evaluation</b> of behaviours, i.e. by using a software to react to.</p></p class="citation"></blockquote><h3 id=89--164310-autodev-automated-ai-driven-development-michele-tufano-et-al-2024>(8/9 | 164/310) AutoDev: Automated AI-Driven Development (Michele Tufano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, Neel Sundaresan. (2024)<br><strong>AutoDev: Automated AI-Driven Development</strong><br><button class=copy-to-clipboard title="AutoDev: Automated AI-Driven Development" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08299v1.pdf filename=2403.08299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot. However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing <b>code,</b> <b>git</b> operations, etc. Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting <b>code</b> <b>snippets</b> and file manipulation within a chat-based interface. To fill this gap, we present AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev&rsquo;s autonomous AI Agents to achieve. These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. They also have access to files, compiler output, build and testing logs, static analysis tools, and more. This enables the AI Agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required. Furthermore, AutoDev establishes a secure development environment by confining all operations within Docker containers. This framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within AutoDev. In our evaluation, we tested AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and 87.8% of Pass@1 for <b>code</b> <b>generation</b> and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment.</p></p class="citation"></blockquote><h3 id=99--165310-log-summarisation-for-defect-evolution-analysis-rares-dolga-et-al-2024>(9/9 | 165/310) Log Summarisation for Defect Evolution Analysis (Rares Dolga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rares Dolga, Ran Zmigrod, Rui Silva, Salwa Alamir, Sameena Shah. (2024)<br><strong>Log Summarisation for Defect Evolution Analysis</strong><br><button class=copy-to-clipboard title="Log Summarisation for Defect Evolution Analysis" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08358v1.pdf filename=2403.08358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Log analysis and monitoring are essential aspects in software maintenance and identifying defects. In particular, the temporal nature and vast size of log data leads to an interesting and important research question: How can logs be summarised and monitored over time? While this has been a fundamental topic of research in the software engineering community, work has typically focused on heuristic-, syntax-, or static-based methods. In this work, we suggest an online semantic-based <b>clustering</b> approach to error logs that dynamically updates the log clusters to enable monitoring code error life-cycles. We also introduce a novel metric to evaluate the performance of temporal log clusters. We test our system and evaluation metric with an industrial dataset and find that our solution outperforms similar systems. We hope that our work encourages further temporal exploration in defect datasets.</p></p class="citation"></blockquote><h2 id=cslg-53>cs.LG (53)</h2><h3 id=153--166310-hrlaif-improvements-in-helpfulness-and-harmlessness-in-open-domain-reinforcement-learning-from-ai-feedback-ang-li-et-al-2024>(1/53 | 166/310) HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback (Ang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ang Li, Qiugen Xiao, Peng Cao, Jian Tang, Yi Yuan, Zijie Zhao, Xiaoyuan Chen, Liang Zhang, Xiangyang Li, Kaitong Yang, Weidong Guo, Yukang Gan, Xu Yu, Daniell Wang, Ying Shan. (2024)<br><strong>HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback</strong><br><button class=copy-to-clipboard title="HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08309v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08309v2.pdf filename=2403.08309v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>AI</b> <b>Feedback</b> (RLAIF) has the advantages of shorter annotation cycles and lower costs over <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF),</b> making it highly efficient during the rapid strategy iteration periods of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> training. Using <b>ChatGPT</b> as a labeler to provide feedback on open-domain <b>prompts</b> in RLAIF training, we observe an increase in human evaluators&rsquo; preference win ratio for model responses, but a decrease in evaluators&rsquo; satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>AI</b> <b>Feedback</b> (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model&rsquo;s helpfulness more robust in training process. Additionally, it employs AI for Red Teaming, further improving the model&rsquo;s harmlessness. Human evaluation results show that HRLAIF inherits the ability of RLAIF to enhance human preference for outcomes at a low cost while also improving the satisfaction rate of responses. Compared to the policy model before <b>Reinforcement</b> <b>Learning</b> <b>(RL),</b> <b>it</b> <b>achieves</b> an increase of 2.08% in satisfaction rate, effectively addressing the issue of a decrease of 4.58% in satisfaction rate after basic RLAIF.</p></p class="citation"></blockquote><h3 id=253--167310-second-order-information-matters-revisiting-machine-unlearning-for-large-language-models-kang-gu-et-al-2024>(2/53 | 167/310) Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models (Kang Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kang Gu, Md Rafi Ur Rashid, Najrin Sultana, Shagufta Mehnaz. (2024)<br><strong>Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models</strong><br><button class=copy-to-clipboard title="Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Machine Unlearning, ChatGPT, Gemini, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10557v1.pdf filename=2403.10557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid development of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> we have witnessed intense competition among the major <b>LLM</b> products like <b>ChatGPT,</b> <b>LLaMa,</b> and <b>Gemini.</b> However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of <b>LLM</b> practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning" problem of <b>LLMs</b> using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are not only data-agnostic/model-agnostic but also proven to be robust in terms of utility preservation or privacy guarantee. Through a comprehensive evaluation with four NLP datasets as well as a case study on real-world datasets, our methods consistently show superiority over the first-order methods.</p></p class="citation"></blockquote><h3 id=353--168310-cleanagent-automating-data-standardization-with-llm-based-agents-danrui-qi-et-al-2024>(3/53 | 168/310) CleanAgent: Automating Data Standardization with LLM-based Agents (Danrui Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danrui Qi, Jiannan Wang. (2024)<br><strong>CleanAgent: Automating Data Standardization with LLM-based Agents</strong><br><button class=copy-to-clipboard title="CleanAgent: Automating Data Standardization with LLM-based Agents" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs.LG<br>Keyword Score: 60<br>Keywords: ChatGPT, Code Generation, Natural Language Understanding, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08291v1.pdf filename=2403.08291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing <b>code</b> <b>to</b> diverse column types pose significant challenges. Although <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>ChatGPT</b> have shown promise in automating this process through <b>natural</b> <b>language</b> <b>understanding</b> and <b>code</b> <b>generation,</b> it still demands expert-level programming knowledge and continuous interaction for <b>prompt</b> refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the <b>code</b> <b>generation</b> of <b>LLM</b> with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of <b>code.</b> <b>Then</b> we introduce the CleanAgent framework integrating Dataprep.Clean and <b>LLM-based</b> agents to automate the data standardization process. With CleanAgent, data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process.</p></p class="citation"></blockquote><h3 id=453--169310-federated-knowledge-graph-unlearning-via-diffusion-model-bingchen-liu-et-al-2024>(4/53 | 169/310) Federated Knowledge Graph Unlearning via Diffusion Model (Bingchen Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingchen Liu, Yuanyuan Fang. (2024)<br><strong>Federated Knowledge Graph Unlearning via Diffusion Model</strong><br><button class=copy-to-clipboard title="Federated Knowledge Graph Unlearning via Diffusion Model" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 56<br>Keywords: Diffusion Model, Graph, Benchmarking, Federated Learning, Knowledge Graph, Knowledge Graph, Machine Unlearning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08554v1.pdf filename=2403.08554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy. <b>Knowledge</b> <b>graph</b> <b>(KG)</b> embedding representation provides a foundation for <b>knowledge</b> <b>reasoning</b> and applications by mapping entities and relations into vector space. <b>Federated</b> <b>KG</b> embedding enables the utilization of <b>knowledge</b> <b>from</b> diverse client sources while safeguarding the privacy of local data. However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into <b>machine</b> <b>unlearning</b> (MU) have been sparked. However, it is challenging to maintain the performance of <b>KG</b> embedding models while forgetting the influence of specific forgotten data on the model. In this paper, we propose FedDM, a novel framework tailored for <b>machine</b> <b>unlearning</b> in <b>federated</b> <b>knowledge</b> <b>graphs.</b> Leveraging <b>diffusion</b> <b>models,</b> we generate noisy data to sensibly mitigate the influence of specific <b>knowledge</b> <b>on</b> FL models while preserving the overall performance concerning the remaining data. We conduct experimental evaluations on <b>benchmark</b> datasets to assess the efficacy of the proposed model. Extensive experiments demonstrate that FedDM yields promising results in <b>knowledge</b> <b>forgetting.</b></p></p class="citation"></blockquote><h3 id=553--170310-multi-objective-optimization-using-adaptive-distributed-reinforcement-learning-jing-tan-et-al-2024>(5/53 | 170/310) Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning (Jing Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Tan, Ramin Khalili, Holger Karl. (2024)<br><strong>Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs.LG, math-OC<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, Few-shot Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08879v1.pdf filename=2403.08879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Intelligent Transportation System (ITS) environment is known to be dynamic and distributed, where participants (vehicle users, operators, etc.) have multiple, changing and possibly conflicting objectives. Although <b>Reinforcement</b> <b>Learning</b> (RL) algorithms are commonly applied to optimize ITS applications such as resource management and offloading, most RL algorithms focus on single objectives. In many situations, converting a multi-objective problem into a single-objective one is impossible, intractable or insufficient, making such RL algorithms inapplicable. We propose a multi-objective, multi-agent <b>reinforcement</b> <b>learning</b> (MARL) algorithm with high learning efficiency and low computational requirements, which automatically triggers adaptive <b>few-shot</b> <b>learning</b> in a dynamic, distributed and noisy environment with sparse and delayed reward. We test our algorithm in an ITS environment with edge cloud computing. Empirical results show that the algorithm is quick to adapt to new environments and performs better in all individual and system metrics compared to the state-of-the-art <b>benchmark.</b> Our algorithm also addresses various practical concerns with its modularized and asynchronous online training method. In addition to the cloud <b>simulation,</b> we test our algorithm on a single-board computer and show that it can make inference in 6 milliseconds.</p></p class="citation"></blockquote><h3 id=653--171310-training-self-localization-models-for-unseen-unfamiliar-places-via-teacher-to-student-data-free-knowledge-transfer-kenta-tsukahara-et-al-2024>(6/53 | 171/310) Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer (Kenta Tsukahara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kenta Tsukahara, Kanji Tanaka, Daiki Iwata. (2024)<br><strong>Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer</strong><br><button class=copy-to-clipboard title="Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.LG<br>Keyword Score: 45<br>Keywords: Black Box, Continual Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10552v1.pdf filename=2403.10552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A typical assumption in state-of-the-art self-localization models is that an annotated training dataset is available in the target workspace. However, this does not always hold when a robot travels in a general open-world. This study introduces a novel training scheme for open-world distributed robot systems. In our scheme, a robot (&ldquo;student&rdquo;) can ask the other robots it meets at unfamiliar places (&ldquo;teachers&rdquo;) for guidance. Specifically, a pseudo-training dataset is reconstructed from the teacher model and thereafter used for <b>continual</b> <b>learning</b> of the student model. Unlike typical <b>knowledge</b> <b>transfer</b> schemes, our scheme introduces only minimal assumptions on the teacher model, such that it can handle various types of open-set teachers, including uncooperative, untrainable (e.g., image retrieval engines), and blackbox teachers (i.e., data privacy). Rather than relying on the availability of private data of teachers as in existing methods, we propose to exploit an assumption that holds universally in self-localization tasks: &ldquo;The teacher model is a self-localization system&rdquo; and to reuse the self-localization system of a teacher as a sole accessible communication channel. We particularly focus on designing an excellent student/questioner whose interactions with teachers can yield effective question-and-answer sequences that can be used as pseudo-training datasets for the student self-localization model. When applied to a generic recursive <b>knowledge</b> <b>distillation</b> scenario, our approach exhibited stable and consistent performance improvement.</p></p class="citation"></blockquote><h3 id=753--172310-simple-and-scalable-strategies-to-continually-pre-train-large-language-models-adam-ibrahim-et-al-2024>(7/53 | 172/310) Simple and Scalable Strategies to Continually Pre-train Large Language Models (Adam Ibrahim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish. (2024)<br><strong>Simple and Scalable Strategies to Continually Pre-train Large Language Models</strong><br><button class=copy-to-clipboard title="Simple and Scalable Strategies to Continually Pre-train Large Language Models" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Continual Learning, Distribution Shift, Distribution Shift, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08763v1.pdf filename=2403.08763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the <b>distribution</b> <b>shift</b> induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation <b>benchmarks.</b> Specifically, we show this for a weak but realistic <b>distribution</b> <b>shift</b> between two commonly used <b>LLM</b> pre-training datasets (English$\rightarrow$English) and a stronger <b>distribution</b> <b>shift</b> (English$\rightarrow$German) at the $405$M parameter model scale with <b>large</b> <b>dataset</b> <b>sizes</b> (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our <b>continual</b> <b>learning</b> strategies match the re-training baseline for a 10B parameter <b>LLM.</b> Our results demonstrate that <b>LLMs</b> can be successfully updated via simple and scalable <b>continual</b> <b>learning</b> strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.</p></p class="citation"></blockquote><h3 id=853--173310-a-physics-driven-graphsage-method-for-physical-process-simulations-described-by-partial-differential-equations-hang-hu-et-al-2024>(8/53 | 173/310) A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations (Hang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Hu, Sidi Wu, Guoxiong Cai, Na Liu. (2024)<br><strong>A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations</strong><br><button class=copy-to-clipboard title="A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: G-1-8, cs-LG, cs.LG, physics-comp-ph<br>Keyword Score: 43<br>Keywords: GraphSAGE, Graph, Simulation, Simulator, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08569v1.pdf filename=2403.08569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physics-informed neural networks (PINNs) have successfully addressed various computational physics problems based on partial differential equations (PDEs). However, while tackling issues related to irregularities like singularities and oscillations, trained solutions usually suffer low accuracy. In addition, most current works only offer the trained solution for predetermined input parameters. If any change occurs in input parameters, <b>transfer</b> <b>learning</b> or retraining is required, and traditional numerical techniques also need an independent <b>simulation.</b> In this work, a physics-driven <b>GraphSAGE</b> approach (PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal basis functions is presented to solve computational problems governed by irregular PDEs and to develop parametric PDE surrogate models. This approach employs <b>graph</b> representations of physical domains, thereby reducing the demands for evaluated points due to local refinement. A distance-related edge feature and a feature mapping strategy are devised to help training and convergence for singularity and oscillation situations, respectively. The merits of the proposed method are demonstrated through a couple of cases. Moreover, the robust PDE surrogate model for heat conduction problems parameterized by the Gaussian random field source is successfully established, which not only provides the solution accurately but is several times faster than the finite element method in our experiments.</p></p class="citation"></blockquote><h3 id=953--174310-fast-inference-of-removal-based-node-influence-weikai-li-et-al-2024>(9/53 | 174/310) Fast Inference of Removal-Based Node Influence (Weikai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weikai Li, Zhiping Xiao, Xiao Luo, Yizhou Sun. (2024)<br><strong>Fast Inference of Removal-Based Node Influence</strong><br><button class=copy-to-clipboard title="Fast Inference of Removal-Based Node Influence" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Counter-factual, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08333v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08333v2.pdf filename=2403.08333v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> are widely utilized to capture the information spreading patterns in <b>graphs.</b> <b>While</b> <b>remarkable</b> performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained <b>GNN</b> model caused by removing a node. A real-world application is, &ldquo;In the task of predicting Twitter accounts&rsquo; polarity, had a particular account been removed, how would others&rsquo; polarity change?&rdquo;. We use the <b>GNN</b> as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. Our target is to obtain the influence score for every node, and a straightforward way is to alternately remove every node and apply the trained <b>GNN</b> on the modified <b>graph</b> <b>to</b> <b>generate</b> new predictions. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as <b>graph</b> <b>adversarial</b> <b>attack</b> and <b>counterfactual</b> explanation, cannot directly satisfy our needs, since their problem settings are different. We propose an efficient, intuitive, and effective method, NOde-Removal-based fAst <b>GNN</b> inference (NORA), which uses the gradient information to approximate the node-removal influence. It only costs one forward propagation and one backpropagation to approximate the influence score for all nodes. Extensive experiments on six datasets and six <b>GNN</b> models verify the effectiveness of NORA. Our code is available at <a href=https://github.com/weikai-li/NORA.git>https://github.com/weikai-li/NORA.git</a>.</p></p class="citation"></blockquote><h3 id=1053--175310-human-alignment-of-large-language-models-through-online-preference-optimisation-daniele-calandriello-et-al-2024>(10/53 | 175/310) Human Alignment of Large Language Models through Online Preference Optimisation (Daniele Calandriello et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, Rishabh Joshi, Zeyu Zheng, Bilal Piot. (2024)<br><strong>Human Alignment of Large Language Models through Online Preference Optimisation</strong><br><button class=copy-to-clipboard title="Human Alignment of Large Language Models through Online Preference Optimisation" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08635v1.pdf filename=2403.08635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring alignment of language models&rsquo; outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF),</b> Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD. This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generations are sampled by the online policy and annotated by a trained preference model. Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play. Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy (between the online and reference policy) similarly as the general Nash-MD algorithm. We compare online-IPO and IPO-MD to different online versions of existing losses on preference data such as DPO and SLiC on a summarisation task.</p></p class="citation"></blockquote><h3 id=1153--176310-data-efficient-sleep-staging-with-synthetic-time-series-pretraining-niklas-grieger-et-al-2024>(11/53 | 176/310) Data-Efficient Sleep Staging with Synthetic Time Series Pretraining (Niklas Grieger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niklas Grieger, Siamak Mehrkanoon, Stephan Bialonski. (2024)<br><strong>Data-Efficient Sleep Staging with Synthetic Time Series Pretraining</strong><br><button class=copy-to-clipboard title="Data-Efficient Sleep Staging with Synthetic Time Series Pretraining" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-QM<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08592v1.pdf filename=2403.08592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as <b>self-supervised</b> <b>learning,</b> have been suggested, but they typically rely on extensive empirical datasets. Inspired by recent advances in computer vision, we propose a pretraining task termed &ldquo;frequency pretraining&rdquo; to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series. Our experiments demonstrate that our method surpasses fully <b>supervised</b> <b>learning</b> in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects. Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond frequencies to enhance sleep staging performance, which is consistent with previous research. We anticipate that our approach will be advantageous across a broad spectrum of applications where EEG data is limited or derived from a small number of subjects, including the domain of brain-computer interfaces.</p></p class="citation"></blockquote><h3 id=1253--177310-sok-reducing-the-vulnerability-of-fine-tuned-language-models-to-membership-inference-attacks-guy-amit-et-al-2024>(12/53 | 177/310) SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks (Guy Amit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guy Amit, Abigail Goldsteen, Ariel Farkash. (2024)<br><strong>SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks</strong><br><button class=copy-to-clipboard title="SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08481v1.pdf filename=2403.08481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require <b>fine-tuning</b> generic base models on customized, proprietary datasets. This <b>fine-tuning</b> data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of <b>fine-tuned</b> <b>large</b> <b>language</b> <b>models</b> to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We find that some training methods provide significantly reduced privacy risk, with the combination of <b>differential</b> <b>privacy</b> and low-rank adaptors achieving the best privacy protection against these attacks.</p></p class="citation"></blockquote><h3 id=1353--178310-autodfp-automatic-data-free-pruning-via-channel-similarity-reconstruction-siqi-li-et-al-2024>(13/53 | 178/310) AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction (Siqi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siqi Li, Jun Chen, Jingyang Xiang, Chengrui Zhu, Yong Liu. (2024)<br><strong>AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction</strong><br><button class=copy-to-clipboard title="AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Pruning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08204v1.pdf filename=2403.08204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structured <b>pruning</b> methods are developed to bridge the gap between the massive scale of neural networks and the limited hardware resources. Most current structured <b>pruning</b> methods rely on training datasets to <b>fine-tune</b> the compressed model, resulting in high computational burdens and being inapplicable for scenarios with stringent requirements on privacy and security. As an alternative, some data-free methods have been proposed, however, these methods often require handcraft parameter tuning and can only achieve inflexible reconstruction. In this paper, we propose the Automatic Data-Free <b>Pruning</b> (AutoDFP) method that achieves automatic <b>pruning</b> and reconstruction without <b>fine-tuning.</b> Our approach is based on the assumption that the loss of information can be partially compensated by retaining focused information from similar channels. Specifically, We formulate data-free <b>pruning</b> as an optimization problem, which can be effectively addressed through <b>reinforcement</b> <b>learning.</b> AutoDFP assesses the similarity of channels for each layer and provides this information to the <b>reinforcement</b> <b>learning</b> agent, guiding the <b>pruning</b> and reconstruction process of the network. We evaluate AutoDFP with multiple networks on multiple datasets, achieving impressive compression results. For instance, on the CIFAR-10 dataset, AutoDFP demonstrates a 2.87% reduction in accuracy loss compared to the recently proposed data-free <b>pruning</b> method DFPC with fewer FLOPs on VGG-16. Furthermore, on the ImageNet dataset, AutoDFP achieves 43.17% higher accuracy than the SOTA method with the same 80% preserved ratio on MobileNet-V1.</p></p class="citation"></blockquote><h3 id=1453--179310-molbind-multimodal-alignment-of-language-molecules-and-proteins-teng-xiao-et-al-2024>(14/53 | 179/310) MolBind: Multimodal Alignment of Language, Molecules, and Proteins (Teng Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teng Xiao, Chao Cui, Huaisheng Zhu, Vasant G. Honavar. (2024)<br><strong>MolBind: Multimodal Alignment of Language, Molecules, and Proteins</strong><br><button class=copy-to-clipboard title="MolBind: Multimodal Alignment of Language, Molecules, and Proteins" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, q-bio-QM<br>Keyword Score: 39<br>Keywords: Graph, Contrastive Learning, Multi-modal, Multi-modal, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08167v1.pdf filename=2403.08167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in biology and chemistry have leveraged <b>multi-modal</b> learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular <b>graphs,</b> 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through <b>contrastive</b> <b>learning,</b> mapping all modalities to a shared feature space for <b>multi-modal</b> semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including <b>graph-language,</b> conformation-language, <b>graph-conformation,</b> and conformation-protein paired data. MolBind shows superior <b>zero-shot</b> <b>learning</b> performance across a wide range of tasks, demonstrating its strong capability of capturing the underlying semantics of multiple modalities.</p></p class="citation"></blockquote><h3 id=1553--180310-usable-xai-10-strategies-towards-exploiting-explainability-in-the-llm-era-xuansheng-wu-et-al-2024>(15/53 | 180/310) Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era (Xuansheng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuansheng Wu, Haiyan Zhao, Yaochen Zhu, Yucheng Shi, Fan Yang, Tianming Liu, Xiaoming Zhai, Wenlin Yao, Jundong Li, Mengnan Du, Ninghao Liu. (2024)<br><strong>Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era</strong><br><button class=copy-to-clipboard title="Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CY, cs-LG, cs.LG<br>Keyword Score: 35<br>Keywords: Black Box, Explainable AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08946v1.pdf filename=2403.08946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Explainable</b> <b>AI</b> (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to <b>LLMs</b> due to their complexity advanced capabilities. Second, as <b>LLMs</b> are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the <b>&ldquo;black</b> <b>box&rdquo;</b> to actively enhancing the productivity and applicability of <b>LLMs</b> in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of <b>LLMs</b> can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of <b>LLMs</b> by analyzing (1) how XAI can benefit <b>LLMs</b> and AI systems, and (2) how <b>LLMs</b> can contribute to the advancement of XAI. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations. The code used in this paper can be found at: <a href=https://github.com/JacksonWuxs/UsableXAI_LLM>https://github.com/JacksonWuxs/UsableXAI_LLM</a>.</p></p class="citation"></blockquote><h3 id=1653--181310-extracting-explanations-justification-and-uncertainty-from-black-box-deep-neural-networks-paul-ardis-et-al-2024>(16/53 | 181/310) Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks (Paul Ardis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Ardis, Arjuna Flenner. (2024)<br><strong>Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks</strong><br><button class=copy-to-clipboard title="Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-10, cs-LG, cs.LG, stat-ML<br>Keyword Score: 35<br>Keywords: Anomaly Detection, Black Box, Out-of-distribution, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08652v1.pdf filename=2403.08652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence. In mission critical applications, it is important to both understand associated DNN <b>reasoning</b> and its supporting evidence. In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs. Our approach is efficient both in terms of memory and computation, and can be applied to any <b>black</b> <b>box</b> DNN without any retraining, including applications to <b>anomaly</b> <b>detection</b> and <b>out-of-distribution</b> detection tasks. We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs.</p></p class="citation"></blockquote><h3 id=1753--182310-towards-efficient-risk-sensitive-policy-gradient-an-iteration-complexity-analysis-rui-liu-et-al-2024>(17/53 | 182/310) Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis (Rui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Liu, Erfaun Noorani, Pratap Tokekar, John S. Baras. (2024)<br><strong>Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis</strong><br><button class=copy-to-clipboard title="Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08955v1.pdf filename=2403.08955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutral counterparts. Our theoretical analysis demonstrates that risk-sensitive REINFORCE can have a reduced number of iterations required for convergence. This leads to improved iteration complexity, as employing the exponential utility does not entail additional computation per iteration. We characterize the conditions under which risk-sensitive algorithms can achieve better iteration complexity. Our <b>simulation</b> results also validate that risk-averse cases can converge and stabilize more quickly after approximately half of the episodes compared to their risk-neutral counterparts.</p></p class="citation"></blockquote><h3 id=1853--183310-unsupervised-learning-of-hybrid-latent-dynamics-a-learn-to-identify-framework-yubo-ye-et-al-2024>(18/53 | 183/310) Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework (Yubo Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yubo Ye, Sumeet Vadhavkar, Xiajun Jiang, Ryan Missel, Huafeng Liu, Linwei Wang. (2024)<br><strong>Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework</strong><br><button class=copy-to-clipboard title="Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Meta Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08194v1.pdf filename=2403.08194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern applications increasingly require <b>unsupervised</b> <b>learning</b> of latent dynamics from high-dimensional time-series. This presents a significant challenge of identifiability: many abstract latent representations may reconstruct observations, yet do they guarantee an adequate identification of the governing dynamics? This paper investigates this challenge from two angles: the use of physics inductive bias specific to the data being modeled, and a learn-to-identify strategy that separates forecasting objectives from the data used for the identification. We combine these two strategies in a novel framework for <b>unsupervised</b> <b>meta-learning</b> <b>of</b> hybrid latent dynamics <b>(Meta-HyLaD)</b> <b>with:</b> 1) a latent dynamic function that hybridize known mathematical expressions of prior physics with neural functions describing its unknown errors, and 2) a <b>meta-learning</b> <b>formulation</b> to learn to separately identify both components of the hybrid dynamics. Through extensive experiments on five physics and one biomedical systems, we provide strong evidence for the benefits of <b>Meta-HyLaD</b> <b>to</b> integrate rich prior knowledge while identifying their gap to observed data.</p></p class="citation"></blockquote><h3 id=1953--184310-semi-supervised-learning-for-anomaly-traffic-detection-via-bidirectional-normalizing-flows-zhangxuan-dang-et-al-2024>(19/53 | 184/310) Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows (Zhangxuan Dang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhangxuan Dang, Yu Zheng, Xinglin Lin, Chunlei Peng, Qiuyu Chen, Xinbo Gao. (2024)<br><strong>Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows</strong><br><button class=copy-to-clipboard title="Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Anomaly Detection, Benchmarking, Benchmarking, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10550v1.pdf filename=2403.10550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid development of the Internet, various types of <b>anomaly</b> <b>traffic</b> are threatening network security. We consider the problem of <b>anomaly</b> <b>network</b> traffic detection and propose a three-stage <b>anomaly</b> <b>detection</b> framework using only normal traffic. Our framework can generate pseudo <b>anomaly</b> <b>samples</b> without prior knowledge of anomalies to achieve the detection of <b>anomaly</b> <b>data.</b> Firstly, we employ a reconstruction method to learn the deep representation of normal samples. Secondly, these representations are normalized to a standard normal distribution using a bidirectional flow module. To simulate <b>anomaly</b> <b>samples,</b> we add noises to the normalized representations which are then passed through the generation direction of the bidirectional flow module. Finally, a simple classifier is trained to differentiate the normal samples and pseudo <b>anomaly</b> <b>samples</b> in the latent space. During inference, our framework requires only two modules to detect anomalous samples, leading to a considerable reduction in model size. According to the experiments, our method achieves the state of-the-art results on the common <b>benchmarking</b> datasets of <b>anomaly</b> <b>network</b> traffic detection. The code is given in the <a href=https://github.com/ZxuanDang/ATD-via-Flows.git>https://github.com/ZxuanDang/ATD-via-Flows.git</a></p></p class="citation"></blockquote><h3 id=2053--185310-causal-graph-neural-networks-for-wildfire-danger-prediction-shan-zhao-et-al-2024>(20/53 | 185/310) Causal Graph Neural Networks for Wildfire Danger Prediction (Shan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shan Zhao, Ioannis Prapas, Ilektra Karasante, Zhitong Xiong, Ioannis Papoutsis, Gustau Camps-Valls, Xiao Xiang Zhu. (2024)<br><strong>Causal Graph Neural Networks for Wildfire Danger Prediction</strong><br><button class=copy-to-clipboard title="Causal Graph Neural Networks for Wildfire Danger Prediction" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08414v1.pdf filename=2403.08414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> that explicitly model the causal mechanism among complex variables via <b>graph</b> <b>learning.</b> <b>The</b> causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology&rsquo;s effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships. Furthermore, SHAP values from our trained model further enhance our understanding of the model&rsquo;s inner workings.</p></p class="citation"></blockquote><h3 id=2153--186310-reduced-jeffries-matusita-distance-a-novel-loss-function-to-improve-generalization-performance-of-deep-classification-models-mohammad-lashkari-et-al-2024>(21/53 | 186/310) Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models (Mohammad Lashkari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Lashkari, Amin Gheibi. (2024)<br><strong>Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models</strong><br><button class=copy-to-clipboard title="Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68Q32, 68T05, 68T45, 68R10, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Node Classification, Graph, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08408v1.pdf filename=2403.08408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The generalization performance of deep neural networks in classification tasks is a major concern in machine learning research. Despite widespread techniques used to diminish the over-fitting issue such as <b>data</b> <b>augmentation,</b> pseudo-labeling, regularization, and ensemble learning, this performance still needs to be enhanced with other approaches. In recent years, it has been theoretically demonstrated that the loss function characteristics i.e. its Lipschitzness and maximum value affect the generalization performance of deep neural networks which can be utilized as a guidance to propose novel distance measures. In this paper, by analyzing the aforementioned characteristics, we introduce a distance called Reduced Jeffries-Matusita as a loss function for training deep classification models to reduce the over-fitting issue. In our experiments, we evaluate the new loss function in two different problems: image classification in computer vision and <b>node</b> <b>classification</b> in the context of <b>graph</b> learning. The results show that the new distance measure stabilizes the training process significantly, enhances the generalization ability, and improves the performance of the models in the Accuracy and F1-score metrics, even if the training set size is small.</p></p class="citation"></blockquote><h3 id=2253--187310-bg-hgnn-toward-scalable-and-efficient-heterogeneous-graph-neural-network-junwei-su-et-al-2024>(22/53 | 187/310) BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network (Junwei Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwei Su, Lingjun Mao, Chuan Wu. (2024)<br><strong>BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network</strong><br><button class=copy-to-clipboard title="BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08207v1.pdf filename=2403.08207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many computer vision and machine learning problems are modelled as learning tasks on heterogeneous <b>graphs,</b> <b>featuring</b> <b>a</b> wide array of relations from diverse types of nodes and edges. Heterogeneous <b>graph</b> <b>neural</b> <b>networks</b> (HGNNs) stand out as a promising neural model class designed for heterogeneous <b>graphs.</b> <b>Built</b> <b>on</b> traditional <b>GNNs,</b> existing HGNNs employ different parameter spaces to model the varied relationships. However, the practical effectiveness of existing HGNNs is often limited to simple heterogeneous <b>graphs</b> <b>with</b> <b>few</b> relation types. This paper first highlights and demonstrates that the standard approach employed by existing HGNNs inevitably leads to parameter explosion and relation collapse, making HGNNs less effective or impractical for complex heterogeneous <b>graphs</b> <b>with</b> <b>numerous</b> relation types. To overcome this issue, we introduce a novel framework, Blend&amp;Grind-HGNN (BG-HGNN), which effectively tackles the challenges by carefully integrating different relations into a unified feature space manageable by a single set of parameters. This results in a refined HGNN method that is more efficient and effective in learning from heterogeneous <b>graphs,</b> <b>especially</b> <b>when</b> the number of relations grows. Our empirical studies illustrate that BG-HGNN significantly surpasses existing HGNNs in terms of parameter efficiency (up to 28.96 $\times$), training throughput (up to 8.12 $\times$), and accuracy (up to 1.07 $\times$).</p></p class="citation"></blockquote><h3 id=2353--188310-one-shot-averaging-for-distributed-tdλ-under-markov-sampling-haoxing-tian-et-al-2024>(23/53 | 188/310) One-Shot Averaging for Distributed TD($λ$) Under Markov Sampling (Haoxing Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxing Tian, Ioannis Ch. Paschalidis, Alex Olshevsky. (2024)<br><strong>One-Shot Averaging for Distributed TD($λ$) Under Markov Sampling</strong><br><button class=copy-to-clipboard title="One-Shot Averaging for Distributed TD($λ$) Under Markov Sampling" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08896v1.pdf filename=2403.08896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a distributed setup for <b>reinforcement</b> <b>learning,</b> where each agent has a copy of the same <b>Markov</b> <b>Decision</b> <b>Process</b> but transitions are sampled from the corresponding <b>Markov</b> <b>chain</b> <b>independently</b> by each agent. We show that in this setting, we can achieve a linear speedup for TD($\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough. Notably, this speedup is achieved by ``one shot averaging,&rsquo;&rsquo; a procedure where the agents run TD($\lambda$) with <b>Markov</b> <b>sampling</b> <b>independently</b> and only average their results after the final step. This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work.</p></p class="citation"></blockquote><h3 id=2453--189310-when-can-we-approximate-wide-contrastive-models-with-neural-tangent-kernels-and-principal-component-analysis-gautham-govind-anil-et-al-2024>(24/53 | 189/310) When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis? (Gautham Govind Anil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gautham Govind Anil, Pascal Esser, Debarghya Ghoshdastidar. (2024)<br><strong>When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?</strong><br><button class=copy-to-clipboard title="When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08673v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08673v1.pdf filename=2403.08673v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>learning</b> is a paradigm for learning representations from unlabelled data that has been highly successful for image and text data. Several recent works have examined <b>contrastive</b> <b>losses</b> to claim that <b>contrastive</b> <b>models</b> effectively learn spectral embeddings, while few works show relations between (wide) <b>contrastive</b> <b>models</b> and kernel principal component analysis (PCA). However, it is not known if trained <b>contrastive</b> <b>models</b> indeed correspond to kernel methods or PCA. In this work, we analyze the training dynamics of two-layer <b>contrastive</b> <b>models,</b> with non-linear activation, and answer when these models are close to PCA or kernel methods. It is well known in the <b>supervised</b> setting that neural networks are equivalent to neural tangent kernel (NTK) machines, and that the NTK of infinitely wide networks remains constant during training. We provide the first convergence results of NTK for <b>contrastive</b> <b>losses,</b> and present a nuanced picture: NTK of wide networks remains almost constant for cosine similarity based <b>contrastive</b> <b>losses,</b> but not for losses based on dot product similarity. We further study the training dynamics of <b>contrastive</b> <b>models</b> with orthogonality constraints on output layer, which is implicitly assumed in works relating <b>contrastive</b> <b>learning</b> to spectral embedding. Our deviation bounds suggest that representations learned by <b>contrastive</b> <b>models</b> are close to the principal components of a certain matrix computed from random features. We empirically show that our theoretical results possibly hold beyond two-layer networks.</p></p class="citation"></blockquote><h3 id=2553--190310-improving-implicit-regularization-of-sgd-with-preconditioning-for-least-square-problems-junwei-su-et-al-2024>(25/53 | 190/310) Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems (Junwei Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwei Su, Difan Zou, Chuan Wu. (2024)<br><strong>Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems</strong><br><button class=copy-to-clipboard title="Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08585v1.pdf filename=2403.08585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of <b>SGD</b> is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of <b>SGD</b> and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of <b>SGD</b> with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned <b>SGD</b> and (standard & preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving <b>SGD</b> with preconditioning. First, we establish excess risk bounds (generalization performance) for preconditioned <b>SGD</b> and ridge regression under an arbitrary preconditions matrix. Second, leveraging the excessive risk characterization of preconditioned <b>SGD</b> and ridge regression, we show that (through construction) there exists a simple preconditioned matrix that can outperform (standard & preconditioned) ridge regression. Finally, we show that our proposed preconditioning matrix is straightforward enough to allow robust estimation from finite samples while maintaining a theoretical advantage over ridge regression. Our empirical results align with our theoretical findings, collectively showcasing the enhanced regularization effect of preconditioned <b>SGD.</b></p></p class="citation"></blockquote><h3 id=2653--191310-learning-to-watermark-llm-generated-text-via-reinforcement-learning-xiaojun-xu-et-al-2024>(26/53 | 191/310) Learning to Watermark LLM-generated Text via Reinforcement Learning (Xiaojun Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaojun Xu, Yuanshun Yao, Yang Liu. (2024)<br><strong>Learning to Watermark LLM-generated Text via Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Learning to Watermark LLM-generated Text via Reinforcement Learning" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10553v1.pdf filename=2403.10553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study how to watermark <b>LLM</b> outputs, i.e. embedding algorithmically detectable signals into <b>LLM-generated</b> text to track misuse. Unlike the current mainstream methods that work with a fixed <b>LLM,</b> we expand the watermark design space by including the <b>LLM</b> tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the <b>LLM</b> weights, and such signals can be detected by a paired detector. We propose a co-training framework based on <b>reinforcement</b> <b>learning</b> that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the <b>LLM</b> to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks). It also allows watermarked model open-sourcing. In addition, if used together with alignment, the extra overhead introduced is low - only training an extra reward model (i.e. our detector). We hope our work can bring more effort into studying a broader watermark design that is not limited to working with a fixed <b>LLM.</b> We open-source the code: <a href=https://github.com/xiaojunxu/learning-to-watermark-llm>https://github.com/xiaojunxu/learning-to-watermark-llm</a> .</p></p class="citation"></blockquote><h3 id=2753--192310-deep-submodular-peripteral-networks-gantavya-bhatt-et-al-2024>(27/53 | 192/310) Deep Submodular Peripteral Networks (Gantavya Bhatt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gantavya Bhatt, Arnav Das, Jeff Bilmes. (2024)<br><strong>Deep Submodular Peripteral Networks</strong><br><button class=copy-to-clipboard title="Deep Submodular Peripteral Networks" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Active Learning, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08199v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08199v2.pdf filename=2403.08199v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a <b>contrastive-learning</b> <b>inspired</b> GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style &ldquo;peripteral&rdquo; loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional <b>contrastive</b> <b>learning,</b> our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strategies for training, including <b>active-learning</b> <b>inspired</b> submodular feedback. We demonstrate DSPNs&rsquo; efficacy in learning submodularity from a costly target submodular function showing superiority in downstream tasks such as experimental design and streaming applications.</p></p class="citation"></blockquote><h3 id=2853--193310-reproducibility-and-geometric-intrinsic-dimensionality-an-investigation-on-graph-neural-network-research-tobias-hille-et-al-2024>(28/53 | 193/310) Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research (Tobias Hille et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Hille, Maximilian Stubbemann, Tom Hanika. (2024)<br><strong>Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research</strong><br><button class=copy-to-clipboard title="Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T01 68T07 68T09 51F99, I-2-6, cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08438v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08438v2.pdf filename=2403.08438v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for <b>graph</b> <b>neural</b> <b>networks.</b> Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data and impeding the training and inference processes. Using the closely linked concept of geometric intrinsic dimension we investigate to which extend the used machine learning models are influenced by the intrinsic dimension of the data sets they are trained on.</p></p class="citation"></blockquote><h3 id=2953--194310-decoupled-federated-learning-on-long-tailed-and-non-iid-data-with-feature-statistics-zhuoxin-chen-et-al-2024>(29/53 | 194/310) Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics (Zhuoxin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoxin Chen, Zhenyu Wu, Yang Ji. (2024)<br><strong>Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics</strong><br><button class=copy-to-clipboard title="Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08364v1.pdf filename=2403.08364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions. This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance. To address this issue, we propose a two-stage Decoupled <b>Federated</b> <b>learning</b> framework using Feature Statistics (DFL-FS). In the first stage, the server estimates the client&rsquo;s class coverage distributions through masked local feature statistics <b>clustering</b> to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage. In the second stage, DFL-FS employs <b>federated</b> <b>feature</b> regeneration based on global feature statistics and utilizes resampling and weighted covariance to calibrate the global classifier to enhance the model&rsquo;s adaptability to long-tailed data distributions. We conducted experiments on CIFAR10-LT and CIFAR100-LT datasets with various long-tailed rates. The results demonstrate that our method outperforms state-of-the-art methods in both accuracy and convergence rate.</p></p class="citation"></blockquote><h3 id=3053--195310-structural-positional-encoding-for-knowledge-integration-in-transformer-based-medical-process-monitoring-christopher-irwin-et-al-2024>(30/53 | 195/310) Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring (Christopher Irwin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher Irwin, Marco Dossena, Giorgio Leonardi, Stefania Montani. (2024)<br><strong>Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring</strong><br><button class=copy-to-clipboard title="Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08836v1.pdf filename=2403.08836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predictive process monitoring is a process mining task aimed at forecasting information about a running process trace, such as the most correct next activity to be executed. In medical domains, predictive process monitoring can provide valuable decision support in atypical and nontrivial situations. Decision support and quality assessment in medicine cannot ignore domain knowledge, in order to be grounded on all the available information (which is not limited to data) and to be really acceptable by end users. In this paper, we propose a predictive process monitoring approach relying on the use of a {\em transformer}, a deep learning architecture based on the attention mechanism. A major contribution of our work lies in the incorporation of ontological domain-specific knowledge, carried out through a <b>graph</b> positional encoding technique. The paper presents and discusses the encouraging experimental result we are collecting in the domain of stroke management.</p></p class="citation"></blockquote><h3 id=3153--196310-representing-molecules-as-random-walks-over-interpretable-grammars-michael-sun-et-al-2024>(31/53 | 196/310) Representing Molecules as Random Walks Over Interpretable Grammars (Michael Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Sun, Minghao Guo, Weize Yuan, Veronika Thost, Crystal Elaine Owens, Aristotle Franklin Grosz, Sharvaa Selvan, Katelyn Zhou, Hassan Mohiuddin, Benjamin J Pedretti, Zachary P Smith, Jie Chen, Wojciech Matusik. (2024)<br><strong>Representing Molecules as Random Walks Over Interpretable Grammars</strong><br><button class=copy-to-clipboard title="Representing Molecules as Random Walks Over Interpretable Grammars" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 13<br>Keywords: Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08147v1.pdf filename=2403.08147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and <b>reasoning</b> over such molecules in terms of <b>graph</b> grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method&rsquo;s chemical interpretability.</p></p class="citation"></blockquote><h3 id=3253--197310-refresh-responsible-and-efficient-feature-reselection-guided-by-shap-values-shubham-sharma-et-al-2024>(32/53 | 197/310) REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values (Shubham Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubham Sharma, Sanghamitra Dutta, Emanuele Albini, Freddy Lecue, Daniele Magazzeni, Manuela Veloso. (2024)<br><strong>REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values</strong><br><button class=copy-to-clipboard title="REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08880v1.pdf filename=2403.08880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feature selection is a crucial step in building machine learning models. This process is often achieved with accuracy as an objective, and can be cumbersome and computationally expensive for large-scale datasets. Several additional model performance characteristics such as <b>fairness</b> and robustness are of importance for model development. As regulations are driving the need for more trustworthy models, deployed models need to be corrected for model characteristics associated with responsible artificial intelligence. When feature selection is done with respect to one model performance characteristic (eg. accuracy), feature selection with secondary model performance characteristics (eg. <b>fairness</b> and robustness) as objectives would require going through the computationally expensive selection process from scratch. In this paper, we introduce the problem of feature \emph{reselection}, so that features can be selected with respect to secondary model performance characteristics efficiently even after a feature selection process has been done with respect to a primary objective. To address this problem, we propose REFRESH, a method to reselect features so that additional constraints that are desirable towards model performance can be achieved without having to train several new models. REFRESH&rsquo;s underlying algorithm is a novel technique using SHAP values and correlation analysis that can approximate for the predictions of a model without having to train these models. Empirical evaluations on three datasets, including a large-scale loan defaulting dataset show that REFRESH can help find alternate models with better model characteristics efficiently. We also discuss the need for reselection and REFRESH based on regulation desiderata.</p></p class="citation"></blockquote><h3 id=3353--198310-implicit-regularization-of-gradient-flow-on-one-layer-softmax-attention-heejune-sheen-et-al-2024>(33/53 | 198/310) Implicit Regularization of Gradient Flow on One-Layer Softmax Attention (Heejune Sheen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heejune Sheen, Siyu Chen, Tianhao Wang, Harrison H. Zhou. (2024)<br><strong>Implicit Regularization of Gradient Flow on One-Layer Softmax Attention</strong><br><button class=copy-to-clipboard title="Implicit Regularization of Gradient Flow on One-Layer Softmax Attention" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Karush-Kuhn-Tucker<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08699v1.pdf filename=2403.08699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately. Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices. Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights. This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training. For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate <b>KKT</b> conditions of the SVM associated with the classification data. Moreover, the results are extended to general weights configurations given proper alignment of the weight matrices&rsquo; singular spaces with the data features at initialization.</p></p class="citation"></blockquote><h3 id=3453--199310-bifurcated-attention-for-single-context-large-batch-sampling-ben-athiwaratkun-et-al-2024>(34/53 | 199/310) Bifurcated Attention for Single-Context Large-Batch Sampling (Ben Athiwaratkun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Jiacheng Guo, Liangfu Chen, Parminder Bhatia, Ramesh Nallapati, Sudipta Sengupta, Bing Xiang. (2024)<br><strong>Bifurcated Attention for Single-Context Large-Batch Sampling</strong><br><button class=copy-to-clipboard title="Bifurcated Attention for Single-Context Large-Batch Sampling" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Rerank<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08845v1.pdf filename=2403.08845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation without substantially increasing latency, enhancing performance when integrated with postprocessing techniques such as <b>reranking.</b></p></p class="citation"></blockquote><h3 id=3553--200310-verifix-post-training-correction-to-improve-label-noise-robustness-with-verified-samples-sangamesh-kodge-et-al-2024>(35/53 | 200/310) Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples (Sangamesh Kodge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangamesh Kodge, Deepak Ravikumar, Gobinda Saha, Kaushik Roy. (2024)<br><strong>Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples</strong><br><button class=copy-to-clipboard title="Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08618v1.pdf filename=2403.08618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or <b>adversarial</b> <b>attacks.</b> Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model&rsquo;s weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix&rsquo;s effectiveness on both synthetic and real-world label noise. Experiments on the CIFAR dataset with 25% synthetic corruption show 7.36% generalization improvements on average. Additionally, we observe generalization improvements of up to 2.63% on naturally corrupted datasets like WebVision1.0 and Clothing1M.</p></p class="citation"></blockquote><h3 id=3653--201310-can-physical-information-aid-the-generalization-ability-of-neural-networks-for-hydraulic-modeling-gianmarco-guglielmo-et-al-2024>(36/53 | 201/310) Can physical information aid the generalization ability of Neural Networks for hydraulic modeling? (Gianmarco Guglielmo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianmarco Guglielmo, Andrea Montessori, Jean-Michel Tucny, Michele La Rocca, Pietro Prestininzi. (2024)<br><strong>Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?</strong><br><button class=copy-to-clipboard title="Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08589v1.pdf filename=2403.08589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from <b>data</b> <b>scarcity,</b> a challenge for machine learning techniques. Consequently, many purely <b>data-driven</b> <b>Neural</b> Networks proved to lack predictive capabilities. In this work, we propose to mitigate such problem by introducing physical information into the training phase. The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts. Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers. Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods. Instead, we envisaged the employment of Neural Networks as neural operators, featuring physical constraints formulated without resorting to PDEs. The proposed novel methodology shares similarities with <b>data</b> <b>augmentation</b> and regularization. We show that incorporating such soft physical information can improve predictive capabilities.</p></p class="citation"></blockquote><h3 id=3753--202310-caformer-rethinking-time-series-analysis-from-causal-perspective-kexuan-zhang-et-al-2024>(37/53 | 202/310) Caformer: Rethinking Time Series Analysis from Causal Perspective (Kexuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kexuan Zhang, Xiaobei Zou, Yang Tang. (2024)<br><strong>Caformer: Rethinking Time Series Analysis from Causal Perspective</strong><br><button class=copy-to-clipboard title="Caformer: Rethinking Time Series Analysis from Causal Perspective" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08572v1.pdf filename=2403.08572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series analysis is a vital task with broad applications in various domains. However, effectively capturing cross-dimension and cross-time dependencies in non-stationary time series poses significant challenges, particularly in the context of environmental factors. The spurious correlation induced by the environment confounds the causal relationships between cross-dimension and cross-time dependencies. In this paper, we introduce a novel framework called Caformer (\underline{\textbf{Ca}}usal Trans\underline{\textbf{former}}) for time series analysis from a causal perspective. Specifically, our framework comprises three components: Dynamic Learner, Environment Learner, and Dependency Learner. The Dynamic Learner unveils dynamic interactions among dimensions, the Environment Learner mitigates spurious correlations caused by environment with a back-door adjustment, and the Dependency Learner aims to infer robust interactions across both time and dimensions. Our Caformer demonstrates consistent state-of-the-art performance across five mainstream time series analysis tasks, including long- and short-term forecasting, imputation, classification, and <b>anomaly</b> <b>detection,</b> with proper interpretability.</p></p class="citation"></blockquote><h3 id=3853--203310-an-analysis-of-human-alignment-of-latent-diffusion-models-lorenz-linhardt-et-al-2024>(38/53 | 203/310) An Analysis of Human Alignment of Latent Diffusion Models (Lorenz Linhardt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenz Linhardt, Marco Morik, Sidney Bender, Naima Elosegui Borras. (2024)<br><strong>An Analysis of Human Alignment of Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="An Analysis of Human Alignment of Latent Diffusion Models" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08469v1.pdf filename=2403.08469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models,</b> trained on large amounts of data, showed remarkable performance for image synthesis. They have high error consistency with humans and low texture bias when used for classification. Furthermore, prior work demonstrated the decomposability of their bottleneck layer representations into semantic directions. In this work, we analyze how well such representations are aligned to human responses on a triplet odd-one-out task. We find that despite the aforementioned observations: I) The representational alignment with humans is comparable to that of models trained only on ImageNet-1k. II) The most aligned layers of the denoiser U-Net are intermediate layers and not the bottleneck. III) Text conditioning greatly improves alignment at high noise levels, hinting at the importance of abstract textual information, especially in the early stage of generation.</p></p class="citation"></blockquote><h3 id=3953--204310-nonlinear-manifold-learning-determines-microgel-size-from-raman-spectroscopy-eleni-d-koronaki-et-al-2024>(39/53 | 204/310) Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy (Eleni D. Koronaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eleni D. Koronaki, Luise F. Kaven, Johannes M. M. Faust, Ioannis G. Kevrekidis, Alexander Mitsos. (2024)<br><strong>Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy</strong><br><button class=copy-to-clipboard title="Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08376v1.pdf filename=2403.08376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Polymer particle size constitutes a crucial characteristic of product quality in polymerization. Raman spectroscopy is an established and reliable process analytical technology for in-line concentration monitoring. Recent approaches and some theoretical considerations show a correlation between Raman signals and particle sizes but do not determine polymer size from Raman spectroscopic measurements accurately and reliably. With this in mind, we propose three alternative machine learning workflows to perform this task, all involving diffusion maps, a nonlinear manifold learning technique for dimensionality reduction: (i) directly from diffusion maps, (ii) alternating diffusion maps, and (iii) conformal <b>autoencoder</b> neural networks. We apply the workflows to a data set of Raman spectra with associated size measured via dynamic light scattering of 47 microgel (cross-linked polymer) samples in a diameter range of 208nm to 483 nm. The conformal <b>autoencoders</b> substantially outperform state-of-the-art methods and results for the first time in a promising prediction of polymer size from Raman spectra.</p></p class="citation"></blockquote><h3 id=4053--205310-data-augmentation-with-automated-machine-learning-approaches-and-performance-comparison-with-classical-data-augmentation-methods-alhassan-mumuni-et-al-2024>(40/53 | 205/310) Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods (Alhassan Mumuni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alhassan Mumuni, Fuseini Mumuni. (2024)<br><strong>Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods</strong><br><button class=copy-to-clipboard title="Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08352v1.pdf filename=2403.08352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate <b>data</b> <b>transformation</b> operations to create new <b>data</b> <b>samples</b> with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated <b>data</b> <b>augmentation</b> methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based <b>data</b> <b>augmentation</b> techniques. We discuss various approaches for accomplishing <b>data</b> <b>augmentation</b> with AutoML, including <b>data</b> <b>manipulation,</b> <b>data</b> <b>integration</b> and <b>data</b> <b>synthesis</b> techniques. We present extensive discussion of techniques for realizing each of the major subtasks of the <b>data</b> <b>augmentation</b> process: search space design, hyperparameter optimization and model evaluation. Finally, we carried out an extensive comparison and analysis of the performance of automated <b>data</b> <b>augmentation</b> techniques and state-of-the-art methods based on classical augmentation approaches. The results show that AutoML methods for <b>data</b> <b>augmentation</b> currently outperform state-of-the-art techniques based on conventional approaches.</p></p class="citation"></blockquote><h3 id=4153--206310-bayesian-optimization-that-limits-search-region-to-lower-dimensions-utilizing-local-gpr-yasunori-taguchi-et-al-2024>(41/53 | 206/310) Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR (Yasunori Taguchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasunori Taguchi, Hiro Gangi. (2024)<br><strong>Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR</strong><br><button class=copy-to-clipboard title="Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08331v1.pdf filename=2403.08331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimization of product and system characteristics is required in many fields, including design and control. Bayesian optimization (BO) is often used when there are high observing costs, because BO theoretically guarantees an upper bound on regret. However, computational costs increase exponentially with the number of parameters to be optimized, decreasing search efficiency. We propose a BO that limits the search region to lower dimensions and utilizes local <b>Gaussian</b> <b>process</b> regression (LGPR) to scale the BO to higher dimensions. LGPR treats the low-dimensional search region as &ldquo;local,&rdquo; improving prediction accuracies there. The LGPR model is trained on a local subset of data specific to that region. This improves prediction accuracy and search efficiency and reduces the time complexity of matrix inversion in the <b>Gaussian</b> <b>process</b> regression. In evaluations with 20D Ackley and Rosenbrock functions, search efficiencies are equal to or higher than those of the compared methods, improved by about 69% and 40% from the case without LGPR. We apply our method to an automatic design task for a power semiconductor device. We successfully reduce the specific on-resistance to 25% better than a conventional method and 3.4% better than without LGPR.</p></p class="citation"></blockquote><h3 id=4253--207310-machine-unlearning-taxonomy-metrics-applications-challenges-and-prospects-na-li-et-al-2024>(42/53 | 207/310) Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects (Na Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Na Li, Chunyi Zhou, Yansong Gao, Hui Chen, Anmin Fu, Zhi Zhang, Yu Shui. (2024)<br><strong>Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects</strong><br><button class=copy-to-clipboard title="Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08254v1.pdf filename=2403.08254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personal digital data is a critical asset, and governments worldwide have enforced laws and regulations to protect data privacy. Data users have been endowed with the right to be forgotten of their data. In the course of <b>machine</b> <b>learning</b> (ML), the forgotten right requires a model provider to delete user data and its subsequent impact on ML models upon user requests. <b>Machine</b> <b>unlearning</b> emerges to address this, which has garnered ever-increasing attention from both industry and academia. While the area has developed rapidly, there is a lack of comprehensive surveys to capture the latest advancements. Recognizing this shortage, we conduct an extensive exploration to map the landscape of <b>machine</b> <b>unlearning</b> including the (fine-grained) taxonomy of unlearning algorithms under centralized and distributed settings, debate on approximate unlearning, verification and evaluation metrics, challenges and solutions for unlearning under different applications, as well as attacks targeting <b>machine</b> <b>unlearning.</b> The survey concludes by outlining potential directions for future research, hoping to serve as a guide for interested scholars.</p></p class="citation"></blockquote><h3 id=4353--208310-page-domain-incremental-adaptation-with-past-agnostic-generative-replay-for-smart-healthcare-chia-hao-li-et-al-2024>(43/53 | 208/310) PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare (Chia-Hao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chia-Hao Li, Niraj K. Jha. (2024)<br><strong>PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare</strong><br><button class=copy-to-clipboard title="PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08197v1.pdf filename=2403.08197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose PAGE, a <b>domain-incremental</b> <b>adaptation</b> strategy with past-agnostic generative replay for smart healthcare. PAGE enables generative replay without the aid of any preserved data or information from prior <b>domains.</b> <b>When</b> adapting to a new <b>domain,</b> <b>it</b> exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous <b>domains.</b> <b>By</b> replaying the synthetic data with the new real data during training, PAGE achieves a good balance between <b>domain</b> <b>adaptation</b> and knowledge retention. In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result. This makes the predictions interpretable and provides statistical guarantees for disease detection in smart healthcare applications. We demonstrate PAGE&rsquo;s effectiveness in <b>domain-incremental</b> <b>disease</b> detection with three distinct disease datasets collected from commercially available WMSs. PAGE achieves highly competitive performance against state-of-the-art with superior scalability, data privacy, and feasibility. Furthermore, PAGE can enable up to 75% reduction in clinical workload with the help of EICP.</p></p class="citation"></blockquote><h3 id=4453--209310-measuring-the-energy-consumption-and-efficiency-of-deep-neural-networks-an-empirical-analysis-and-design-recommendations-charles-edison-tripp-et-al-2024>(44/53 | 209/310) Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations (Charles Edison Tripp et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charles Edison Tripp, Jordan Perr-Sauer, Jamil Gafur, Amabarish Nag, Avi Purkayastha, Sagi Zisman, Erik A. Bensen. (2024)<br><strong>Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations</strong><br><button class=copy-to-clipboard title="Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08151v1.pdf filename=2403.08151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the so-called <code>Red-AI'' trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures. We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network </code>shapes&rsquo;&rsquo;, and 14 depths on both CPU and GPU hardware collected using node-level watt-meters. This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects. We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy. Our analysis also uncovers a surprising, hardware-mediated non-linear relationship between energy efficiency and network design, challenging the assumption that reducing the number of parameters or FLOPs is the best way to achieve greater energy efficiency. Highlighting the need for cache-considerate algorithm development, we suggest a combined approach to energy efficient network, algorithm, and hardware design. This work contributes to the fields of sustainable computing and Green AI, offering practical guidance for creating more energy-efficient neural networks and promoting sustainable AI.</p></p class="citation"></blockquote><h3 id=4553--210310-a-sparsity-principle-for-partially-observable-causal-representation-learning-danru-xu-et-al-2024>(45/53 | 210/310) A Sparsity Principle for Partially Observable Causal Representation Learning (Danru Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danru Xu, Dingling Yao, Sébastien Lachapelle, Perouz Taslakian, Julius von Kügelgen, Francesco Locatello, Sara Magliacane. (2024)<br><strong>A Sparsity Principle for Partially Observable Causal Representation Learning</strong><br><button class=copy-to-clipboard title="A Sparsity Principle for Partially Observable Causal Representation Learning" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 8<br>Keywords: Benchmarking, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08335v1.pdf filename=2403.08335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal <b>representation</b> <b>learning</b> aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variables by enforcing sparsity in the inferred <b>representation.</b> <b>Experiments</b> on different simulated datasets and established <b>benchmarks</b> highlight the effectiveness of our approach in recovering the ground-truth latents.</p></p class="citation"></blockquote><h3 id=4653--211310-structural-perspective-on-constraint-based-learning-of-markov-networks-tuukka-korhonen-et-al-2024>(46/53 | 211/310) Structural perspective on constraint-based learning of Markov networks (Tuukka Korhonen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuukka Korhonen, Fedor V. Fomin, Pekka Parviainen. (2024)<br><strong>Structural perspective on constraint-based learning of Markov networks</strong><br><button class=copy-to-clipboard title="Structural perspective on constraint-based learning of Markov networks" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DM, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08562v1.pdf filename=2403.08562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Markov networks are probabilistic graphical models that employ undirected <b>graphs</b> to depict conditional independence relationships among variables. Our focus lies in constraint-based structure learning, which entails learning the undirected <b>graph</b> from data through the execution of conditional independence tests. We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets. These bounds uncover an exciting interplay between the structural properties of the <b>graph</b> and the amount of tests required to learn a Markov network. The starting point of our work is that the <b>graph</b> parameter maximum pairwise connectivity, $\kappa$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the <b>graph,</b> is responsible for the sizes of independence tests required to learn the <b>graph.</b> On one hand, we show that at least one test with the size of the conditioning set at least $\kappa$ is always necessary. On the other hand, we prove that any <b>graph</b> can be learned by performing tests of size at most $\kappa$. This completely resolves the question of the minimum size of conditioning sets required to learn the <b>graph.</b> When it comes to the number of tests, our upper bound on the sizes of conditioning sets implies that every $n$-vertex <b>graph</b> can be learned by at most $n^{\kappa}$ tests with conditioning sets of sizes at most $\kappa$. We show that for any upper bound $q$ on the sizes of the conditioning sets, there exist <b>graphs</b> with $O(n q)$ vertices that require at least $n^{\Omega(\kappa)}$ tests to learn. This lower bound holds even when the treewidth and the maximum degree of the <b>graph</b> are at most $\kappa+2$. On the positive side, we prove that every <b>graph</b> of bounded treewidth can be learned by a polynomial number of tests with conditioning sets of sizes at most $2\kappa$.</p></p class="citation"></blockquote><h3 id=4753--212310-learning-enhanced-neighborhood-selection-for-the-vehicle-routing-problem-with-time-windows-willem-feijen-et-al-2024>(47/53 | 212/310) Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows (Willem Feijen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Willem Feijen, Guido Schäfer, Koen Dekker, Seppo Pieterse. (2024)<br><strong>Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows</strong><br><button class=copy-to-clipboard title="Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 90-05, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08839v1.pdf filename=2403.08839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large Neighborhood Search (LNS) is a universal approach that is broadly applicable and has proven to be highly efficient in practice for solving optimization problems. We propose to integrate machine learning (ML) into LNS to assist in deciding which parts of the solution should be destroyed and repaired in each iteration of LNS. We refer to our new approach as Learning-Enhanced Neighborhood Selection (LENS for short). Our approach is universally applicable, i.e., it can be applied to any LNS algorithm to amplify the workings of the destroy algorithm. In this paper, we demonstrate the potential of LENS on the fundamental Vehicle Routing Problem with Time Windows (VRPTW). We implemented an LNS algorithm for VRPTW and collected data on generated novel training instances derived from well-known, extensively utilized <b>benchmark</b> datasets. We trained our LENS approach with this data and compared the experimental results of our approach with two <b>benchmark</b> algorithms: a random neighborhood selection method to show that LENS learns to make informed choices and an oracle neighborhood selection method to demonstrate the potential of our LENS approach. With LENS, we obtain results that significantly improve the quality of the solutions.</p></p class="citation"></blockquote><h3 id=4853--213310-predictive-clustering-of-vessel-behavior-based-on-hierarchical-trajectory-representation-rui-zhang-et-al-2024>(48/53 | 213/310) Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation (Rui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Zhang, Hanyue Wu, Zhenzhong Yin, Zhu Xiao, Yong Xiong, Kezhong Liu. (2024)<br><strong>Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation</strong><br><button class=copy-to-clipboard title="Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08838v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08838v2.pdf filename=2403.08838v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vessel trajectory <b>clustering,</b> which aims to find similar trajectory patterns, has been widely leveraged in overwater applications. Most traditional methods use predefined rules and thresholds to identify discrete vessel behaviors. They aim for high-quality <b>clustering</b> and conduct <b>clustering</b> on entire sequences, whether the original trajectory or its sub-trajectories, failing to represent their evolution. To resolve this problem, we propose a Predictive <b>Clustering</b> of Hierarchical Vessel Behavior (PC-HiV). PC-HiV first uses hierarchical representations to transform every trajectory into a behavioral sequence. Then, it predicts evolution at each timestamp of the sequence based on the representations. By applying predictive <b>clustering</b> and latent encoding, PC-HiV improves <b>clustering</b> and predictions simultaneously. Experiments on real AIS datasets demonstrate PC-HiV&rsquo;s superiority over existing methods, showcasing its effectiveness in capturing behavioral evolution discrepancies between vessel types (tramp vs. liner) and within emission control areas. Results show that our method outperforms NN-Kmeans and Robust DAA by 3.9% and 6.4% of the purity score.</p></p class="citation"></blockquote><h3 id=4953--214310-karina-an-efficient-deep-learning-model-for-global-weather-forecast-minjong-cheon-et-al-2024>(49/53 | 214/310) KARINA: An Efficient Deep Learning Model for Global Weather Forecast (Minjong Cheon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minjong Cheon, Yo-Hwan Choi, Seon-Yu Kang, Yumi Choi, Jeong-Gil Lee, Daehyun Kang. (2024)<br><strong>KARINA: An Efficient Deep Learning Model for Global Weather Forecast</strong><br><button class=copy-to-clipboard title="KARINA: An Efficient Deep Learning Model for Global Weather Forecast" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, physics-ao-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10555v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10555v1.pdf filename=2403.10555v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based, data-driven models are gaining prevalence in climate research, particularly for global weather prediction. However, training the global weather data at high resolution requires massive computational resources. Therefore, we present a new model named KARINA to overcome the substantial computational demands typical of this field. This model achieves forecasting accuracy comparable to higher-resolution counterparts with significantly less computational resources, requiring only 4 NVIDIA A100 GPUs and less than 12 hours of training. KARINA combines ConvNext, SENet, and Geocyclic Padding to enhance weather forecasting at a 2.5{\deg} resolution, which could filter out high-frequency noise. Geocyclic Padding preserves pixels at the lateral boundary of the input image, thereby maintaining atmospheric flow continuity in the spherical Earth. SENet dynamically improves feature response, advancing atmospheric process modeling, particularly in the vertical column process as numerous channels. In this vein, KARINA sets new <b>benchmarks</b> in weather forecasting accuracy, surpassing existing models like the ECMWF S2S reforecasts at a lead time of up to 7 days. Remarkably, KARINA achieved competitive performance even when compared to the recently developed models (Pangu-Weather, GraphCast, ClimaX, and FourCastNet) trained with high-resolution data having 100 times larger pixels. Conclusively, KARINA significantly advances global weather forecasting by efficiently modeling Earth&rsquo;s atmosphere with improved accuracy and resource efficiency.</p></p class="citation"></blockquote><h3 id=5053--215310-scattered-mixture-of-experts-implementation-shawn-tan-et-al-2024>(50/53 | 215/310) Scattered Mixture-of-Experts Implementation (Shawn Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shawn Tan, Yikang Shen, Rameswar Panda, Aaron Courville. (2024)<br><strong>Scattered Mixture-of-Experts Implementation</strong><br><button class=copy-to-clipboard title="Scattered Mixture-of-Experts Implementation" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08245v1.pdf filename=2403.08245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE) on GPUs. ScatterMoE builds upon existing implementations, and overcoming some of the limitations to improve inference and training speed, and memory footprint. This implementation achieves this by avoiding padding and making excessive copies of the input. We introduce ParallelLinear, the main component we use to build our implementation and the various kernels used to speed up the operation. We <b>benchmark</b> our implementation against Megablocks, and show that it enables a higher throughput and lower memory footprint. We also show how ParallelLinear enables extension of the Mixture-of-Experts concept by demonstrating with an implementation of Mixture of Attention.</p></p class="citation"></blockquote><h3 id=5153--216310-robust-decision-aggregation-with-adversarial-experts-yongkang-guo-et-al-2024>(51/53 | 216/310) Robust Decision Aggregation with Adversarial Experts (Yongkang Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongkang Guo, Yuqing Kong. (2024)<br><strong>Robust Decision Aggregation with Adversarial Experts</strong><br><button class=copy-to-clipboard title="Robust Decision Aggregation with Adversarial Experts" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08222v1.pdf filename=2403.08222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a binary decision aggregation problem in the presence of both truthful and adversarial experts. The truthful experts will report their private signals truthfully with proper incentive, while the adversarial experts can report arbitrarily. The decision maker needs to design a robust aggregator to forecast the true state of the world based on the reports of experts. The decision maker does not know the specific information structure, which is a joint distribution of signals, states, and strategies of adversarial experts. We want to find the optimal aggregator minimizing regret under the worst information structure. The regret is defined by the difference in expected loss between the aggregator and a <b>benchmark</b> who makes the optimal decision given the joint distribution and reports of truthful experts. We prove that when the truthful experts are symmetric and adversarial experts are not too numerous, the truncated mean is optimal, which means that we remove some lowest reports and highest reports and take averaging among the left reports. Moreover, for many settings, the optimal aggregators are in the family of piecewise linear functions. The regret is independent of the total number of experts but only depends on the ratio of adversaries. We evaluate our aggregators by numerical experiment in an ensemble learning task. We also obtain some negative results for the aggregation problem with adversarial experts under some more general information structures and experts&rsquo; report space.</p></p class="citation"></blockquote><h3 id=5253--217310-paddingflow-improving-normalizing-flows-with-padding-dimensional-noise-qinglong-meng-et-al-2024>(52/53 | 217/310) PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise (Qinglong Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinglong Meng, Chongkun Xia, Xueqian Wang. (2024)<br><strong>PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise</strong><br><button class=copy-to-clipboard title="PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08216v1.pdf filename=2403.08216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Normalizing flow is a generative modeling approach with efficient sampling. However, Flow-based models suffer two issues, which are manifold and discrete data. If the target distribution is a manifold, which means the dimension of the latent target distribution and the dimension of the data distribution are unmatched, flow-based models might perform badly. Discrete data makes flow-based models collapse into a degenerate mixture of point masses. In this paper, to sidestep such two issues we propose PaddingFlow, a novel dequantization method, which improves normalizing flows with padding-dimensional noise. PaddingFlow is easy to implement, computationally cheap, widely suitable for various tasks, and generates samples that are unbiased estimations of the data. Especially, our method can overcome the limitation of existing dequantization methods that have to change the data distribution, which might degrade performance. We validate our method on the main <b>benchmarks</b> of unconditional density estimation, including five tabular datasets and four image datasets for VAE models, and the IK experiments which are conditional density estimation. The results show that PaddingFlow can provide improvement on all tasks in this paper.</p></p class="citation"></blockquote><h3 id=5353--218310-learning-driven-physically-aware-large-scale-circuit-gate-sizing-yuyang-ye-et-al-2024>(53/53 | 218/310) Learning-driven Physically-aware Large-scale Circuit Gate Sizing (Yuyang Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyang Ye, Peng Xu, Lizheng Ren, Tinghuan Chen, Hao Yan, Bei Yu, Longxing Shi. (2024)<br><strong>Learning-driven Physically-aware Large-scale Circuit Gate Sizing</strong><br><button class=copy-to-clipboard title="Learning-driven Physically-aware Large-scale Circuit Gate Sizing" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-ET, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08193v1.pdf filename=2403.08193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gate sizing plays an important role in timing optimization after physical design. Existing machine learning-based gate sizing works cannot optimize timing on multiple timing paths simultaneously and neglect the physical constraint on layouts. They cause sub-optimal sizing solutions and low-efficiency issues when compared with commercial gate sizing tools. In this work, we propose a learning-driven physically-aware gate sizing framework to optimize timing performance on large-scale circuits efficiently. In our gradient descent optimization-based work, for obtaining accurate gradients, a <b>multi-modal</b> gate sizing-aware timing model is achieved via learning timing information on multiple timing paths and physical information on multiple-scaled layouts jointly. Then, gradient generation based on the sizing-oriented estimator and adaptive back-propagation are developed to update gate sizes. Our results demonstrate that our work achieves higher timing performance improvements in a faster way compared with the commercial gate sizing tool.</p></p class="citation"></blockquote><h2 id=csro-18>cs.RO (18)</h2><h3 id=118--219310-language-grounded-dynamic-scene-graphs-for-interactive-object-search-with-mobile-manipulation-daniel-honerkamp-et-al-2024>(1/18 | 219/310) Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation (Daniel Honerkamp et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Honerkamp, Martin Büchner, Fabien Despinoy, Tim Welschehold, Abhinav Valada. (2024)<br><strong>Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation</strong><br><button class=copy-to-clipboard title="Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 63<br>Keywords: Graph, Simulation, Simulator, Zero-shot, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08605v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08605v2.pdf filename=2403.08605v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To fully leverage the capabilities of mobile manipulation robots, it is imperative that they are able to autonomously execute long-horizon tasks in <b>large</b> <b>unexplored</b> <b>environments.</b> While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown emergent <b>reasoning</b> skills on arbitrary tasks, existing work primarily concentrates on explored environments, typically focusing on either navigation or manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel approach that grounds language models within structured representations derived from open-vocabulary scene <b>graphs,</b> dynamically updated as the environment is explored. We tightly interleave these representations with an object-centric action space. The resulting approach is <b>zero-shot,</b> open-vocabulary, and readily extendable to a spectrum of mobile manipulation and household robotic tasks. We demonstrate the effectiveness of MoMa-LLM in a novel semantic interactive search task in <b>large</b> <b>realistic</b> <b>indoor</b> environments. In extensive experiments in both <b>simulation</b> and the real world, we show substantially improved search efficiency compared to conventional baselines and state-of-the-art approaches, as well as its applicability to more abstract tasks. We make the code publicly available at <a href=http://moma-llm.cs.uni-freiburg.de>http://moma-llm.cs.uni-freiburg.de</a>.</p></p class="citation"></blockquote><h3 id=218--220310-empowering-robotics-with-large-language-models-osmag-map-comprehension-with-llms-fujing-xie-et-al-2024>(2/18 | 220/310) Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs (Fujing Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fujing Xie, Sören Schwertfeger. (2024)<br><strong>Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs</strong><br><button class=copy-to-clipboard title="Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 53<br>Keywords: Graph, Fine-tuning, Fine-tuning, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08228v1.pdf filename=2403.08228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated great potential in robotic applications by providing essential general knowledge for situations that can not be pre-programmed beforehand. Generally speaking, mobile robots need to understand maps to execute tasks such as localization or navigation. In this letter, we address the problem of enabling <b>LLMs</b> to comprehend Area <b>Graph,</b> a text-based map representation, in order to enhance their applicability in the field of mobile robotics. Area <b>Graph</b> is a hierarchical, topometric semantic map representation utilizing polygons to demark areas such as rooms, corridors or buildings. In contrast to commonly used map representations, such as occupancy grid maps or point clouds, osmAG (Area <b>Graph</b> in OpensStreetMap format) is stored in a XML textual format naturally readable by <b>LLMs.</b> Furthermore, conventional robotic algorithms such as localization and path planning are compatible with osmAG, facilitating this map representation comprehensible by <b>LLMs,</b> traditional robotic algorithms and humans. Our experiments show that with a proper map representation, <b>LLMs</b> possess the capability to understand maps and answer queries based on that understanding. Following simple <b>fine-tuning</b> of LLaMA2 models, it surpassed <b>ChatGPT-3.5</b> in tasks involving topology and hierarchy understanding. Our dataset, dataset generation code, <b>fine-tuned</b> LoRA adapters can be accessed at <a href=https://github.com/xiefujing/LLM-osmAG-Comprehension>https://github.com/xiefujing/LLM-osmAG-Comprehension</a>.</p></p class="citation"></blockquote><h3 id=318--221310-copa-general-robotic-manipulation-through-spatial-constraints-of-parts-with-foundation-models-haoxu-huang-et-al-2024>(3/18 | 221/310) CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models (Haoxu Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, Yang Gao. (2024)<br><strong>CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models</strong><br><button class=copy-to-clipboard title="CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 45<br>Keywords: Foundation Model, Geometry, Grounding, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08248v1.pdf filename=2403.08248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within <b>foundation</b> <b>models</b> to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ <b>foundation</b> <b>vision-language</b> models (VLMs) to select the object&rsquo;s grasping part through a novel coarse-to-fine <b>grounding</b> mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial <b>geometry</b> constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal <b>prompt</b> engineering and without additional training. Project page: <a href=https://copa-2024.github.io/>https://copa-2024.github.io/</a></p></p class="citation"></blockquote><h3 id=418--222310-neuromorphic-force-control-in-an-industrial-task-validating-energy-and-latency-benefits-camilo-amaya-et-al-2024>(4/18 | 222/310) Neuromorphic force-control in an industrial task: validating energy and latency benefits (Camilo Amaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Camilo Amaya, Evan Eames, Gintautas Palinauskas, Alexander Perzylo, Yulia Sandamirskaya, Axel von Arnim. (2024)<br><strong>Neuromorphic force-control in an industrial task: validating energy and latency benefits</strong><br><button class=copy-to-clipboard title="Neuromorphic force-control in an industrial task: validating energy and latency benefits" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08928v1.pdf filename=2403.08928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As robots become smarter and more ubiquitous, optimizing the power consumption of intelligent compute becomes imperative towards ensuring the sustainability of technological advancements. Neuromorphic computing hardware makes use of biologically inspired neural architectures to achieve energy and latency improvements compared to conventional von Neumann computing architecture. Applying these benefits to robots has been demonstrated in several works in the field of neurorobotics, typically on relatively simple control tasks. Here, we introduce an example of neuromorphic computing applied to the real-world industrial task of object insertion. We trained a spiking neural network (SNN) to perform force-torque feedback control using a <b>reinforcement</b> <b>learning</b> approach in <b>simulation.</b> We then ported the SNN to the Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At inference time we show latency competitive with current CPU/GPU architectures, two orders of magnitude less energy usage in comparison to traditional low-energy edge-hardware. We offer this example as a proof of concept implementation of a neuromoprhic controller in real-world robotic setting, highlighting the benefits of neuromorphic hardware for the development of intelligent controllers for robots.</p></p class="citation"></blockquote><h3 id=518--223310-real-time-3d-semantic-occupancy-prediction-for-autonomous-vehicles-using-memory-efficient-sparse-convolution-samuel-sze-et-al-2024>(5/18 | 223/310) Real-time 3D semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution (Samuel Sze et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Sze, Lars Kunze. (2024)<br><strong>Real-time 3D semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution</strong><br><button class=copy-to-clipboard title="Real-time 3D semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08748v1.pdf filename=2403.08748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In autonomous vehicles, understanding the surrounding 3D environment of the ego vehicle in real-time is essential. A compact way to represent scenes while encoding geometric distances and semantic object information is via 3D semantic occupancy maps. State of the art 3D mapping methods leverage <b>transformers</b> with cross-attention mechanisms to elevate 2D vision-centric camera features into the 3D domain. However, these methods encounter significant challenges in real-time applications due to their high computational demands during inference. This limitation is particularly problematic in autonomous vehicles, where GPU resources must be shared with other tasks such as localization and planning. In this paper, we introduce an approach that extracts features from front-view 2D camera images and LiDAR scans, then employs a sparse <b>convolution</b> <b>network</b> (Minkowski Engine), for 3D semantic occupancy prediction. Given that outdoor scenes in autonomous driving scenarios are inherently sparse, the utilization of sparse <b>convolution</b> <b>is</b> particularly apt. By jointly solving the problems of 3D scene completion of sparse scenes and 3D semantic segmentation, we provide a more efficient learning framework suitable for real-time applications in autonomous vehicles. We also demonstrate competitive accuracy on the nuScenes dataset.</p></p class="citation"></blockquote><h3 id=618--224310-perceive-with-confidence-statistical-safety-assurances-for-navigation-with-learning-based-perception-anushri-dixit-et-al-2024>(6/18 | 224/310) Perceive With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception (Anushri Dixit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anushri Dixit, Zhiting Mei, Meghan Booker, Mariko Storey-Matsutani, Allen Z. Ren, Anirudha Majumdar. (2024)<br><strong>Perceive With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception</strong><br><button class=copy-to-clipboard title="Perceive With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Distribution Shift, Distribution Shift, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08185v1.pdf filename=2403.08185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rapid advances in perception have enabled large pre-trained models to be used out of the box for processing high-dimensional, noisy, and partial observations of the world into rich geometric representations (e.g., occupancy predictions). However, safe integration of these models onto robots remains challenging due to a lack of reliable performance in unfamiliar environments. In this work, we present a framework for rigorously quantifying the uncertainty of pre-trained perception models for occupancy prediction in order to provide end-to-end statistical safety assurances for navigation. We build on techniques from conformal prediction for producing a calibrated perception system that lightly processes the outputs of a pre-trained model while ensuring generalization to novel environments and robustness to <b>distribution</b> <b>shifts</b> in states when perceptual outputs are used in conjunction with a planner. The calibrated system can be used in combination with any safe planner to provide an end-to-end statistical assurance on safety in a new environment with a user-specified threshold $1-\epsilon$. We evaluate the resulting approach - which we refer to as Perceive with Confidence (PwC) - with experiments in <b>simulation</b> and on hardware where a quadruped robot navigates through indoor environments containing objects unseen during training or calibration. These experiments validate the safety assurances provided by PwC and demonstrate significant improvements in empirical safety rates compared to baselines.</p></p class="citation"></blockquote><h3 id=718--225310-continuous-object-state-recognition-for-cooking-robots-using-pre-trained-vision-language-models-and-black-box-optimization-kento-kawaharazuka-et-al-2024>(7/18 | 225/310) Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization (Kento Kawaharazuka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kento Kawaharazuka, Naoaki Kanazawa, Yoshiki Obinata, Kei Okada, Masayuki Inaba. (2024)<br><strong>Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization</strong><br><button class=copy-to-clipboard title="Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 25<br>Keywords: Black Box, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08239v1.pdf filename=2403.08239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem. On the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time. In addition, the state changes of food are complex and cannot be easily described by manual programming. Therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale <b>vision-language</b> models. By using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking. We also show that by adjusting the weighting of each text <b>prompt</b> based on fitting the similarity changes to a sigmoid function and then performing <b>black-box</b> <b>optimization,</b> more accurate and robust continuous state recognition can be achieved. We demonstrate the effectiveness and limitations of this method by performing the recognition of water boiling, butter melting, egg cooking, and onion stir-frying.</p></p class="citation"></blockquote><h3 id=818--226310-iamcv-multi-scenario-vehicle-interaction-dataset-novel-certad-et-al-2024>(8/18 | 226/310) IAMCV Multi-Scenario Vehicle Interaction Dataset (Novel Certad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Novel Certad, Enrico del Re, Helena Korndörfer, Gregory Schröder, Walter Morales-Alvarez, Sebastian Tschernuth, Delgermaa Gankhuyag, Luigi del Re, Cristina Olaverri-Monreal. (2024)<br><strong>IAMCV Multi-Scenario Vehicle Interaction Dataset</strong><br><button class=copy-to-clipboard title="IAMCV Multi-Scenario Vehicle Interaction Dataset" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-ET, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Object Detection, Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08455v1.pdf filename=2403.08455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The acquisition and analysis of high-quality sensor data constitute an essential requirement in shaping the development of fully autonomous driving systems. This process is indispensable for enhancing road safety and ensuring the effectiveness of the technological advancements in the automotive industry. This study introduces the Interaction of Autonomous and Manually-Controlled Vehicles (IAMCV) dataset, a novel and extensive dataset focused on inter-vehicle interactions. The dataset, enriched with a sophisticated array of sensors such as Light Detection and Ranging, cameras, Inertial Measurement Unit/Global Positioning System, and vehicle bus data acquisition, provides a comprehensive representation of real-world driving scenarios that include roundabouts, intersections, country roads, and highways, recorded across diverse locations in Germany. Furthermore, the study shows the versatility of the IAMCV dataset through several proof-of-concept use cases. Firstly, an <b>unsupervised</b> trajectory <b>clustering</b> algorithm illustrates the dataset&rsquo;s capability in categorizing vehicle movements without the need for labeled training data. Secondly, we compare an online camera calibration method with the Robot Operating System-based standard, using images captured in the dataset. Finally, a preliminary test employing the YOLOv8 <b>object-detection</b> <b>model</b> is conducted, augmented by reflections on the transferability of <b>object</b> <b>detection</b> across various LIDAR resolutions. These use cases underscore the practical utility of the collected dataset, emphasizing its potential to advance research and innovation in the area of intelligent vehicles.</p></p class="citation"></blockquote><h3 id=918--227310-difftactile-a-physics-based-differentiable-tactile-simulator-for-contact-rich-robotic-manipulation-zilin-si-et-al-2024>(9/18 | 227/310) DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation (Zilin Si et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilin Si, Gu Zhang, Qingwei Ben, Branden Romero, Zhou Xian, Chao Liu, Chuang Gan. (2024)<br><strong>DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation</strong><br><button class=copy-to-clipboard title="DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08716v1.pdf filename=2403.08716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce DIFFTACTILE, a physics-based differentiable tactile <b>simulation</b> system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting <b>simulations</b> of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in <b>simulation</b> using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. Code and supplementary materials are available at the project website <a href=https://difftactile.github.io/>https://difftactile.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1018--228310-compliant-hierarchical-control-for-arbitrary-equality-and-inequality-tasks-with-strict-and-soft-priorities-gianluca-garofalo-2024>(10/18 | 228/310) Compliant Hierarchical Control for Arbitrary Equality and Inequality Tasks with Strict and Soft Priorities (Gianluca Garofalo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluca Garofalo. (2024)<br><strong>Compliant Hierarchical Control for Arbitrary Equality and Inequality Tasks with Strict and Soft Priorities</strong><br><button class=copy-to-clipboard title="Compliant Hierarchical Control for Arbitrary Equality and Inequality Tasks with Strict and Soft Priorities" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08491v1.pdf filename=2403.08491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When a robotic system is redundant with respect to a given task, the remaining degrees of freedom can be used to satisfy additional objectives. With current robotic systems having more and more degrees of freedom, this can lead to an entire hierarchy of tasks that need to be solved according to given priorities. In this paper, the first compliant control strategy is presented that allows to consider an arbitrary number of equality and inequality tasks, while still preserving the natural inertia of the robot. The approach is therefore a generalization of a passivity-based controller to the case of an arbitrary number of equality and inequality tasks. The key idea of the method is to use a Weighted Hierarchical Quadratic Problem to extract the set of active tasks and use the latter to perform a coordinate transformation that inertially decouples the tasks. Thereby unifying the line of research focusing on optimization-based and passivity-based multi-task controllers. The method is validated in <b>simulation.</b></p></p class="citation"></blockquote><h3 id=1118--229310-grf-based-predictive-flocking-control-with-dynamic-pattern-formation-chenghao-yu-et-al-2024>(11/18 | 229/310) GRF-based Predictive Flocking Control with Dynamic Pattern Formation (Chenghao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenghao Yu, Dengyu Zhang, Qingrui Zhang. (2024)<br><strong>GRF-based Predictive Flocking Control with Dynamic Pattern Formation</strong><br><button class=copy-to-clipboard title="GRF-based Predictive Flocking Control with Dynamic Pattern Formation" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08434v1.pdf filename=2403.08434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is promising but challenging to design flocking control for a robot swarm to autonomously follow changing patterns or shapes in a optimal distributed manner. The optimal flocking control with dynamic pattern formation is, therefore, investigated in this paper. A predictive flocking control algorithm is proposed based on a Gibbs random field (GRF), where bio-inspired potential energies are used to charaterize <code>robot-robot'' and </code>robot-environment&rsquo;&rsquo; interactions. Specialized performance-related energies, e.g., motion smoothness, are introduced in the proposed design to improve the flocking behaviors. The optimal control is obtained by maximizing a posterior distribution of a GRF. A region-based shape control is accomplished for pattern formation in light of a mean shift technique. The proposed algorithm is evaluated via the comparison with two state-of-the-art flocking control methods in an environment with obstacles. Both numerical <b>simulations</b> and real-world experiments are conducted to demonstrate the efficiency of the proposed design.</p></p class="citation"></blockquote><h3 id=1218--230310-a-novel-feature-learning-based-bio-inspired-neural-network-for-real-time-collision-free-rescue-of-multi-robot-systems-junfei-li-et-al-2024>(12/18 | 230/310) A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems (Junfei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junfei Li, Simon X. Yang. (2024)<br><strong>A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems</strong><br><button class=copy-to-clipboard title="A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08238v1.pdf filename=2403.08238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural disasters and urban accidents drive the demand for rescue robots to provide safer, faster, and more efficient rescue trajectories. In this paper, a feature learning-based bio-inspired neural network (FLBBINN) is proposed to quickly generate a heuristic rescue path in complex and dynamic environments, as traditional approaches usually cannot provide a satisfactory solution to real-time responses to sudden environmental changes. The neurodynamic model is incorporated into the feature learning method that can use environmental information to improve path planning strategies. Task assignment and collision-free rescue trajectory are generated through robot poses and the dynamic landscape of neural activity. A dual-channel scale filter, a neural activity channel, and a secondary distance fusion are employed to extract and filter feature neurons. After completion of the feature learning process, a neurodynamics-based feature matrix is established to quickly generate the new heuristic rescue paths with parameter-driven topological adaptability. The proposed FLBBINN aims to reduce the computational complexity of the neural network-based approach and enable the feature learning method to achieve real-time responses to environmental changes. Several <b>simulations</b> and experiments have been conducted to evaluate the performance of the proposed FLBBINN. The results show that the proposed FLBBINN would significantly improve the speed, efficiency, and optimality for rescue operations.</p></p class="citation"></blockquote><h3 id=1318--231310-a-direct-algorithm-for-multi-gyroscope-infield-calibration-tianheng-wang-et-al-2024>(13/18 | 231/310) A Direct Algorithm for Multi-Gyroscope Infield Calibration (Tianheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianheng Wang, Stergios I. Roumeliotis. (2024)<br><strong>A Direct Algorithm for Multi-Gyroscope Infield Calibration</strong><br><button class=copy-to-clipboard title="A Direct Algorithm for Multi-Gyroscope Infield Calibration" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08177v1.pdf filename=2403.08177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the problem of estimating the rotational extrinsics, as well as the scale factors of two gyroscopes rigidly mounted on the same device. In particular, we formulate the problem as a least-squares minimization and introduce a direct algorithm that computes the estimated quantities without any iterations, hence avoiding local minima and improving efficiency. Furthermore, we show that the rotational extrinsics are observable while the scale factors can be determined up to global scale for general configurations of the gyroscopes. To this end, we also study special placements of the gyroscopes where a pair, or all, of their axes are parallel and analyze their impact on the scale factors&rsquo; observability. Lastly, we evaluate our algorithm in <b>simulations</b> and real-world experiments to assess its performance as a function of key motion and sensor characteristics.</p></p class="citation"></blockquote><h3 id=1418--232310-effective-underwater-glider-path-planning-in-dynamic-3d-environments-using-multi-point-potential-fields-hanzhi-yang-et-al-2024>(14/18 | 232/310) Effective Underwater Glider Path Planning in Dynamic 3D Environments Using Multi-Point Potential Fields (Hanzhi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzhi Yang, Nina Mahmoudian. (2024)<br><strong>Effective Underwater Glider Path Planning in Dynamic 3D Environments Using Multi-Point Potential Fields</strong><br><button class=copy-to-clipboard title="Effective Underwater Glider Path Planning in Dynamic 3D Environments Using Multi-Point Potential Fields" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08163v1.pdf filename=2403.08163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Underwater gliders (UGs) have emerged as highly effective unmanned vehicles for ocean exploration. However, their operation in dynamic and complex underwater environments necessitates robust path-planning strategies. Previous studies have primarily focused on global energy or time-efficient path planning in explored environments, overlooking challenges posed by unpredictable flow conditions and unknown obstacles in varying and dynamic areas like fjords and near-harbor waters. This paper introduces and improves a real-time path planning method, Multi-Point Potential Field (MPPF), tailored for UGs operating in 3D space as they are constrained by buoyancy propulsion and internal actuation. The proposed MPPF method addresses obstacles, flow fields, and local minima, enhancing the efficiency and robustness of UG path planning. A low-cost prototype, the Research Oriented Underwater Glider for Hands-on Investigative Engineering (ROUGHIE), is utilized for validation. Through case studies and <b>simulations,</b> the efficacy of the enhanced MPPF method is demonstrated, highlighting its potential for real-world applications in underwater exploration.</p></p class="citation"></blockquote><h3 id=1518--233310-naturalvlm-leveraging-fine-grained-natural-language-for-affordance-guided-visual-manipulation-ran-xu-et-al-2024>(15/18 | 233/310) NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation (Ran Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ran Xu, Yan Shen, Xiaoqi Li, Ruihai Wu, Hao Dong. (2024)<br><strong>NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation</strong><br><button class=copy-to-clipboard title="NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08355v1.pdf filename=2403.08355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enabling home-assistant robots to perceive and manipulate a diverse range of 3D objects based on human language instructions is a pivotal challenge. Prior research has predominantly focused on simplistic and task-oriented instructions, i.e., &ldquo;Slide the top drawer open&rdquo;. However, many real-world tasks demand intricate multi-step <b>reasoning,</b> and without human instructions, these will become extremely difficult for robot manipulation. To address these challenges, we introduce a comprehensive <b>benchmark,</b> NrVLM, comprising 15 distinct manipulation tasks, containing over 4500 episodes meticulously annotated with fine-grained language instructions. We split the long-term task process into several steps, with each step having a natural language instruction. Moreover, we propose a novel learning framework that completes the manipulation task step-by-step according to the fine-grained instructions. Specifically, we first identify the instruction to execute, taking into account visual observations and the end-effector&rsquo;s current state. Subsequently, our approach facilitates explicit learning through action-prompts and perception-prompts to promote manipulation-aware cross-modality alignment. Leveraging both visual observations and linguistic guidance, our model outputs a sequence of actionable predictions for manipulation, including contact points and end-effector poses. We evaluate our method and baselines using the proposed <b>benchmark</b> NrVLM. The experimental results demonstrate the effectiveness of our approach. For additional details, please refer to <a href=https://sites.google.com/view/naturalvlm>https://sites.google.com/view/naturalvlm</a>.</p></p class="citation"></blockquote><h3 id=1618--234310-synchronized-dual-arm-rearrangement-via-cooperative-mtsp-wenhao-li-et-al-2024>(16/18 | 234/310) Synchronized Dual-arm Rearrangement via Cooperative mTSP (Wenhao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhao Li, Shishun Zhang, Sisi Dai, Hui Huang, Ruizhen Hu, Xiaohong Chen, Kai Xu. (2024)<br><strong>Synchronized Dual-arm Rearrangement via Cooperative mTSP</strong><br><button class=copy-to-clipboard title="Synchronized Dual-arm Rearrangement via Cooperative mTSP" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08191v1.pdf filename=2403.08191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synchronized dual-arm rearrangement is widely studied as a common scenario in industrial applications. It often faces scalability challenges due to the computational complexity of robotic arm rearrangement and the high-dimensional nature of dual-arm planning. To address these challenges, we formulated the problem as cooperative mTSP, a variant of mTSP where agents share cooperative costs, and utilized <b>reinforcement</b> <b>learning</b> for its solution. Our approach involved representing rearrangement tasks using a task state <b>graph</b> that captured spatial relationships and a cooperative cost matrix that provided details about action costs. Taking these representations as observations, we designed an attention-based network to effectively combine them and provide rational task scheduling. Furthermore, a cost predictor is also introduced to directly evaluate actions during both training and planning, significantly expediting the planning process. Our experimental results demonstrate that our approach outperforms existing methods in terms of both performance and planning efficiency.</p></p class="citation"></blockquote><h3 id=1718--235310-spaceoctopus-an-octopus-inspired-motion-planning-framework-for-multi-arm-space-robot-wenbo-zhao-et-al-2024>(17/18 | 235/310) SpaceOctopus: An Octopus-inspired Motion Planning Framework for Multi-arm Space Robot (Wenbo Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenbo Zhao, Shengjie Wang, Yixuan Fan, Yang Gao, Tao Zhang. (2024)<br><strong>SpaceOctopus: An Octopus-inspired Motion Planning Framework for Multi-arm Space Robot</strong><br><button class=copy-to-clipboard title="SpaceOctopus: An Octopus-inspired Motion Planning Framework for Multi-arm Space Robot" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08219v1.pdf filename=2403.08219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Space robots have played a critical role in autonomous maintenance and space junk removal. Multi-arm space robots can efficiently complete the target capture and base reorientation tasks due to their flexibility and the collaborative capabilities between the arms. However, the complex coupling properties arising from both the multiple arms and the free-floating base present challenges to the motion planning problems of multi-arm space robots. We observe that the octopus elegantly achieves similar goals when grabbing prey and escaping from danger. Inspired by the distributed control of octopuses&rsquo; limbs, we develop a multi-level decentralized motion planning framework to manage the movement of different arms of space robots. This motion planning framework integrates naturally with the multi-agent <b>reinforcement</b> <b>learning</b> (MARL) paradigm. The results indicate that our method outperforms the previous method (centralized training). Leveraging the flexibility of the decentralized framework, we reassemble policies trained for different tasks, enabling the space robot to complete trajectory planning tasks while adjusting the base attitude without further learning. Furthermore, our experiments confirm the superior robustness of our method in the face of external disturbances, changing base masses, and even the failure of one arm.</p></p class="citation"></blockquote><h3 id=1818--236310-multi-fidelity-reinforcement-learning-for-time-optimal-quadrotor-re-planning-gilhyun-ryou-et-al-2024>(18/18 | 236/310) Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor Re-planning (Gilhyun Ryou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gilhyun Ryou, Geoffrey Wang, Sertac Karaman. (2024)<br><strong>Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor Re-planning</strong><br><button class=copy-to-clipboard title="Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor Re-planning" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08152v1.pdf filename=2403.08152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-speed online trajectory planning for UAVs poses a significant challenge due to the need for precise modeling of complex dynamics while also being constrained by computational limitations. This paper presents a multi-fidelity <b>reinforcement</b> <b>learning</b> method (MFRL) that aims to effectively create a realistic dynamics model and simultaneously train a planning policy that can be readily deployed in real-time applications. The proposed method involves the co-training of a planning policy and a reward estimator; the latter predicts the performance of the policy&rsquo;s output and is trained efficiently through multi-fidelity Bayesian optimization. This optimization approach models the correlation between different fidelity levels, thereby constructing a high-fidelity model based on a low-fidelity foundation, which enables the accurate development of the reward model with limited high-fidelity experiments. The framework is further extended to include real-world flight experiments in <b>reinforcement</b> <b>learning</b> training, allowing the reward model to precisely reflect real-world constraints and broadening the policy&rsquo;s applicability to real-world scenarios. We present rigorous evaluations by training and testing the planning policy in both simulated and real-world environments. The resulting trained policy not only generates faster and more reliable trajectories compared to the baseline snap minimization method, but it also achieves trajectory updates in 2 ms on average, while the baseline method takes several minutes.</p></p class="citation"></blockquote><h2 id=eesssy-6>eess.SY (6)</h2><h3 id=16--237310-llm-assisted-light-leveraging-large-language-model-capabilities-for-human-mimetic-traffic-signal-control-in-complex-urban-environments-maonan-wang-et-al-2024>(1/6 | 237/310) LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments (Maonan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maonan Wang, Aoyu Pang, Yuheng Kan, Man-On Pun, Chung Shue Chen, Bo Huang. (2024)<br><strong>LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments</strong><br><button class=copy-to-clipboard title="LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 60<br>Keywords: Reinforcement Learning, Simulation, Simulator, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08337v1.pdf filename=2403.08337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or <b>reinforcement</b> <b>learning</b> (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into TSC, harnessing their advanced <b>reasoning</b> and decision-making faculties. Specifically, a hybrid framework that augments <b>LLMs</b> with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the <b>LLM</b> at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a <b>simulation</b> platform is developed to corroborate the efficacy of the proposed framework. The findings from our <b>simulations</b> attest to the system&rsquo;s adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of <b>LLMs</b> into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at \href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}.</p></p class="citation"></blockquote><h3 id=26--238310-mechanism-design-optimization-through-cad-based-bayesian-optimization-and-quantified-constraints-abdelmajid-ben-yahya-et-al-2024>(2/6 | 238/310) Mechanism Design Optimization through CAD-Based Bayesian Optimization and Quantified Constraints (Abdelmajid Ben Yahya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelmajid Ben Yahya, Santiago Ramos Garces, Nick Van Oosterwyck, Annie Cuyt, Stijn Derammelaere. (2024)<br><strong>Mechanism Design Optimization through CAD-Based Bayesian Optimization and Quantified Constraints</strong><br><button class=copy-to-clipboard title="Mechanism Design Optimization through CAD-Based Bayesian Optimization and Quantified Constraints" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: 74P20, G-1-6, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08473v1.pdf filename=2403.08473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research delves into optimizing mechanism design, with an emphasis on the energy efficiency and the expansive design possibilities of reciprocating mechanisms. It investigates how to efficiently integrate Computer-Aided Design (CAD) <b>simulations</b> with Bayesian Optimization (BO) and a constrained design space, aiming to enhance the design optimization process beyond the confines of traditional kinematic and dynamic analysis. The study sets out to create a novel optimization framework that merges CAD <b>simulations</b> with a BO strategy. Initially, the feasibility of a mechanism design is assessed through CAD-motion <b>simulations,</b> which gauge its practicality. Upon deeming a design feasible, an evaluation via CAD-motion <b>simulations</b> is conducted to ascertain the objective value. This research proposes utilizing non-parametric Gaussian processes for crafting a surrogate model of the objective function, considering the design space&rsquo;s static and dynamic constraints. The findings reveal that the introduced CAD-based Bayesian Optimization framework adeptly identifies optimal design parameters that minimize root mean square (RMS) torque while complying with predetermined constraints. This method markedly diminishes the complexity seen in analytical approaches, rendering it adaptable to intricate mechanisms and practicable for machine builders. The framework evidences the utility of integrating constraints in the optimization process, showing promise for attaining globally optimal designs efficiently. A case study on an emergency ventilator, with three design parameters, demonstrates a 71% RMS torque reduction after 255 CAD-based evaluations, underscoring the approach&rsquo;s effectiveness and its potential for refining mechanism design optimization.</p></p class="citation"></blockquote><h3 id=36--239310-remote-ugv-control-via-practical-wireless-channels-a-model-predictive-control-approach-inghao-cao-et-al-2024>(3/6 | 239/310) Remote UGV Control via Practical Wireless Channels: A Model Predictive Control Approach (inghao Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>inghao Cao, Subhan Khan, Wanchun Liu, Yonghui Li, Branka Vucetic. (2024)<br><strong>Remote UGV Control via Practical Wireless Channels: A Model Predictive Control Approach</strong><br><button class=copy-to-clipboard title="Remote UGV Control via Practical Wireless Channels: A Model Predictive Control Approach" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08398v1.pdf filename=2403.08398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In addressing wireless networked control systems (WNCS) subject to unexpected packet loss and uncertainties, this paper presents a practical Model Predictive Control (MPC) based control scheme with considerations of of packet dropouts, latency, process noise and measurement noise. A discussion of the quasi-static Rayleigh fading channel is presented herein to enhance the realism of the underlying assumption in a real-world context. To achieve a desirable performance, the proposed control scheme leverages the predictive capabilities of a direct multiple shooting MPC, employs a compensation strategy to mitigate the impact of wireless channel imperfections. Instead of feeding noisy measurements into the MPC, we employ an Extended Kalman Filter (EKF) to mitigate the influence of measurement noise and process disturbances. Finally, we implement the proposed MPC algorithm on a simulated Unmanned Ground Vehicle (UGV) and conduct a series of experiments to evaluate the performance of our control scheme across various scenarios. Through our <b>simulation</b> results and comparative analyses, we have substantiated the effectiveness and improvements brought about by our approach through the utilization of multiple metrics.</p></p class="citation"></blockquote><h3 id=46--240310-probabilistic-metaplasticity-for-continual-learning-with-memristors-fatima-tuz-zohora-et-al-2024>(4/6 | 240/310) Probabilistic Metaplasticity for Continual Learning with Memristors (Fatima Tuz Zohora et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatima Tuz Zohora, Vedant Karia, Nicholas Soures, Dhireesha Kudithipudi. (2024)<br><strong>Probabilistic Metaplasticity for Continual Learning with Memristors</strong><br><button class=copy-to-clipboard title="Probabilistic Metaplasticity for Continual Learning with Memristors" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08718v1.pdf filename=2403.08718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crossbar architectures utilizing memristor devices hold promise to address <b>continual</b> <b>learning</b> challenges in resource-constrained edge devices. However, these nanoscale devices often exhibit low precision and high variability in conductance modulation, rendering them unsuitable for <b>continual</b> <b>learning</b> solutions that consolidate weights through precise modulation. This issue can be circumvented by accumulating weight gradients in auxiliary high-precision memory and updating memristor weights when gradients are equivalent to memristor weight resolution. However, it leads to frequent memory access, high memory overhead, and energy dissipation. In this research, we propose probabilistic metaplasticity, which consolidates weights by modulating their update probability rather than magnitude. The proposed mechanism eliminates high-precision modification to weight magnitude and consequently, high-precision memory for gradient accumulation. We demonstrate the efficacy of the proposed mechanism by integrating probabilistic metaplasticity into a spiking network trained on an error threshold with low-precision memristor weights. Evaluations of two <b>continual</b> <b>learning</b> <b>benchmarks</b> show that probabilistic metaplasticity consumes ~67% lower memory for additional parameters and up to two orders of magnitude lower energy during parameter updates compared to an auxiliary memory-based solution while achieving state-of-the-art performance. The proposed model shows potential for energy-efficient <b>continual</b> <b>learning</b> with low-precision emerging devices.</p></p class="citation"></blockquote><h3 id=56--241310-ventilation-and-temperature-control-for-energy-efficient-and-healthy-buildings-a-differentiable-pde-approach-yuexin-bian-et-al-2024>(5/6 | 241/310) Ventilation and Temperature Control for Energy-efficient and Healthy Buildings: A Differentiable PDE Approach (Yuexin Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuexin Bian, Xiaohan Fu, Rajesh K. Gupta, Yuanyuan Shi. (2024)<br><strong>Ventilation and Temperature Control for Energy-efficient and Healthy Buildings: A Differentiable PDE Approach</strong><br><button class=copy-to-clipboard title="Ventilation and Temperature Control for Energy-efficient and Healthy Buildings: A Differentiable PDE Approach" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08996v1.pdf filename=2403.08996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel framework for building learning and control, focusing on ventilation and thermal management to enhance energy efficiency. We validate the performance of the proposed framework in system model learning via two case studies: a synthetic study focusing on the joint learning of temperature and CO2 fields, and an application to a real-world dataset for CO2 field learning. For building control, we demonstrate that the proposed framework can optimize the control actions and significantly reduce the energy cost while maintaining a comfort and healthy indoor environment. When compared to existing traditional methods, an optimization-based method with ODE models and <b>reinforcement</b> <b>learning,</b> our approach can significantly reduce the energy consumption while guarantees all the safety-critical air quality and control constraints. Promising future research directions involve validating and improving the proposed PDE models through accurate estimation of airflow fields within indoor environments. Additionally, incorporating uncertainty modeling into the PDE framework for HVAC control presents an opportunity to enhance the efficiency and reliability of building HVAC system management.</p></p class="citation"></blockquote><h3 id=66--242310-prototyping-and-experimental-results-for-environment-aware-millimeter-wave-beam-alignment-via-channel-knowledge-map-zhuoyin-dai-et-al-2024>(6/6 | 242/310) Prototyping and Experimental Results for Environment-Aware Millimeter Wave Beam Alignment via Channel Knowledge Map (Zhuoyin Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoyin Dai, Di Wu, Zhenjun Dong, Kun Li, Dingyang Ding, Sihan Wang, Yong Zeng. (2024)<br><strong>Prototyping and Experimental Results for Environment-Aware Millimeter Wave Beam Alignment via Channel Knowledge Map</strong><br><button class=copy-to-clipboard title="Prototyping and Experimental Results for Environment-Aware Millimeter Wave Beam Alignment via Channel Knowledge Map" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SP, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08200v1.pdf filename=2403.08200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Channel knowledge map (CKM), which aims to directly reflect the intrinsic channel properties of the local wireless environment, is a novel technique for achieving environmentaware communication. In this paper, to alleviate the large training overhead in millimeter wave (mmWave) beam alignment, an environment-aware and training-free beam alignment prototype is established based on a typical CKM, termed beam index map (BIM). To this end, a general CKM construction method is first presented, and an indoor BIM is constructed offline to learn the candidate transmit and receive beam index pairs for each grid in the experimental area. Furthermore, based on the location information of the receiver (or the dynamic obstacles) from the ultra-wide band (UWB) positioning system, the established BIM is used to achieve training-free beam alignment by directly providing the beam indexes for the transmitter and receiver. Three typical scenarios are considered in the experiment, including quasi-static environment with line-of-sight (LoS) link, quasistatic environment without LoS link and dynamic environment. Besides, the receiver orientation measured from the gyroscope is also used to help CKM predict more accurate beam indexes. The experiment results show that compared with the <b>benchmark</b> location-based beam alignment strategy, the CKM-based beam alignment strategy can achieve much higher received power, which is close to that achieved by exhaustive beam search, but with significantly reduced training overhead.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--243310-paperclip-associating-astronomical-observations-and-natural-language-with-multi-modal-models-siddharth-mishra-sharma-et-al-2024>(1/1 | 243/310) PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models (Siddharth Mishra-Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddharth Mishra-Sharma, Yiding Song, Jesse Thaler. (2024)<br><strong>PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models</strong><br><button class=copy-to-clipboard title="PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-CL, cs-CV, cs-IR, cs-LG<br>Keyword Score: 53<br>Keywords: Fine-tuning, Foundation Model, Multi-modal, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08851v1.pdf filename=2403.08851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is <b>fine-tuned</b> from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally <b>summarized</b> via guided generation using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Using observations from the Hubble Space Telescope (HST) as an example, we show that the <b>fine-tuned</b> model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a given observation). Our study demonstrates the potential for using generalist <b>foundation</b> <b>models</b> rather than task-specific models for interacting with astronomical data by leveraging text as an interface.</p></p class="citation"></blockquote><h2 id=csir-6>cs.IR (6)</h2><h3 id=16--244310-domain-adaptation-for-dense-retrieval-and-conversational-dense-retrieval-through-self-supervision-by-meticulous-pseudo-relevance-labeling-minghan-li-et-al-2024>(1/6 | 244/310) Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling (Minghan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minghan Li, Eric Gaussier. (2024)<br><strong>Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling</strong><br><button class=copy-to-clipboard title="Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 50<br>Keywords: Adversarial Learning, Dense Retrieval, Fine-tuning, T5, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08970v1.pdf filename=2403.08970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have demonstrated that the ability of <b>dense</b> <b>retrieval</b> models to generalize to target <b>domains</b> <b>with</b> different distributions is limited, which contrasts with the results obtained with interaction-based models. Prior attempts to mitigate this challenge involved leveraging <b>adversarial</b> <b>learning</b> and query generation approaches, but both approaches nevertheless resulted in limited improvements. In this paper, we propose to combine the query-generation approach with a self-supervision approach in which pseudo-relevance labels are automatically generated on the target <b>domain.</b> <b>To</b> accomplish this, a <b>T5-3B</b> model is utilized for pseudo-positive labeling, and meticulous hard negatives are chosen. We also apply this strategy on conversational <b>dense</b> <b>retrieval</b> model for conversational search. A similar pseudo-labeling approach is used, but with the addition of a query-rewriting module to rewrite conversational queries for subsequent labeling. This proposed approach enables a model&rsquo;s <b>domain</b> <b>adaptation</b> with real queries and documents from the target dataset. Experiments on standard <b>dense</b> <b>retrieval</b> and conversational <b>dense</b> <b>retrieval</b> models both demonstrate improvements on baseline models when they are <b>fine-tuned</b> on the pseudo-relevance labeled data.</p></p class="citation"></blockquote><h3 id=26--245310-foundation-models-and-information-retrieval-in-digital-pathology-h-r-tizhoosh-2024>(2/6 | 245/310) Foundation Models and Information Retrieval in Digital Pathology (H. R. Tizhoosh, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H. R. Tizhoosh. (2024)<br><strong>Foundation Models and Information Retrieval in Digital Pathology</strong><br><button class=copy-to-clipboard title="Foundation Models and Information Retrieval in Digital Pathology" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CV, cs-IR, cs-LG, cs.IR, eess-IV<br>Keyword Score: 40<br>Keywords: Foundation Model, Generative AI, Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12090v1.pdf filename=2403.12090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper reviews the state-of-the-art of <b>foundation</b> <b>models,</b> <b>LLMs,</b> <b>generative</b> <b>AI,</b> <b>information</b> <b>retrieval</b> and CBIR in digital pathology</p></p class="citation"></blockquote><h3 id=36--246310-towards-unified-modeling-for-positive-and-negative-preferences-in-sign-aware-recommendation-yuting-liu-et-al-2024>(3/6 | 246/310) Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation (Yuting Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuting Liu, Yizhou Dang, Yuliang Liang, Qiang Liu, Guibing Guo, Jianzhe Zhao, Xingwei Wang. (2024)<br><strong>Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation</strong><br><button class=copy-to-clipboard title="Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs-SI, cs.IR<br>Keyword Score: 33<br>Keywords: Graph, Convolution, Convolutional Neural Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08246v1.pdf filename=2403.08246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, sign-aware <b>graph</b> <b>recommendation</b> has drawn much attention as it will learn users&rsquo; negative preferences besides positive ones from both positive and negative interactions (i.e., links in a <b>graph)</b> with items. To accommodate the different semantics of negative and positive links, existing works utilize two independent encoders to model users&rsquo; positive and negative preferences, respectively. However, these approaches cannot learn the negative preferences from high-order heterogeneous interactions between users and items formed by multiple links with different signs, resulting in inaccurate and incomplete negative user preferences. To cope with these intractable issues, we propose a novel \textbf{L}ight \textbf{S}igned \textbf{G}raph <b>Convolution</b> <b>Network</b> specifically for \textbf{Rec}ommendation (\textbf{LSGRec}), which adopts a unified modeling approach to simultaneously model high-order users&rsquo; positive and negative preferences on a signed user-item interaction <b>graph.</b> Specifically, for the negative preferences within high-order heterogeneous interactions, first-order negative preferences are captured by the negative links, while high-order negative preferences are propagated along positive edges. Then, <b>recommendation</b> results are generated based on positive preferences and optimized with negative ones. Finally, we train representations of users and items through different auxiliary tasks. Extensive experiments on three real-world datasets demonstrate that our method outperforms existing baselines regarding performance and computational efficiency. Our code is available at \url{https://anonymous.4open.science/r/LSGRec-BB95}.</p></p class="citation"></blockquote><h3 id=46--247310-discrete-semantic-tokenization-for-deep-ctr-prediction-qijiong-liu-et-al-2024>(4/6 | 247/310) Discrete Semantic Tokenization for Deep CTR Prediction (Qijiong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, Xiao-Ming Wu. (2024)<br><strong>Discrete Semantic Tokenization for Deep CTR Prediction</strong><br><button class=copy-to-clipboard title="Discrete Semantic Tokenization for Deep CTR Prediction" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Quantization, Recommendation, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08206v1.pdf filename=2403.08206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings and then caches them, prioritizes space over time. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic <b>tokenization</b> approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST <b>quantizes</b> dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user&ndash;item token pair. Our experimental results on news <b>recommendation</b> showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.</p></p class="citation"></blockquote><h3 id=56--248310-ilciter-evidence-grounded-interpretable-local-citation-recommendation-sayar-ghosh-roy-et-al-2024>(5/6 | 248/310) ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation (Sayar Ghosh Roy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayar Ghosh Roy, Jiawei Han. (2024)<br><strong>ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation</strong><br><button class=copy-to-clipboard title="ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08737v1.pdf filename=2403.08737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing Machine Learning approaches for local citation <b>recommendation</b> directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers. Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited <b>recommendation</b> interpretability. To alleviate this, we introduce the evidence-grounded local citation <b>recommendation</b> task, where the target latent space comprises evidence spans for recommending specific papers. Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature. Unlike past formulations that simply output <b>recommendations,</b> ILCiteR retrieves ranked lists of evidence span and recommended paper pairs. Secondly, previously proposed neural models for citation <b>recommendation</b> require expensive training on massive labeled data, ideally after every significant update to the pool of candidate papers. In contrast, ILCiteR relies solely on distant supervision from a dynamic evidence database and pre-trained <b>Transformer-based</b> Language Models without any model training. We contribute a novel dataset for the evidence-grounded local citation <b>recommendation</b> task and demonstrate the efficacy of our proposed conditional neural rank-ensembling approach for re-ranking evidence spans.</p></p class="citation"></blockquote><h3 id=66--249310-nlqxform-ui-a-natural-language-interface-for-querying-dblp-interactively-ruijie-wang-et-al-2024>(6/6 | 249/310) NLQxform-UI: A Natural Language Interface for Querying DBLP Interactively (Ruijie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruijie Wang, Zhiruo Zhang, Luca Rossetto, Florian Ruosch, Abraham Bernstein. (2024)<br><strong>NLQxform-UI: A Natural Language Interface for Querying DBLP Interactively</strong><br><button class=copy-to-clipboard title="NLQxform-UI: A Natural Language Interface for Querying DBLP Interactively" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08475v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08475v1.pdf filename=2403.08475v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the DBLP computer science bibliography has been prominently used for searching scholarly information, such as publications, scholars, and venues. However, its current search service lacks the capability to handle complex queries, which limits the usability of DBLP. In this paper, we present NLQxform-UI, a web-based natural language interface that enables users to query DBLP directly with complex natural language questions. NLQxform-UI automatically translates given questions into SPARQL queries and executes the queries over the DBLP <b>knowledge</b> <b>graph</b> to retrieve answers. The querying process is presented to users in an interactive manner, which improves the transparency of the system and helps examine the returned answers. Also, intermediate results in the querying process can be previewed and manually altered to improve the accuracy of the system. NLQxform-UI has been completely open-sourced: <a href=https://github.com/ruijie-wang-uzh/NLQxform-UI>https://github.com/ruijie-wang-uzh/NLQxform-UI</a>.</p></p class="citation"></blockquote><h2 id=csit-7>cs.IT (7)</h2><h3 id=17--250310-meta-learning-based-fronthaul-compression-for-cloud-radio-access-networks-ruihua-qiao-et-al-2024>(1/7 | 250/310) Meta-Learning-Based Fronthaul Compression for Cloud Radio Access Networks (Ruihua Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruihua Qiao, Tao Jiang, Wei Yu. (2024)<br><strong>Meta-Learning-Based Fronthaul Compression for Cloud Radio Access Networks</strong><br><button class=copy-to-clipboard title="Meta-Learning-Based Fronthaul Compression for Cloud Radio Access Networks" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 43<br>Keywords: Graph Attention Networks, Benchmarking, Meta Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09004v1.pdf filename=2403.09004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the fronthaul compression problem in a user-centric cloud radio access network, in which single-antenna users are served by a central processor (CP) cooperatively via a cluster of remote radio heads (RRHs). To satisfy the fronthaul capacity constraint, this paper proposes a transform-compress-forward scheme, which consists of well-designed transformation matrices and uniform quantizers. The transformation matrices perform dimension reduction in the uplink and dimension expansion in the downlink. To reduce the communication overhead for designing the transformation matrices, this paper further proposes a deep learning framework to first learn a suboptimal transformation matrix at each RRH based on the local channel state information (CSI), and then to refine it iteratively. To facilitate the refinement process, we propose an efficient signaling scheme that only requires the transmission of low-dimensional effective CSI and its gradient between the CP and RRH, and further, a <b>meta-learning</b> <b>based</b> <b>gated</b> recurrent unit network to reduce the number of signaling transmission rounds. For the sum-rate maximization problem, <b>simulation</b> results show that the proposed two-stage neural network can perform close to the fully cooperative global CSI based <b>benchmark</b> with significantly reduced communication overhead for both the uplink and the downlink. Moreover, using the first stage alone can already outperform the existing local CSI based <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=27--251310-meta-reinforcement-learning-for-resource-allocation-in-aerial-active-ris-assisted-networks-with-rate-splitting-multiple-access-sajad-faramarzi-et-al-2024>(2/7 | 251/310) Meta Reinforcement Learning for Resource Allocation in Aerial Active-RIS-assisted Networks with Rate-Splitting Multiple Access (Sajad Faramarzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sajad Faramarzi, Sepideh Javadi, Farshad Zeinali, Hosein Zarini, Mohammad Robat Mili, Mehdi Bennis, Yonghui Li, Kai-Kit Wong. (2024)<br><strong>Meta Reinforcement Learning for Resource Allocation in Aerial Active-RIS-assisted Networks with Rate-Splitting Multiple Access</strong><br><button class=copy-to-clipboard title="Meta Reinforcement Learning for Resource Allocation in Aerial Active-RIS-assisted Networks with Rate-Splitting Multiple Access" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 40<br>Keywords: Meta Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08648v1.pdf filename=2403.08648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mounting a reconfigurable intelligent surface (RIS) on an unmanned aerial vehicle (UAV) holds promise for improving traditional terrestrial network performance. Unlike conventional methods deploying passive RIS on UAVs, this study delves into the efficacy of an aerial active RIS (AARIS). Specifically, the downlink transmission of an AARIS network is investigated, where the base station (BS) leverages rate-splitting multiple access (RSMA) for effective interference management and benefits from the support of an AARIS for jointly amplifying and reflecting the BS&rsquo;s transmit signals. Considering both the non-trivial energy consumption of the active RIS and the limited energy storage of the UAV, we propose an innovative element selection strategy for optimizing the on/off status of RIS elements, which adaptively and remarkably manages the system&rsquo;s power consumption. To this end, a resource management problem is formulated, aiming to maximize the system energy efficiency (EE) by jointly optimizing the transmit beamforming at the BS, the element activation, the phase shift and the amplification factor at the RIS, the RSMA common data rate at users, as well as the UAV&rsquo;s trajectory. Due to the dynamicity nature of UAV and user mobility, a deep <b>reinforcement</b> <b>learning</b> (DRL) algorithm is designed for resource allocation, utilizing <b>meta-learning</b> <b>to</b> adaptively handle fast time-varying system dynamics. <b>Simulations</b> indicate that incorporating an active RIS at the UAV leads to substantial EE gain, compared to passive RIS-aided UAV. We observe the superiority of the RSMA-based AARIS system in terms of EE, compared to existing approaches adopting non-orthogonal multiple access (NOMA).</p></p class="citation"></blockquote><h3 id=37--252310-maximum-channel-coding-rate-of-finite-block-length-mimo-faster-than-nyquist-signaling-zichao-zhang-et-al-2024>(3/7 | 252/310) Maximum Channel Coding Rate of Finite Block Length MIMO Faster-Than-Nyquist Signaling (Zichao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zichao Zhang, Melda Yuksel, Halim Yanikomeroglu, Benjamin K. Ng, Chan-Tong Lam. (2024)<br><strong>Maximum Channel Coding Rate of Finite Block Length MIMO Faster-Than-Nyquist Signaling</strong><br><button class=copy-to-clipboard title="Maximum Channel Coding Rate of Finite Block Length MIMO Faster-Than-Nyquist Signaling" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08989v1.pdf filename=2403.08989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pursuit of higher data rates and efficient spectrum utilization in modern communication technologies necessitates novel solutions. In order to provide insights into improving spectral efficiency and reducing latency, this study investigates the maximum channel coding rate (MCCR) of finite block length (FBL) multiple-input multiple-output (MIMO) faster-than-Nyquist (FTN) channels. By optimizing power allocation, we derive the system&rsquo;s MCCR expression. <b>Simulation</b> results are compared with the existing literature to reveal the benefits of FTN in FBL transmission.</p></p class="citation"></blockquote><h3 id=47--253310-low-complexity-beam-training-for-multi-ris-assisted-multi-user-communications-yuan-xu-et-al-2024>(4/7 | 253/310) Low-Complexity Beam Training for Multi-RIS-Assisted Multi-User Communications (Yuan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Xu, Chongwen Huang, Wei Li, Zhaohui Yang, Xiaoming Chen, Zhaoyang Zhang, Chau Yuen, Mérouane Debbah. (2024)<br><strong>Low-Complexity Beam Training for Multi-RIS-Assisted Multi-User Communications</strong><br><button class=copy-to-clipboard title="Low-Complexity Beam Training for Multi-RIS-Assisted Multi-User Communications" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08339v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08339v2.pdf filename=2403.08339v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the beam training problem in the multi-user millimeter wave (mmWave) communication system, where multiple reconfigurable intelligent surfaces (RISs) are deployed to improve the coverage and the achievable rate. However, existing beam training techniques in mmWave systems suffer from the high complexity (i.e., exponential order) and low identification accuracy. To address these problems, we propose a novel hashing multi-arm beam (HMB) training scheme that reduces the training complexity to the logarithmic order with the high accuracy. Specifically, we first design a generation mechanism for HMB codebooks. Then, we propose a demultiplexing algorithm based on the soft decision to distinguish signals from different RIS reflective links. Finally, we utilize a multi-round voting mechanism to align the beams. <b>Simulation</b> results show that the proposed HMB training scheme enables simultaneous training for multiple RISs and multiple users, and reduces the beam training overhead to the logarithmic level. Moreover, it also shows that our proposed scheme can significantly improve the identification accuracy by at least 20% compared to existing beam training techniques.</p></p class="citation"></blockquote><h3 id=57--254310-handoffs-in-user-centric-cell-free-mimo-networks-a-pomdp-framework-hussein-a-ammar-et-al-2024>(5/7 | 254/310) Handoffs in User-Centric Cell-Free MIMO Networks: A POMDP Framework (Hussein A. Ammar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hussein A. Ammar, Raviraj Adve, Shahram Shahbazpanahi, Gary Boudreau, Kothapalli Venkata Srinivas. (2024)<br><strong>Handoffs in User-Centric Cell-Free MIMO Networks: A POMDP Framework</strong><br><button class=copy-to-clipboard title="Handoffs in User-Centric Cell-Free MIMO Networks: A POMDP Framework" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-DC, cs-IT, cs-NI, cs-SY, cs.IT, eess-SP, eess-SY, math-IT<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08900v1.pdf filename=2403.08900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of managing handoffs (HOs) in user-centric cell-free massive MIMO (UC-mMIMO) networks. Motivated by the importance of controlling the number of HOs and by the correlation between efficient HO decisions and the temporal evolution of the channel conditions, we formulate a partially observable <b>Markov</b> <b>decision</b> <b>process</b> (POMDP) with the state space representing the discrete versions of the large-scale fading and the action space representing the association decisions of the user with the access points (APs). We develop a novel algorithm that employs this model to derive a HO policy for a mobile user based on current and future rewards. To alleviate the high complexity of our POMDP, we follow a divide-and-conquer approach by breaking down the POMDP formulation into sub-problems, each solved separately. Then, the policy and the candidate pool of APs for the sub-problem that produced the best total expected reward are used to perform HOs within a specific time horizon. We then introduce modifications to our algorithm to decrease the number of HOs. The results show that half of the number of HOs in the UC-mMIMO networks can be eliminated. Namely, our novel solution can control the number of HOs while maintaining a rate guarantee, where a 47%-70% reduction of the cumulative number of HOs is observed in networks with a density of 125 APs per km2. Most importantly, our results show that a POMDP-based HO scheme is promising to control HOs.</p></p class="citation"></blockquote><h3 id=67--255310-improved-trade-offs-between-amortization-and-download-bandwidth-for-linear-hss-keller-blackwell-et-al-2024>(6/7 | 255/310) Improved Trade-offs Between Amortization and Download Bandwidth for Linear HSS (Keller Blackwell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keller Blackwell, Mary Wootters. (2024)<br><strong>Improved Trade-offs Between Amortization and Download Bandwidth for Linear HSS</strong><br><button class=copy-to-clipboard title="Improved Trade-offs Between Amortization and Download Bandwidth for Linear HSS" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08719v1.pdf filename=2403.08719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A Homomorphic Secret Sharing (HSS) scheme is a secret-sharing scheme that shares a secret $x$ among $s$ servers, and additionally allows an output client to reconstruct some function $f(x)$ using information that can be locally computed by each server. A key parameter in HSS schemes is download rate, which quantifies how much information the output client needs to download from the servers. Often, download rate is improved by amortizing over $\ell$ instances of the problem, making $\ell$ also a key parameter of interest. Recent work (Fosli, Ishai, Kolobov, and Wootters 2022) established a limit on the download rate of linear HSS schemes for computing low-degree polynomials and constructed schemes that achieve this optimal download rate; their schemes required amortization over $\ell = \Omega(s \log(s))$ instances of the problem. Subsequent work (Blackwell and Wootters, 2023) completely characterized linear HSS schemes that achieve optimal download rate in terms of a coding-theoretic notion termed optimal labelweight codes. A consequence of this characterization was that $\ell = \Omega(s \log(s))$ is in fact necessary to achieve optimal download rate. In this paper, we characterize all linear HSS schemes, showing that schemes of any download rate are equivalent to a generalization of optimal labelweight codes. This equivalence is constructive and provides a way to obtain an explicit linear HSS scheme from any linear code. Using this characterization, we present explicit linear HSS schemes with slightly sub-optimal rate but with much improved amortization $\ell = O(s)$. Our constructions are based on algebraic <b>geometry</b> codes (specifically Hermitian codes and Goppa codes).</p></p class="citation"></blockquote><h3 id=77--256310-coverage-and-rate-analysis-for-integrated-sensing-and-communication-networks-xu-gan-et-al-2024>(7/7 | 256/310) Coverage and Rate Analysis for Integrated Sensing and Communication Networks (Xu Gan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Gan, Chongwen Huang, Zhaohui Yang, Xiaoming Chen, Jiguang He, Zhaoyang Zhang, Chau Yuen, Yong Liang Guan, Mérouane Debbah. (2024)<br><strong>Coverage and Rate Analysis for Integrated Sensing and Communication Networks</strong><br><button class=copy-to-clipboard title="Coverage and Rate Analysis for Integrated Sensing and Communication Networks" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08343v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08343v1.pdf filename=2403.08343v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrated sensing and communication (ISAC) is increasingly recognized as a pivotal technology for next-generation cellular networks, offering mutual benefits in both sensing and communication capabilities. This advancement necessitates a re-examination of the fundamental limits within networks where these two functions coexist via shared spectrum and infrastructures. However, traditional stochastic <b>geometry-based</b> performance analyses are confined to either communication or sensing networks separately. This paper bridges this gap by introducing a generalized stochastic <b>geometry</b> framework in ISAC networks. Based on this framework, we define and calculate the coverage and ergodic rate of sensing and communication performance under resource constraints. Then, we shed light on the fundamental limits of ISAC networks by presenting theoretical results for the coverage rate of the unified performance, taking into account the coupling effects of dual functions in coexistence networks. Further, we obtain the analytical formulations for evaluating the ergodic sensing rate constrained by the maximum communication rate, and the ergodic communication rate constrained by the maximum sensing rate. Extensive numerical results validate the accuracy of all theoretical derivations, and also indicate that denser networks significantly enhance ISAC coverage. Specifically, increasing the base station density from $1$ $\text{km}^{-2}$ to $10$ $\text{km}^{-2}$ can boost the ISAC coverage rate from $1.4%$ to $39.8%$. Further, results also reveal that with the increase of the constrained sensing rate, the ergodic communication rate improves significantly, but the reverse is not obvious.</p></p class="citation"></blockquote><h2 id=csma-4>cs.MA (4)</h2><h3 id=14--257310-cultural-evolution-in-populations-of-large-language-models-jérémy-perez-et-al-2024>(1/4 | 257/310) Cultural evolution in populations of Large Language Models (Jérémy Perez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jérémy Perez, Corentin Léger, Marcela Ovando-Tellez, Chris Foulon, Joan Dussauld, Pierre-Yves Oudeyer, Clément Moulin-Frier. (2024)<br><strong>Cultural evolution in populations of Large Language Models</strong><br><button class=copy-to-clipboard title="Cultural evolution in populations of Large Language Models" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: 68T50, I-2-7, cs-AI, cs-MA, cs.MA, q-bio-PE<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08882v1.pdf filename=2403.08882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research in cultural evolution aims at providing causal explanations for the change of culture over time. Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods. While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models. This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms. We here propose that leveraging the capacity of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to mimic human behavior may be fruitful to address this gap. On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake. Indeed, as artificial agents are bound to participate more and more to the evolution of culture, it is crucial to better understand the dynamics of machine-generated cultural evolution. We here present a framework for simulating cultural evolution in populations of <b>LLMs,</b> allowing the manipulation of variables known to be important in cultural evolution, such as network structure, personality, and the way social information is aggregated and transformed. The software we developed for conducting these <b>simulations</b> is open-source and features an intuitive user-interface, which we hope will help to build bridges between the fields of cultural evolution and generative artificial intelligence.</p></p class="citation"></blockquote><h3 id=24--258310-autonomous-underground-freight-transport-systems----the-future-of-urban-logistics-lasse-bienzeisler-et-al-2024>(2/4 | 258/310) Autonomous Underground Freight Transport Systems &ndash; The Future of Urban Logistics? (Lasse Bienzeisler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lasse Bienzeisler, Torben Lelke, Bernhard Friedrich. (2024)<br><strong>Autonomous Underground Freight Transport Systems &ndash; The Future of Urban Logistics?</strong><br><button class=copy-to-clipboard title="Autonomous Underground Freight Transport Systems -- The Future of Urban Logistics?" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs-SY, cs.MA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08841v1.pdf filename=2403.08841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We design a concept for an autonomous underground freight transport system for Hanover, Germany. To evaluate the resulting system changes in overall traffic flows from an environmental perspective, we carried out an agent-based traffic <b>simulation</b> with MATSim. Our <b>simulations</b> indicate comparatively low impacts on network-wide traffic volumes. Local CO2 emissions, on the other hand, could be reduced by up to 32 %. In total, the shuttle system can replace more than 18 % of the vehicles in use with conventional combustion engines. Thus, an autonomous underground freight transportation system can contribute to environmentally friendly and economical transportation of urban goods on the condition of cooperative use of the system.</p></p class="citation"></blockquote><h3 id=34--259310-beyond-joint-demonstrations-personalized-expert-guidance-for-efficient-multi-agent-reinforcement-learning-peihong-yu-et-al-2024>(3/4 | 259/310) Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning (Peihong Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peihong Yu, Manav Mishra, Alec Koppel, Carl Busart, Priya Narayan, Dinesh Manocha, Amrit Bedi, Pratap Tokekar. (2024)<br><strong>Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs-RO, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08936v1.pdf filename=2403.08936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space. While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations. In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team. These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts. To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to cooperate, namely personalized expert-guided MARL (PegMARL). This algorithm utilizes two discriminators: the first provides incentives based on the alignment of policy behavior with demonstrations, and the second regulates incentives based on whether the behavior leads to the desired objective. We evaluate PegMARL using personalized demonstrations in both discrete and continuous environments. The results demonstrate that PegMARL learns near-optimal policies even when provided with suboptimal demonstrations, and outperforms state-of-the-art MARL algorithms in solving coordinated tasks. We also showcase PegMARL&rsquo;s capability to leverage joint demonstrations in the StarCraft scenario and converge effectively even with demonstrations from non-co-trained policies.</p></p class="citation"></blockquote><h3 id=44--260310-emergence-of-social-norms-in-large-language-model-based-agent-societies-siyue-ren-et-al-2024>(4/4 | 260/310) Emergence of Social Norms in Large Language Model-based Agent Societies (Siyue Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyue Ren, Zhiyao Cui, Ruiqi Song, Zhen Wang, Shuyue Hu. (2024)<br><strong>Emergence of Social Norms in Large Language Model-based Agent Societies</strong><br><button class=copy-to-clipboard title="Emergence of Social Norms in Large Language Model-based Agent Societies" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-CY, cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08251v1.pdf filename=2403.08251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of social norms has attracted much interest in a wide array of disciplines, ranging from social science and cognitive science to artificial intelligence. In this paper, we propose the first generative agent architecture that empowers the emergence of social norms within a population of <b>large</b> <b>language</b> <b>model-based</b> agents. Our architecture, named CRSEC, consists of four modules: Creation & Representation, Spreading, Evaluation, and Compliance. Our architecture addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents&rsquo; communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents&rsquo; planning and actions. Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our architecture to establish social norms and reduce social conflicts within <b>large</b> <b>language</b> <b>model-based</b> multi-agent systems. The positive outcomes of our human evaluation, conducted with 30 evaluators, further affirm the effectiveness of our approach.</p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--261310-an-efficient-end-to-end-approach-to-noise-invariant-speech-features-via-multi-task-learning-heitor-r-guimarães-et-al-2024>(1/1 | 261/310) An Efficient End-to-End Approach to Noise Invariant Speech Features via Multi-Task Learning (Heitor R. Guimarães et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heitor R. Guimarães, Arthur Pimentel, Anderson R. Avila, Mehdi Rezagholizadeh, Boxing Chen, Tiago H. Falk. (2024)<br><strong>An Efficient End-to-End Approach to Noise Invariant Speech Features via Multi-Task Learning</strong><br><button class=copy-to-clipboard title="An Efficient End-to-End Approach to Noise Invariant Speech Features via Multi-Task Learning" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 38<br>Keywords: Benchmarking, Knowledge Distillation, Knowledge Distillation, Representation Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08654v1.pdf filename=2403.08654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> speech <b>representation</b> <b>learning</b> enables the extraction of meaningful features from raw waveforms. These features can then be efficiently used across multiple downstream tasks. However, two significant issues arise when considering the deployment of such methods ``in-the-wild": (i) Their large size, which can be prohibitive for edge applications; and (ii) their robustness to detrimental factors, such as noise and/or reverberation, that can heavily degrade the performance of such systems. In this work, we propose RobustDistiller, a novel <b>knowledge</b> <b>distillation</b> mechanism that tackles both problems jointly. Simultaneously to the <b>distillation</b> recipe, we apply a multi-task learning objective to encourage the network to learn noise-invariant <b>representations</b> <b>by</b> denoising the input. The proposed mechanism is evaluated on twelve different downstream tasks. It outperforms several <b>benchmarks</b> regardless of noise type, or noise and reverberation levels. Experimental results show that the new Student model with 23M parameters can achieve results comparable to the Teacher model with 95M parameters. Lastly, we show that the proposed recipe can be applied to other <b>distillation</b> methodologies, such as the recent DPWavLM. For reproducibility, code and model checkpoints will be made available at \mbox{\url{https://github.com/Hguimaraes/robustdistiller}}.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--262310-exploring-prompt-engineering-practices-in-the-enterprise-michael-desmond-et-al-2024>(1/5 | 262/310) Exploring Prompt Engineering Practices in the Enterprise (Michael Desmond et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Desmond, Michelle Brachman. (2024)<br><strong>Exploring Prompt Engineering Practices in the Enterprise</strong><br><button class=copy-to-clipboard title="Exploring Prompt Engineering Practices in the Enterprise" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08950v1.pdf filename=2403.08950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interaction with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is primarily carried out via <b>prompting.</b> A <b>prompt</b> is a natural language instruction designed to elicit certain behaviour or output from a model. In theory, natural language <b>prompts</b> enable non-experts to interact with and leverage <b>LLMs.</b> However, for complex tasks and tasks with specific requirements, <b>prompt</b> design is not trivial. Creating effective <b>prompts</b> requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal. We hypothesize that the way in which users iterate on their <b>prompts</b> can provide insight into how they think <b>prompting</b> and models work, as well as the kinds of support needed for more efficient <b>prompt</b> engineering. To better understand <b>prompt</b> engineering practices, we analyzed sessions of <b>prompt</b> editing behavior, categorizing the parts of <b>prompts</b> users iterated on and the types of changes they made. We discuss design implications and future directions based on these <b>prompt</b> engineering practices.</p></p class="citation"></blockquote><h3 id=25--263310-the-full-scale-assembly-simulation-testbed-fast-dataset-alec-g-moore-et-al-2024>(2/5 | 263/310) The Full-scale Assembly Simulation Testbed (FAST) Dataset (Alec G. Moore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alec G. Moore, Tiffany D. Do, Nayan N. Chawla, Antonia Jimenez Iriarte, Ryan P. McMahan. (2024)<br><strong>The Full-scale Assembly Simulation Testbed (FAST) Dataset</strong><br><button class=copy-to-clipboard title="The Full-scale Assembly Simulation Testbed (FAST) Dataset" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-LG, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08969v1.pdf filename=2403.08969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains. One constraint for this research area is the dearth of open datasets. In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly <b>Simulation</b> Testbed (FAST). This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR. In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers.</p></p class="citation"></blockquote><h3 id=35--264310-academiaos-automating-grounded-theory-development-in-qualitative-research-with-large-language-models-thomas-übellacker-2024>(3/5 | 264/310) AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models (Thomas Übellacker, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Übellacker. (2024)<br><strong>AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models</strong><br><button class=copy-to-clipboard title="AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-IR, cs.HC<br>Keyword Score: 20<br>Keywords: Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08844v1.pdf filename=2403.08844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AcademiaOS is a first attempt to automate grounded theory development in qualitative research with <b>large</b> <b>language</b> <b>models.</b> Using recent <b>large</b> <b>language</b> <b>models&rsquo;</b> language understanding, generation, and <b>reasoning</b> capabilities, AcademiaOS codes curated qualitative raw data such as interview transcripts and develops themes and dimensions to further develop a grounded theoretical model, affording novel insights. A user study (n=19) suggests that the system finds acceptance in the academic community and exhibits the potential to augment humans in qualitative research. AcademiaOS has been made open-source for others to build upon and adapt to their use cases.</p></p class="citation"></blockquote><h3 id=45--265310-ai-coach-for-badminton-dhruv-toshniwal-et-al-2024>(4/5 | 265/310) AI coach for badminton (Dhruv Toshniwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhruv Toshniwal, Arpit Patil, Nancy Vachhani. (2024)<br><strong>AI coach for badminton</strong><br><button class=copy-to-clipboard title="AI coach for badminton" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08956v1.pdf filename=2403.08956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the competitive realm of sports, optimal performance necessitates rigorous management of nutrition and physical conditioning. Specifically, in badminton, the agility and precision required make it an ideal candidate for motion analysis through video analytics. This study leverages advanced neural network methodologies to dissect video footage of badminton matches, aiming to extract detailed insights into player kinetics and biomechanics. Through the analysis of stroke mechanics, including hand-hip coordination, leg positioning, and the execution angles of strokes, the research aims to derive predictive models that can suggest improvements in stance, technique, and muscle orientation. These <b>recommendations</b> are designed to mitigate erroneous techniques, reduce the risk of joint fatigue, and enhance overall performance. Utilizing a vast array of data available online, this research correlates players&rsquo; physical attributes with their in-game movements to identify muscle activation patterns during play. The goal is to offer personalized training and nutrition strategies that align with the specific biomechanical demands of badminton, thereby facilitating targeted performance enhancements.</p></p class="citation"></blockquote><h3 id=55--266310-a-virtual-environment-for-collaborative-inspection-in-additive-manufacturing-vuthea-chheang-et-al-2024>(5/5 | 266/310) A Virtual Environment for Collaborative Inspection in Additive Manufacturing (Vuthea Chheang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vuthea Chheang, Brian Thomas Weston, Robert William Cerda, Brian Au, Brian Giera, Peer-Timo Bremer, Haichao Miao. (2024)<br><strong>A Virtual Environment for Collaborative Inspection in Additive Manufacturing</strong><br><button class=copy-to-clipboard title="A Virtual Environment for Collaborative Inspection in Additive Manufacturing" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-DC, cs-HC, cs.HC<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08940v1.pdf filename=2403.08940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries. A defining feature for many AM parts is the complex internal <b>geometry</b> enabled by the printing process. However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces. Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team. In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts. Geographically separated experts can virtually inspect and jointly discuss data. It also supports VR and non-VR users, who can be spectators in the VR environment. Various features for data exploration and inspection are developed and enhanced via real-time synchronization. We followed usability and interface verification guidelines using Nielsen&rsquo;s heuristics approach. Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback. Results reveal potential benefits, applicability, and current limitations. The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings.</p></p class="citation"></blockquote><h2 id=csgt-4>cs.GT (4)</h2><h3 id=14--267310-language-based-game-theory-in-the-age-of-artificial-intelligence-valerio-capraro-et-al-2024>(1/4 | 267/310) Language-based game theory in the age of artificial intelligence (Valerio Capraro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valerio Capraro, Roberto Di Paolo, Matjaz Perc, Veronica Pizziol. (2024)<br><strong>Language-based game theory in the age of artificial intelligence</strong><br><button class=copy-to-clipboard title="Language-based game theory in the age of artificial intelligence" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-CY, cs-GT, cs.GT, econ-TH<br>Keyword Score: 30<br>Keywords: Generative AI, Sentiment Analysis, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08944v1.pdf filename=2403.08944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding human behaviour in decision problems and strategic interactions has wide-ranging applications in economics, psychology, and artificial intelligence. Game theory offers a robust foundation for this understanding, based on the idea that individuals aim to maximize a utility function. However, the exact factors influencing strategy choices remain elusive. While traditional models try to explain human behaviour as a function of the outcomes of available actions, recent experimental research reveals that linguistic content significantly impacts decision-making, thus <b>prompting</b> a paradigm shift from outcome-based to language-based utility functions. This shift is more urgent than ever, given the advancement of <b>generative</b> <b>AI,</b> which has the potential to support humans in making critical decisions through language-based interactions. We propose <b>sentiment</b> <b>analysis</b> as a fundamental tool for this shift and take an initial step by analyzing 61 experimental instructions from the dictator game, an economic game capturing the balance between self-interest and the interest of others, which is at the core of many social interactions. Our meta-analysis shows that <b>sentiment</b> <b>analysis</b> can explain human behaviour beyond economic outcomes. We discuss future research directions. We hope this work sets the stage for a novel game theoretical approach that emphasizes the importance of language in human decisions.</p></p class="citation"></blockquote><h3 id=24--268310-strategizing-against-q-learners-a-control-theoretical-approach-yuksel-arslantas-et-al-2024>(2/4 | 268/310) Strategizing against Q-learners: A Control-theoretical Approach (Yuksel Arslantas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuksel Arslantas, Ege Yuceel, Muhammed O. Sayin. (2024)<br><strong>Strategizing against Q-learners: A Control-theoretical Approach</strong><br><button class=copy-to-clipboard title="Strategizing against Q-learners: A Control-theoretical Approach" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-GT, cs.GT, math-OC<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Quantization, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08906v1.pdf filename=2403.08906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the susceptibility of the Q-learning algorithm (a classical and widely used <b>reinforcement</b> <b>learning</b> method) to strategic manipulation of sophisticated opponents in games. We quantify how much a strategically sophisticated agent can exploit a naive Q-learner if she knows the opponent&rsquo;s Q-learning algorithm. To this end, we formulate the strategic actor&rsquo;s problem as a <b>Markov</b> <b>decision</b> <b>process</b> (with a continuum state space encompassing all possible Q-values) as if the Q-learning algorithm is the underlying dynamical system. We also present a <b>quantization-based</b> approximation scheme to tackle the continuum state space and analyze its performance both analytically and numerically.</p></p class="citation"></blockquote><h3 id=34--269310-learning-how-to-strategically-disclose-information-raj-kiriti-velicheti-et-al-2024>(3/4 | 269/310) Learning How to Strategically Disclose Information (Raj Kiriti Velicheti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raj Kiriti Velicheti, Melih Bastopcu, S. Rasoul Etesami, Tamer Başar. (2024)<br><strong>Learning How to Strategically Disclose Information</strong><br><button class=copy-to-clipboard title="Learning How to Strategically Disclose Information" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-IT, cs-LG, cs-SY, cs.GT, eess-SY, math-IT, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08741v1.pdf filename=2403.08741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Strategic information disclosure, in its simplest form, considers a game between an information provider (sender) who has access to some private information that an information receiver is interested in. While the receiver takes an action that affects the utilities of both players, the sender can design information (or modify beliefs) of the receiver through signal commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg equilibrium for this game traditionally requires the sender to have access to the receiver&rsquo;s objective. In this work, we consider an online version of information design where a sender interacts with a receiver of an unknown type who is adversarially chosen at each round. Restricting attention to Gaussian prior and quadratic costs for the sender and the receiver, we show that $\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback, where $T$ is the total number of interactions between the sender and the receiver. Further, we propose a novel parametrization that allows the sender to achieve $\mathcal{O}(\sqrt{T})$ regret for a general convex utility function. We then consider the Bayesian Persuasion problem with an additional cost term in the objective function, which penalizes signaling policies that are more informative and obtain $\mathcal{O}(\log(T))$ regret. Finally, we establish a sublinear regret bound for the partial information feedback setting and provide <b>simulations</b> to support our theoretical results.</p></p class="citation"></blockquote><h3 id=44--270310-an-algorithmic-theory-of-simplicity-in-mechanism-design-diodato-ferraioli-et-al-2024>(4/4 | 270/310) An Algorithmic Theory of Simplicity in Mechanism Design (Diodato Ferraioli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diodato Ferraioli, Carmine Ventre. (2024)<br><strong>An Algorithmic Theory of Simplicity in Mechanism Design</strong><br><button class=copy-to-clipboard title="An Algorithmic Theory of Simplicity in Mechanism Design" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-MA, cs.GT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08610v1.pdf filename=2403.08610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A growing body of work in economics and computation focuses on the trade-off between implementability and simplicity in mechanism design. The goal is to develop a theory that not only allows to design an incentive structure easy to grasp for imperfectly rational agents, but also understand the ensuing limitations on the class of mechanisms that enforce it. In this context, the concept of OSP mechanisms has assumed a prominent role since they provably account for the absence of contingent <b>reasoning</b> skills, a specific cognitive limitation. For single-dimensional agents, it is known that OSP mechanisms need to use certain greedy algorithms. In this work, we introduce a notion that interpolates between OSP and SOSP, a more stringent notion where agents only plan a subset of their own future moves. We provide an algorithmic characterization of this novel class of mechanisms for single-dimensional domains and binary allocation problems, that precisely measures the interplay between simplicity and implementability. We build on this to show how mechanisms based on reverse greedy algorithms (a.k.a., deferred acceptance auctions) are algorithmically more robust to imperfectly rationality than those adopting greedy algorithms.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--271310-digital-twin-assisted-reinforcement-learning-for-resource-aware-microservice-offloading-in-edge-computing-xiangchun-chen-et-al-2024>(1/2 | 271/310) Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing (Xiangchun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangchun Chen, Jiannong Cao, Zhixuan Liang, Yuvraj Sahni, Mingjin Zhang. (2024)<br><strong>Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing</strong><br><button class=copy-to-clipboard title="Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-LG, cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08687v1.pdf filename=2403.08687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute microservices from end devices. Microservice offloading, a fundamentally important problem, decides when and where microservices are executed upon the arrival of services. However, the dynamic nature of the real-world CEC environment often leads to inefficient microservice offloading strategies, resulting in underutilized resources and network congestion. To address this challenge, we formulate an online joint microservice offloading and bandwidth allocation problem, JMOBA, to minimize the average completion time of services. In this paper, we introduce a novel microservice offloading algorithm, DTDRLMO, which leverages deep <b>reinforcement</b> <b>learning</b> (DRL) and digital twin technology. Specifically, we employ digital twin techniques to predict and adapt to changing edge node loads and network conditions of CEC in real-time. Furthermore, this approach enables the generation of an efficient offloading plan, selecting the most suitable edge node for each microservice. <b>Simulation</b> results on real-world and synthetic datasets demonstrate that DTDRLMO outperforms heuristic and learning-based methods in average service completion time.</p></p class="citation"></blockquote><h3 id=22--272310-from-channel-measurement-to-training-data-for-phy-layer-ai-applications-michael-zentarra-et-al-2024>(2/2 | 272/310) From Channel Measurement to Training Data for PHY Layer AI Applications (Michael Zentarra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Zentarra, Julian Ahrens, Lia Ahrens. (2024)<br><strong>From Channel Measurement to Training Data for PHY Layer AI Applications</strong><br><button class=copy-to-clipboard title="From Channel Measurement to Training Data for PHY Layer AI Applications" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08317v1.pdf filename=2403.08317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning-based techniques such as artificial intelligence (AI) and machine learning (ML) play an increasingly important role in the development of future communication networks. The success of a learning algorithm depends on the quality and quantity of the available training data. In the physical layer (PHY), channel information data can be obtained either through measurement campaigns or through <b>simulations</b> based on predefined channel models. Performing measurements can be time consuming while only gaining information about one specific position or scenario. Simulated data, on the other hand, are more generalized and reflect in most cases not a real environment but instead, a statistical approximation based on a mathematical model. This paper presents a procedure for acquiring channel data by means of fast and flexible software defined radio (SDR) based channel measurements along with a method for a parameter extraction that provides configuration input to the simulator. The procedure from the measurement to the simulated channel data is demonstrated in two exemplary propagation scenarios. It is shown, that in both cases the simulated data is in good accordance to the measurements</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--273310-self-supervised-learning-for-covariance-estimation-tzvi-diskin-et-al-2024>(1/1 | 273/310) Self-Supervised Learning for Covariance Estimation (Tzvi Diskin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tzvi Diskin, Ami Wiesel. (2024)<br><strong>Self-Supervised Learning for Covariance Estimation</strong><br><button class=copy-to-clipboard title="Self-Supervised Learning for Covariance Estimation" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 30<br>Keywords: Foundation Model, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08662v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08662v1.pdf filename=2403.08662v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the use of deep learning for covariance estimation. We propose to globally learn a neural network that will then be applied locally at inference time. Leveraging recent advancements in <b>self-supervised</b> <b>foundational</b> <b>models,</b> we train the network without any labeling by simply masking different samples and learning to predict their covariance given their surrounding neighbors. The architecture is based on the popular attention mechanism. Its main advantage over classical methods is the automatic exploitation of global characteristics without any distributional assumptions or regularization. It can be pre-trained as a <b>foundation</b> <b>model</b> and then be repurposed for various downstream tasks, e.g., adaptive target detection in radar or hyperspectral imagery.</p></p class="citation"></blockquote><h2 id=csce-5>cs.CE (5)</h2><h3 id=15--274310-evaluating-the-efficiency-and-cost-effectiveness-of-rpb-based-co2-capture-a-comprehensive-approach-to-simultaneous-design-and-operating-condition-optimization-howoun-jung-et-al-2024>(1/5 | 274/310) Evaluating the Efficiency and Cost-effectiveness of RPB-based CO2 Capture: A Comprehensive Approach to Simultaneous Design and Operating Condition Optimization (Howoun Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Howoun Jung, Nohjin Park, Jay H. Lee. (2024)<br><strong>Evaluating the Efficiency and Cost-effectiveness of RPB-based CO2 Capture: A Comprehensive Approach to Simultaneous Design and Operating Condition Optimization</strong><br><button class=copy-to-clipboard title="Evaluating the Efficiency and Cost-effectiveness of RPB-based CO2 Capture: A Comprehensive Approach to Simultaneous Design and Operating Condition Optimization" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 30<br>Keywords: Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08244v1.pdf filename=2403.08244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite ongoing global initiatives to reduce CO2 emissions, implementing large-scale CO2 capture using amine solvents is fraught with economic uncertainties and technical hurdles. The Rotating Packed Bed (RPB) presents a promising alternative to traditional packed towers, offering compact design and adaptability. Nonetheless, scaling RPB processes to an industrial level is challenging due to the nascent nature of its application. The complexity of designing RPB units, setting operating conditions, and evaluating process performance adds layers of difficulty to the adoption of RPB-based systems in industries. This study introduces an optimization-driven design and evaluation for CO2 capture processes utilizing RPB columns. By employing detailed process <b>simulation,</b> we aim to concurrently optimize unit design and operating parameters, underscoring its advantage over conventional sequential approaches. Our process design method integrates heuristic design <b>recommendations</b> as constraints, resulting in 9.4% to 12.7% cost savings compared to conventional sequential design methods. Furthermore, our comprehensive process-level analysis reveals that using concentrated MEA solvent can yield total cost savings of 13.4% to 25.0% compared to the standard 30wt% MEA solvent. Additionally, the RPB unit can deliver an 8.5 to 23.6 times reduction in packing volume. While the commercial-scale feasibility of RPB technology has been established, the advancement of this field hinges on acquiring a broader and more robust dataset from commercial-scale implementations. Employing strategic methods like modularization could significantly reduce the entry barriers for CO2 capture projects, facilitating their broader adoption and implementation.</p></p class="citation"></blockquote><h3 id=25--275310-a-framework-for-strategic-discovery-of-credible-neural-network-surrogate-models-under-uncertainty-pratyush-kumar-singh-et-al-2024>(2/5 | 275/310) A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty (Pratyush Kumar Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pratyush Kumar Singh, Kathryn A. Farrell-Maupin, Danial Faghihi. (2024)<br><strong>A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty</strong><br><button class=copy-to-clipboard title="A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-LG, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08901v1.pdf filename=2403.08901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity <b>simulations</b> of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making. This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters. The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty. Leveraging these principles, OPAL-surrogate introduces a systematic and efficient strategy for balancing the trade-off between model complexity, accuracy, and prediction uncertainty. The effectiveness of OPAL-surrogate is demonstrated through two modeling problems, including the deformation of porous materials for building insulation and turbulent combustion flow for the ablation of solid fuels within hybrid rocket motors.</p></p class="citation"></blockquote><h3 id=35--276310-improved-bass-model-using-sales-proportional-average-for-one-condition-of-mono-peak-curves-ahmad-abu-sleem-et-al-2024>(3/5 | 276/310) Improved bass model using sales proportional average for one condition of mono peak curves (Ahmad Abu Sleem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Abu Sleem, Mohammed Alromema, Mohammad A. M. Abdel-Aal. (2024)<br><strong>Improved bass model using sales proportional average for one condition of mono peak curves</strong><br><button class=copy-to-clipboard title="Improved bass model using sales proportional average for one condition of mono peak curves" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08993v1.pdf filename=2403.08993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>&ldquo;This study provides a modified Bass model to deal with trend curves for basic issues of relevance to individuals from all over the world, for which we collected 16 data sets from 2004 to 2022 and that are available on Google servers as &ldquo;google trends&rdquo;. It was discovered that the Bass model did not forecast well for curves that have a mono peak with a sharp decrease to some level then have semi-stable with small decrement sales for a long time, thus a new parameter based on r1 and r2 (ratios of average sales) was introduced, which improved the model&rsquo;s prediction ability and provided better results. The model was also applied to a data set taken from the Kaggle website about a subscriber digital product offering for financial services that include newsletters, webinars, and investment <b>recommendations.</b> The data contain 508932 data points about the products sold during 2016-2022. Compared to the traditional Bass model, the modified model showed better results in dealing with this condition, as the expected curve shape was closer to real sales, and the sum of squares error (SSE) value was reduced to a ratio ranging between (36.35-79.3%). Therefore, the improved model can be relied upon in these conditions.&rdquo;</p></p class="citation"></blockquote><h3 id=45--277310-a-comparative-analysis-of-transient-finite-strain-coupled-diffusion-deformation-theories-for-hydrogels-jorge-humberto-urrea-quintero-et-al-2024>(4/5 | 277/310) A comparative analysis of transient finite-strain coupled diffusion-deformation theories for hydrogels (Jorge-Humberto Urrea-Quintero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorge-Humberto Urrea-Quintero, Michele Marino, Thomas Wick, Udo Nackenhorst. (2024)<br><strong>A comparative analysis of transient finite-strain coupled diffusion-deformation theories for hydrogels</strong><br><button class=copy-to-clipboard title="A comparative analysis of transient finite-strain coupled diffusion-deformation theories for hydrogels" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cond-mat-mtrl-sci, cond-mat-soft, cs-CE, cs.CE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08972v1.pdf filename=2403.08972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents a comparative review and classification between some well-known thermodynamically consistent models of hydrogel behavior in a large deformation setting, specifically focusing on solvent absorption/desorption and its impact on mechanical deformation and network swelling. The proposed discussion addresses formulation aspects, general mathematical classification of the governing equations, and numerical implementation issues based on the finite element method. The theories are presented in a unified framework demonstrating that, despite not being evident in some cases, all of them follow equivalent thermodynamic arguments. A detailed numerical analysis is carried out where Taylor-Hood elements are employed in the spatial discretization to satisfy the inf-sup condition and to prevent spurious numerical oscillations. The resulting discrete problems are solved using the FEniCS platform through consistent variational formulations, employing both monolithic and staggered approaches. We conduct <b>benchmark</b> tests on various hydrogel structures, demonstrating that major differences arise from the chosen volumetric response of the hydrogel. The significance of this choice is frequently underestimated in the state-of-the-art literature but has been shown to have substantial implications on the resulting hydrogel behavior.</p></p class="citation"></blockquote><h3 id=55--278310-model-order-reduction-for-transient-coupled-diffusion-deformation-of-hydrogels-gopal-agarwal-et-al-2024>(5/5 | 278/310) Model order reduction for transient coupled diffusion-deformation of hydrogels (Gopal Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gopal Agarwal, Jorge-Humberto Urrea-Quintero, Henning Wessels, Thomas Wick. (2024)<br><strong>Model order reduction for transient coupled diffusion-deformation of hydrogels</strong><br><button class=copy-to-clipboard title="Model order reduction for transient coupled diffusion-deformation of hydrogels" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cond-mat-mtrl-sci, cond-mat-soft, cs-CE, cs.CE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08968v1.pdf filename=2403.08968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a reduced-order model (ROM) for analyzing the transient diffusion-deformation of hydrogels. The full-order model (FOM) describing hydrogel transient behavior consists of a coupled system of partial differential equations in which chemical potential and displacements are coupled. This system is formulated in a monolithic fashion and solved using the Finite Element Method (FEM). The ROM employs proper orthogonal decomposition as a model order reduction approach. We test the ROM performance through <b>benchmark</b> tests on hydrogel swelling behavior and a case study simulating co-axial printing. Finally, we embed the ROM into an optimization problem to identify the model material parameters of the coupled problem using full-field data. We verify that the ROM can predict hydrogels&rsquo; diffusion-deformation evolution and material properties, significantly reducing computation time compared to the FOM. The results demonstrate the ROM&rsquo;s accuracy and computational efficiency. This work paths the way towards advanced practical applications of ROMs, e.g., in the context of feedback error control in hydrogel 3D printing.</p></p class="citation"></blockquote><h2 id=cssy-1>cs.SY (1)</h2><h3 id=11--279310-differential-privacy-in-nonlinear-dynamical-systems-with-tracking-performance-guarantees-dhrubajit-chowdhury-et-al-2024>(1/1 | 279/310) Differential Privacy in Nonlinear Dynamical Systems with Tracking Performance Guarantees (Dhrubajit Chowdhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhrubajit Chowdhury, Raman Goyal, Shantanu Rane. (2024)<br><strong>Differential Privacy in Nonlinear Dynamical Systems with Tracking Performance Guarantees</strong><br><button class=copy-to-clipboard title="Differential Privacy in Nonlinear Dynamical Systems with Tracking Performance Guarantees" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SY<br>Categories: cs-SY, cs.SY, eess-SY<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08181v1.pdf filename=2403.08181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel approach to make the tracking error of a class of nonlinear systems differentially private in addition to guaranteeing the tracking error performance. We use funnel control to make the tracking error evolve within a performance funnel that is pre-specified by the user. We make the performance funnel differentially private by adding a bounded continuous noise generated from an Ornstein-Uhlenbeck-type process. Since the funnel controller is a function of the performance funnel, the noise adds randomized perturbation to the control input. We show that, as a consequence of the <b>differential</b> <b>privacy</b> of the performance funnel, the tracking error is also differentially private. As a result, the tracking error is bounded by the noisy funnel boundary while maintaining privacy. We show a <b>simulation</b> result to demonstrate the framework.</p></p class="citation"></blockquote><h2 id=cssi-3>cs.SI (3)</h2><h3 id=13--280310-link-prediction-for-social-networks-using-representation-learning-and-heuristic-based-features-samarth-khanna-et-al-2024>(1/3 | 280/310) Link Prediction for Social Networks using Representation Learning and Heuristic-based Features (Samarth Khanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samarth Khanna, Sree Bhattacharyya, Sudipto Ghosh, Kushagra Agarwal, Asit Kumar Das. (2024)<br><strong>Link Prediction for Social Networks using Representation Learning and Heuristic-based Features</strong><br><button class=copy-to-clipboard title="Link Prediction for Social Networks using Representation Learning and Heuristic-based Features" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-LG, cs-SI, cs.SI<br>Keyword Score: 28<br>Keywords: Graph, Graph Neural Network, Recommendation, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08613v1.pdf filename=2403.08613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exponential growth in scale and relevance of social networks enable them to provide expansive insights. Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating <b>recommendations</b> to influence analysis. Several categories of solutions exist for the same. Here, we explore various feature extraction techniques to generate <b>representations</b> <b>of</b> nodes and edges in a social network that allow us to predict missing links. We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, <b>Graph</b> <b>Neural</b> <b>Networks,</b> and <b>Graph</b> <b>Heuristics,</b> <b>followed</b> by modeling with ensemble classifiers and custom Neural Networks. Further, we propose combining heuristic-based features and learned <b>representations</b> <b>that</b> demonstrate improved performance for the link prediction task on social network datasets. Using this method to generate accurate <b>recommendations</b> for many applications is a matter of further study that appears very promising. The code for all the experiments has been made public.</p></p class="citation"></blockquote><h3 id=23--281310-negative-impact-of-online-political-incivility-on-willingness-to-see-political-comments-kohei-nishi-2024>(2/3 | 281/310) Negative Impact of Online Political Incivility on Willingness to See Political Comments (Kohei Nishi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kohei Nishi. (2024)<br><strong>Negative Impact of Online Political Incivility on Willingness to See Political Comments</strong><br><button class=copy-to-clipboard title="Negative Impact of Online Political Incivility on Willingness to See Political Comments" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08372v1.pdf filename=2403.08372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been significant attention on online political incivility. While previous research suggests that uncivil political comments lead people to be less willing to see more comments on the same issue, two critical questions have received limited exploration: (1) Are people exposed to uncivil political comments less willing to see other comments from the person who posted the uncivil comment?; (2) Are people exposed to uncivil political comments less willing to see comments from people who have different thoughts than them? To address these questions, the present study conducted a preregistered online survey experiment targeting Japanese citizens, focusing on the pro- vs anti-Kishida cabinet conflict in Japan. The results show that the participants were less willing to see other comments by the person who posted the comment when the comment was uncivil than when it was civil. In addition, the anti-Kishida participants were less willing to see political opinions posted online by people who have different thoughts than them when the comment was uncivil than when it was civil, while the participants in the other subgroups did not show a similar tendency. These findings suggest that uncivil expressions in online political communication might <b>prompt</b> people to avoid reading opinions from those who have different thoughts than them, which might promote political echo chambers.</p></p class="citation"></blockquote><h3 id=33--282310-an-improvement-on-the-louvain-algorithm-using-random-walks-duy-hieu-do-et-al-2024>(3/3 | 282/310) An improvement on the Louvain algorithm using random walks (Duy Hieu Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duy Hieu Do, Thi Ha Duong Phan. (2024)<br><strong>An improvement on the Louvain algorithm using random walks</strong><br><button class=copy-to-clipboard title="An improvement on the Louvain algorithm using random walks" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08313v1.pdf filename=2403.08313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We will present improvements to famous algorithms for community detection, namely Newman&rsquo;s spectral method algorithm and the Louvain algorithm. The Newman algorithm begins by treating the original <b>graph</b> as a single cluster, then repeats the process to split each cluster into two, based on the signs of the eigenvector corresponding to the secondlargest eigenvalue. Our improvement involves replacing the time-consuming computation of eigenvalues with a random walk during the splitting process. The Louvain algorithm iteratively performs the following steps until no increase in modularity can be achieved anymore: each step consists of two phases, phase 1 for partitioning the <b>graph</b> into clusters, and phase 2 for constructing a new <b>graph</b> where each vertex represents one cluster obtained from phase 1. We propose an improvement to this algorithm by adding our random walk algorithm as an additional phase for refining clusters obtained from phase 1. It maintains a complexity comparable to the Louvain algorithm while exhibiting superior efficiency. To validate the robustness and effectiveness of our proposed algorithms, we conducted experiments using randomly generated <b>graphs</b> and real-world data.</p></p class="citation"></blockquote><h2 id=mathna-4>math.NA (4)</h2><h3 id=14--283310-efficient-geometric-markov-chain-monte-carlo-for-nonlinear-bayesian-inversion-enabled-by-derivative-informed-neural-operators-lianghao-cao-et-al-2024>(1/4 | 283/310) Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators (Lianghao Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lianghao Cao, Thomas O&rsquo;Leary-Roseberry, Omar Ghattas. (2024)<br><strong>Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators</strong><br><button class=copy-to-clipboard title="Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-LG, cs-NA, math-NA, math.NA, stat-CO, stat-ML<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08220v1.pdf filename=2403.08220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local <b>geometry,</b> it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model <b>simulations.</b> We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional operator learning using input&ndash;output samples often demands a prohibitively large number of model <b>simulations.</b> In this work, we present an extension of derivative-informed operator learning [O&rsquo;Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] using input&ndash;output&ndash;derivative training samples. Such a learning method leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observable and its parametric derivative at a significantly lower training cost than the conventional method. Cost and error analysis for reduced basis DINO surrogates are provided. Numerical studies on PDE-constrained Bayesian inversion demonstrate that DINO-driven MCMC generates effective posterior samples 3&ndash;9 times faster than geometric MCMC and 60&ndash;97 times faster than prior <b>geometry-based</b> MCMC. Furthermore, the training cost of DINO surrogates breaks even after collecting merely 10&ndash;25 effective posterior samples compared to geometric MCMC.</p></p class="citation"></blockquote><h3 id=24--284310-interpolatory-model-order-reduction-of-large-scale-dynamical-systems-with-root-mean-squared-error-measures-sean-reiter-et-al-2024>(2/4 | 284/310) Interpolatory model order reduction of large-scale dynamical systems with root mean squared error measures (Sean Reiter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sean Reiter, Steffen W. R. Werner. (2024)<br><strong>Interpolatory model order reduction of large-scale dynamical systems with root mean squared error measures</strong><br><button class=copy-to-clipboard title="Interpolatory model order reduction of large-scale dynamical systems with root mean squared error measures" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, cs-SY, eess-SY, math-DS, math-NA, math-OC, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08894v1.pdf filename=2403.08894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The root mean squared error is an important measure used in a variety of applications such as structural dynamics and acoustics to model averaged deviations from standard behavior. For large-scale systems, <b>simulations</b> of this quantity quickly become computationally prohibitive. Classical model order reduction techniques attempt to resolve this issue via the construction of surrogate models that emulate the root mean squared error measure using an intermediate linear system. However, this approach requires a potentially large number of linear outputs, which can be disadvantageous in the design of reduced-order models. In this work, we consider directly the root mean squared error as the quantity of interest using the concept of quadratic-output models and propose several new model reduction techniques for the construction of appropriate surrogates. We test the proposed methods on a model for the vibrational response of a plate with tuned vibration absorbers.</p></p class="citation"></blockquote><h3 id=34--285310-non-linear-collision-induced-breakage-equation-finite-volume-and-semi-analytical-methods-sanjiv-kumar-bariwal-et-al-2024>(3/4 | 285/310) Non-linear collision-induced breakage equation: finite volume and semi-analytical methods (Sanjiv Kumar Bariwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanjiv Kumar Bariwal, Saddam Hussain, Rajesh Kumar. (2024)<br><strong>Non-linear collision-induced breakage equation: finite volume and semi-analytical methods</strong><br><button class=copy-to-clipboard title="Non-linear collision-induced breakage equation: finite volume and semi-analytical methods" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-AP, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08457v1.pdf filename=2403.08457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The non-linear collision-induced breakage equation has significant applications in particulate processes. Two semi-analytical techniques, namely homotopy analysis method (HAM) and accelerated homotopy perturbation method (AHPM) are investigated along with the well-known finite volume method (FVM) to comprehend the dynamical behavior of the non-linear system, i.e., the concentration function, the total number and the total mass of the particles in the system. The theoretical convergence analyses of the series solutions of HAM and AHPM are discussed. In addition, the error estimations of the truncated solutions of both methods equip the maximum absolute error bound. To justify the applicability and accuracy of these methods, numerical <b>simulations</b> are compared with the findings of FVM and analytical solutions considering three physical problems.</p></p class="citation"></blockquote><h3 id=44--286310-tangential-fixpoint-iterations-for-gromov-wasserstein-barycenters-florian-beier-et-al-2024>(4/4 | 286/310) Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters (Florian Beier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Beier, Robert Beinert. (2024)<br><strong>Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters</strong><br><button class=copy-to-clipboard title="Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65K10, 28A33, 58C30, 65D05, cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08612v1.pdf filename=2403.08612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Gromov-Wasserstein (GW) transport problem is a relaxation of classic optimal transport, which seeks a transport between two measures while preserving their internal <b>geometry.</b> Due to meeting this theoretical underpinning, it is a valuable tool for the analysis of objects that do not possess a natural embedding or should be studied independently of it. Prime applications can thus be found in e.g. shape matching, classification and interpolation tasks. To tackle the latter, one theoretically justified approach is the employment of multi-marginal GW transport and GW barycenters, which are Fr'echet means with respect to the GW distance. However, because the computation of GW itself already poses a quadratic and non-convex optimization problem, the determination of GW barycenters is a hard task and algorithms for their computation are scarce. In this paper, we revisit a known procedure for the determination of Fr'echet means in Riemannian manifolds via tangential approximations in the context of GW. We provide a characterization of barycenters in the GW tangent space, which ultimately gives rise to a fixpoint iteration for approximating GW barycenters using multi-marginal plans. We propose a relaxation of this fixpoint iteration and show that it monotonously decreases the barycenter loss. In certain cases our proposed method naturally provides us with barycentric embeddings. The resulting algorithm is capable of producing qualitative shape interpolations between multiple 3d shapes with support sizes of over thousands of points in reasonable time. In addition, we verify our method on shape classification and multi-graph matching tasks.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--287310-learnable-community-aware-transformer-for-brain-connectome-analysis-with-token-clustering-yanting-yang-et-al-2024>(1/1 | 287/310) Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering (Yanting Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanting Yang, Beidi Zhao, Zhuohao Ni, Yize Zhao, Xiaoxiao Li. (2024)<br><strong>Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering</strong><br><button class=copy-to-clipboard title="Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-LG, eess-IV, q-bio-NC, q-bio.NC<br>Keyword Score: 23<br>Keywords: Clustering, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08203v1.pdf filename=2403.08203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neuroscientific research has revealed that the complex brain network can be organized into distinct functional communities, each characterized by a cohesive group of regions of interest (ROIs) with strong interconnections. These communities play a crucial role in comprehending the functional organization of the brain and its implications for neurological conditions, including Autism Spectrum Disorder (ASD) and biological differences, such as in gender. Traditional models have been constrained by the necessity of predefined community clusters, limiting their flexibility and adaptability in deciphering the brain&rsquo;s functional organization. Furthermore, these models were restricted by a fixed number of communities, hindering their ability to accurately represent the brain&rsquo;s dynamic nature. In this study, we present a token <b>clustering</b> brain <b>transformer-based</b> model ($\texttt{TC-BrainTF}$) for joint community <b>clustering</b> and classification. Our approach proposes a novel token <b>clustering</b> (TC) module based on the <b>transformer</b> architecture, which utilizes learnable <b>prompt</b> tokens with orthogonal loss where each ROI embedding is projected onto the <b>prompt</b> embedding space, effectively <b>clustering</b> ROIs into communities and reducing the dimensions of the node representation via merging with communities. Our results demonstrate that our learnable community-aware model $\texttt{TC-BrainTF}$ offers improved accuracy in identifying ASD and classifying genders through rigorous testing on ABIDE and HCP datasets. Additionally, the qualitative analysis on $\texttt{TC-BrainTF}$ has demonstrated the effectiveness of the designed TC module and its relevance to neuroscience interpretations.</p></p class="citation"></blockquote><h2 id=mathoc-3>math.OC (3)</h2><h3 id=13--288310-a-constrained-tracking-controller-for-ramp-and-sinusoidal-reference-signals-using-robust-positive-invariance-geovana-franca-dos-santos-et-al-2024>(1/3 | 288/310) A Constrained Tracking Controller for Ramp and Sinusoidal Reference Signals using Robust Positive Invariance (Geovana Franca dos Santos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geovana Franca dos Santos, Eugenio B. Castelan, Walter Lucia. (2024)<br><strong>A Constrained Tracking Controller for Ramp and Sinusoidal Reference Signals using Robust Positive Invariance</strong><br><button class=copy-to-clipboard title="A Constrained Tracking Controller for Ramp and Sinusoidal Reference Signals using Robust Positive Invariance" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08987v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08987v2.pdf filename=2403.08987v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an output feedback controller capable of ensuring steady-state offset-free tracking for ramp and sinusoidal reference signals while ensuring local stability and state and input constraints fulfillment. The proposed solution is derived by jointly exploiting the internal model principle, polyhedral robust positively invariant arguments, and the Extended Farkas&rsquo; Lemma. In particular, by considering a generic class of output feedback controller equipped with a feedforward term, a proportional effect, and a double integrator, we offline design the controller&rsquo;s gains by means of a single bilinear optimization problem. A peculiar feature of the proposed design is that the sets of all the admissible reference signals and the plant&rsquo;s initial conditions are also offline determined. <b>Simulation</b> results are provided to testify to the effectiveness of the proposed tracking controller and its capability to deal with both state and input constraints.</p></p class="citation"></blockquote><h3 id=23--289310-regret-analysis-of-policy-optimization-over-submanifolds-for-linearly-constrained-online-lqg-ting-jui-chang-et-al-2024>(2/3 | 289/310) Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG (Ting-Jui Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting-Jui Chang, Shahin Shahrampour. (2024)<br><strong>Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG</strong><br><button class=copy-to-clipboard title="Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08553v1.pdf filename=2403.08553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancement in online optimization and control has provided novel tools to study online linear quadratic regulator (LQR) problems, where cost matrices are varying adversarially over time. However, the controller parameterization of existing works may not satisfy practical conditions like sparsity due to physical connections. In this work, we study online linear quadratic Gaussian problems with a given linear constraint imposed on the controller. Inspired by the recent work of [1] which proposed, for a linearly constrained policy optimization of an offline LQR, a second order method equipped with a Riemannian metric that emerges naturally in the context of optimal control problems, we propose online optimistic Newton on manifold (OONM) which provides an online controller based on the prediction on the first and second order information of the function sequence. To quantify the proposed algorithm, we leverage the notion of regret defined as the sub-optimality of its cumulative cost to that of a (locally) minimizing controller sequence and provide the regret bound in terms of the path-length of the minimizer sequence. <b>Simulation</b> results are also provided to verify the property of OONM.</p></p class="citation"></blockquote><h3 id=33--290310-exponential-stability-of-parametric-optimization-based-controllers-via-lure-contractivity-alexander-davydov-et-al-2024>(3/3 | 290/310) Exponential Stability of Parametric Optimization-Based Controllers via Lur&rsquo;e Contractivity (Alexander Davydov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Davydov, Francesco Bullo. (2024)<br><strong>Exponential Stability of Parametric Optimization-Based Controllers via Lur&rsquo;e Contractivity</strong><br><button class=copy-to-clipboard title="Exponential Stability of Parametric Optimization-Based Controllers via Lur'e Contractivity" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08159v1.pdf filename=2403.08159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this letter, we investigate sufficient conditions for the exponential stability of LTI systems driven by controllers derived from parametric optimization problems. Our primary focus is on parametric projection controllers, namely parametric programs whose objective function is the squared distance to a nominal controller. Leveraging the virtual system method of analysis and a novel contractivity result for Lur&rsquo;e systems, we establish a sufficient LMI condition for the exponential stability of an LTI system with a parametric projection-based controller. Separately, we prove additional results for single-integrator systems. Finally, we apply our results to state-dependent saturated control systems and control barrier function-based control and provide numerical <b>simulations.</b></p></p class="citation"></blockquote><h2 id=statml-4>stat.ML (4)</h2><h3 id=14--291310-towards-model-agnostic-posterior-approximation-for-fast-and-accurate-variational-autoencoders-yaniv-yacoby-et-al-2024>(1/4 | 291/310) Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders (Yaniv Yacoby et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaniv Yacoby, Weiwei Pan, Finale Doshi-Velez. (2024)<br><strong>Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders</strong><br><button class=copy-to-clipboard title="Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08941v1.pdf filename=2403.08941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inference for <b>Variational</b> <b>Autoencoders</b> (VAEs) consists of learning two models: (1) a generative model, which transforms a simple distribution over a latent space into the distribution over observed data, and (2) an inference model, which approximates the posterior of the latent codes given data. The two components are learned jointly via a lower bound to the generative model&rsquo;s log marginal likelihood. In early phases of joint training, the inference model poorly approximates the latent code posteriors. Recent work showed that this leads optimization to get stuck in local optima, negatively impacting the learned generative model. As such, recent work suggests ensuring a high-quality inference model via iterative training: maximizing the objective function relative to the inference model before every update to the generative model. Unfortunately, iterative training is inefficient, requiring heuristic criteria for reverting from iterative to joint training for speed. Here, we suggest an inference method that trains the generative and inference models independently. It approximates the posterior of the true model a priori; fixing this posterior approximation, we then maximize the lower bound relative to only the generative model. By conventional wisdom, this approach should rely on the true prior and likelihood of the true model to approximate its posterior (which are unknown). However, we show that we can compute a deterministic, model-agnostic posterior approximation (MAPA) of the true model&rsquo;s posterior. We then use MAPA to develop a proof-of-concept inference method. We present preliminary results on low-dimensional synthetic data that (1) MAPA captures the trend of the true posterior, and (2) our MAPA-based inference performs better density estimation with less computation than baselines. Lastly, we present a roadmap for scaling the MAPA-based inference method to high-dimensional data.</p></p class="citation"></blockquote><h3 id=24--292310-a-non-asymptotic-theory-of-kernel-ridge-regression-deterministic-equivalents-test-error-and-gcv-estimator-theodor-misiakiewicz-et-al-2024>(2/4 | 292/310) A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator (Theodor Misiakiewicz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Theodor Misiakiewicz, Basil Saeed. (2024)<br><strong>A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator</strong><br><button class=copy-to-clipboard title="A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08938v1.pdf filename=2403.08938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider learning an unknown target function $f_<em>$ using kernel ridge regression (KRR) given i.i.d. data $(u_i,y_i)$, $i\leq n$, where $u_i \in U$ is a covariate vector and $y_i = f_</em> (u_i) +\varepsilon_i \in \mathbb{R}$. A recent string of work has empirically shown that the test error of KRR can be well approximated by a closed-form estimate derived from an `equivalent&rsquo; sequence model that only depends on the spectrum of the kernel operator. However, a theoretical justification for this equivalence has so far relied either on restrictive assumptions &ndash; such as subgaussian independent eigenfunctions &ndash; , or asymptotic derivations for specific kernels in high dimensions. In this paper, we prove that this equivalence holds for a general class of problems satisfying some spectral and concentration properties on the kernel eigendecomposition. Specifically, we establish in this setting a non-asymptotic deterministic approximation for the test error of KRR &ndash; with explicit non-asymptotic bounds &ndash; that only depends on the eigenvalues and the target function alignment to the eigenvectors of the kernel. Our proofs rely on a careful derivation of deterministic equivalents for random matrix functionals in the dimension free regime pioneered by Cheng and Montanari (2022). We apply this setting to several classical examples and show an excellent agreement between theoretical predictions and numerical <b>simulations.</b> These results rely on having access to the eigendecomposition of the kernel operator. Alternatively, we prove that, under this same setting, the generalized cross-validation (GCV) estimator concentrates on the test error uniformly over a range of ridge regularization parameter that includes zero (the interpolating solution). As a consequence, the GCV estimator can be used to estimate from data the test error and optimal regularization parameter for KRR.</p></p class="citation"></blockquote><h3 id=34--293310-multifidelity-linear-regression-for-scientific-machine-learning-from-scarce-data-elizabeth-qian-et-al-2024>(3/4 | 293/310) Multifidelity linear regression for scientific machine learning from scarce data (Elizabeth Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elizabeth Qian, Anirban Chaudhuri, Dayoung Kang, Vignesh Sella. (2024)<br><strong>Multifidelity linear regression for scientific machine learning from scarce data</strong><br><button class=copy-to-clipboard title="Multifidelity linear regression for scientific machine learning from scarce data" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CE, cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08627v1.pdf filename=2403.08627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning (ML) methods, which fit to data the parameters of a given parameterized model class, have garnered significant interest as potential methods for learning surrogate models for complex engineering systems for which traditional <b>simulation</b> is expensive. However, in many scientific and engineering settings, generating high-fidelity data on which to train ML models is expensive, and the available budget for generating training data is limited. ML models trained on the resulting scarce high-fidelity data have high variance and are sensitive to vagaries of the training data set. We propose a new multifidelity training approach for scientific machine learning that exploits the scientific context where data of varying fidelities and costs are available; for example high-fidelity data may be generated by an expensive fully resolved physics <b>simulation</b> whereas lower-fidelity data may arise from a cheaper model based on simplifying assumptions. We use the multifidelity data to define new multifidelity Monte Carlo estimators for the unknown parameters of linear regression models, and provide theoretical analyses that guarantee the approach&rsquo;s accuracy and improved robustness to small training budgets. Numerical results verify the theoretical analysis and demonstrate that multifidelity learned models trained on scarce high-fidelity data and additional low-fidelity data achieve order-of-magnitude lower model variance than standard models trained on only high-fidelity data of comparable cost. This illustrates that in the scarce data regime, our multifidelity training strategy yields models with lower expected error than standard training approaches.</p></p class="citation"></blockquote><h3 id=44--294310-asymptotics-of-random-feature-regression-beyond-the-linear-scaling-regime-hong-hu-et-al-2024>(4/4 | 294/310) Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime (Hong Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hong Hu, Yue M. Lu, Theodor Misiakiewicz. (2024)<br><strong>Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime</strong><br><button class=copy-to-clipboard title="Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08160v1.pdf filename=2403.08160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters $p$? How should we choose $p$ relative to the <b>sample</b> <b>size</b> $n$ to achieve optimal test error? In this paper, we investigate the example of random feature ridge regression (RFRR). This model can be seen either as a finite-rank approximation to kernel ridge regression (KRR), or as a simplified model for neural networks trained in the so-called lazy regime. We consider covariates uniformly distributed on the $d$-dimensional sphere and compute sharp asymptotics for the RFRR test error in the high-dimensional polynomial scaling, where $p,n,d \to \infty$ while $p/ d^{\kappa_1}$ and $n / d^{\kappa_2}$ stay constant, for all $\kappa_1 , \kappa_2 \in \mathbb{R}_{>0}$. These asymptotics precisely characterize the impact of the number of random features and regularization parameter on the test performance. In particular, RFRR exhibits an intuitive trade-off between approximation and generalization power. For $n = o(p)$, the <b>sample</b> <b>size</b> $n$ is the bottleneck and RFRR achieves the same performance as KRR (which is equivalent to taking $p = \infty$). On the other hand, if $p = o(n)$, the number of random features $p$ is the limiting factor and RFRR test error matches the approximation error of the random feature model class (akin to taking $n = \infty$). Finally, a double descent appears at $n= p$, a phenomenon that was previously only characterized in the linear scaling $\kappa_1 = \kappa_2 = 1$.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--295310-multi-product-hamiltonian-simulation-with-explicit-commutator-scaling-junaid-aftab-et-al-2024>(1/2 | 295/310) Multi-product Hamiltonian simulation with explicit commutator scaling (Junaid Aftab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junaid Aftab, Dong An, Konstantina Trivisa. (2024)<br><strong>Multi-product Hamiltonian simulation with explicit commutator scaling</strong><br><button class=copy-to-clipboard title="Multi-product Hamiltonian simulation with explicit commutator scaling" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-NA, math-NA, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08922v1.pdf filename=2403.08922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The well-conditioned multi-product formula (MPF), proposed by [Low, Kliuchnikov, and Wiebe, 2019], is a simple high-order time-independent Hamiltonian <b>simulation</b> algorithm that implements a linear combination of standard product formulas of low order. While the MPF aims to simultaneously exploit commutator scaling among Hamiltonians and achieve near-optimal time and precision dependence, its lack of a rigorous error bound on the nested commutators renders its practical advantage ambiguous. In this work, we conduct a rigorous complexity analysis of the well-conditioned MPF, demonstrating explicit commutator scaling and near-optimal time and precision dependence at the same time. Using our improved complexity analysis, we present several applications of practical interest where the MPF based on a second-order product formula can achieve a polynomial speedup in both system size and evolution time, as well as an exponential speedup in precision, compared to second-order and even higher-order product formulas. Compared to post-Trotter methods, the MPF based on a second-order product formula can achieve polynomially better scaling in system size, with only poly-logarithmic overhead in evolution time and precision.</p></p class="citation"></blockquote><h3 id=22--296310-efficiently-verifiable-quantum-advantage-on-near-term-analog-quantum-simulators-zhenning-liu-et-al-2024>(2/2 | 296/310) Efficiently verifiable quantum advantage on near-term analog quantum simulators (Zhenning Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenning Liu, Dhruv Devulapalli, Dominik Hangleiter, Yi-Kai Liu, Alicia J. Kollár, Alexey V. Gorshkov, Andrew M. Childs. (2024)<br><strong>Efficiently verifiable quantum advantage on near-term analog quantum simulators</strong><br><button class=copy-to-clipboard title="Efficiently verifiable quantum advantage on near-term analog quantum simulators" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CC, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08195v1.pdf filename=2403.08195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing schemes for demonstrating quantum computational advantage are subject to various practical restrictions, including the hardness of verification and challenges in experimental implementation. Meanwhile, analog quantum simulators have been realized in many experiments to study novel physics. In this work, we propose a quantum advantage protocol based on single-step Feynman-Kitaev verification of an analog quantum <b>simulation,</b> in which the verifier need only run an $O(\lambda^2)$-time classical computation, and the prover need only prepare $O(1)$ samples of a history state and perform $O(\lambda^2)$ single-qubit measurements, for a security parameter $\lambda$. We also propose a near-term feasible strategy for honest provers and discuss potential experimental realizations.</p></p class="citation"></blockquote><h2 id=csai-4>cs.AI (4)</h2><h3 id=14--297310-meta-operators-for-enabling-parallel-planning-using-deep-reinforcement-learning-ángel-aso-mollar-et-al-2024>(1/4 | 297/310) Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning (Ángel Aso-Mollar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ángel Aso-Mollar, Eva Onaindia. (2024)<br><strong>Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08910v1.pdf filename=2403.08910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a growing interest in the application of <b>Reinforcement</b> <b>Learning</b> (RL) techniques to AI planning with the aim to come up with general policies. Typically, the mapping of the transition model of AI planning to the state transition system of a <b>Markov</b> <b>Decision</b> <b>Process</b> is established by assuming a one-to-one correspondence of the respective action spaces. In this paper, we introduce the concept of meta-operator as the result of simultaneously applying multiple planning operators, and we show that including meta-operators in the RL action space enables new planning perspectives to be addressed using RL, such as parallel planning. Our research aims to analyze the performance and complexity of including meta-operators in the RL process, concretely in domains where satisfactory outcomes have not been previously achieved using usual generalized planning models. The main objective of this article is thus to pave the way towards a redefinition of the RL action space in a manner that is more closely aligned with the planning perspective.</p></p class="citation"></blockquote><h3 id=24--298310-specification-overfitting-in-artificial-intelligence-benjamin-roth-et-al-2024>(2/4 | 298/310) Specification Overfitting in Artificial Intelligence (Benjamin Roth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Roth, Pedro Henrique Luz de Araujo, Yuxi Xia, Saskia Kaltenbrunner, Christoph Korab. (2024)<br><strong>Specification Overfitting in Artificial Intelligence</strong><br><button class=copy-to-clipboard title="Specification Overfitting in Artificial Intelligence" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Fairness, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08425v1.pdf filename=2403.08425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning (ML) and artificial intelligence (AI) approaches are often criticized for their inherent bias and for their lack of control, accountability, and transparency. Consequently, regulatory bodies struggle with containing this technology&rsquo;s potential negative side effects. High-level requirements such as <b>fairness</b> and robustness need to be formalized into concrete specification metrics, imperfect proxies that capture isolated aspects of the underlying requirements. Given possible trade-offs between different metrics and their vulnerability to over-optimization, integrating specification metrics in system development processes is not trivial. This paper defines specification overfitting, a scenario where systems focus excessively on specified metrics to the detriment of high-level requirements and task performance. We present an extensive literature survey to categorize how researchers propose, measure, and optimize specification metrics in several AI fields (e.g., natural language processing, computer vision, <b>reinforcement</b> <b>learning).</b> Using a keyword-based search on papers from major AI conferences and journals between 2018 and mid-2023, we identify and analyze 74 papers that propose or optimize specification metrics. We find that although most papers implicitly address specification overfitting (e.g., by reporting more than one specification metric), they rarely discuss which role specification metrics should play in system development or explicitly define the scope and assumptions behind metric formulations.</p></p class="citation"></blockquote><h3 id=34--299310-a-short-review-on-novel-approaches-for-maximum-clique-problem-from-classical-algorithms-to-graph-neural-networks-and-quantum-algorithms-raffaele-marino-et-al-2024>(3/4 | 299/310) A Short Review on Novel Approaches for Maximum Clique Problem: from Classical algorithms to Graph Neural Networks and Quantum algorithms (Raffaele Marino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raffaele Marino, Lorenzo Buffoni, Bogdan Zavalnij. (2024)<br><strong>A Short Review on Novel Approaches for Maximum Clique Problem: from Classical algorithms to Graph Neural Networks and Quantum algorithms</strong><br><button class=copy-to-clipboard title="A Short Review on Novel Approaches for Maximum Clique Problem: from Classical algorithms to Graph Neural Networks and Quantum algorithms" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cond-mat-dis-nn, cs-AI, cs-DS, cs-LG, cs.AI, math-OC, quant-ph<br>Keyword Score: 16<br>Keywords: Graph, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09742v1.pdf filename=2403.09742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This manuscript provides a comprehensive review of the Maximum Clique Problem, a computational problem that involves finding subsets of vertices in a <b>graph</b> <b>that</b> <b>are</b> all pairwise adjacent to each other. The manuscript covers in a simple way classical algorithms for solving the problem and includes a review of recent developments in <b>graph</b> <b>neural</b> <b>networks</b> and quantum algorithms. The review concludes with <b>benchmarks</b> for testing classical as well as new learning, and quantum algorithms.</p></p class="citation"></blockquote><h3 id=44--300310-optimizing-risk-averse-human-ai-hybrid-teams-andrew-fuchs-et-al-2024>(4/4 | 300/310) Optimizing Risk-averse Human-AI Hybrid Teams (Andrew Fuchs et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Fuchs, Andrea Passarella, Marco Conti. (2024)<br><strong>Optimizing Risk-averse Human-AI Hybrid Teams</strong><br><button class=copy-to-clipboard title="Optimizing Risk-averse Human-AI Hybrid Teams" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08386v1.pdf filename=2403.08386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team. The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread. However, their behavior is not error-free, making hybrid teams a very suitable solution. As such, we consider methods for improving performance for these teams of humans and AI systems. For hybrid teams, we will refer to both the humans and AI systems as agents. To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard <b>Reinforcement</b> <b>Learning</b> scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents. We further guide the manager&rsquo;s learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior. We demonstrate the optimality of our manager&rsquo;s performance in several grid environments which include failure states which terminate an episode and should be avoided. We perform our experiments with teams of agents with varying degrees of acceptable risk, in the form of proximity to a failure state, and measure the manager&rsquo;s ability to make effective delegation decisions with respect to its own risk-based constraints, then compare these to the optimal decisions. Our results show our manager can successfully learn desirable delegations which result in team paths near/exactly optimal with respect to path length and number of delegations.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--301310-gpt-ontology-and-caabac-a-tripartite-personalized-access-control-model-anchored-by-compliance-context-and-attribute-raza-nowrozy-et-al-2024>(1/1 | 301/310) GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control Model Anchored by Compliance, Context and Attribute (Raza Nowrozy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raza Nowrozy, Khandakar Ahmed, Hua Wang. (2024)<br><strong>GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control Model Anchored by Compliance, Context and Attribute</strong><br><button class=copy-to-clipboard title="GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control Model Anchored by Compliance, Context and Attribute" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CR, cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: GPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08264v1.pdf filename=2403.08264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As digital healthcare evolves, the security of electronic health records (EHR) becomes increasingly crucial. This study presents the <b>GPT-Onto-CAABAC</b> framework, integrating Generative Pretrained <b>Transformer</b> <b>(GPT),</b> medical-legal ontologies and Context-Aware Attribute-Based Access Control (CAABAC) to enhance EHR access security. Unlike traditional models, <b>GPT-Onto-CAABAC</b> dynamically interprets policies and adapts to changing healthcare and legal environments, offering customized access control solutions. Through empirical evaluation, this framework is shown to be effective in improving EHR security by accurately aligning access decisions with complex regulatory and situational requirements. The findings suggest its broader applicability in sectors where access control must meet stringent compliance and adaptability standards.</p></p class="citation"></blockquote><h2 id=csdm-3>cs.DM (3)</h2><h3 id=13--302310-improved-dynamics-for-the-maximum-common-subgraph-problem-davide-guidobene-et-al-2024>(1/3 | 302/310) Improved Dynamics for the Maximum Common Subgraph Problem (Davide Guidobene et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Guidobene, Guido Cera. (2024)<br><strong>Improved Dynamics for the Maximum Common Subgraph Problem</strong><br><button class=copy-to-clipboard title="Improved Dynamics for the Maximum Common Subgraph Problem" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: G-2-2, cs-DM, cs.DM<br>Keyword Score: 16<br>Keywords: Graph, Clustering, Hierarchical Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08703v1.pdf filename=2403.08703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Maximum Common Subgraph (MCS) problem plays a crucial role across various domains, bridging theoretical exploration and practical applications in fields like bioinformatics and social network analysis. Despite its wide applicability, MCS is notoriously challenging and is classified as an NP-Complete (NPC) problem. This study introduces new heuristics aimed at mitigating these challenges through the reformulation of the MCS problem as the Maximum Clique and its complement, the Maximum Independent Set. Our first heuristic leverages the Motzkin-Straus theorem to reformulate the Maximum Clique Problem as a constrained optimization problem, continuing the work of Pelillo in Replicator Equations, Maximal Cliques, and <b>Graph</b> Isomorphism (1999) with replicator dynamics and introducing annealed imitation heuristics as in Dominant Sets and <b>Hierarchical</b> <b>Clustering</b> (Pavan and Pelillo, 2003) to improve chances of convergence to better local optima. The second technique applies heuristics drawn upon strategies for the Maximum Independent Set problem to efficiently reduce <b>graph</b> sizes as used by Akiwa and Iwata in 2014. This enables faster computation and, in many instances, yields near-optimal solutions. Furthermore we look at the implementation of both techniques in a single algorithm and find that it is a promising approach. Our techniques were tested on randomly generated Erd\H{o}s-R'enyi <b>graph</b> pairs. Results indicate the potential for application and substantial impact on future research directions.</p></p class="citation"></blockquote><h3 id=23--303310-on-sampling-diluted-spin-glasses-using-glauber-dynamics-charilaos-efthymiou-et-al-2024>(2/3 | 303/310) On sampling diluted Spin Glasses using Glauber dynamics (Charilaos Efthymiou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charilaos Efthymiou, Kostas Zampetakis. (2024)<br><strong>On sampling diluted Spin Glasses using Glauber dynamics</strong><br><button class=copy-to-clipboard title="On sampling diluted Spin Glasses using Glauber dynamics" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM, math-PR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08921v1.pdf filename=2403.08921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spin-glasses are Gibbs distributions that have been studied in CS for many decades. Recently, they have gained renewed attention as they emerge naturally in learning, inference, optimisation etc. We consider the Edwards-Anderson (EA) spin-glass distribution at inverse temperature $\beta$ when the underlying <b>graph</b> is an instance of $G(n,d/n)$. This is the random <b>graph</b> on $n$ vertices where each edge appears independently with probability $d/n$ and $d=\Theta(1)$. We study the problem of approximate sampling from this distribution using Glauber dynamics. For a range of $\beta$ that depends on $d$ and for typical instances of the EA model on $G(n,d/n)$, we show that the corresponding Glauber dynamics exhibits mixing time $O(n^{2+\frac{3}{\log^2 d}})$. The range of $\beta$ for which we obtain our rapid-mixing results correspond to the expected influence being $&lt;1/d$; we conjecture that this is the best possible. Unlike the mean-field spin-glasses, where the problem has been studied before, the diluted case has not. We utilise the well-known path-coupling technique. In the standard Glauber dynamics on $G(n,d/n)$, one has to deal with the so-called effect of high degree vertices. Here, rather than considering degrees, it is more natural to use a different measure on the vertices called aggregate influence. We build on the block-construction approach proposed by [Dyer et al. 2006] to circumvent the problem of high-degree vertices. Specifically, we first establish rapid mixing for an appropriately defined block-dynamics. We design this dynamics such that vertices of large aggregate influence are placed deep inside their blocks. Then, we obtain rapid mixing for the Glauber dynamics utilising a comparison argument.</p></p class="citation"></blockquote><h3 id=33--304310-ensuring-connectedness-for-the-maximum-quasi-clique-and-densest-k-subgraph-problems-daniela-scherer-dos-santos-et-al-2024>(3/3 | 304/310) Ensuring connectedness for the Maximum Quasi-clique and Densest $k$-subgraph problems (Daniela Scherer dos Santos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniela Scherer dos Santos, Kathrin Klamroth, Pedro Martins, Luís Paquete. (2024)<br><strong>Ensuring connectedness for the Maximum Quasi-clique and Densest $k$-subgraph problems</strong><br><button class=copy-to-clipboard title="Ensuring connectedness for the Maximum Quasi-clique and Densest $k$-subgraph problems" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08534v1.pdf filename=2403.08534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given an undirected <b>graph</b> $G$, a quasi-clique is a subgraph of $G$ whose density is at least $\gamma$ $(0 &lt; \gamma \leq 1)$. Two optimization problems can be defined for quasi-cliques: the Maximum Quasi-Clique (MQC) Problem, which finds a quasi-clique with maximum vertex cardinality, and the Densest $k$-Subgraph (DKS) Problem, which finds the densest subgraph given a fixed cardinality constraint. Most existing approaches to solve both problems often disregard the requirement of connectedness, which may lead to solutions containing isolated components that are meaningless for many real-life applications. To address this issue, we propose two flow-based connectedness constraints to be integrated into known Mixed-Integer Linear Programming (MILP) formulations for either MQC or DKS problems. We compare the performance of MILP formulations enhanced with our connectedness constraints in terms of both running time and number of solved instances against existing approaches that ensure quasi-clique connectedness. Experimental results demonstrate that our constraints are quite competitive, making them valuable for practical applications requiring connectedness.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--305310-leveraging-non-decimated-wavelet-packet-features-and-transformer-models-for-time-series-forecasting-guy-p-nason-et-al-2024>(1/1 | 305/310) Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting (Guy P Nason et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guy P Nason, James L. Wei. (2024)<br><strong>Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting</strong><br><button class=copy-to-clipboard title="Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: 62M10, 62M45, cs-LG, stat-ME, stat.ME<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08630v1.pdf filename=2403.08630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions. Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase. Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors. The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs. Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-temporal models, and both statistical and deep learning-based methods. The latter include state-of-the-art <b>transformer-based</b> neural network architectures. Our experiments suggest significant benefit in replacing higher-order lagged features with wavelet features across all examined non-temporal methods for one-step-forward forecasting, and modest benefit when used as inputs for temporal deep learning-based models for long-horizon forecasting.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--306310-measures-of-relevance-to-the-success-of-streaming-platforms-juan-carlos-gonçalves-dosantos-et-al-2024>(1/1 | 306/310) Measures of relevance to the success of streaming platforms (Juan Carlos Gonçalves-Dosantos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Carlos Gonçalves-Dosantos, Ricardo Martínez, Joaquín Sánchez-Soriano. (2024)<br><strong>Measures of relevance to the success of streaming platforms</strong><br><button class=copy-to-clipboard title="Measures of relevance to the success of streaming platforms" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: cs-GT, econ-TH, econ.TH<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08421v1.pdf filename=2403.08421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital streaming platforms, including Twitch, Spotify, Netflix, Disney, and Kindle, have emerged as one of the main sources of entertainment with significant growth potential. Many of these platforms distribute royalties among streamers, artists, producers, or writers based on their impact. In this paper, we measure the relevance of each of these contributors to the overall success of the platform, which is information that can play a key role in revenue allocation. We perform an axiomatic analysis to provide normative foundations for three relevance metrics: the uniform, the proportional, and the subscriber-proportional indicators. The last two indicators implement the so-called pro-rata and user-centric models, which are extensively applied to distribute revenues in the music streaming market. The axioms we propose formalize different principles of <b>fairness,</b> stability, and non-manipulability, and are tailor-made for the streaming context. We complete our analysis with a case study that measures the influence of the 19 most-followed streamers worldwide on the Twitch platform.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--307310-the-q-ary-gilbert-varshamov-bound-can-be-improved-for-all-but-finitely-many-positive-integers-q-xue-bin-liang-2024>(1/1 | 307/310) The q-ary Gilbert-Varshamov bound can be improved for all but finitely many positive integers q (Xue-Bin Liang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xue-Bin Liang. (2024)<br><strong>The q-ary Gilbert-Varshamov bound can be improved for all but finitely many positive integers q</strong><br><button class=copy-to-clipboard title="The q-ary Gilbert-Varshamov bound can be improved for all but finitely many positive integers q" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, cs-IT, math-CO, math-IT, math-NT, math.CO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08727v1.pdf filename=2403.08727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For any positive integer $q\geq 2$ and any real number $\delta\in(0,1)$, let $\alpha_q(n,\delta n)$ denote the maximum size of a subset of $\mathbb{Z}<em>q^n$ with minimum Hamming distance at least $\delta n$, where $\mathbb{Z}<em>q={0,1,\dotsc,q-1}$ and $n\in\mathbb{N}$. The asymptotic rate function is defined by $ R_q(\delta) = \limsup</em>{n\rightarrow\infty}\frac{1}{n}\log_q\alpha_q(n,\delta n). $ The famous $q$-ary asymptotic Gilbert-Varshamov bound, obtained in the 1950s, states that [ R_q(\delta) \geq 1 - \delta\log_q(q-1)-\delta\log_q\frac{1}{\delta}-(1-\delta)\log_q\frac{1}{1-\delta} \stackrel{\mathrm{def}}{=}R</em>\mathrm{GV}(\delta,q) ] for all positive integers $q\geq 2$ and $0&lt;\delta&lt;1-q^{-1}$. In the case that $q$ is an even power of a prime with $q\geq 49$, the $q$-ary Gilbert-Varshamov bound was firstly improved by using algebraic <b>geometry</b> codes in the works of Tsfasman, Vladut, and Zink and of Ihara in the 1980s. The further investigation in algebraic <b>geometry</b> codes has shown that the $q$-ary Gilbert-Varshamov bound can also be improved in the case that $q$ is an odd power of a prime but not a prime with $q > 125$. However, it remains a long-standing open problem whether the $q$-ary Gilbert-Varshamov bound would be tight for those infinitely many integers $q$ which is a prime, except for Fermat primes not less than 257, and which is a generic positive integer not being a prime power. In this paper, we prove that the $q$-ary Gilbert-Varshamov bound can be improved for all but finitely many positive integers $q\geq 2$. It is shown that $ R_q(1/2) > R_\mathrm{GV}(1/2,q) $ for all integers $q > \exp(29)$. Furthermore, we show that the growth of the rate function $R_q(\delta)$ for $\delta\in(0,1)$ fixed and $q$ growing large has a nontrivial lower bound. These new lower bounds are achieved by using codes from <b>geometry</b> of numbers introduced by Lenstra in the 1980s.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--308310-approximating-small-sparse-cuts-aditya-anand-et-al-2024>(1/2 | 308/310) Approximating Small Sparse Cuts (Aditya Anand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Anand, Euiwoong Lee, Jason Li, Thatchaphol Saranurak. (2024)<br><strong>Approximating Small Sparse Cuts</strong><br><button class=copy-to-clipboard title="Approximating Small Sparse Cuts" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08983v1.pdf filename=2403.08983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study polynomial-time approximation algorithms for (edge/vertex) Sparsest Cut and Small Set Expansion in terms of $k$, the number of edges or vertices cut in the optimal solution. Our main results are $\mathcal{O}(\text{polylog}, k)$-approximation algorithms for various versions in this setting. Our techniques involve an extension of the notion of sample sets (Feige and Mahdian STOC'06), originally developed for small balanced cuts, to sparse cuts in general. We then show how to combine this notion of sample sets with two algorithms, one based on an existing framework of LP rounding and another new algorithm based on the cut-matching game, to get such approximation algorithms. Our cut-matching game algorithm can be viewed as a local version of the cut-matching game by Khandekar, Khot, Orecchia and Vishnoi and certifies an expansion of every vertex set of size $s$ in $\mathcal{O}(\log s)$ rounds. These techniques may be of independent interest. As corollaries of our results, we also obtain an $\mathcal{O}(\log opt)$-approximation for min-max <b>graph</b> partitioning, where $opt$ is the min-max value of the optimal cut, and improve the bound on the size of multicut mimicking networks computable in polynomial time.</p></p class="citation"></blockquote><h3 id=22--309310-worst-case-to-expander-case-reductions-derandomized-and-generalized-amir-abboud-et-al-2024>(2/2 | 309/310) Worst-Case to Expander-Case Reductions: Derandomized and Generalized (Amir Abboud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Abboud, Nathan Wallheimer. (2024)<br><strong>Worst-Case to Expander-Case Reductions: Derandomized and Generalized</strong><br><button class=copy-to-clipboard title="Worst-Case to Expander-Case Reductions: Derandomized and Generalized" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08394v1.pdf filename=2403.08394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A recent paper by Abboud and Wallheimer [ITCS 2023] presents self-reductions for various fundamental <b>graph</b> problems, that transform worst-case instances to expanders, thus proving that the complexity remains unchanged if the input is assumed to be an expander. An interesting corollary of their self-reductions is that, if some problem admit such reduction, then the popular algorithmic paradigm based on expander-decompositions is useless against it. In this paper, we improve their core gadget, which augments a <b>graph</b> to make it an expander while retaining its important structure. Our new core construction has the benefit of being simple to analyze and generalize, while obtaining the following results: 1. A derandomization of the self-reductions, showing that the equivalence between worst-case and expander-case holds even for deterministic algorithms, and ruling out the use of expander-decompositions as a derandomization tool. 2. An extension of the results to other models of computation, such as the Fully Dynamic model and the Congested Clique model. In the former, we either improve or provide an alternative approach to some recent hardness results for dynamic expander <b>graphs,</b> by Henzinger, Paz, and Sricharan [ESA 2022]. In addition, we continue this line of research by designing new self-reductions for more problems, such as Max-Cut and dynamic Densest Subgraph, and demonstrating that the core gadget can be utilized to lift lower bounds based on the OMv Conjecture to expanders.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--310310-timed-strategies-for-real-time-rewrite-theories-carlos-olarte-et-al-2024>(1/1 | 310/310) Timed Strategies for Real-Time Rewrite Theories (Carlos Olarte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Olarte, Peter Csaba Ölveczky. (2024)<br><strong>Timed Strategies for Real-Time Rewrite Theories</strong><br><button class=copy-to-clipboard title="Timed Strategies for Real-Time Rewrite Theories" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08920v1.pdf filename=2403.08920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we propose a language for conveniently defining a wide range of execution strategies for real-time rewrite theories, and provide Maude-strategy-implemented versions of most Real-Time Maude analysis methods, albeit with user-defined discrete and timed strategies. We also identify a new time sampling strategy that should provide both efficient and exhaustive analysis for many distributed real-time systems. We exemplify the use of our language and its analyses on a simple round trip time protocol, and compare the performance of standard Maude search with our strategy-implemented reachability analyses on the CASH scheduling algorithm <b>benchmark.</b></p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.14</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.16</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-51>cs.CL (51)</a><ul><li><a href=#151--1310-zero-shot-and-few-shot-generation-strategies-for-artificial-clinical-records-erlend-frayling-et-al-2024>(1/51 | 1/310) Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records (Erlend Frayling et al., 2024)</a></li><li><a href=#251--2310-rich-semantic-knowledge-enhanced-large-language-models-for-few-shot-chinese-spell-checking-ming-dong-et-al-2024>(2/51 | 2/310) Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking (Ming Dong et al., 2024)</a></li><li><a href=#351--3310-large-language-models-are-contrastive-reasoners-liang-yao-2024>(3/51 | 3/310) Large Language Models are Contrastive Reasoners (Liang Yao, 2024)</a></li><li><a href=#451--4310-pet-sql-a-prompt-enhanced-two-stage-text-to-sql-framework-with-cross-consistency-zhishuai-li-et-al-2024>(4/51 | 4/310) PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency (Zhishuai Li et al., 2024)</a></li><li><a href=#551--5310-lmstyle-benchmark-evaluating-text-style-transfer-for-chatbots-jianlin-chen-2024>(5/51 | 5/310) LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots (Jianlin Chen, 2024)</a></li><li><a href=#651--6310-from-human-experts-to-machines-an-llm-supported-approach-to-ontology-and-knowledge-graph-construction-vamsi-krishna-kommineni-et-al-2024>(6/51 | 6/310) From human experts to machines: An LLM supported approach to ontology and knowledge graph construction (Vamsi Krishna Kommineni et al., 2024)</a></li><li><a href=#751--7310-can-large-language-models-identify-authorship-baixiang-huang-et-al-2024>(7/51 | 7/310) Can Large Language Models Identify Authorship? (Baixiang Huang et al., 2024)</a></li><li><a href=#851--8310-sotopia-π-interactive-learning-of-socially-intelligent-language-agents-ruiyi-wang-et-al-2024>(8/51 | 8/310) SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language Agents (Ruiyi Wang et al., 2024)</a></li><li><a href=#951--9310-devbench-a-comprehensive-benchmark-for-software-development-bowen-li-et-al-2024>(9/51 | 9/310) DevBench: A Comprehensive Benchmark for Software Development (Bowen Li et al., 2024)</a></li><li><a href=#1051--10310-teams-rl-teaching-llms-to-teach-themselves-better-instructions-via-reinforcement-learning-shangding-gu-et-al-2024>(10/51 | 10/310) TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning (Shangding Gu et al., 2024)</a></li><li><a href=#1151--11310-generative-pretrained-structured-transformers-unsupervised-syntactic-language-models-at-scale-xiang-hu-et-al-2024>(11/51 | 11/310) Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale (Xiang Hu et al., 2024)</a></li><li><a href=#1251--12310-steering-llms-towards-unbiased-responses-a-causality-guided-debiasing-framework-jingling-li-et-al-2024>(12/51 | 12/310) Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework (Jingling Li et al., 2024)</a></li><li><a href=#1351--13310-the-human-factor-in-detecting-errors-of-large-language-models-a-systematic-literature-review-and-future-research-directions-christian-a-schiller-2024>(13/51 | 13/310) The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions (Christian A. Schiller, 2024)</a></li><li><a href=#1451--14310-detecting-hallucination-and-coverage-errors-in-retrieval-augmented-generation-for-controversial-topics-tyler-a-chang-et-al-2024>(14/51 | 14/310) Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics (Tyler A. Chang et al., 2024)</a></li><li><a href=#1551--15310-mastering-text-code-and-math-simultaneously-via-fusing-highly-specialized-language-models-ning-ding-et-al-2024>(15/51 | 15/310) Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models (Ning Ding et al., 2024)</a></li><li><a href=#1651--16310-recipe4u-student-chatgpt-interaction-dataset-in-efl-writing-education-jieun-han-et-al-2024>(16/51 | 16/310) RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education (Jieun Han et al., 2024)</a></li><li><a href=#1751--17310-boosting-disfluency-detection-with-large-language-model-as-disfluency-generator-zhenrong-cheng-et-al-2024>(17/51 | 17/310) Boosting Disfluency Detection with Large Language Model as Disfluency Generator (Zhenrong Cheng et al., 2024)</a></li><li><a href=#1851--18310-call-me-when-necessary-llms-can-efficiently-and-faithfully-reason-over-structured-environments-sitao-cheng-et-al-2024>(18/51 | 18/310) Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments (Sitao Cheng et al., 2024)</a></li><li><a href=#1951--19310-data-oriented-dynamic-fine-tuning-parameter-selection-strategy-for-fish-mask-based-efficient-fine-tuning-ming-dong-et-al-2024>(19/51 | 19/310) Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning (Ming Dong et al., 2024)</a></li><li><a href=#2051--20310-gemma-open-models-based-on-gemini-research-and-technology-gemma-team-et-al-2024>(20/51 | 20/310) Gemma: Open Models Based on Gemini Research and Technology (Gemma Team et al., 2024)</a></li><li><a href=#2151--21310-speechcolab-leaderboard-an-open-source-platform-for-automatic-speech-recognition-evaluation-jiayu-du-et-al-2024>(21/51 | 21/310) SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech Recognition Evaluation (Jiayu Du et al., 2024)</a></li><li><a href=#2251--22310-moleculeqa-a-dataset-to-evaluate-factual-accuracy-in-molecular-comprehension-xingyu-lu-et-al-2024>(22/51 | 22/310) MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension (Xingyu Lu et al., 2024)</a></li><li><a href=#2351--23310-evaluating-large-language-models-as-generative-user-simulators-for-conversational-recommendation-se-eun-yoon-et-al-2024>(23/51 | 23/310) Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation (Se-eun Yoon et al., 2024)</a></li><li><a href=#2451--24310-medinsight-a-multi-source-context-augmentation-framework-for-generating-patient-centric-medical-responses-using-large-language-models-subash-neupane-et-al-2024>(24/51 | 24/310) MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models (Subash Neupane et al., 2024)</a></li><li><a href=#2551--25310-automatic-interactive-evaluation-for-large-language-models-with-state-aware-patient-simulator-yusheng-liao-et-al-2024>(25/51 | 25/310) Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator (Yusheng Liao et al., 2024)</a></li><li><a href=#2651--26310-autoregressive-score-generation-for-multi-trait-essay-scoring-heejin-do-et-al-2024>(26/51 | 26/310) Autoregressive Score Generation for Multi-trait Essay Scoring (Heejin Do et al., 2024)</a></li><li><a href=#2751--27310-embedded-translations-for-low-resource-automated-glossing-changbing-yang-et-al-2024>(27/51 | 27/310) Embedded Translations for Low-resource Automated Glossing (Changbing Yang et al., 2024)</a></li><li><a href=#2851--28310-automatic-speech-recognition-asr-for-the-diagnosis-of-pronunciation-of-speech-sound-disorders-in-korean-children-taekyung-ahn-et-al-2024>(28/51 | 28/310) Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of Speech Sound Disorders in Korean children (Taekyung Ahn et al., 2024)</a></li><li><a href=#2951--29310-strengthening-multimodal-large-language-model-with-bootstrapped-preference-optimization-renjie-pi-et-al-2024>(29/51 | 29/310) Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization (Renjie Pi et al., 2024)</a></li><li><a href=#3051--30310-evaluating-the-application-of-large-language-models-to-generate-feedback-in-programming-education-sven-jacobs-et-al-2024>(30/51 | 30/310) Evaluating the Application of Large Language Models to Generate Feedback in Programming Education (Sven Jacobs et al., 2024)</a></li><li><a href=#3151--31310-improving-acoustic-word-embeddings-through-correspondence-training-of-self-supervised-speech-representations-amit-meghanani-et-al-2024>(31/51 | 31/310) Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations (Amit Meghanani et al., 2024)</a></li><li><a href=#3251--32310-token-alignment-via-character-matching-for-subword-completion-ben-athiwaratkun-et-al-2024>(32/51 | 32/310) Token Alignment via Character Matching for Subword Completion (Ben Athiwaratkun et al., 2024)</a></li><li><a href=#3351--33310-non-discrimination-criteria-for-generative-language-models-sara-sterlie-et-al-2024>(33/51 | 33/310) Non-discrimination Criteria for Generative Language Models (Sara Sterlie et al., 2024)</a></li><li><a href=#3451--34310-do-large-language-models-solve-arc-visual-analogies-like-people-do-gustaw-opiełka-et-al-2024>(34/51 | 34/310) Do Large Language Models Solve ARC Visual Analogies Like People Do? (Gustaw Opiełka et al., 2024)</a></li><li><a href=#3551--35310-smart-submodular-data-mixture-strategy-for-instruction-tuning-h-s-v-n-s-kowndinya-renduchintala-et-al-2024>(35/51 | 35/310) SMART: Submodular Data Mixture Strategy for Instruction Tuning (H S V N S Kowndinya Renduchintala et al., 2024)</a></li><li><a href=#3651--36310-overleafcopilot-empowering-academic-writing-in-overleaf-with-large-language-models-haomin-wen-et-al-2024>(36/51 | 36/310) OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models (Haomin Wen et al., 2024)</a></li><li><a href=#3751--37310-skipformer-a-skip-and-recover-strategy-for-efficient-speech-recognition-wenjing-zhu-et-al-2024>(37/51 | 37/310) Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition (Wenjing Zhu et al., 2024)</a></li><li><a href=#3851--38310-research-on-the-application-of-deep-learning-based-bert-model-in-sentiment-analysis-yichao-wu-et-al-2024>(38/51 | 38/310) Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis (Yichao Wu et al., 2024)</a></li><li><a href=#3951--39310-autoguide-automated-generation-and-selection-of-state-aware-guidelines-for-large-language-model-agents-yao-fu-et-al-2024>(39/51 | 39/310) AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents (Yao Fu et al., 2024)</a></li><li><a href=#4051--40310-the-garden-of-forking-paths-observing-dynamic-parameters-distribution-in-large-language-models-carlo-nicolini-et-al-2024>(40/51 | 40/310) The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models (Carlo Nicolini et al., 2024)</a></li><li><a href=#4151--41310-do-language-models-care-about-text-quality-evaluating-web-crawled-corpora-across-11-languages-rik-van-noord-et-al-2024>(41/51 | 41/310) Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora Across 11 Languages (Rik van Noord et al., 2024)</a></li><li><a href=#4251--42310-language-models-scale-reliably-with-over-training-and-on-downstream-tasks-samir-yitzhak-gadre-et-al-2024>(42/51 | 42/310) Language models scale reliably with over-training and on downstream tasks (Samir Yitzhak Gadre et al., 2024)</a></li><li><a href=#4351--43310-authorship-verification-based-on-the-likelihood-ratio-of-grammar-models-andrea-nini-et-al-2024>(43/51 | 43/310) Authorship Verification based on the Likelihood Ratio of Grammar Models (Andrea Nini et al., 2024)</a></li><li><a href=#4451--44310-learning-to-describe-for-predicting-zero-shot-drug-drug-interactions-fangqi-zhu-et-al-2024>(44/51 | 44/310) Learning to Describe for Predicting Zero-shot Drug-Drug Interactions (Fangqi Zhu et al., 2024)</a></li><li><a href=#4551--45310-knowledge-conflicts-for-llms-a-survey-rongwu-xu-et-al-2024>(45/51 | 45/310) Knowledge Conflicts for LLMs: A Survey (Rongwu Xu et al., 2024)</a></li><li><a href=#4651--46310-is-context-helpful-for-chat-translation-evaluation-sweta-agrawal-et-al-2024>(46/51 | 46/310) Is Context Helpful for Chat Translation Evaluation? (Sweta Agrawal et al., 2024)</a></li><li><a href=#4751--47310-streamingdialogue-prolonged-dialogue-learning-via-long-context-compression-with-minimal-losses-jia-nan-li-et-al-2024>(47/51 | 47/310) StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses (Jia-Nan Li et al., 2024)</a></li><li><a href=#4851--48310-from-um-to-yeah-producing-predicting-and-regulating-information-flow-in-human-conversation-claire-augusta-bergey-et-al-2024>(48/51 | 48/310) From &lsquo;um&rsquo; to &lsquo;yeah&rsquo;: Producing, predicting, and regulating information flow in human conversation (Claire Augusta Bergey et al., 2024)</a></li><li><a href=#4951--49310-towards-personalized-evaluation-of-large-language-models-with-an-anonymous-crowd-sourcing-platform-mingyue-cheng-et-al-2024>(49/51 | 49/310) Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform (Mingyue Cheng et al., 2024)</a></li><li><a href=#5051--50310-validating-and-exploring-large-geographic-corpora-jonathan-dunn-2024>(50/51 | 50/310) Validating and Exploring Large Geographic Corpora (Jonathan Dunn, 2024)</a></li><li><a href=#5151--51310-rethinking-loss-functions-for-fact-verification-yuta-mukobara-et-al-2024>(51/51 | 51/310) Rethinking Loss Functions for Fact Verification (Yuta Mukobara et al., 2024)</a></li></ul></li><li><a href=#cscv-85>cs.CV (85)</a><ul><li><a href=#185--52310-tina-think-interaction-and-action-framework-for-zero-shot-vision-language-navigation-dingbang-li-et-al-2024>(1/85 | 52/310) TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation (Dingbang Li et al., 2024)</a></li><li><a href=#285--53310-unleashing-the-power-of-meta-tuning-for-few-shot-generalization-through-sparse-interpolated-experts-shengzhuang-chen-et-al-2024>(2/85 | 53/310) Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts (Shengzhuang Chen et al., 2024)</a></li><li><a href=#385--54310-efficient-prompt-tuning-of-large-vision-language-model-for-fine-grained-ship-classification-long-lan-et-al-2024>(3/85 | 54/310) Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification (Long Lan et al., 2024)</a></li><li><a href=#485--55310-coronetgan-controlled-pruning-of-gans-via-hypernetworks-aman-kumar-et-al-2024>(4/85 | 55/310) CoroNetGAN: Controlled Pruning of GANs via Hypernetworks (Aman Kumar et al., 2024)</a></li><li><a href=#585--56310-lafs-landmark-based-facial-self-supervised-learning-for-face-recognition-zhonglin-sun-et-al-2024>(5/85 | 56/310) LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition (Zhonglin Sun et al., 2024)</a></li><li><a href=#685--57310-language-driven-visual-consensus-for-zero-shot-semantic-segmentation-zicheng-zhang-et-al-2024>(6/85 | 57/310) Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation (Zicheng Zhang et al., 2024)</a></li><li><a href=#785--58310-activating-wider-areas-in-image-super-resolution-cheng-cheng-et-al-2024>(7/85 | 58/310) Activating Wider Areas in Image Super-Resolution (Cheng Cheng et al., 2024)</a></li><li><a href=#885--59310-attack-deterministic-conditional-image-generative-models-for-diverse-and-controllable-generation-tianyi-chu-et-al-2024>(8/85 | 59/310) Attack Deterministic Conditional Image Generative Models for Diverse and Controllable Generation (Tianyi Chu et al., 2024)</a></li><li><a href=#985--60310-coin-a-benchmark-of-continual-instruction-tuning-for-multimodel-large-language-model-cheng-chen-et-al-2024>(9/85 | 60/310) CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model (Cheng Chen et al., 2024)</a></li><li><a href=#1085--61310-dialoggen-multi-modal-interactive-dialogue-system-for-multi-turn-text-to-image-generation-minbin-huang-et-al-2024>(10/85 | 61/310) DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation (Minbin Huang et al., 2024)</a></li><li><a href=#1185--62310-a-multimodal-fusion-network-for-student-emotion-recognition-based-on-transformer-and-tensor-product-ao-xiang-et-al-2024>(11/85 | 62/310) A Multimodal Fusion Network For Student Emotion Recognition Based on Transformer and Tensor Product (Ao Xiang et al., 2024)</a></li><li><a href=#1285--63310-meter-a-mobile-vision-transformer-architecture-for-monocular-depth-estimation-l-papa-et-al-2024>(12/85 | 63/310) METER: a mobile vision transformer architecture for monocular depth estimation (L. Papa et al., 2024)</a></li><li><a href=#1385--64310-fogguard-guarding-yolo-against-fog-using-perceptual-loss-soheil-gharatappeh-et-al-2024>(13/85 | 64/310) FogGuard: guarding YOLO against fog using perceptual loss (Soheil Gharatappeh et al., 2024)</a></li><li><a href=#1485--65310-holmes-holonym-meronym-based-semantic-inspection-for-convolutional-image-classifiers-francesco-dibitonto-et-al-2024>(14/85 | 65/310) HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers (Francesco Dibitonto et al., 2024)</a></li><li><a href=#1585--66310-multiscale-low-frequency-memory-network-for-improved-feature-extraction-in-convolutional-neural-networks-fuzhi-wu-et-al-2024>(15/85 | 66/310) Multiscale Low-Frequency Memory Network for Improved Feature Extraction in Convolutional Neural Networks (Fuzhi Wu et al., 2024)</a></li><li><a href=#1685--67310-nerf-supervised-feature-point-detection-and-description-ali-youssef-et-al-2024>(16/85 | 67/310) NeRF-Supervised Feature Point Detection and Description (Ali Youssef et al., 2024)</a></li><li><a href=#1785--68310-mim4d-masked-modeling-with-multi-view-video-for-autonomous-driving-representation-learning-jialv-zou-et-al-2024>(17/85 | 68/310) MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving Representation Learning (Jialv Zou et al., 2024)</a></li><li><a href=#1885--69310-masked-generative-story-transformer-with-character-guidance-and-caption-augmentation-christos-papadimitriou-et-al-2024>(18/85 | 69/310) Masked Generative Story Transformer with Character Guidance and Caption Augmentation (Christos Papadimitriou et al., 2024)</a></li><li><a href=#1985--70310-dam-dynamic-adapter-merging-for-continual-video-qa-learning-feng-cheng-et-al-2024>(19/85 | 70/310) DAM: Dynamic Adapter Merging for Continual Video QA Learning (Feng Cheng et al., 2024)</a></li><li><a href=#2085--71310-gaussctrl-multi-view-consistent-text-driven-3d-gaussian-splatting-editing-jing-wu-et-al-2024>(20/85 | 71/310) GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing (Jing Wu et al., 2024)</a></li><li><a href=#2185--72310-deep-learning-for-in-orbit-cloud-segmentation-and-classification-in-hyperspectral-satellite-data-daniel-kovac-et-al-2024>(21/85 | 72/310) Deep Learning for In-Orbit Cloud Segmentation and Classification in Hyperspectral Satellite Data (Daniel Kovac et al., 2024)</a></li><li><a href=#2285--73310-onevos-unifying-video-object-segmentation-with-all-in-one-transformer-framework-wanyun-li-et-al-2024>(22/85 | 73/310) OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework (Wanyun Li et al., 2024)</a></li><li><a href=#2385--74310-pig-aggression-classification-using-cnn-transformers-and-recurrent-networks-junior-silva-souza-et-al-2024>(23/85 | 74/310) Pig aggression classification using CNN, Transformers and Recurrent Networks (Junior Silva Souza et al., 2024)</a></li><li><a href=#2485--75310-optimized-detection-and-classification-on-gtrsb-advancing-traffic-sign-recognition-with-convolutional-neural-networks-dhruv-toshniwal-et-al-2024>(24/85 | 75/310) Optimized Detection and Classification on GTRSB: Advancing Traffic Sign Recognition with Convolutional Neural Networks (Dhruv Toshniwal et al., 2024)</a></li><li><a href=#2585--76310-vlogger-multimodal-diffusion-for-embodied-avatar-synthesis-enric-corona-et-al-2024>(25/85 | 76/310) VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis (Enric Corona et al., 2024)</a></li><li><a href=#2685--77310-pathm3-a-multimodal-multi-task-multiple-instance-learning-framework-for-whole-slide-image-classification-and-captioning-qifeng-zhou-et-al-2024>(26/85 | 77/310) PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning (Qifeng Zhou et al., 2024)</a></li><li><a href=#2785--78310-envision3d-one-image-to-3d-with-anchor-views-interpolation-yatian-pang-et-al-2024>(27/85 | 78/310) Envision3D: One Image to 3D with Anchor Views Interpolation (Yatian Pang et al., 2024)</a></li><li><a href=#2885--79310-refractive-colmap-refractive-structure-from-motion-revisited-mengkun-she-et-al-2024>(28/85 | 79/310) Refractive COLMAP: Refractive Structure-from-Motion Revisited (Mengkun She et al., 2024)</a></li><li><a href=#2985--80310-consistent-prompting-for-rehearsal-free-continual-learning-zhanxin-gao-et-al-2024>(29/85 | 80/310) Consistent Prompting for Rehearsal-Free Continual Learning (Zhanxin Gao et al., 2024)</a></li><li><a href=#3085--81310-using-deep-learning-for-morphological-classification-in-pigs-with-a-focus-on-sanitary-monitoring-eduardo-bedin-et-al-2024>(30/85 | 81/310) Using Deep Learning for Morphological Classification in Pigs with a Focus on Sanitary Monitoring (Eduardo Bedin et al., 2024)</a></li><li><a href=#3185--82310-clip-bevformer-enhancing-multi-view-image-based-bev-detector-with-ground-truth-flow-chenbin-pan-et-al-2024>(31/85 | 82/310) CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow (Chenbin Pan et al., 2024)</a></li><li><a href=#3285--83310-icontra-toward-thematic-collection-design-via-interactive-concept-transfer-dinh-khoi-vo-et-al-2024>(32/85 | 83/310) iCONTRA: Toward Thematic Collection Design Via Interactive Concept Transfer (Dinh-Khoi Vo et al., 2024)</a></li><li><a href=#3385--84310-data-augmentation-in-human-centric-vision-wentao-jiang-et-al-2024>(33/85 | 84/310) Data Augmentation in Human-Centric Vision (Wentao Jiang et al., 2024)</a></li><li><a href=#3485--85310-improved-yolov5-based-on-attention-mechanism-and-fasternet-for-foreign-object-detection-on-railway-and-airway-tracks-zongqing-qi-et-al-2024>(34/85 | 85/310) Improved YOLOv5 Based on Attention Mechanism and FasterNet for Foreign Object Detection on Railway and Airway tracks (Zongqing Qi et al., 2024)</a></li><li><a href=#3585--86310-model-will-tell-training-membership-inference-for-diffusion-models-xiaomeng-fu-et-al-2024>(35/85 | 86/310) Model Will Tell: Training Membership Inference for Diffusion Models (Xiaomeng Fu et al., 2024)</a></li><li><a href=#3685--87310-an-empirical-study-of-parameter-efficient-fine-tuning-on-vision-language-pre-train-model-yuxin-tian-et-al-2024>(36/85 | 87/310) An Empirical Study of Parameter Efficient Fine-tuning on Vision-Language Pre-train Model (Yuxin Tian et al., 2024)</a></li><li><a href=#3785--88310-raf-gi-towards-robust-accurate-and-fast-convergent-gradient-inversion-attack-in-federated-learning-can-liu-et-al-2024>(37/85 | 88/310) RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion Attack in Federated Learning (Can Liu et al., 2024)</a></li><li><a href=#3885--89310-stmpl-human-soft-tissue-simulation-anton-agafonov-et-al-2024>(38/85 | 89/310) STMPL: Human Soft-Tissue Simulation (Anton Agafonov et al., 2024)</a></li><li><a href=#3985--90310-styledyrf-zero-shot-4d-style-transfer-for-dynamic-neural-radiance-fields-hongbin-xu-et-al-2024>(39/85 | 90/310) StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance Fields (Hongbin Xu et al., 2024)</a></li><li><a href=#4085--91310-mgic-a-multi-label-gradient-inversion-attack-based-on-canny-edge-detection-on-federated-learning-can-liu-et-al-2024>(40/85 | 91/310) MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge Detection on Federated Learning (Can Liu et al., 2024)</a></li><li><a href=#4185--92310-vigface-virtual-identity-generation-model-for-face-image-synthesis-minsoo-kim-et-al-2024>(41/85 | 92/310) VIGFace: Virtual Identity Generation Model for Face Image Synthesis (Minsoo Kim et al., 2024)</a></li><li><a href=#4285--93310-point-cloud-compression-via-constrained-optimal-transport-zezeng-li-et-al-2024>(42/85 | 93/310) Point Cloud Compression via Constrained Optimal Transport (Zezeng Li et al., 2024)</a></li><li><a href=#4385--94310-lix-implicitly-infusing-spatial-geometric-prior-knowledge-into-visual-semantic-segmentation-for-autonomous-driving-sicen-guo-et-al-2024>(43/85 | 94/310) LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving (Sicen Guo et al., 2024)</a></li><li><a href=#4485--95310-shadowremovalnet-efficient-real-time-shadow-removal-alzayat-saleh-et-al-2024>(44/85 | 95/310) ShadowRemovalNet: Efficient Real-Time Shadow Removal (Alzayat Saleh et al., 2024)</a></li><li><a href=#4585--96310-monoocc-digging-into-monocular-semantic-occupancy-prediction-yupeng-zheng-et-al-2024>(45/85 | 96/310) MonoOcc: Digging into Monocular Semantic Occupancy Prediction (Yupeng Zheng et al., 2024)</a></li><li><a href=#4685--97310-repair-rank-correlation-and-noisy-pair-half-replacing-with-memory-for-noisy-correspondence-ruochen-zheng-et-al-2024>(46/85 | 97/310) REPAIR: Rank Correlation and Noisy Pair Half-replacing with Memory for Noisy Correspondence (Ruochen Zheng et al., 2024)</a></li><li><a href=#4785--98310-secg-semantic-enhanced-3d-visual-grounding-via-cross-modal-graph-attention-feng-xiao-et-al-2024>(47/85 | 98/310) SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention (Feng Xiao et al., 2024)</a></li><li><a href=#4885--99310-pnesm-arbitrary-3d-scene-stylization-via-prompt-based-neural-style-mapping-jiafu-chen-et-al-2024>(48/85 | 99/310) PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping (Jiafu Chen et al., 2024)</a></li><li><a href=#4985--100310-cart-caltech-aerial-rgb-thermal-dataset-in-the-wild-connor-lee-et-al-2024>(49/85 | 100/310) CART: Caltech Aerial RGB-Thermal Dataset in the Wild (Connor Lee et al., 2024)</a></li><li><a href=#5085--101310-actiondiffusion-an-action-aware-diffusion-model-for-procedure-planning-in-instructional-videos-lei-shi-et-al-2024>(50/85 | 101/310) ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning in Instructional Videos (Lei Shi et al., 2024)</a></li><li><a href=#5185--102310-towards-dense-and-accurate-radar-perception-via-efficient-cross-modal-diffusion-model-ruibin-zhang-et-al-2024>(51/85 | 102/310) Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal Diffusion Model (Ruibin Zhang et al., 2024)</a></li><li><a href=#5285--103310-iterative-online-image-synthesis-via-diffusion-model-for-imbalanced-classification-shuhan-li-et-al-2024>(52/85 | 103/310) Iterative Online Image Synthesis via Diffusion Model for Imbalanced Classification (Shuhan Li et al., 2024)</a></li><li><a href=#5385--104310-hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation-zhonghan-zhao-et-al-2024>(53/85 | 104/310) Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation (Zhonghan Zhao et al., 2024)</a></li><li><a href=#5485--105310-ig-fiqa-improving-face-image-quality-assessment-through-intra-class-variance-guidance-robust-to-inaccurate-pseudo-labels-minsoo-kim-et-al-2024>(54/85 | 105/310) IG-FIQA: Improving Face Image Quality Assessment through Intra-class Variance Guidance robust to Inaccurate Pseudo-Labels (Minsoo Kim et al., 2024)</a></li><li><a href=#5585--106310-make-me-happier-evoking-emotions-through-image-diffusion-models-qing-lin-et-al-2024>(55/85 | 106/310) Make Me Happier: Evoking Emotions Through Image Diffusion Models (Qing Lin et al., 2024)</a></li><li><a href=#5685--107310-ntire-2023-image-shadow-removal-challenge-technical-report-team-iim_tti-yuki-kondo-et-al-2024>(56/85 | 107/310) NTIRE 2023 Image Shadow Removal Challenge Technical Report: Team IIM_TTI (Yuki Kondo et al., 2024)</a></li><li><a href=#5785--108310-unveiling-the-truth-exploring-human-gaze-patterns-in-fake-images-giuseppe-cartella-et-al-2024>(57/85 | 108/310) Unveiling the Truth: Exploring Human Gaze Patterns in Fake Images (Giuseppe Cartella et al., 2024)</a></li><li><a href=#5885--109310-federated-data-model-xiao-chen-et-al-2024>(58/85 | 109/310) Federated Data Model (Xiao Chen et al., 2024)</a></li><li><a href=#5985--110310-artvista-gateway-to-empower-anyone-into-artist-trong-vu-hoang-et-al-2024>(59/85 | 110/310) ARtVista: Gateway To Empower Anyone Into Artist (Trong-Vu Hoang et al., 2024)</a></li><li><a href=#6085--111310-ambient-diffusion-posterior-sampling-solving-inverse-problems-with-diffusion-models-trained-on-corrupted-data-asad-aali-et-al-2024>(60/85 | 111/310) Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data (Asad Aali et al., 2024)</a></li><li><a href=#6185--112310-haifit-human-centered-ai-for-fashion-image-translation-jianan-jiang-et-al-2024>(61/85 | 112/310) HAIFIT: Human-Centered AI for Fashion Image Translation (Jianan Jiang et al., 2024)</a></li><li><a href=#6285--113310-scaling-up-dynamic-human-scene-interaction-modeling-nan-jiang-et-al-2024>(62/85 | 113/310) Scaling Up Dynamic Human-Scene Interaction Modeling (Nan Jiang et al., 2024)</a></li><li><a href=#6385--114310-aigcs-confuse-ai-too-investigating-and-explaining-synthetic-image-induced-hallucinations-in-large-vision-language-models-yifei-gao-et-al-2024>(63/85 | 114/310) AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models (Yifei Gao et al., 2024)</a></li><li><a href=#6485--115310-unilidar-bridge-the-domain-gap-among-different-lidars-for-continual-learning-zikun-xu-et-al-2024>(64/85 | 115/310) UniLiDAR: Bridge the domain gap among different LiDARs for continual learning (Zikun Xu et al., 2024)</a></li><li><a href=#6585--116310-gaussian-splatting-in-style-abhishek-saroha-et-al-2024>(65/85 | 116/310) Gaussian Splatting in Style (Abhishek Saroha et al., 2024)</a></li><li><a href=#6685--117310-noisediffusion-correcting-noise-for-image-interpolation-with-diffusion-models-beyond-spherical-linear-interpolation-pengfei-zheng-et-al-2024>(66/85 | 117/310) NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation (PengFei Zheng et al., 2024)</a></li><li><a href=#6785--118310-pfstorer-personalized-face-restoration-and-super-resolution-tuomas-varanka-et-al-2024>(67/85 | 118/310) PFStorer: Personalized Face Restoration and Super-Resolution (Tuomas Varanka et al., 2024)</a></li><li><a href=#6885--119310-low-cost-and-real-time-industrial-human-action-recognitions-based-on-large-scale-foundation-models-wensheng-liang-et-al-2024>(68/85 | 119/310) Low-Cost and Real-Time Industrial Human Action Recognitions Based on Large-Scale Foundation Models (Wensheng Liang et al., 2024)</a></li><li><a href=#6985--120310-tackling-the-singularities-at-the-endpoints-of-time-intervals-in-diffusion-models-pengze-zhang-et-al-2024>(69/85 | 120/310) Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models (Pengze Zhang et al., 2024)</a></li><li><a href=#7085--121310-mitigate-target-level-insensitivity-of-infrared-small-target-detection-via-posterior-distribution-modeling-haoqing-li-et-al-2024>(70/85 | 121/310) Mitigate Target-level Insensitivity of Infrared Small Target Detection via Posterior Distribution Modeling (Haoqing Li et al., 2024)</a></li><li><a href=#7185--122310-improved-image-based-pose-regressor-models-for-underwater-environments-luyuan-peng-et-al-2024>(71/85 | 122/310) Improved Image-based Pose Regressor Models for Underwater Environments (Luyuan Peng et al., 2024)</a></li><li><a href=#7285--123310-identity-aware-dual-constraint-network-for-cloth-changing-person-re-identification-peini-guo-et-al-2024>(72/85 | 123/310) Identity-aware Dual-constraint Network for Cloth-Changing Person Re-identification (Peini Guo et al., 2024)</a></li><li><a href=#7385--124310-follow-your-click-open-domain-regional-image-animation-via-short-prompts-yue-ma-et-al-2024>(73/85 | 124/310) Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts (Yue Ma et al., 2024)</a></li><li><a href=#7485--125310-sketch2manga-shaded-manga-screening-from-sketch-with-diffusion-models-jian-lin-et-al-2024>(74/85 | 125/310) Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models (Jian Lin et al., 2024)</a></li><li><a href=#7585--126310-p2lhapwearable-sensor-based-human-activity-recognition-segmentation-and-forecast-through-patch-to-label-seq2seq-transformer-shuangjian-li-et-al-2024>(75/85 | 126/310) P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer (Shuangjian Li et al., 2024)</a></li><li><a href=#7685--127310-versatile-defense-against-adversarial-attacks-on-image-recognition-haibo-zhang-et-al-2024>(76/85 | 127/310) Versatile Defense Against Adversarial Attacks on Image Recognition (Haibo Zhang et al., 2024)</a></li><li><a href=#7785--128310-fastmac-stochastic-spectral-sampling-of-correspondence-graph-yifei-zhang-et-al-2024>(77/85 | 128/310) FastMAC: Stochastic Spectral Sampling of Correspondence Graph (Yifei Zhang et al., 2024)</a></li><li><a href=#7885--129310-representing-anatomical-trees-by-denoising-diffusion-of-implicit-neural-fields-ashish-sinha-et-al-2024>(78/85 | 129/310) Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields (Ashish Sinha et al., 2024)</a></li><li><a href=#7985--130310-slcf-net-sequential-lidar-camera-fusion-for-semantic-scene-completion-using-a-3d-recurrent-u-net-helin-cao-et-al-2024>(79/85 | 130/310) SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net (Helin Cao et al., 2024)</a></li><li><a href=#8085--131310-3dfires-few-image-3d-reconstruction-for-scenes-with-hidden-surface-linyi-jin-et-al-2024>(80/85 | 131/310) 3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface (Linyi Jin et al., 2024)</a></li><li><a href=#8185--132310-himap-hybrid-representation-learning-for-end-to-end-vectorized-hd-map-construction-yi-zhou-et-al-2024>(81/85 | 132/310) HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction (Yi Zhou et al., 2024)</a></li><li><a href=#8285--133310-occfiner-offboard-occupancy-refinement-with-hybrid-propagation-hao-shi-et-al-2024>(82/85 | 133/310) OccFiner: Offboard Occupancy Refinement with Hybrid Propagation (Hao Shi et al., 2024)</a></li><li><a href=#8385--134310-drfer-learning-disentangled-representations-for-3d-facial-expression-recognition-hebeizi-li-et-al-2024>(83/85 | 134/310) DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition (Hebeizi Li et al., 2024)</a></li><li><a href=#8485--135310-prago-differentiable-multi-view-pose-optimization-from-objectness-detections-matteo-taiana-et-al-2024>(84/85 | 135/310) PRAGO: Differentiable Multi-View Pose Optimization From Objectness Detections (Matteo Taiana et al., 2024)</a></li><li><a href=#8585--136310-better-fit-accommodate-variations-in-clothing-types-for-virtual-try-on-xuanpu-zhang-et-al-2024>(85/85 | 136/310) Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on (Xuanpu Zhang et al., 2024)</a></li></ul></li><li><a href=#cssd-3>cs.SD (3)</a><ul><li><a href=#13--137310-em-tts-efficiently-trained-low-resource-mongolian-lightweight-text-to-speech-ziqi-liang-et-al-2024>(1/3 | 137/310) EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech (Ziqi Liang et al., 2024)</a></li><li><a href=#23--138310-end-to-end-amp-modeling-from-data-to-controllable-guitar-amplifier-models-lauri-juvela-et-al-2024>(2/3 | 138/310) End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier Models (Lauri Juvela et al., 2024)</a></li><li><a href=#33--139310-from-weak-to-strong-sound-event-labels-using-adaptive-change-point-detection-and-active-learning-john-martinsson-et-al-2024>(3/3 | 139/310) From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning (John Martinsson et al., 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#11--140310-scvgae-a-novel-approach-using-zinb-based-variational-graph-autoencoder-for-single-cell-rna-seq-imputation-yoshitaka-inoue-2024>(1/1 | 140/310) scVGAE: A Novel Approach using ZINB-Based Variational Graph Autoencoder for Single-Cell RNA-Seq Imputation (Yoshitaka Inoue, 2024)</a></li></ul></li><li><a href=#eessiv-13>eess.IV (13)</a><ul><li><a href=#113--141310-segmentation-of-knee-bones-for-osteoarthritis-assessment-a-comparative-analysis-of-supervised-few-shot-and-zero-shot-learning-approaches-yun-xin-teoh-et-al-2024>(1/13 | 141/310) Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches (Yun Xin Teoh et al., 2024)</a></li><li><a href=#213--142310-7t-mri-synthesization-from-3t-acquisitions-qiming-cui-et-al-2024>(2/13 | 142/310) 7T MRI Synthesization from 3T Acquisitions (Qiming Cui et al., 2024)</a></li><li><a href=#313--143310-md-dose-a-diffusion-model-based-on-the-mamba-for-radiotherapy-dose-prediction-linjie-fu-et-al-2024>(3/13 | 143/310) MD-Dose: A Diffusion Model based on the Mamba for Radiotherapy Dose Prediction (Linjie Fu et al., 2024)</a></li><li><a href=#413--144310-diffusion-models-with-implicit-guidance-for-medical-anomaly-detection-cosmin-i-bercea-et-al-2024>(4/13 | 144/310) Diffusion Models with Implicit Guidance for Medical Anomaly Detection (Cosmin I. Bercea et al., 2024)</a></li><li><a href=#513--145310-exploiting-structural-consistency-of-chest-anatomy-for-unsupervised-anomaly-detection-in-radiography-images-tiange-xiang-et-al-2024>(5/13 | 145/310) Exploiting Structural Consistency of Chest Anatomy for Unsupervised Anomaly Detection in Radiography Images (Tiange Xiang et al., 2024)</a></li><li><a href=#613--146310-robust-covid-19-detection-in-ct-images-with-clip-li-lin-et-al-2024>(6/13 | 146/310) Robust COVID-19 Detection in CT Images with CLIP (Li Lin et al., 2024)</a></li><li><a href=#713--147310-diffusion-based-iterative-counterfactual-explanations-for-fetal-ultrasound-image-quality-assessment-paraskevas-pegios-et-al-2024>(7/13 | 147/310) Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment (Paraskevas Pegios et al., 2024)</a></li><li><a href=#813--148310-focusmae-gallbladder-cancer-detection-from-ultrasound-videos-with-focused-masked-autoencoders-soumen-basu-et-al-2024>(8/13 | 148/310) FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders (Soumen Basu et al., 2024)</a></li><li><a href=#913--149310-spatiotemporal-diffusion-model-with-paired-sampling-for-accelerated-cardiac-cine-mri-shihan-qiu-et-al-2024>(9/13 | 149/310) Spatiotemporal Diffusion Model with Paired Sampling for Accelerated Cardiac Cine MRI (Shihan Qiu et al., 2024)</a></li><li><a href=#1013--150310-clinically-feasible-diffusion-reconstruction-for-highly-accelerated-cardiac-cine-mri-shihan-qiu-et-al-2024>(10/13 | 150/310) Clinically Feasible Diffusion Reconstruction for Highly-Accelerated Cardiac Cine MRI (Shihan Qiu et al., 2024)</a></li><li><a href=#1113--151310-gaussianimage-1000-fps-image-representation-and-compression-by-2d-gaussian-splatting-xinjie-zhang-et-al-2024>(11/13 | 151/310) GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting (Xinjie Zhang et al., 2024)</a></li><li><a href=#1213--152310-content-aware-masked-image-modeling-transformer-for-stereo-image-compression-xinjie-zhang-et-al-2024>(12/13 | 152/310) Content-aware Masked Image Modeling Transformer for Stereo Image Compression (Xinjie Zhang et al., 2024)</a></li><li><a href=#1313--153310-pre-examinations-improve-automated-metastases-detection-on-cranial-mri-katerina-deike-hofmann-et-al-2024>(13/13 | 153/310) Pre-examinations Improve Automated Metastases Detection on Cranial MRI (Katerina Deike-Hofmann et al., 2024)</a></li></ul></li><li><a href=#cscr-3>cs.CR (3)</a><ul><li><a href=#13--154310-review-of-generative-ai-methods-in-cybersecurity-yagmur-yigit-et-al-2024>(1/3 | 154/310) Review of Generative AI Methods in Cybersecurity (Yagmur Yigit et al., 2024)</a></li><li><a href=#23--155310-tastle-distract-large-language-models-for-automatic-jailbreak-attack-zeguan-xiao-et-al-2024>(2/3 | 155/310) Tastle: Distract Large Language Models for Automatic Jailbreak Attack (Zeguan Xiao et al., 2024)</a></li><li><a href=#33--156310-advancing-security-in-ai-systems-a-novel-approach-to-detecting-backdoors-in-deep-neural-networks-khondoker-murad-hossain-et-al-2024>(3/3 | 156/310) Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks (Khondoker Murad Hossain et al., 2024)</a></li></ul></li><li><a href=#csse-9>cs.SE (9)</a><ul><li><a href=#19--157310-software-vulnerability-and-functionality-assessment-using-llms-rasmus-ingemann-tuffveson-jensen-et-al-2024>(1/9 | 157/310) Software Vulnerability and Functionality Assessment using LLMs (Rasmus Ingemann Tuffveson Jensen et al., 2024)</a></li><li><a href=#29--158310-search-based-optimisation-of-llm-learning-shots-for-story-point-estimation-vali-tawosi-et-al-2024>(2/9 | 158/310) Search-based Optimisation of LLM Learning Shots for Story Point Estimation (Vali Tawosi et al., 2024)</a></li><li><a href=#39--159310-a-picture-is-worth-a-thousand-words-exploring-diagram-and-video-based-oop-exercises-to-counter-llm-over-reliance-bruno-pereira-cipriano-et-al-2024>(3/9 | 159/310) A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance (Bruno Pereira Cipriano et al., 2024)</a></li><li><a href=#49--160310-bugs-in-large-language-models-generated-code-an-empirical-study-florian-tambon-et-al-2024>(4/9 | 160/310) Bugs in Large Language Models Generated Code: An Empirical Study (Florian Tambon et al., 2024)</a></li><li><a href=#59--161310-teaching-machines-to-code-smart-contract-translation-with-llms-rabimba-karanjai-et-al-2024>(5/9 | 161/310) Teaching Machines to Code: Smart Contract Translation with LLMs (Rabimba Karanjai et al., 2024)</a></li><li><a href=#69--162310-system-for-systematic-literature-review-using-multiple-ai-agents-concept-and-an-empirical-evaluation-abdul-malik-sami-et-al-2024>(6/9 | 162/310) System for systematic literature review using multiple AI agents: Concept and an empirical evaluation (Abdul Malik Sami et al., 2024)</a></li><li><a href=#79--163310-understanding-and-evaluating-developer-behaviour-in-programming-tasks-martin-schröer-et-al-2024>(7/9 | 163/310) Understanding and Evaluating Developer Behaviour in Programming Tasks (Martin Schröer et al., 2024)</a></li><li><a href=#89--164310-autodev-automated-ai-driven-development-michele-tufano-et-al-2024>(8/9 | 164/310) AutoDev: Automated AI-Driven Development (Michele Tufano et al., 2024)</a></li><li><a href=#99--165310-log-summarisation-for-defect-evolution-analysis-rares-dolga-et-al-2024>(9/9 | 165/310) Log Summarisation for Defect Evolution Analysis (Rares Dolga et al., 2024)</a></li></ul></li><li><a href=#cslg-53>cs.LG (53)</a><ul><li><a href=#153--166310-hrlaif-improvements-in-helpfulness-and-harmlessness-in-open-domain-reinforcement-learning-from-ai-feedback-ang-li-et-al-2024>(1/53 | 166/310) HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback (Ang Li et al., 2024)</a></li><li><a href=#253--167310-second-order-information-matters-revisiting-machine-unlearning-for-large-language-models-kang-gu-et-al-2024>(2/53 | 167/310) Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models (Kang Gu et al., 2024)</a></li><li><a href=#353--168310-cleanagent-automating-data-standardization-with-llm-based-agents-danrui-qi-et-al-2024>(3/53 | 168/310) CleanAgent: Automating Data Standardization with LLM-based Agents (Danrui Qi et al., 2024)</a></li><li><a href=#453--169310-federated-knowledge-graph-unlearning-via-diffusion-model-bingchen-liu-et-al-2024>(4/53 | 169/310) Federated Knowledge Graph Unlearning via Diffusion Model (Bingchen Liu et al., 2024)</a></li><li><a href=#553--170310-multi-objective-optimization-using-adaptive-distributed-reinforcement-learning-jing-tan-et-al-2024>(5/53 | 170/310) Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning (Jing Tan et al., 2024)</a></li><li><a href=#653--171310-training-self-localization-models-for-unseen-unfamiliar-places-via-teacher-to-student-data-free-knowledge-transfer-kenta-tsukahara-et-al-2024>(6/53 | 171/310) Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer (Kenta Tsukahara et al., 2024)</a></li><li><a href=#753--172310-simple-and-scalable-strategies-to-continually-pre-train-large-language-models-adam-ibrahim-et-al-2024>(7/53 | 172/310) Simple and Scalable Strategies to Continually Pre-train Large Language Models (Adam Ibrahim et al., 2024)</a></li><li><a href=#853--173310-a-physics-driven-graphsage-method-for-physical-process-simulations-described-by-partial-differential-equations-hang-hu-et-al-2024>(8/53 | 173/310) A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations (Hang Hu et al., 2024)</a></li><li><a href=#953--174310-fast-inference-of-removal-based-node-influence-weikai-li-et-al-2024>(9/53 | 174/310) Fast Inference of Removal-Based Node Influence (Weikai Li et al., 2024)</a></li><li><a href=#1053--175310-human-alignment-of-large-language-models-through-online-preference-optimisation-daniele-calandriello-et-al-2024>(10/53 | 175/310) Human Alignment of Large Language Models through Online Preference Optimisation (Daniele Calandriello et al., 2024)</a></li><li><a href=#1153--176310-data-efficient-sleep-staging-with-synthetic-time-series-pretraining-niklas-grieger-et-al-2024>(11/53 | 176/310) Data-Efficient Sleep Staging with Synthetic Time Series Pretraining (Niklas Grieger et al., 2024)</a></li><li><a href=#1253--177310-sok-reducing-the-vulnerability-of-fine-tuned-language-models-to-membership-inference-attacks-guy-amit-et-al-2024>(12/53 | 177/310) SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks (Guy Amit et al., 2024)</a></li><li><a href=#1353--178310-autodfp-automatic-data-free-pruning-via-channel-similarity-reconstruction-siqi-li-et-al-2024>(13/53 | 178/310) AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction (Siqi Li et al., 2024)</a></li><li><a href=#1453--179310-molbind-multimodal-alignment-of-language-molecules-and-proteins-teng-xiao-et-al-2024>(14/53 | 179/310) MolBind: Multimodal Alignment of Language, Molecules, and Proteins (Teng Xiao et al., 2024)</a></li><li><a href=#1553--180310-usable-xai-10-strategies-towards-exploiting-explainability-in-the-llm-era-xuansheng-wu-et-al-2024>(15/53 | 180/310) Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era (Xuansheng Wu et al., 2024)</a></li><li><a href=#1653--181310-extracting-explanations-justification-and-uncertainty-from-black-box-deep-neural-networks-paul-ardis-et-al-2024>(16/53 | 181/310) Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks (Paul Ardis et al., 2024)</a></li><li><a href=#1753--182310-towards-efficient-risk-sensitive-policy-gradient-an-iteration-complexity-analysis-rui-liu-et-al-2024>(17/53 | 182/310) Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis (Rui Liu et al., 2024)</a></li><li><a href=#1853--183310-unsupervised-learning-of-hybrid-latent-dynamics-a-learn-to-identify-framework-yubo-ye-et-al-2024>(18/53 | 183/310) Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework (Yubo Ye et al., 2024)</a></li><li><a href=#1953--184310-semi-supervised-learning-for-anomaly-traffic-detection-via-bidirectional-normalizing-flows-zhangxuan-dang-et-al-2024>(19/53 | 184/310) Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows (Zhangxuan Dang et al., 2024)</a></li><li><a href=#2053--185310-causal-graph-neural-networks-for-wildfire-danger-prediction-shan-zhao-et-al-2024>(20/53 | 185/310) Causal Graph Neural Networks for Wildfire Danger Prediction (Shan Zhao et al., 2024)</a></li><li><a href=#2153--186310-reduced-jeffries-matusita-distance-a-novel-loss-function-to-improve-generalization-performance-of-deep-classification-models-mohammad-lashkari-et-al-2024>(21/53 | 186/310) Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models (Mohammad Lashkari et al., 2024)</a></li><li><a href=#2253--187310-bg-hgnn-toward-scalable-and-efficient-heterogeneous-graph-neural-network-junwei-su-et-al-2024>(22/53 | 187/310) BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network (Junwei Su et al., 2024)</a></li><li><a href=#2353--188310-one-shot-averaging-for-distributed-tdλ-under-markov-sampling-haoxing-tian-et-al-2024>(23/53 | 188/310) One-Shot Averaging for Distributed TD($λ$) Under Markov Sampling (Haoxing Tian et al., 2024)</a></li><li><a href=#2453--189310-when-can-we-approximate-wide-contrastive-models-with-neural-tangent-kernels-and-principal-component-analysis-gautham-govind-anil-et-al-2024>(24/53 | 189/310) When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis? (Gautham Govind Anil et al., 2024)</a></li><li><a href=#2553--190310-improving-implicit-regularization-of-sgd-with-preconditioning-for-least-square-problems-junwei-su-et-al-2024>(25/53 | 190/310) Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems (Junwei Su et al., 2024)</a></li><li><a href=#2653--191310-learning-to-watermark-llm-generated-text-via-reinforcement-learning-xiaojun-xu-et-al-2024>(26/53 | 191/310) Learning to Watermark LLM-generated Text via Reinforcement Learning (Xiaojun Xu et al., 2024)</a></li><li><a href=#2753--192310-deep-submodular-peripteral-networks-gantavya-bhatt-et-al-2024>(27/53 | 192/310) Deep Submodular Peripteral Networks (Gantavya Bhatt et al., 2024)</a></li><li><a href=#2853--193310-reproducibility-and-geometric-intrinsic-dimensionality-an-investigation-on-graph-neural-network-research-tobias-hille-et-al-2024>(28/53 | 193/310) Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research (Tobias Hille et al., 2024)</a></li><li><a href=#2953--194310-decoupled-federated-learning-on-long-tailed-and-non-iid-data-with-feature-statistics-zhuoxin-chen-et-al-2024>(29/53 | 194/310) Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics (Zhuoxin Chen et al., 2024)</a></li><li><a href=#3053--195310-structural-positional-encoding-for-knowledge-integration-in-transformer-based-medical-process-monitoring-christopher-irwin-et-al-2024>(30/53 | 195/310) Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring (Christopher Irwin et al., 2024)</a></li><li><a href=#3153--196310-representing-molecules-as-random-walks-over-interpretable-grammars-michael-sun-et-al-2024>(31/53 | 196/310) Representing Molecules as Random Walks Over Interpretable Grammars (Michael Sun et al., 2024)</a></li><li><a href=#3253--197310-refresh-responsible-and-efficient-feature-reselection-guided-by-shap-values-shubham-sharma-et-al-2024>(32/53 | 197/310) REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values (Shubham Sharma et al., 2024)</a></li><li><a href=#3353--198310-implicit-regularization-of-gradient-flow-on-one-layer-softmax-attention-heejune-sheen-et-al-2024>(33/53 | 198/310) Implicit Regularization of Gradient Flow on One-Layer Softmax Attention (Heejune Sheen et al., 2024)</a></li><li><a href=#3453--199310-bifurcated-attention-for-single-context-large-batch-sampling-ben-athiwaratkun-et-al-2024>(34/53 | 199/310) Bifurcated Attention for Single-Context Large-Batch Sampling (Ben Athiwaratkun et al., 2024)</a></li><li><a href=#3553--200310-verifix-post-training-correction-to-improve-label-noise-robustness-with-verified-samples-sangamesh-kodge-et-al-2024>(35/53 | 200/310) Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples (Sangamesh Kodge et al., 2024)</a></li><li><a href=#3653--201310-can-physical-information-aid-the-generalization-ability-of-neural-networks-for-hydraulic-modeling-gianmarco-guglielmo-et-al-2024>(36/53 | 201/310) Can physical information aid the generalization ability of Neural Networks for hydraulic modeling? (Gianmarco Guglielmo et al., 2024)</a></li><li><a href=#3753--202310-caformer-rethinking-time-series-analysis-from-causal-perspective-kexuan-zhang-et-al-2024>(37/53 | 202/310) Caformer: Rethinking Time Series Analysis from Causal Perspective (Kexuan Zhang et al., 2024)</a></li><li><a href=#3853--203310-an-analysis-of-human-alignment-of-latent-diffusion-models-lorenz-linhardt-et-al-2024>(38/53 | 203/310) An Analysis of Human Alignment of Latent Diffusion Models (Lorenz Linhardt et al., 2024)</a></li><li><a href=#3953--204310-nonlinear-manifold-learning-determines-microgel-size-from-raman-spectroscopy-eleni-d-koronaki-et-al-2024>(39/53 | 204/310) Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy (Eleni D. Koronaki et al., 2024)</a></li><li><a href=#4053--205310-data-augmentation-with-automated-machine-learning-approaches-and-performance-comparison-with-classical-data-augmentation-methods-alhassan-mumuni-et-al-2024>(40/53 | 205/310) Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods (Alhassan Mumuni et al., 2024)</a></li><li><a href=#4153--206310-bayesian-optimization-that-limits-search-region-to-lower-dimensions-utilizing-local-gpr-yasunori-taguchi-et-al-2024>(41/53 | 206/310) Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR (Yasunori Taguchi et al., 2024)</a></li><li><a href=#4253--207310-machine-unlearning-taxonomy-metrics-applications-challenges-and-prospects-na-li-et-al-2024>(42/53 | 207/310) Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects (Na Li et al., 2024)</a></li><li><a href=#4353--208310-page-domain-incremental-adaptation-with-past-agnostic-generative-replay-for-smart-healthcare-chia-hao-li-et-al-2024>(43/53 | 208/310) PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare (Chia-Hao Li et al., 2024)</a></li><li><a href=#4453--209310-measuring-the-energy-consumption-and-efficiency-of-deep-neural-networks-an-empirical-analysis-and-design-recommendations-charles-edison-tripp-et-al-2024>(44/53 | 209/310) Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations (Charles Edison Tripp et al., 2024)</a></li><li><a href=#4553--210310-a-sparsity-principle-for-partially-observable-causal-representation-learning-danru-xu-et-al-2024>(45/53 | 210/310) A Sparsity Principle for Partially Observable Causal Representation Learning (Danru Xu et al., 2024)</a></li><li><a href=#4653--211310-structural-perspective-on-constraint-based-learning-of-markov-networks-tuukka-korhonen-et-al-2024>(46/53 | 211/310) Structural perspective on constraint-based learning of Markov networks (Tuukka Korhonen et al., 2024)</a></li><li><a href=#4753--212310-learning-enhanced-neighborhood-selection-for-the-vehicle-routing-problem-with-time-windows-willem-feijen-et-al-2024>(47/53 | 212/310) Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows (Willem Feijen et al., 2024)</a></li><li><a href=#4853--213310-predictive-clustering-of-vessel-behavior-based-on-hierarchical-trajectory-representation-rui-zhang-et-al-2024>(48/53 | 213/310) Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation (Rui Zhang et al., 2024)</a></li><li><a href=#4953--214310-karina-an-efficient-deep-learning-model-for-global-weather-forecast-minjong-cheon-et-al-2024>(49/53 | 214/310) KARINA: An Efficient Deep Learning Model for Global Weather Forecast (Minjong Cheon et al., 2024)</a></li><li><a href=#5053--215310-scattered-mixture-of-experts-implementation-shawn-tan-et-al-2024>(50/53 | 215/310) Scattered Mixture-of-Experts Implementation (Shawn Tan et al., 2024)</a></li><li><a href=#5153--216310-robust-decision-aggregation-with-adversarial-experts-yongkang-guo-et-al-2024>(51/53 | 216/310) Robust Decision Aggregation with Adversarial Experts (Yongkang Guo et al., 2024)</a></li><li><a href=#5253--217310-paddingflow-improving-normalizing-flows-with-padding-dimensional-noise-qinglong-meng-et-al-2024>(52/53 | 217/310) PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise (Qinglong Meng et al., 2024)</a></li><li><a href=#5353--218310-learning-driven-physically-aware-large-scale-circuit-gate-sizing-yuyang-ye-et-al-2024>(53/53 | 218/310) Learning-driven Physically-aware Large-scale Circuit Gate Sizing (Yuyang Ye et al., 2024)</a></li></ul></li><li><a href=#csro-18>cs.RO (18)</a><ul><li><a href=#118--219310-language-grounded-dynamic-scene-graphs-for-interactive-object-search-with-mobile-manipulation-daniel-honerkamp-et-al-2024>(1/18 | 219/310) Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation (Daniel Honerkamp et al., 2024)</a></li><li><a href=#218--220310-empowering-robotics-with-large-language-models-osmag-map-comprehension-with-llms-fujing-xie-et-al-2024>(2/18 | 220/310) Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs (Fujing Xie et al., 2024)</a></li><li><a href=#318--221310-copa-general-robotic-manipulation-through-spatial-constraints-of-parts-with-foundation-models-haoxu-huang-et-al-2024>(3/18 | 221/310) CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models (Haoxu Huang et al., 2024)</a></li><li><a href=#418--222310-neuromorphic-force-control-in-an-industrial-task-validating-energy-and-latency-benefits-camilo-amaya-et-al-2024>(4/18 | 222/310) Neuromorphic force-control in an industrial task: validating energy and latency benefits (Camilo Amaya et al., 2024)</a></li><li><a href=#518--223310-real-time-3d-semantic-occupancy-prediction-for-autonomous-vehicles-using-memory-efficient-sparse-convolution-samuel-sze-et-al-2024>(5/18 | 223/310) Real-time 3D semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution (Samuel Sze et al., 2024)</a></li><li><a href=#618--224310-perceive-with-confidence-statistical-safety-assurances-for-navigation-with-learning-based-perception-anushri-dixit-et-al-2024>(6/18 | 224/310) Perceive With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception (Anushri Dixit et al., 2024)</a></li><li><a href=#718--225310-continuous-object-state-recognition-for-cooking-robots-using-pre-trained-vision-language-models-and-black-box-optimization-kento-kawaharazuka-et-al-2024>(7/18 | 225/310) Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization (Kento Kawaharazuka et al., 2024)</a></li><li><a href=#818--226310-iamcv-multi-scenario-vehicle-interaction-dataset-novel-certad-et-al-2024>(8/18 | 226/310) IAMCV Multi-Scenario Vehicle Interaction Dataset (Novel Certad et al., 2024)</a></li><li><a href=#918--227310-difftactile-a-physics-based-differentiable-tactile-simulator-for-contact-rich-robotic-manipulation-zilin-si-et-al-2024>(9/18 | 227/310) DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation (Zilin Si et al., 2024)</a></li><li><a href=#1018--228310-compliant-hierarchical-control-for-arbitrary-equality-and-inequality-tasks-with-strict-and-soft-priorities-gianluca-garofalo-2024>(10/18 | 228/310) Compliant Hierarchical Control for Arbitrary Equality and Inequality Tasks with Strict and Soft Priorities (Gianluca Garofalo, 2024)</a></li><li><a href=#1118--229310-grf-based-predictive-flocking-control-with-dynamic-pattern-formation-chenghao-yu-et-al-2024>(11/18 | 229/310) GRF-based Predictive Flocking Control with Dynamic Pattern Formation (Chenghao Yu et al., 2024)</a></li><li><a href=#1218--230310-a-novel-feature-learning-based-bio-inspired-neural-network-for-real-time-collision-free-rescue-of-multi-robot-systems-junfei-li-et-al-2024>(12/18 | 230/310) A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems (Junfei Li et al., 2024)</a></li><li><a href=#1318--231310-a-direct-algorithm-for-multi-gyroscope-infield-calibration-tianheng-wang-et-al-2024>(13/18 | 231/310) A Direct Algorithm for Multi-Gyroscope Infield Calibration (Tianheng Wang et al., 2024)</a></li><li><a href=#1418--232310-effective-underwater-glider-path-planning-in-dynamic-3d-environments-using-multi-point-potential-fields-hanzhi-yang-et-al-2024>(14/18 | 232/310) Effective Underwater Glider Path Planning in Dynamic 3D Environments Using Multi-Point Potential Fields (Hanzhi Yang et al., 2024)</a></li><li><a href=#1518--233310-naturalvlm-leveraging-fine-grained-natural-language-for-affordance-guided-visual-manipulation-ran-xu-et-al-2024>(15/18 | 233/310) NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation (Ran Xu et al., 2024)</a></li><li><a href=#1618--234310-synchronized-dual-arm-rearrangement-via-cooperative-mtsp-wenhao-li-et-al-2024>(16/18 | 234/310) Synchronized Dual-arm Rearrangement via Cooperative mTSP (Wenhao Li et al., 2024)</a></li><li><a href=#1718--235310-spaceoctopus-an-octopus-inspired-motion-planning-framework-for-multi-arm-space-robot-wenbo-zhao-et-al-2024>(17/18 | 235/310) SpaceOctopus: An Octopus-inspired Motion Planning Framework for Multi-arm Space Robot (Wenbo Zhao et al., 2024)</a></li><li><a href=#1818--236310-multi-fidelity-reinforcement-learning-for-time-optimal-quadrotor-re-planning-gilhyun-ryou-et-al-2024>(18/18 | 236/310) Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor Re-planning (Gilhyun Ryou et al., 2024)</a></li></ul></li><li><a href=#eesssy-6>eess.SY (6)</a><ul><li><a href=#16--237310-llm-assisted-light-leveraging-large-language-model-capabilities-for-human-mimetic-traffic-signal-control-in-complex-urban-environments-maonan-wang-et-al-2024>(1/6 | 237/310) LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments (Maonan Wang et al., 2024)</a></li><li><a href=#26--238310-mechanism-design-optimization-through-cad-based-bayesian-optimization-and-quantified-constraints-abdelmajid-ben-yahya-et-al-2024>(2/6 | 238/310) Mechanism Design Optimization through CAD-Based Bayesian Optimization and Quantified Constraints (Abdelmajid Ben Yahya et al., 2024)</a></li><li><a href=#36--239310-remote-ugv-control-via-practical-wireless-channels-a-model-predictive-control-approach-inghao-cao-et-al-2024>(3/6 | 239/310) Remote UGV Control via Practical Wireless Channels: A Model Predictive Control Approach (inghao Cao et al., 2024)</a></li><li><a href=#46--240310-probabilistic-metaplasticity-for-continual-learning-with-memristors-fatima-tuz-zohora-et-al-2024>(4/6 | 240/310) Probabilistic Metaplasticity for Continual Learning with Memristors (Fatima Tuz Zohora et al., 2024)</a></li><li><a href=#56--241310-ventilation-and-temperature-control-for-energy-efficient-and-healthy-buildings-a-differentiable-pde-approach-yuexin-bian-et-al-2024>(5/6 | 241/310) Ventilation and Temperature Control for Energy-efficient and Healthy Buildings: A Differentiable PDE Approach (Yuexin Bian et al., 2024)</a></li><li><a href=#66--242310-prototyping-and-experimental-results-for-environment-aware-millimeter-wave-beam-alignment-via-channel-knowledge-map-zhuoyin-dai-et-al-2024>(6/6 | 242/310) Prototyping and Experimental Results for Environment-Aware Millimeter Wave Beam Alignment via Channel Knowledge Map (Zhuoyin Dai et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--243310-paperclip-associating-astronomical-observations-and-natural-language-with-multi-modal-models-siddharth-mishra-sharma-et-al-2024>(1/1 | 243/310) PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models (Siddharth Mishra-Sharma et al., 2024)</a></li></ul></li><li><a href=#csir-6>cs.IR (6)</a><ul><li><a href=#16--244310-domain-adaptation-for-dense-retrieval-and-conversational-dense-retrieval-through-self-supervision-by-meticulous-pseudo-relevance-labeling-minghan-li-et-al-2024>(1/6 | 244/310) Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling (Minghan Li et al., 2024)</a></li><li><a href=#26--245310-foundation-models-and-information-retrieval-in-digital-pathology-h-r-tizhoosh-2024>(2/6 | 245/310) Foundation Models and Information Retrieval in Digital Pathology (H. R. Tizhoosh, 2024)</a></li><li><a href=#36--246310-towards-unified-modeling-for-positive-and-negative-preferences-in-sign-aware-recommendation-yuting-liu-et-al-2024>(3/6 | 246/310) Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation (Yuting Liu et al., 2024)</a></li><li><a href=#46--247310-discrete-semantic-tokenization-for-deep-ctr-prediction-qijiong-liu-et-al-2024>(4/6 | 247/310) Discrete Semantic Tokenization for Deep CTR Prediction (Qijiong Liu et al., 2024)</a></li><li><a href=#56--248310-ilciter-evidence-grounded-interpretable-local-citation-recommendation-sayar-ghosh-roy-et-al-2024>(5/6 | 248/310) ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation (Sayar Ghosh Roy et al., 2024)</a></li><li><a href=#66--249310-nlqxform-ui-a-natural-language-interface-for-querying-dblp-interactively-ruijie-wang-et-al-2024>(6/6 | 249/310) NLQxform-UI: A Natural Language Interface for Querying DBLP Interactively (Ruijie Wang et al., 2024)</a></li></ul></li><li><a href=#csit-7>cs.IT (7)</a><ul><li><a href=#17--250310-meta-learning-based-fronthaul-compression-for-cloud-radio-access-networks-ruihua-qiao-et-al-2024>(1/7 | 250/310) Meta-Learning-Based Fronthaul Compression for Cloud Radio Access Networks (Ruihua Qiao et al., 2024)</a></li><li><a href=#27--251310-meta-reinforcement-learning-for-resource-allocation-in-aerial-active-ris-assisted-networks-with-rate-splitting-multiple-access-sajad-faramarzi-et-al-2024>(2/7 | 251/310) Meta Reinforcement Learning for Resource Allocation in Aerial Active-RIS-assisted Networks with Rate-Splitting Multiple Access (Sajad Faramarzi et al., 2024)</a></li><li><a href=#37--252310-maximum-channel-coding-rate-of-finite-block-length-mimo-faster-than-nyquist-signaling-zichao-zhang-et-al-2024>(3/7 | 252/310) Maximum Channel Coding Rate of Finite Block Length MIMO Faster-Than-Nyquist Signaling (Zichao Zhang et al., 2024)</a></li><li><a href=#47--253310-low-complexity-beam-training-for-multi-ris-assisted-multi-user-communications-yuan-xu-et-al-2024>(4/7 | 253/310) Low-Complexity Beam Training for Multi-RIS-Assisted Multi-User Communications (Yuan Xu et al., 2024)</a></li><li><a href=#57--254310-handoffs-in-user-centric-cell-free-mimo-networks-a-pomdp-framework-hussein-a-ammar-et-al-2024>(5/7 | 254/310) Handoffs in User-Centric Cell-Free MIMO Networks: A POMDP Framework (Hussein A. Ammar et al., 2024)</a></li><li><a href=#67--255310-improved-trade-offs-between-amortization-and-download-bandwidth-for-linear-hss-keller-blackwell-et-al-2024>(6/7 | 255/310) Improved Trade-offs Between Amortization and Download Bandwidth for Linear HSS (Keller Blackwell et al., 2024)</a></li><li><a href=#77--256310-coverage-and-rate-analysis-for-integrated-sensing-and-communication-networks-xu-gan-et-al-2024>(7/7 | 256/310) Coverage and Rate Analysis for Integrated Sensing and Communication Networks (Xu Gan et al., 2024)</a></li></ul></li><li><a href=#csma-4>cs.MA (4)</a><ul><li><a href=#14--257310-cultural-evolution-in-populations-of-large-language-models-jérémy-perez-et-al-2024>(1/4 | 257/310) Cultural evolution in populations of Large Language Models (Jérémy Perez et al., 2024)</a></li><li><a href=#24--258310-autonomous-underground-freight-transport-systems----the-future-of-urban-logistics-lasse-bienzeisler-et-al-2024>(2/4 | 258/310) Autonomous Underground Freight Transport Systems &ndash; The Future of Urban Logistics? (Lasse Bienzeisler et al., 2024)</a></li><li><a href=#34--259310-beyond-joint-demonstrations-personalized-expert-guidance-for-efficient-multi-agent-reinforcement-learning-peihong-yu-et-al-2024>(3/4 | 259/310) Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning (Peihong Yu et al., 2024)</a></li><li><a href=#44--260310-emergence-of-social-norms-in-large-language-model-based-agent-societies-siyue-ren-et-al-2024>(4/4 | 260/310) Emergence of Social Norms in Large Language Model-based Agent Societies (Siyue Ren et al., 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--261310-an-efficient-end-to-end-approach-to-noise-invariant-speech-features-via-multi-task-learning-heitor-r-guimarães-et-al-2024>(1/1 | 261/310) An Efficient End-to-End Approach to Noise Invariant Speech Features via Multi-Task Learning (Heitor R. Guimarães et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--262310-exploring-prompt-engineering-practices-in-the-enterprise-michael-desmond-et-al-2024>(1/5 | 262/310) Exploring Prompt Engineering Practices in the Enterprise (Michael Desmond et al., 2024)</a></li><li><a href=#25--263310-the-full-scale-assembly-simulation-testbed-fast-dataset-alec-g-moore-et-al-2024>(2/5 | 263/310) The Full-scale Assembly Simulation Testbed (FAST) Dataset (Alec G. Moore et al., 2024)</a></li><li><a href=#35--264310-academiaos-automating-grounded-theory-development-in-qualitative-research-with-large-language-models-thomas-übellacker-2024>(3/5 | 264/310) AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models (Thomas Übellacker, 2024)</a></li><li><a href=#45--265310-ai-coach-for-badminton-dhruv-toshniwal-et-al-2024>(4/5 | 265/310) AI coach for badminton (Dhruv Toshniwal et al., 2024)</a></li><li><a href=#55--266310-a-virtual-environment-for-collaborative-inspection-in-additive-manufacturing-vuthea-chheang-et-al-2024>(5/5 | 266/310) A Virtual Environment for Collaborative Inspection in Additive Manufacturing (Vuthea Chheang et al., 2024)</a></li></ul></li><li><a href=#csgt-4>cs.GT (4)</a><ul><li><a href=#14--267310-language-based-game-theory-in-the-age-of-artificial-intelligence-valerio-capraro-et-al-2024>(1/4 | 267/310) Language-based game theory in the age of artificial intelligence (Valerio Capraro et al., 2024)</a></li><li><a href=#24--268310-strategizing-against-q-learners-a-control-theoretical-approach-yuksel-arslantas-et-al-2024>(2/4 | 268/310) Strategizing against Q-learners: A Control-theoretical Approach (Yuksel Arslantas et al., 2024)</a></li><li><a href=#34--269310-learning-how-to-strategically-disclose-information-raj-kiriti-velicheti-et-al-2024>(3/4 | 269/310) Learning How to Strategically Disclose Information (Raj Kiriti Velicheti et al., 2024)</a></li><li><a href=#44--270310-an-algorithmic-theory-of-simplicity-in-mechanism-design-diodato-ferraioli-et-al-2024>(4/4 | 270/310) An Algorithmic Theory of Simplicity in Mechanism Design (Diodato Ferraioli et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--271310-digital-twin-assisted-reinforcement-learning-for-resource-aware-microservice-offloading-in-edge-computing-xiangchun-chen-et-al-2024>(1/2 | 271/310) Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing (Xiangchun Chen et al., 2024)</a></li><li><a href=#22--272310-from-channel-measurement-to-training-data-for-phy-layer-ai-applications-michael-zentarra-et-al-2024>(2/2 | 272/310) From Channel Measurement to Training Data for PHY Layer AI Applications (Michael Zentarra et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--273310-self-supervised-learning-for-covariance-estimation-tzvi-diskin-et-al-2024>(1/1 | 273/310) Self-Supervised Learning for Covariance Estimation (Tzvi Diskin et al., 2024)</a></li></ul></li><li><a href=#csce-5>cs.CE (5)</a><ul><li><a href=#15--274310-evaluating-the-efficiency-and-cost-effectiveness-of-rpb-based-co2-capture-a-comprehensive-approach-to-simultaneous-design-and-operating-condition-optimization-howoun-jung-et-al-2024>(1/5 | 274/310) Evaluating the Efficiency and Cost-effectiveness of RPB-based CO2 Capture: A Comprehensive Approach to Simultaneous Design and Operating Condition Optimization (Howoun Jung et al., 2024)</a></li><li><a href=#25--275310-a-framework-for-strategic-discovery-of-credible-neural-network-surrogate-models-under-uncertainty-pratyush-kumar-singh-et-al-2024>(2/5 | 275/310) A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty (Pratyush Kumar Singh et al., 2024)</a></li><li><a href=#35--276310-improved-bass-model-using-sales-proportional-average-for-one-condition-of-mono-peak-curves-ahmad-abu-sleem-et-al-2024>(3/5 | 276/310) Improved bass model using sales proportional average for one condition of mono peak curves (Ahmad Abu Sleem et al., 2024)</a></li><li><a href=#45--277310-a-comparative-analysis-of-transient-finite-strain-coupled-diffusion-deformation-theories-for-hydrogels-jorge-humberto-urrea-quintero-et-al-2024>(4/5 | 277/310) A comparative analysis of transient finite-strain coupled diffusion-deformation theories for hydrogels (Jorge-Humberto Urrea-Quintero et al., 2024)</a></li><li><a href=#55--278310-model-order-reduction-for-transient-coupled-diffusion-deformation-of-hydrogels-gopal-agarwal-et-al-2024>(5/5 | 278/310) Model order reduction for transient coupled diffusion-deformation of hydrogels (Gopal Agarwal et al., 2024)</a></li></ul></li><li><a href=#cssy-1>cs.SY (1)</a><ul><li><a href=#11--279310-differential-privacy-in-nonlinear-dynamical-systems-with-tracking-performance-guarantees-dhrubajit-chowdhury-et-al-2024>(1/1 | 279/310) Differential Privacy in Nonlinear Dynamical Systems with Tracking Performance Guarantees (Dhrubajit Chowdhury et al., 2024)</a></li></ul></li><li><a href=#cssi-3>cs.SI (3)</a><ul><li><a href=#13--280310-link-prediction-for-social-networks-using-representation-learning-and-heuristic-based-features-samarth-khanna-et-al-2024>(1/3 | 280/310) Link Prediction for Social Networks using Representation Learning and Heuristic-based Features (Samarth Khanna et al., 2024)</a></li><li><a href=#23--281310-negative-impact-of-online-political-incivility-on-willingness-to-see-political-comments-kohei-nishi-2024>(2/3 | 281/310) Negative Impact of Online Political Incivility on Willingness to See Political Comments (Kohei Nishi, 2024)</a></li><li><a href=#33--282310-an-improvement-on-the-louvain-algorithm-using-random-walks-duy-hieu-do-et-al-2024>(3/3 | 282/310) An improvement on the Louvain algorithm using random walks (Duy Hieu Do et al., 2024)</a></li></ul></li><li><a href=#mathna-4>math.NA (4)</a><ul><li><a href=#14--283310-efficient-geometric-markov-chain-monte-carlo-for-nonlinear-bayesian-inversion-enabled-by-derivative-informed-neural-operators-lianghao-cao-et-al-2024>(1/4 | 283/310) Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators (Lianghao Cao et al., 2024)</a></li><li><a href=#24--284310-interpolatory-model-order-reduction-of-large-scale-dynamical-systems-with-root-mean-squared-error-measures-sean-reiter-et-al-2024>(2/4 | 284/310) Interpolatory model order reduction of large-scale dynamical systems with root mean squared error measures (Sean Reiter et al., 2024)</a></li><li><a href=#34--285310-non-linear-collision-induced-breakage-equation-finite-volume-and-semi-analytical-methods-sanjiv-kumar-bariwal-et-al-2024>(3/4 | 285/310) Non-linear collision-induced breakage equation: finite volume and semi-analytical methods (Sanjiv Kumar Bariwal et al., 2024)</a></li><li><a href=#44--286310-tangential-fixpoint-iterations-for-gromov-wasserstein-barycenters-florian-beier-et-al-2024>(4/4 | 286/310) Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters (Florian Beier et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--287310-learnable-community-aware-transformer-for-brain-connectome-analysis-with-token-clustering-yanting-yang-et-al-2024>(1/1 | 287/310) Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering (Yanting Yang et al., 2024)</a></li></ul></li><li><a href=#mathoc-3>math.OC (3)</a><ul><li><a href=#13--288310-a-constrained-tracking-controller-for-ramp-and-sinusoidal-reference-signals-using-robust-positive-invariance-geovana-franca-dos-santos-et-al-2024>(1/3 | 288/310) A Constrained Tracking Controller for Ramp and Sinusoidal Reference Signals using Robust Positive Invariance (Geovana Franca dos Santos et al., 2024)</a></li><li><a href=#23--289310-regret-analysis-of-policy-optimization-over-submanifolds-for-linearly-constrained-online-lqg-ting-jui-chang-et-al-2024>(2/3 | 289/310) Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG (Ting-Jui Chang et al., 2024)</a></li><li><a href=#33--290310-exponential-stability-of-parametric-optimization-based-controllers-via-lure-contractivity-alexander-davydov-et-al-2024>(3/3 | 290/310) Exponential Stability of Parametric Optimization-Based Controllers via Lur&rsquo;e Contractivity (Alexander Davydov et al., 2024)</a></li></ul></li><li><a href=#statml-4>stat.ML (4)</a><ul><li><a href=#14--291310-towards-model-agnostic-posterior-approximation-for-fast-and-accurate-variational-autoencoders-yaniv-yacoby-et-al-2024>(1/4 | 291/310) Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders (Yaniv Yacoby et al., 2024)</a></li><li><a href=#24--292310-a-non-asymptotic-theory-of-kernel-ridge-regression-deterministic-equivalents-test-error-and-gcv-estimator-theodor-misiakiewicz-et-al-2024>(2/4 | 292/310) A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator (Theodor Misiakiewicz et al., 2024)</a></li><li><a href=#34--293310-multifidelity-linear-regression-for-scientific-machine-learning-from-scarce-data-elizabeth-qian-et-al-2024>(3/4 | 293/310) Multifidelity linear regression for scientific machine learning from scarce data (Elizabeth Qian et al., 2024)</a></li><li><a href=#44--294310-asymptotics-of-random-feature-regression-beyond-the-linear-scaling-regime-hong-hu-et-al-2024>(4/4 | 294/310) Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime (Hong Hu et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--295310-multi-product-hamiltonian-simulation-with-explicit-commutator-scaling-junaid-aftab-et-al-2024>(1/2 | 295/310) Multi-product Hamiltonian simulation with explicit commutator scaling (Junaid Aftab et al., 2024)</a></li><li><a href=#22--296310-efficiently-verifiable-quantum-advantage-on-near-term-analog-quantum-simulators-zhenning-liu-et-al-2024>(2/2 | 296/310) Efficiently verifiable quantum advantage on near-term analog quantum simulators (Zhenning Liu et al., 2024)</a></li></ul></li><li><a href=#csai-4>cs.AI (4)</a><ul><li><a href=#14--297310-meta-operators-for-enabling-parallel-planning-using-deep-reinforcement-learning-ángel-aso-mollar-et-al-2024>(1/4 | 297/310) Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning (Ángel Aso-Mollar et al., 2024)</a></li><li><a href=#24--298310-specification-overfitting-in-artificial-intelligence-benjamin-roth-et-al-2024>(2/4 | 298/310) Specification Overfitting in Artificial Intelligence (Benjamin Roth et al., 2024)</a></li><li><a href=#34--299310-a-short-review-on-novel-approaches-for-maximum-clique-problem-from-classical-algorithms-to-graph-neural-networks-and-quantum-algorithms-raffaele-marino-et-al-2024>(3/4 | 299/310) A Short Review on Novel Approaches for Maximum Clique Problem: from Classical algorithms to Graph Neural Networks and Quantum algorithms (Raffaele Marino et al., 2024)</a></li><li><a href=#44--300310-optimizing-risk-averse-human-ai-hybrid-teams-andrew-fuchs-et-al-2024>(4/4 | 300/310) Optimizing Risk-averse Human-AI Hybrid Teams (Andrew Fuchs et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--301310-gpt-ontology-and-caabac-a-tripartite-personalized-access-control-model-anchored-by-compliance-context-and-attribute-raza-nowrozy-et-al-2024>(1/1 | 301/310) GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control Model Anchored by Compliance, Context and Attribute (Raza Nowrozy et al., 2024)</a></li></ul></li><li><a href=#csdm-3>cs.DM (3)</a><ul><li><a href=#13--302310-improved-dynamics-for-the-maximum-common-subgraph-problem-davide-guidobene-et-al-2024>(1/3 | 302/310) Improved Dynamics for the Maximum Common Subgraph Problem (Davide Guidobene et al., 2024)</a></li><li><a href=#23--303310-on-sampling-diluted-spin-glasses-using-glauber-dynamics-charilaos-efthymiou-et-al-2024>(2/3 | 303/310) On sampling diluted Spin Glasses using Glauber dynamics (Charilaos Efthymiou et al., 2024)</a></li><li><a href=#33--304310-ensuring-connectedness-for-the-maximum-quasi-clique-and-densest-k-subgraph-problems-daniela-scherer-dos-santos-et-al-2024>(3/3 | 304/310) Ensuring connectedness for the Maximum Quasi-clique and Densest $k$-subgraph problems (Daniela Scherer dos Santos et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--305310-leveraging-non-decimated-wavelet-packet-features-and-transformer-models-for-time-series-forecasting-guy-p-nason-et-al-2024>(1/1 | 305/310) Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting (Guy P Nason et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--306310-measures-of-relevance-to-the-success-of-streaming-platforms-juan-carlos-gonçalves-dosantos-et-al-2024>(1/1 | 306/310) Measures of relevance to the success of streaming platforms (Juan Carlos Gonçalves-Dosantos et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--307310-the-q-ary-gilbert-varshamov-bound-can-be-improved-for-all-but-finitely-many-positive-integers-q-xue-bin-liang-2024>(1/1 | 307/310) The q-ary Gilbert-Varshamov bound can be improved for all but finitely many positive integers q (Xue-Bin Liang, 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--308310-approximating-small-sparse-cuts-aditya-anand-et-al-2024>(1/2 | 308/310) Approximating Small Sparse Cuts (Aditya Anand et al., 2024)</a></li><li><a href=#22--309310-worst-case-to-expander-case-reductions-derandomized-and-generalized-amir-abboud-et-al-2024>(2/2 | 309/310) Worst-Case to Expander-Case Reductions: Derandomized and Generalized (Amir Abboud et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--310310-timed-strategies-for-real-time-rewrite-theories-carlos-olarte-et-al-2024>(1/1 | 310/310) Timed Strategies for Real-Time Rewrite Theories (Carlos Olarte et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>