<!doctype html><html><head><title>High-order Proximity Preserved Embedding for Dynamic Networks</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="High-order Proximity Preserved Embedding for Dynamic Networks"><meta property="og:description" content="Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Zhu, D., Cui, P., Zhang, Z., Pei, J., & Zhu, W. (2018).
High-Order Proximity Preserved Embedding for Dynamic Networks.
IEEE Transactions on Knowledge and Data Engineering, 30(11), 2134–2144.
https://doi.org/10.1109/TKDE.2018.2822283 Abstract Network embedding, aiming to embed a network into a low dimensional vector space while preserving the inherent structural properties of the network, has attracted considerable attention. However, most existing embedding methods focus on the static network while neglecting the evolving characteristic of real-world networks."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202206/20220618223844/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-18T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-18T00:00:00+00:00"><meta name=description content="High-order Proximity Preserved Embedding for Dynamic Networks"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script>
<script language=javascript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/papers/>Papers</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul class=active><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a class=active href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/papers/202206/20220618223844/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Jun 18, 2022</p></div><div class=title><h1>High-order Proximity Preserved Embedding for Dynamic Networks</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/atround-2 class="btn, btn-sm">Round-2</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2018 class="btn, btn-sm">2018</a></li><li class=rounded><a href=/akitenkrad-blog/tags/dsblogcatalog class="btn, btn-sm">BlogCatalog</a></li><li class=rounded><a href=/akitenkrad-blog/tags/dscatster class="btn, btn-sm">Catster</a></li><li class=rounded><a href=/akitenkrad-blog/tags/dsyoutube-friendships class="btn, btn-sm">Youtube-friendships</a></li></ul></div><div class=post-content id=post-content><ul><li><input checked disabled type=checkbox> Round-1: Overview</li><li><input checked disabled type=checkbox> Round-2: Model Implementation Details</li><li><input disabled type=checkbox> Round-3: Experiments</li></ul><h2 id=citation>Citation</h2><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation>Zhu, D., Cui, P., Zhang, Z., Pei, J., & Zhu, W. (2018).<br>High-Order Proximity Preserved Embedding for Dynamic Networks.<br>IEEE Transactions on Knowledge and Data Engineering, 30(11), 2134–2144.<br><a href=https://doi.org/10.1109/TKDE.2018.2822283>https://doi.org/10.1109/TKDE.2018.2822283</a></p class="citation"></blockquote><h2 id=abstract>Abstract</h2><blockquote><p>Network embedding, aiming to embed a network into a low dimensional vector space while preserving the inherent structural properties of the network, has attracted considerable attention. However, most existing embedding methods focus on the static network while neglecting the evolving characteristic of real-world networks. Meanwhile, most of previous methods cannot well preserve the high-order proximity, which is a critical structural property of networks. These problems motivate us to seek an effective and efficient way to preserve the high-order proximity in embedding vectors when the networks evolve over time. In this paper, we propose a novel method of Dynamic High-order Proximity preserved Embedding (DHPE). Specifically, we adopt the generalized SVD (GSVD) to preserve the high-order proximity. Then, by transforming the GSVD problem to a generalized eigenvalue problem, we propose a generalized eigen perturbation to incrementally update the results of GSVD to incorporate the changes of dynamic networks. Further, we propose an accelerated solution to the DHPE model so that it achieves a linear time complexity with respect to the number of nodes and number of changed edges in the network. Our empirical experiments on one synthetic network and several real-world networks demonstrate the effectiveness and efficiency of the proposed method.</p></blockquote><h2 id=background--whats-new>Background & What&rsquo;s New</h2><ul><li>動的なグラフ構造データにおいて，<strong>High-order Proximity</strong> はグラフの構造的な特徴を捉えるための重要な特徴量であるが，既存手法の多くは High-order Proximity をうまく扱えない<ul><li>要因の一つは，既存手法の多くが Static Graph を対象としたものでありノードやエッジが追加・削除された場合の High-order Proximity の変動を捉えることができないことである</li></ul></li><li>グラフ構造データにおいては，そもそも High-order Proximity の計算自体が負荷の高い処理である<ul><li>近年，Ou et al. (2016) において，Generalized SVD(GSVD) を用いることによって High-order Proximity を陽に計算することなく特徴量を保持したままベクトルに変換する方法が提案された</li><li>しかし，動的なグラフにおいて GSVD をどのように活用することができるか，という点については研究の余地が残されていた</li></ul></li><li>動的な無向グラフにおいて，High-order Proximity を保持したEmbeddingを算出できる手法として，<strong>DHPE</strong> を提案した<ul><li>GSVDを適用した後，問題を一般化された固有値問題に変換し，行列摂動理論を用いてグラフの動的な構造変化に対応した</li></ul></li><li>行列摂動理論を用いた計算は複雑になりがちで効率性のボトルネックになりうるが，この点を改善する方法を提案し，計算効率を大きく向上させた</li></ul><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Asymmetric Transitivity Preserving Graph Embedding (Mingdong Ou et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Mingdong Ou, Peng Cui, J. Pei, Ziwei Zhang, Wenwu Zhu. (2016)<br><strong>Asymmetric Transitivity Preserving Graph Embedding</strong><br>KDD<br><a href=https://www.semanticscholar.org/paper/07627bf7eb649220ffbcdf6bf233e3a4a76e8590>Paper Link</a><br>Influential Citation Count (117), SS-ID (07627bf7eb649220ffbcdf6bf233e3a4a76e8590)</p><p><strong>ABSTRACT</strong><br>Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding (HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.</p></p class="details-citation"></div></div></details><h2 id=dataset>Dataset</h2><ul><li><a href=https://www.re3data.org/repository/r3d100010959>BlogCatalog</a></li><li><a href=http://konect.cc/networks/petster-friendships-cat/>Catster</a></li><li><a href=http://konect.cc/networks/com-youtube/>Youtube1</a></li></ul><h2 id=model-description>Model Description</h2><p>ステップ $t$ における Dynamic Network を</p><p>$$
\begin{array}{l}
G^{(t)} = \lbrace V^{(t)}, E^{(t)} \rbrace
\end{array} \tag{1} \\
\text{where} \hspace{10pt} \begin{array}{l}
V^{(t)} = \left\lbrace v_1^{(t)}, v_2^{(t)}, \ldots, v_N^{(t)} \right\rbrace
\end{array}
$$</p><p>と表すこととし，$G^{(t)}$ の Embedding と high-order proximity をそれぞれ</p><p>$$
\begin{array}{l}
\text{Embedding} & \mapsto U^{(t)} \in \mathbb{R}^{N \times d} \\
\text{High-order Proximity} & \mapsto S^{(t)} \hspace{10pt} (S_{ij}^{(t)} \text{ is the proximity between }v_i^{(t)}\text{ and }v_j^{(t)})
\end{array} \tag{2} \\
\text{where} \hspace{10pt} \begin{array}{l}
d & \mapsto \text{embedding dimension} \\
S^{(t)} & \in \mathbb{R}^N
\end{array}
$$</p><p>と表す．</p><p>Dynamic Network Embedding の計算プロセスは次の2つのステップに分けられる．</p><style>.definition-box{position:relative;margin:2em 0;padding:1rem 1em .5rem 1rem;border:solid 3px #585858;border-radius:8px}.definition-box .box-title{position:absolute;display:inline-block;top:-13px;left:10px;padding:0 9px;line-height:1;font-size:19px;background:#fafafc;color:#585858;font-weight:700}.definition-box p{margin:0;padding:0}</style><div class=definition-box><span class=box-title>Step 1. Static network embedding</span><p>given adjacency matrix $A^{(t)}$, at time step $t$;<br>output the embedding matrix
$$
U^{(t)}
$$
using static model.</p></div><style>.definition-box{position:relative;margin:2em 0;padding:1rem 1em .5rem 1rem;border:solid 3px #585858;border-radius:8px}.definition-box .box-title{position:absolute;display:inline-block;top:-13px;left:10px;padding:0 9px;line-height:1;font-size:19px;background:#fafafc;color:#585858;font-weight:700}.definition-box p{margin:0;padding:0}</style><div class=definition-box><span class=box-title>Step 2. Dynamic network embedding</span><p>given adjacency matrix $\lbrace A^{(t+1)}, A^{(t+2)}, \ldots, A^{(t+i)} \rbrace$, at time steps $\lbrace t+1, t+2, \ldots, t+i \rbrace$ and the embedding matrix $U^{(t)}$ at time step $t$;<br>output the embedding matrix
$$
\lbrace U^{(t+1)}, U^{(t+2)}, \ldots, U^{(t+i)} \rbrace
$$
at time steps $\lbrace t+1, t+2, \ldots, t+i \rbrace$.</p></div><h4 id=gsvd-based-static-model>GSVD-Based Static Model</h4><p>Ou et al. (2016) において提案されている手法にしたがって，次の目的関数を考える．</p><p>$$
\begin{array}{l}
\min \left\lVert S - U{U^\prime}^\mathsf{T} \right\rVert_F^2 \\
\end{array} \tag{3} \\
\text{where} \hspace{10pt} \begin{array}{l}
U, U^\prime &\in& \mathbb{R}^{N \times d} \\
S &\in& \mathbb{R}^N
\end{array}
$$</p><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Asymmetric Transitivity Preserving Graph Embedding (Mingdong Ou et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Mingdong Ou, Peng Cui, J. Pei, Ziwei Zhang, Wenwu Zhu. (2016)<br><strong>Asymmetric Transitivity Preserving Graph Embedding</strong><br>KDD<br><a href=https://www.semanticscholar.org/paper/07627bf7eb649220ffbcdf6bf233e3a4a76e8590>Paper Link</a><br>Influential Citation Count (117), SS-ID (07627bf7eb649220ffbcdf6bf233e3a4a76e8590)</p><p><strong>ABSTRACT</strong><br>Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding (HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.</p></p class="details-citation"></div></div></details><br><p>High-order Proximity の指標として広く用いられている Katz Index を $S$ として採用する．</p><p>$$
\begin{array}{l}
S^{\text{Katz}} & = M_a^{-1} M_b \\
M_a & = (I - \beta A) \\
M_b & = \beta A
\end{array} \tag{4} \\
\text{where} \hspace{10pt} \begin{array}{l}
I & \mapsto & \text{Identity Matrix} \\
\beta & \mapsto & \text{determines how fast the weight of a path decays} \\
& & \text{when the length of path grows}
\end{array}
$$</p><p>$\beta$ は収束に大きく影響するので，適切に設定する必要がある．</p><p>Ou et al. (2016) で提案されている通り，ここで Generalized SVD (GSVD) を適用することによって，$S$ を計算することなく $S$ の特異値と特異ベクトルを得ることができる．<br>すなわち，目的関数における最適なEmbeddingは以下で与えられる．</p><p>$$
\begin{array}{l}
U & = \left[ \sqrt{\sigma_1} \boldsymbol{v}_1^l, \ldots, \sqrt{\sigma_d}\boldsymbol{v}_d^l\right] \\
U^\prime & = \left[ \sqrt{\sigma_1} \boldsymbol{v}_1^r, \ldots, \sqrt{\sigma_d}\boldsymbol{v}_d^r\right]
\end{array} \tag{5} \\
\text{where} \hspace{10pt} \begin{array}{l}
\lbrace \sigma_1, \ldots, \sigma_N \rbrace & \mapsto \text{the singular values of }S \text{ sorted in descending order} \\
\boldsymbol{v}_i^l, \boldsymbol{v}_i^r & \mapsto \text{corresponding left and right singular vectors of }\sigma_i
\end{array}
$$</p><p>また，Ou et al. (2016) によれば，GSVD-Based Static Model のエラーバウンドは</p><p>$$
\begin{equation}
\left\lVert S - U{U^\prime}^\mathsf{T} \right\rVert_F^2 = \sum_{i=d+1}^N \sigma_i^2
\end{equation} \tag{6}
$$</p><p>である．</p><h4 id=problem-transformation-for-dynamic-model>Problem Transformation for Dynamic Model</h4><p>$\Delta A$ および $U^{(t)}$ が与えられたときに， $U^{(t)}$ を $U^{(t+1)}$ へ更新する方法について検討する．<br>GSVDのアウトプットは下記で与えられる．</p><p>$$
\begin{array}{l}
S^{(t)} & = {M_a^{(t)}}^{-1} M_b^{(t)} = V^{l(t)} \Sigma^{(t)} {V^{r(t)}}^\mathsf{T} \\
\Sigma^{(t)} &= \text{diag} \left( \sigma_1^{(t)}, \ldots, \sigma_N^{(t)} \right) \\
\end{array} \tag{7} \\
\text{where} \hspace{10pt} \begin{array}{l}
V^{l(t)}, V^{r(t)} & \mapsto \text{singular vectors in matrices}
\end{array}
$$</p><p>上式を直接計算して $S^{(t+1)}$ を算出するのは非常に計算コストが高いため，GSVDを次のように一般化固有値問題へ変形する．<br>無向グラフにおいては，$A$ および $S$ は対称行列となるので，</p><p>$$
\begin{array}{l}
M_a^{-1} M_b X & = \Lambda X \\
\Lambda & = \text{diag} \left( \lambda_1, \ldots, \lambda_N \right) \\
\lambda_i & = \sigma_i \cdot \text{sgn} \left( \boldsymbol{v}_i^l \cdot \boldsymbol{v}_i^r \right) \\
X & = V^l
\end{array} \tag{8} \\
\text{where} \hspace{10pt} \begin{array}{l}
\lbrace \lambda_i \rbrace & \mapsto \text{eigenvalues of }S\text{ in descending order} \\
X & \mapsto \text{a matrix which contains the corresponding eigen vectors of }\lambda_i \\
\text{sgn} & \mapsto \text{Sign function}
\end{array}
$$</p><p>となり，</p><p>$$
\begin{equation}
M_b X = M_a \Lambda X
\end{equation} \tag{9}
$$</p><p>を得る．<br>上式は明らかに固有値問題の一般化された形式であり，その計算結果をGSVDに逆変形することができる．すなわち，</p><p>$$
\begin{array}{l}
\boldsymbol{v}_i^l & = \boldsymbol{x}_i \sigma_i \\
& = \lvert \lambda_i \rvert \boldsymbol{v}_i^r \\
& = \boldsymbol{x}_i \cdot \text{sgn} \left( \lambda_i \right)
\end{array} \tag{10}
$$</p><p>となる．したがって，$\Sigma^{(t)}$，$V^{l(t)}$， $V^{r(t)}$ が与えられれば，(8) によって $X^{(t)}$ および $\Lambda^{(t)}$ を計算することができ，$X^{(t+1)}$ および $\Lambda^{(t+1)}$ が得られれば，(10) によって $\Sigma^{(t)}$，$V^{l(t)}$， $V^{r(t)}$ を計算することができる．</p><p>よって，次の問題は $X^{(t)}$ を $X^{(t+1)}$ に効率よく更新する計算をどのように実装するか，というものとなる．</p><h4 id=generalized-eigen-perturbation>Generalized Eigen Perturbation</h4><p>隣接行列の直前のタイムステップからの差分 $\Delta A$ を受け取って，$M_a$，$M_b$ の差分は以下のように計算できる</p><p>$$
\begin{array}{l}
\Delta M_a & = -\beta \Delta A \\
\Delta M_b & = \beta \Delta A
\end{array} \tag{11}
$$</p><p>したがって，固有値と固有ベクトルの差分について，(9) より</p><p>$$
\begin{array}{l}
\left( M_b + \Delta M_b \right)\left( X + \Delta X \right) = \left( M_a + \Delta M_a \right)\left(\Lambda + \Delta \Lambda \right)\left( X + \Delta X \right)
\end{array} \tag{12}
$$</p><p>を得る．特定の固有値，固有ベクトルのペアについては，</p><p>$$
\begin{array}{l}
\left( M_b + \Delta M_b \right)\left( \boldsymbol{x}_i + \Delta \boldsymbol{x}_i \right) = \left( \lambda_i + \Delta \lambda_i \right) \left( M_a + \Delta M_a \right) \left( \boldsymbol{x}_i + \Delta \boldsymbol{x}_i \right)
\end{array} \tag{13}
$$</p><p>となる．</p><p>まず $\Delta \lambda_i$ を計算する．</p><p>$\Delta M_b x_i = \lambda_i M_a \boldsymbol{x}_i$ であることおよび， $\Delta M_b \Delta \boldsymbol{x}_i$，$\lambda_i \Delta M_a \Delta \boldsymbol{x}_i$ などの高次項は精度への影響が小さいことがわかっているため除外できることを考慮し，さらに両辺に左から ${\boldsymbol{x}_i}^\mathsf{T}$ をかけることによって，(13) は</p><p>$$
\begin{equation}
{\boldsymbol{x}_i}^\mathsf{T} M_b \Delta \boldsymbol{x}_i + {\boldsymbol{x}_i}^\mathsf{T} \Delta M_b \boldsymbol{x}_i = \lambda_i {\boldsymbol{x}_i}^\mathsf{T} M_a \Delta \boldsymbol{x}_i + \lambda_i {\boldsymbol{x}_i}^\mathsf{T} \Delta M_a \boldsymbol{x}_i + \Delta \lambda_i {\boldsymbol{x}_i}^\mathsf{T} M_a \boldsymbol{x}_i
\end{equation} \tag{14}
$$</p><p>のように変形することができる．</p><p>$M_a$ と $M_b$ は対称行列であるから，結局</p><p>$$
\begin{equation}
{\boldsymbol{x}_i}^\mathsf{T} M_b = \lambda_i {\boldsymbol{x}_i}^\mathsf{T} M_a
\end{equation} \tag{15}
$$</p><p>を得る．したがって，</p><p>$$
\begin{equation}
\Delta \lambda_i = \frac{H_b(i, i) - \lambda_i H_a(i, i)}{F_a(i, i)}
\end{equation} \tag{16} \\
\text{where} \hspace{10pt} \begin{array}{l}
H_a(i, j) = {\boldsymbol{x}_i}^\mathsf{T} \Delta M_a \boldsymbol{x}_j \\
H_b(i, j) = {\boldsymbol{x}_i}^\mathsf{T} \Delta M_b \boldsymbol{x}_j \\
F_a(i, j) = {\boldsymbol{x}_i}^\mathsf{T} M_a \boldsymbol{x}_j \\
F_b(i, j) = {\boldsymbol{x}_i}^\mathsf{T} M_b \boldsymbol{x}_j
\end{array}
$$</p><p>となる．</p><p>次に $\Delta \boldsymbol{x}_i$ を計算する．</p><p>行列摂動理論により，グラフの隣り合うタイムステップ間での差異はスムーズなものであり，線形近似できると考えられるので，</p><p>$$
\begin{array}{l}
\begin{array}{l}
\Delta \boldsymbol{x}_i = {\displaystyle\sum_{j=1, j \neq i}^d} \alpha_{ij} \boldsymbol{x}_j
\end{array} \tag{17} \\
\text{where} \hspace{10pt} \begin{array}{l}
\alpha_{ij} \mapsto \text{the coefficient indicating the contribution of }x_j\text{ to }\Delta \boldsymbol{x}_i
\end{array}
\end{array}
$$</p><p>と表せる．したがって，</p><p>$$
\begin{array}{l}
\begin{array}{l}
\alpha_i = W^{-1}B
\end{array} \tag{18} \\
\text{where} \hspace{10pt} \begin{array}{l}
\alpha_i = \left[ \alpha_{i1}, \ldots, \alpha_{i(i-1)}, \alpha_{i(i+1)}, \ldots, \alpha_{id} \right] \hspace{10pt} (\alpha_{ij} \text{ for } 1 \leq j \leq d, j \neq i) \\
B(p) = H_b(p, i) - (\lambda_i + \Delta \lambda_i) H_a (p, i) - \Delta \lambda_i F_a (p, i) \\
W(p, j) = (\lambda_i + \Delta \lambda_i) {\displaystyle \sum_{j=1, j \neq i}^d} H_a (p, j) - {\displaystyle \sum_{j=1, j \neq i}^d} H_b (p, j) + (\lambda_i + \Delta \lambda_i) {\displaystyle \sum_{j=1, j \neq i}^d} F_a (p, j) - {\displaystyle \sum_{j=1, j \neq i}^d} F_b (p, j)
\end{array}
\end{array}
$$</p><p>となる．</p><p>(16)，(17)，(18) から固有値と固有ベクトルのタイムステップ間の差分が計算できるので， $\Lambda^{(t+1)}$，$X^{(t+1)}$ を計算することができる．</p><figure><img src=algorithm.png width=100%><figcaption>Algorithm</figcaption></figure><h2 id=results>Results</h2><h3 id=settings>Settings</h3><h4 id=tasks>Tasks</h4><ul><li>無向ネットワークの Embedding において，high-order proximity を捉える性能を検証する</li><li>Dynamic Network における提案手法 (DHPE) の有効性を検証する</li><li>提案手法 (DHPE) の効率性を検証する</li></ul><h4 id=baseline-methods>Baseline Methods</h4><p><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>LINE (Jian Tang et al., 2015)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation>Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Q. Mei. (2015)<br><strong>LINE: Large-scale Information Network Embedding</strong><br>WWW<br><a href=https://www.semanticscholar.org/paper/0834e74304b547c9354b6d7da6fa78ef47a48fa8>Paper Link</a><br>Influential Citation Count (867), SS-ID (0834e74304b547c9354b6d7da6fa78ef47a48fa8)<br><strong>ABSTRACT</strong><br>This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,&rsquo;&rsquo; which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\footnote{\url{https://github.com/tangjianpku/LINE}}.</p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>DeepWalk (Bryan Perozzi et al., 2014)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation>Bryan Perozzi, Rami Al-Rfou, S. Skiena. (2014)<br><strong>DeepWalk: online learning of social representations</strong><br>KDD<br><a href=https://www.semanticscholar.org/paper/fff114cbba4f3ba900f33da574283e3de7f26c83>Paper Link</a><br>Influential Citation Count (1396), SS-ID (fff114cbba4f3ba900f33da574283e3de7f26c83)<br><strong>ABSTRACT</strong><br>We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk&rsquo;s latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk&rsquo;s representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk&rsquo;s representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.</p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>node2vec (Aditya Grover et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation>Aditya Grover, J. Leskovec. (2016)<br><strong>node2vec: Scalable Feature Learning for Networks</strong><br>KDD<br><a href=https://www.semanticscholar.org/paper/36ee2c8bd605afd48035d15fdc6b8c8842363376>Paper Link</a><br>Influential Citation Count (1164), SS-ID (36ee2c8bd605afd48035d15fdc6b8c8842363376)<br><strong>ABSTRACT</strong><br>Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node&rsquo;s network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.</p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>GraRep (Shaosheng Cao et al., 2015)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation>Shaosheng Cao, Wei Lu, Qiongkai Xu. (2015)<br><strong>GraRep: Learning Graph Representations with Global Structural Information</strong><br>CIKM<br><a href=https://www.semanticscholar.org/paper/c2fd72cb2a77941e655b5d949d0d59b01e173c3b>Paper Link</a><br>Influential Citation Count (142), SS-ID (c2fd72cb2a77941e655b5d949d0d59b01e173c3b)<br><strong>ABSTRACT</strong><br>In this paper, we present {GraRep}, a novel model for learning vertex representations of weighted graphs. This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. We also formally analyze the connections between our work and several previous research efforts, including the DeepWalk model of Perozzi et al. as well as the skip-gram model with negative sampling of Mikolov et al. We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization. Empirical results demonstrate that our representation significantly outperforms other state-of-the-art methods in such tasks.</p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>TRIP (C. Chen et al., 2015)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation>C. Chen, Hanghang Tong. (2015)<br><strong>Fast Eigen-Functions Tracking on Dynamic Graphs</strong><br>SDM<br><a href=https://www.semanticscholar.org/paper/ddc3dc43095173a6b5f0a6f5fbdde79e1abac34d>Paper Link</a><br>Influential Citation Count (3), SS-ID (ddc3dc43095173a6b5f0a6f5fbdde79e1abac34d)<br><strong>ABSTRACT</strong><br>Many important graph parameters can be expressed as eigenfunctions of its adjacency matrix. Examples include epidemic threshold, graph robustness, etc. It is often of key importance to accurately monitor these parameters. For example, knowing that Ebola virus has already been brought to the US continent, to avoid the virus from spreading away, it is important to know which emerging connections among related people would cause great reduction on the epidemic threshold of the network. However, most, if not all, of the existing algorithms computing these measures assume that the input graph is static, despite the fact that almost all real graphs are evolving over time. In this paper, we propose two online algorithms to track the eigen-functions of a dynamic graph with linear complexity wrt the number of nodes and number of changed edges in the graph. The key idea is to leverage matrix perturbation theory to efficiently update the top eigen-pairs of the underlying graph without recomputing them from scratch at each time stamp. Experiment results demonstrate that our methods can reach up to 20× speedup with precision more than 80% for fairly long period of time.</p class="details-citation"></div></div></details></p><ul><li>embedding dimention $d \mapsto 100$</li><li>$\beta \mapsto 0.8 / r$ ($r$ は隣接行列のスペクトル半径)</li></ul><h4 id=evaluation-metrics>Evaluation Metrics</h4><ul><li><p>RMSE
$$
\begin{equation}
\text{RMSE} = \sqrt{\frac{\left\lVert S - U{U^\prime}^\mathsf{T} \right\rVert _F^2}{N^2}}
\end{equation} \tag{19}
$$</p></li><li><p>Precision@k
$$
\begin{equation}
\text{Precision@k} = \frac{\left\lvert\lbrace (i,j)|(i,j) \in E_p \cap E_o\rbrace\right\rvert}{\lvert E_p \rvert}
\end{equation} \tag{20} \\
\text{where} \hspace{10pt} \begin{array}{l}
E_p & \mapsto \text{the set of predicted top }k\text{ edges} \\
E_o & \mapsto \text{ the set of observed edges} \\
\lvert \cdot \rvert & \mapsto \text{the size of a set}
\end{array}
$$</p></li><li><p>Mean Average Precision (MAP) @k
$$
\begin{array}{l}
\text{AP@}k(i) & = {\displaystyle \frac{\sum_{j=1}^k \text{Precision@}j(i) \cdot \delta_i(j)}{\sum_{j=1}^k \delta_i(j)}} \\
\text{MAP@}k & = {\displaystyle \frac{\sum_{v_i \in V} \text{AP@}k(i)}{\lvert V \rvert}}
\end{array} \tag{21} \\
\text{where} \hspace{10pt} \begin{array}{l}
\text{Precision@}j(i) & \mapsto \text{Precision@}j\text{ for node }v_i \\
\delta_i(j) & = \left\lbrace \begin{array}{l}
1 & \text{if } v_i \text{ and } v_j \text{ have an edge} \\
0 & \text{otherwise}
\end{array}\right.
\end{array}
$$</p></li></ul><h3 id=effectiveness-of-the-static-model>Effectiveness of the Static Model</h3><figure><img src=figure-1.png width=100%><figcaption>Link Prediction on static networks</figcaption></figure><h3 id=effectiveness-of-the-dynamic-model>Effectiveness of the Dynamic Model</h3><h4 id=high-order-proximity-approximation>High-Order Proximity Approximation</h4><figure><img src=figure-2.png width=100%><figcaption>High-Order Proximity Approximation</figcaption></figure><h4 id=link-prediction>Link Prediction</h4><figure><img src=figure-3.png width=100%><figcaption>Link Prediction</figcaption></figure><h4 id=node-reccomencation>Node Reccomencation</h4><figure><img src=figure-4.png width=100%><figcaption>Node Reccomenation</figcaption></figure><h4 id=multi-label-classification>Multi-Label Classification</h4><figure><img src=figure-5.png width=100%><figcaption>Multi-Label Classification</figcaption></figure><h2 id=references>References</h2><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Asymmetric Transitivity Preserving Graph Embedding (Mingdong Ou et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Mingdong Ou, Peng Cui, J. Pei, Ziwei Zhang, Wenwu Zhu. (2016)<br><strong>Asymmetric Transitivity Preserving Graph Embedding</strong><br>KDD<br><a href=https://www.semanticscholar.org/paper/07627bf7eb649220ffbcdf6bf233e3a4a76e8590>Paper Link</a><br>Influential Citation Count (117), SS-ID (07627bf7eb649220ffbcdf6bf233e3a4a76e8590)</p><p><strong>ABSTRACT</strong><br>Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding (HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>LINE: Large-scale Information Network Embedding (Jian Tang et al., 2015)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Q. Mei. (2015)<br><strong>LINE: Large-scale Information Network Embedding</strong><br>WWW<br><a href=https://www.semanticscholar.org/paper/0834e74304b547c9354b6d7da6fa78ef47a48fa8>Paper Link</a><br>Influential Citation Count (867), SS-ID (0834e74304b547c9354b6d7da6fa78ef47a48fa8)</p><p><strong>ABSTRACT</strong><br>This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,&rsquo;&rsquo; which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\footnote{\url{https://github.com/tangjianpku/LINE}}.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Scalable learning of collective behavior based on sparse social dimensions (Lei Tang et al., 2009)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Lei Tang, Huan Liu. (2009)<br><strong>Scalable learning of collective behavior based on sparse social dimensions</strong><br>CIKM<br><a href=https://www.semanticscholar.org/paper/094f9616e15f4e64e7afd9d7f5a1b092bbc83738>Paper Link</a><br>Influential Citation Count (33), SS-ID (094f9616e15f4e64e7afd9d7f5a1b092bbc83738)</p><p><strong>ABSTRACT</strong><br>The study of collective behavior is to understand how individuals behave in a social network environment. Oceans of data generated by social media like Facebook, Twitter, Flickr and YouTube present opportunities and challenges to studying collective behavior in a large scale. In this work, we aim to learn to predict collective behavior in social media. In particular, given information about some individuals, how can we infer the behavior of unobserved individuals in the same network? A social-dimension based approach is adopted to address the heterogeneity of connections presented in social media. However, the networks in social media are normally of colossal size, involving hundreds of thousands or even millions of actors. The scale of networks entails scalable learning of models for collective behavior prediction. To address the scalability issue, we propose an edge-centric clustering scheme to extract sparse social dimensions. With sparse social dimensions, the social-dimension based approach can efficiently handle networks of millions of actors while demonstrating comparable prediction performance as other non-scalable methods.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>A General Framework for Content-enhanced Network Representation Learning (Xiaofei Sun et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Xiaofei Sun, Jiang Guo, Xiao Ding, Ting Liu. (2016)<br><strong>A General Framework for Content-enhanced Network Representation Learning</strong><br>ArXiv<br><a href=https://www.semanticscholar.org/paper/1268d2ae95f0b128678d6ce033ba8ea7f0d98be1>Paper Link</a><br>Influential Citation Count (12), SS-ID (1268d2ae95f0b128678d6ce033ba8ea7f0d98be1)</p><p><strong>ABSTRACT</strong><br>This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world net- works with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Model‐based clustering for social networks (M. Handcock et al., 2007)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>M. Handcock, A. Raftery, J. Tantrum. (2007)<br><strong>Model‐based clustering for social networks</strong></p><p><a href=https://www.semanticscholar.org/paper/157c24a3df3203622cfc2ffd514e6ea10b019a14>Paper Link</a><br>Influential Citation Count (78), SS-ID (157c24a3df3203622cfc2ffd514e6ea10b019a14)</p><p><strong>ABSTRACT</strong><br>Summary.  Network models are widely used to represent relations between interacting units or actors. Network data often exhibit transitivity, meaning that two actors that have ties to a third actor are more likely to be tied than actors that do not, homophily by attributes of the actors or dyads, and clustering. Interest often focuses on finding clusters of actors or ties, and the number of groups in the data is typically unknown. We propose a new model, the latent position cluster model, under which the probability of a tie between two actors depends on the distance between them in an unobserved Euclidean ‘social space’, and the actors’ locations in the latent social space arise from a mixture of distributions, each corresponding to a cluster. We propose two estimation methods: a two‐stage maximum likelihood method and a fully Bayesian method that uses Markov chain Monte Carlo sampling. The former is quicker and simpler, but the latter performs better. We also propose a Bayesian way of determining the number of clusters that are present by using approximate conditional Bayes factors. Our model represents transitivity, homophily by attributes and clustering simultaneously and does not require the number of clusters to be known. The model makes it easy to simulate realistic networks with clustering, which are potentially useful as inputs to models of more complex systems of which the network is part, such as epidemic models of infectious disease. We apply the model to two networks of social relations. A free software package in the R statistical language, latentnet, is available to analyse data by using the model.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Visualizing Data using t-SNE (L. V. D. Maaten et al., 2008)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>L. V. D. Maaten, Geoffrey E. Hinton. (2008)<br><strong>Visualizing Data using t-SNE</strong></p><p><a href=https://www.semanticscholar.org/paper/1c46943103bd7b7a2c7be86859995a4144d1938b>Paper Link</a><br>Influential Citation Count (873), SS-ID (1c46943103bd7b7a2c7be86859995a4144d1938b)</p><p><strong>ABSTRACT</strong><br>We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>TIMERS: Error-Bounded SVD Restart on Dynamic Networks (Ziwei Zhang et al., 2017)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Ziwei Zhang, Peng Cui, J. Pei, Xiao Wang, Wenwu Zhu. (2017)<br><strong>TIMERS: Error-Bounded SVD Restart on Dynamic Networks</strong><br>AAAI<br><a href=https://www.semanticscholar.org/paper/1ca27cb4d43e7f3fbc1c4196252168702d7b3b3e>Paper Link</a><br>Influential Citation Count (10), SS-ID (1ca27cb4d43e7f3fbc1c4196252168702d7b3b3e)</p><p><strong>ABSTRACT</strong><br>Singular Value Decomposition (SVD) is a popular approach in various network applications, such as link prediction and network parameter characterization. Incremental SVD approaches are proposed to process newly changed nodes and edges in dynamic networks. However, incremental SVD approaches suffer from serious error accumulation inevitably due to approximation on incremental updates. SVD restart is an effective approach to reset the aggregated error, but when to restart SVD for dynamic networks is not addressed in literature. In this paper, we propose TIMERS, Theoretically Instructed Maximum-Error-bounded Restart of SVD, a novel approach which optimally sets the restart time in order to reduce error accumulation in time. Specifically, we monitor the margin between reconstruction loss of incremental updates and the minimum loss in SVD model. To reduce the complexity of monitoring, we theoretically develop a lower bound of SVD minimum loss for dynamic networks and use the bound to replace the minimum loss in monitoring. By setting a maximum tolerated error as a threshold, we can trigger SVD restart automatically when the margin exceeds this threshold. We prove that the time complexity of our method is linear with respect to the number of local dynamic changes, and our method is general across different types of dynamic networks. We conduct extensive experiments on several synthetic and real dynamic networks. The experimental results demonstrate that our proposed method significantly outperforms the existing methods by reducing 27% to 42% in terms of the maximum error for dynamic network reconstruction when fixing the number of restarts. Our method reduces the number of restarts by 25% to 50% when fixing the maximum error tolerated.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>LIBLINEAR: A Library for Large Linear Classification (Rong-En Fan et al., 2008)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin. (2008)<br><strong>LIBLINEAR: A Library for Large Linear Classification</strong><br>J. Mach. Learn. Res.<br><a href=https://www.semanticscholar.org/paper/268a4f8da15a42f3e0e71691f760ff5edbf9cec8>Paper Link</a><br>Influential Citation Count (907), SS-ID (268a4f8da15a42f3e0e71691f760ff5edbf9cec8)</p><p><strong>ABSTRACT</strong><br>LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Incorporate Group Information to Enhance Network Embedding (Jifan Chen et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Jifan Chen, Qi Zhang, Xuanjing Huang. (2016)<br><strong>Incorporate Group Information to Enhance Network Embedding</strong><br>CIKM<br><a href=https://www.semanticscholar.org/paper/332ec914469af4ecbc4ada0631773febc030406e>Paper Link</a><br>Influential Citation Count (4), SS-ID (332ec914469af4ecbc4ada0631773febc030406e)</p><p><strong>ABSTRACT</strong><br>The problem of representing large-scale networks with low-dimensional vectors has received considerable attention in recent years. Except the networks that include only vertices and edges, a variety of networks contain information about groups or communities. For example, on Facebook, in addition to users and the follower-followee relations between them, users can also create and join groups. However, previous studies have rarely utilized this valuable information to generate embeddings of vertices. In this paper, we investigate a novel method for learning the network embeddings with valuable group information for large-scale networks. The proposed methods take both the inner structures of the groups and the information across groups into consideration. Experimental results demonstrate that the embeddings generated by the proposed methods significantly outperform state-of-the-art network embedding methods on two different scale real-world network</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>A global geometric framework for nonlinear dimensionality reduction. (J. Tenenbaum et al., 2000)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>J. Tenenbaum, V. De Silva, J. Langford. (2000)<br><strong>A global geometric framework for nonlinear dimensionality reduction.</strong><br>Science<br><a href=https://www.semanticscholar.org/paper/3537fcd0ff99a3b3cb3d279012df826358420556>Paper Link</a><br>Influential Citation Count (1200), SS-ID (3537fcd0ff99a3b3cb3d279012df826358420556)</p><p><strong>ABSTRACT</strong><br>Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>node2vec: Scalable Feature Learning for Networks (Aditya Grover et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Aditya Grover, J. Leskovec. (2016)<br><strong>node2vec: Scalable Feature Learning for Networks</strong><br>KDD<br><a href=https://www.semanticscholar.org/paper/36ee2c8bd605afd48035d15fdc6b8c8842363376>Paper Link</a><br>Influential Citation Count (1164), SS-ID (36ee2c8bd605afd48035d15fdc6b8c8842363376)</p><p><strong>ABSTRACT</strong><br>Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node&rsquo;s network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Revisiting Semi-Supervised Learning with Graph Embeddings (Zhilin Yang et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Zhilin Yang, William W. Cohen, R. Salakhutdinov. (2016)<br><strong>Revisiting Semi-Supervised Learning with Graph Embeddings</strong><br>ICML<br><a href=https://www.semanticscholar.org/paper/3d846cb01f6a975554035d2210b578ca61344b22>Paper Link</a><br>Influential Citation Count (186), SS-ID (3d846cb01f6a975554035d2210b578ca61344b22)</p><p><strong>ABSTRACT</strong><br>We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Fast, Warped Graph Embedding: Unifying Framework and One-Click Algorithm (Siheng Chen et al., 2017)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Siheng Chen, Sufeng Niu, L. Akoglu, J. Kovacevic, C. Faloutsos. (2017)<br><strong>Fast, Warped Graph Embedding: Unifying Framework and One-Click Algorithm</strong><br>ArXiv<br><a href=https://www.semanticscholar.org/paper/406b5b1f350a4f51391da29fcfa2a6800dc33973>Paper Link</a><br>Influential Citation Count (4), SS-ID (406b5b1f350a4f51391da29fcfa2a6800dc33973)</p><p><strong>ABSTRACT</strong><br>What is the best way to describe a user in a social network with just a few numbers? Mathematically, this is equivalent to assigning a vector representation to each node in a graph, a process called graph embedding. We propose a novel framework, GEM-D that unifies most of the past algorithms such as LapEigs, DeepWalk and node2vec. GEM-D achieves its goal by decomposing any graph embedding algorithm into three building blocks: node proximity function, warping function and loss function. Based on thorough analysis of GEM-D, we propose a novel algorithm, called UltimateWalk, which outperforms the most-recently proposed state-of-the-art DeepWalk and node2vec. The contributions of this work are: (1) The proposed framework, GEM-D unifies the past graph embedding algorithms and provides a general recipe of how to design a graph embedding; (2) the nonlinearlity in the warping function contributes significantly to the quality of embedding and the exponential function is empirically optimal; (3) the proposed algorithm, UltimateWalk is one-click (no user-defined parameters), scalable and has a closed-form solution.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Matrix Computations (A. Chrzȩszczyk et al., 2011)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation>A. Chrzȩszczyk, J. Kochanowski. (2011)<br><strong>Matrix Computations</strong><br>Encyclopedia of Parallel Computing<br><a href=https://www.semanticscholar.org/paper/444d70e3331b5083b40ef32e49390ef683a65e67>Paper Link</a><br>Influential Citation Count (569), SS-ID (444d70e3331b5083b40ef32e49390ef683a65e67)</p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Heterogeneous Information Network Embedding for Meta Path based Proximity (Zhipeng Huang et al., 2017)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Zhipeng Huang, N. Mamoulis. (2017)<br><strong>Heterogeneous Information Network Embedding for Meta Path based Proximity</strong><br>ArXiv<br><a href=https://www.semanticscholar.org/paper/52a150d6a098ef142bece099dadaa613fddbae50>Paper Link</a><br>Influential Citation Count (14), SS-ID (52a150d6a098ef142bece099dadaa613fddbae50)</p><p><strong>ABSTRACT</strong><br>A network embedding is a representation of a large graph in a low-dimensional space, where vertices are modeled as vectors. The objective of a good embedding is to preserve the proximity between vertices in the original graph. This way, typical search and mining methods can be applied in the embedded space with the help of off-the-shelf multidimensional indexing approaches. Existing network embedding techniques focus on homogeneous networks, where all vertices are considered to belong to a single class.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Combining content and link for classification using matrix factorization (Shenghuo Zhu et al., 2007)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Shenghuo Zhu, Kai Yu, Yun Chi, Yihong Gong. (2007)<br><strong>Combining content and link for classification using matrix factorization</strong><br>SIGIR<br><a href=https://www.semanticscholar.org/paper/5c58ad9a6c09782814a7d048bebd6ef1609c0fb4>Paper Link</a><br>Influential Citation Count (19), SS-ID (5c58ad9a6c09782814a7d048bebd6ef1609c0fb4)</p><p><strong>ABSTRACT</strong><br>The world wide web contains rich textual contents that areinterconnected via complex hyperlinks. This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample. It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure. The research in this direction has recently received considerable attention but are still in an early stage. Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features. Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors. Further analysis can be performed based on the compact representation of web pages. In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Graph Embedding and Extensions: A General Framework for Dimensionality Reduction (Shuicheng Yan et al., 2007)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Shuicheng Yan, Dong Xu, Benyu Zhang, HongJiang Zhang, Qiang Yang, Stephen Lin. (2007)<br><strong>Graph Embedding and Extensions: A General Framework for Dimensionality Reduction</strong><br>IEEE Transactions on Pattern Analysis and Machine Intelligence<br><a href=https://www.semanticscholar.org/paper/69381b5efd97e7c55f51c2730caccab3d632d4d2>Paper Link</a><br>Influential Citation Count (311), SS-ID (69381b5efd97e7c55f51c2730caccab3d632d4d2)</p><p><strong>ABSTRACT</strong><br>A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identification (Ting Chen et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Ting Chen, Yizhou Sun. (2016)<br><strong>Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identification</strong><br>WSDM<br><a href=https://www.semanticscholar.org/paper/6b183d2297cb493a57dbc875689ab2430d870043>Paper Link</a><br>Influential Citation Count (14), SS-ID (6b183d2297cb493a57dbc875689ab2430d870043)</p><p><strong>ABSTRACT</strong><br>In this paper, we study the problem of author identification under double-blind review setting, which is to identify potential authors given information of an anonymized paper. Different from existing approaches that rely heavily on feature engineering, we propose to use network embedding approach to address the problem, which can automatically represent nodes into lower dimensional feature vectors. However, there are two major limitations in recent studies on network embedding: (1) they are usually general-purpose embedding methods, which are independent of the specific tasks; and (2) most of these approaches can only deal with homogeneous networks, where the heterogeneity of the network is ignored. Hence, challenges faced here are two folds: (1) how to embed the network under the guidance of the author identification task, and (2) how to select the best type of information due to the heterogeneity of the network. To address the challenges, we propose a task-guided and path-augmented heterogeneous network embedding model. In our model, nodes are first embedded as vectors in latent feature space. Embeddings are then shared and jointly trained according to task-specific and network-general objectives. We extend the existing unsupervised network embedding to incorporate meta paths in heterogeneous networks, and select paths according to the specific task. The guidance from author identification task for network embedding is provided both explicitly in joint training and implicitly during meta path selection. Our experiments demonstrate that by using path-augmented network embedding with task guidance, our model can obtain significantly better accuracy at identifying the true authors comparing to existing methods.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Graphs over time: densification laws, shrinking diameters and possible explanations (J. Leskovec et al., 2005)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>J. Leskovec, J. Kleinberg, C. Faloutsos. (2005)<br><strong>Graphs over time: densification laws, shrinking diameters and possible explanations</strong><br>KDD &lsquo;05<br><a href=https://www.semanticscholar.org/paper/788b6f36a2b7cab86a5a29000e8b7cde25b85e73>Paper Link</a><br>Influential Citation Count (182), SS-ID (788b6f36a2b7cab86a5a29000e8b7cde25b85e73)</p><p><strong>ABSTRACT</strong><br>How do real graphs evolve over time? What are &ldquo;normal&rdquo; growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time.Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time, with the number of edges growing super-linearly in the number of nodes. Second, the average distance between nodes often shrinks over time, in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)).Existing graph generation models do not exhibit these types of behavior, even at a qualitative level. We provide a new graph generator, based on a &ldquo;forest fire&rdquo; spreading process, that has a simple, intuitive justification, requires very few parameters (like the &ldquo;flammability&rdquo; of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Structural Deep Embedding for Hyper-Networks (Ke Tu et al., 2017)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Ke Tu, Peng Cui, Xiao Wang, Fei Wang, Wenwu Zhu. (2017)<br><strong>Structural Deep Embedding for Hyper-Networks</strong><br>AAAI<br><a href=https://www.semanticscholar.org/paper/7b9cdf953223aa27ea548fa3a62d77d67723b0e2>Paper Link</a><br>Influential Citation Count (12), SS-ID (7b9cdf953223aa27ea548fa3a62d77d67723b0e2)</p><p><strong>ABSTRACT</strong><br>Network embedding has recently attracted lots of attentions in data mining. Existing network embedding methods mainly focus on networks with pairwise relationships. In real world, however, the relationships among data points could go beyond pairwise, i.e., three or more objects are involved in each relationship represented by a hyperedge, thus forming hyper-networks. These hyper-networks pose great challenges to existing network embedding methods when the hyperedges are indecomposable, that is to say, any subset of nodes in a hyperedge cannot form another hyperedge. These indecomposable hyperedges are especially common in heterogeneous networks. In this paper, we propose a novel Deep Hyper-Network Embedding (DHNE) model to embed hyper-networks with indecomposable hyperedges. More specifically, we theoretically prove that any linear similarity metric in embedding space commonly used in existing methods cannot maintain the indecomposibility property in hyper-networks, and thus propose a new deep model to realize a non-linear tuplewise similarity function while preserving both local and global proximities in the formed embedding space. We conduct extensive experiments on four different types of hyper-networks, including a GPS network, an online social network, a drug network and a semantic network. The empirical results demonstrate that our method can significantly and consistently outperform the state-of-the-art algorithms.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Latent Space Approaches to Social Network Analysis (Peter D. Hoff et al., 2002)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Peter D. Hoff, A. Raftery, M. Handcock. (2002)<br><strong>Latent Space Approaches to Social Network Analysis</strong></p><p><a href=https://www.semanticscholar.org/paper/82e4390c043754d5af22d48964a42a891f81e8b3>Paper Link</a><br>Influential Citation Count (177), SS-ID (82e4390c043754d5af22d48964a42a891f81e8b3)</p><p><strong>ABSTRACT</strong><br>Network models are widely used to represent relational information among interacting units. In studies of social networks, recent emphasis has been placed on random graph models where the nodes usually represent individual social actors and the edges represent the presence of a specified relation between actors. We develop a class of models where the probability of a relation between actors depends on the positions of individuals in an unobserved “social space.” We make inference for the social space within maximum likelihood and Bayesian frameworks, and propose Markov chain Monte Carlo procedures for making inference on latent positions and the effects of observed covariates. We present analyses of three standard datasets from the social networks literature, and compare the method to an alternative stochastic blockmodeling approach. In addition to improving on model fit for these datasets, our method provides a visual and interpretable model-based spatial representation of social relationships and improves on existing methods by allowing the statistical uncertainty in the social space to be quantified and graphically represented.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Unsupervised Large Graph Embedding (F. Nie et al., 2017)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>F. Nie, Wei Zhu, Xuelong Li. (2017)<br><strong>Unsupervised Large Graph Embedding</strong><br>AAAI<br><a href=https://www.semanticscholar.org/paper/9ad503ff70a2b3a1ddebc96683ed73c7fcd0840b>Paper Link</a><br>Influential Citation Count (6), SS-ID (9ad503ff70a2b3a1ddebc96683ed73c7fcd0840b)</p><p><strong>ABSTRACT</strong><br>There are many successful spectral based unsupervised dimensionality reduction methods, including Laplacian Eigenmap (LE), Locality Preserving Projection (LPP), Spectral Regression (SR), etc. LPP and SR are two different linear spectral based methods, however, we discover that LPP and SR are equivalent, if the symmetric similarity matrix is doubly stochastic, Positive Semi-Definite (PSD) and with rank p, where p is the reduced dimension. The discovery promotes us to seek low-rank and doubly stochastic similarity matrix, we then propose an unsupervised linear dimensionality reduction method, called Unsupervised Large Graph Embedding (ULGE). ULGE starts with similar idea as LPP, it adopts an efficient approach to construct similarity matrix and then performs spectral analysis efficiently, the computational complexity can reduce to O(ndm), which is a significant improvement compared to conventional spectral based methods which need O(n^2d) at least, where n, d and m are the number of samples, dimensions and anchors, respectively. Extensive experiments on several public available data sets demonstrate the efficiency and effectiveness of the proposed method.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering (Mikhail Belkin et al., 2001)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Mikhail Belkin, P. Niyogi. (2001)<br><strong>Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</strong><br>NIPS<br><a href=https://www.semanticscholar.org/paper/9d16c547d15a08091e68c86a99731b14366e3f0d>Paper Link</a><br>Influential Citation Count (370), SS-ID (9d16c547d15a08091e68c86a99731b14366e3f0d)</p><p><strong>ABSTRACT</strong><br>Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Towards a Generalized Singular Value Decomposition (C. Paige et al., 1981)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>C. Paige, M. Saunders. (1981)<br><strong>Towards a Generalized Singular Value Decomposition</strong></p><p><a href=https://www.semanticscholar.org/paper/9d2e9f807557b5cb50340402a0df45da53e5ba13>Paper Link</a><br>Influential Citation Count (59), SS-ID (9d2e9f807557b5cb50340402a0df45da53e5ba13)</p><p><strong>ABSTRACT</strong><br>We suggest a form for, and give a constructive derivation of, the generalized singular value decomposition of any two matrices having the same number of columns. We outline its desirable characteristics and compare it to an earlier suggestion by Van Loan [SIAM J. Numer. Anal., 13 (1976), pp. 76–83]. The present form largely follows from the work of Van Loan, but is slightly more general and computationally more amenable than that in the paper cited. We also prove a useful extension of a theorem of Stewart [SIAM Rev. 19 (1977), pp. 634–662] on unitary decompositions of submatrices of a unitary matrix.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Relational learning via latent social dimensions (Lei Tang et al., 2009)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Lei Tang, Huan Liu. (2009)<br><strong>Relational learning via latent social dimensions</strong><br>KDD<br><a href=https://www.semanticscholar.org/paper/a505e4c2bf30cd88afe483f7541409e2ba5ab3d4>Paper Link</a><br>Influential Citation Count (74), SS-ID (a505e4c2bf30cd88afe483f7541409e2ba5ab3d4)</p><p><strong>ABSTRACT</strong><br>Social media such as blogs, Facebook, Flickr, etc., presents data in a network format rather than classical IID distribution. To address the interdependency among data instances, relational learning has been proposed, and collective inference based on network connectivity is adopted for prediction. However, connections in social media are often multi-dimensional. An actor can connect to another actor for different reasons, e.g., alumni, colleagues, living in the same city, sharing similar interests, etc. Collective inference normally does not differentiate these connections. In this work, we propose to extract latent social dimensions based on network information, and then utilize them as features for discriminative learning. These social dimensions describe diverse affiliations of actors hidden in the network, and the discriminative learning can automatically determine which affiliations are better aligned with the class labels. Such a scheme is preferred when multiple diverse relations are associated with the same network. We conduct extensive experiments on social media data (one from a real-world blog site and the other from a popular content sharing site). Our model outperforms representative relational learning methods based on collective inference, especially when few labeled data are available. The sensitivity of this model and its connection to existing methods are also examined.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Nonlinear dimensionality reduction by locally linear embedding. (S. Roweis et al., 2000)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>S. Roweis, L. Saul. (2000)<br><strong>Nonlinear dimensionality reduction by locally linear embedding.</strong><br>Science<br><a href=https://www.semanticscholar.org/paper/afcd6da7637ddeef6715109aca248da7a24b1c65>Paper Link</a><br>Influential Citation Count (1579), SS-ID (afcd6da7637ddeef6715109aca248da7a24b1c65)</p><p><strong>ABSTRACT</strong><br>Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>GraRep: Learning Graph Representations with Global Structural Information (Shaosheng Cao et al., 2015)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Shaosheng Cao, Wei Lu, Qiongkai Xu. (2015)<br><strong>GraRep: Learning Graph Representations with Global Structural Information</strong><br>CIKM<br><a href=https://www.semanticscholar.org/paper/c2fd72cb2a77941e655b5d949d0d59b01e173c3b>Paper Link</a><br>Influential Citation Count (142), SS-ID (c2fd72cb2a77941e655b5d949d0d59b01e173c3b)</p><p><strong>ABSTRACT</strong><br>In this paper, we present {GraRep}, a novel model for learning vertex representations of weighted graphs. This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. We also formally analyze the connections between our work and several previous research efforts, including the DeepWalk model of Perozzi et al. as well as the skip-gram model with negative sampling of Mikolov et al. We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization. Empirical results demonstrate that our representation significantly outperforms other state-of-the-art methods in such tasks.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>A Survey on Network Embedding (Peng Cui et al., 2017)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Peng Cui, Xiao Wang, J. Pei, Wenwu Zhu. (2017)<br><strong>A Survey on Network Embedding</strong><br>IEEE Transactions on Knowledge and Data Engineering<br><a href=https://www.semanticscholar.org/paper/ce840188f3395815201b7da49f9bb40d24fc046a>Paper Link</a><br>Influential Citation Count (31), SS-ID (ce840188f3395815201b7da49f9bb40d24fc046a)</p><p><strong>ABSTRACT</strong><br>Network embedding assigns nodes in a network to low-dimensional representations and effectively preserves the network structure. Recently, a significant amount of progresses have been made toward this emerging network analysis paradigm. In this survey, we focus on categorizing and then reviewing the current development on network embedding methods, and point out its future research directions. We first summarize the motivation of network embedding. We discuss the classical graph embedding algorithms and their relationship with network embedding. Afterwards and primarily, we provide a comprehensive overview of a large number of network embedding methods in a systematic manner, covering the structure- and property-preserving network embedding methods, the network embedding methods with side information, and the advanced information preserving network embedding methods. Moreover, several evaluation approaches for network embedding and some useful online resources, including the network data sets and softwares, are reviewed, too. Finally, we discuss the framework of exploiting these network embedding methods to build an effective system and point out some potential future directions.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Structural Deep Network Embedding (Daixin Wang et al., 2016)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Daixin Wang, Peng Cui, Wenwu Zhu. (2016)<br><strong>Structural Deep Network Embedding</strong><br>KDD<br><a href=https://www.semanticscholar.org/paper/d0b7c8828f0fca4dd901674e8fb5bd464a187664>Paper Link</a><br>Influential Citation Count (241), SS-ID (d0b7c8828f0fca4dd901674e8fb5bd464a187664)</p><p><strong>ABSTRACT</strong><br>Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>An Introduction to Linear Algebra (J. Esser, 2006)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>J. Esser. (2006)<br><strong>An Introduction to Linear Algebra</strong></p><p><a href=https://www.semanticscholar.org/paper/d2af753f6ae3711ed6fcd6fdac5bba70b7dae3a8>Paper Link</a><br>Influential Citation Count (66), SS-ID (d2af753f6ae3711ed6fcd6fdac5bba70b7dae3a8)</p><p><strong>ABSTRACT</strong><br>Class notes on vectors, linear combination, basis, span. 1 Vectors Vectors on the plane are ordered pairs of real numbers (a, b) such as (0, 1), (1, 0), (1, 2), (−1, 1). The plane is denoted by R, also known as Euclidean 2-space. Vectors in our physical space are ordered triples (a, b, c) such as (1, 0, 0), (0, 0, 1), (0, 1, 0), (1, 1, 2), (1,−1, 2). All such vectors form Euclidean 3-space, or R. In general, the set of all ordered n-tuples (x1, x2, · · · , xn) is Euclidean n-space. 1.1 Addition and Scalar Multiplication Let v = [v1, v2, · · · , vn] and w = [w1, w2, · · · , wn] be two vectors in R. Then vector addition is: v + w = [v1 + w1, v2 + w2, · · · , vn + wn], vector subtraction is: v − w = [v1 − w1, v2 − w2, · · · , vn − wn]. For any number r (scalar), vector scalar multiplication is: r v = [r v1, r v2, · · · , r vn]. Two nonzero vectors v and w in R are said to be parallel if one is a scalar mulplication of the other, v = r w. If r > 0 (r &lt; 0), they point in the same (opposite) direction. Example 1: are the two vectors v = [2, 1, 3,−5], and w = [6, 3, 9,−15] parallel ? Yes, w = 3 v. ∗Department of Mathematics, UCI, Irvine, CA 92617. 1 1.2 Linear Combination Given vectors v, v, · · · , v in R, and scalars r1, r2, · · · , rk in R, the vector: r1 v 1 + r2 v 2 + · · ·+ rk v, (1.1) is called a linear combination of the vectors v, v, · · · , v with scalar coefficients r1, r2, · · · , rk. Example 2: any vector [a1, a2] in R 2 can be expressed as a unique linear combination of the two vectors [1, 0] and [0, 1]: [a1, a2] = r1[1, 0] + r2[0, 1], (1.2) if and only if r1 = a1, r2 = a2. We call [1, 0] and [0, 1] standard basis vectors of R , often denoted by e and e. Similarly, standard basis vectors in R are: e = [1, 0, 0], e = [0, 1, 0], e = [0, 0, 1]. Standard basis vectors of R are e, e, · · · , e, where e, 1 ≤ j ≤ n, is the vector with zero components except that the j-th component equals 1. Each vector in R is a unique linear combination of the standard basis vectors. 1.3 Span Let v, v, · · · , v be vectors in R. The span of these vectors is the set of all linear combinations of them, and is denoted by sp(v, v, · · · , v). Example 3: the span of [1, 0] and [0, 1] is R. The span of v = [1,−2] and v = [7,−14] is the line along [1,−2] instead of R, because v is a scalar multiple of v (or v is in sp(v)). Example 4: Let v = [1, 3], w = [−2, 5], determine if [−1, 19] is in sp(v,w) and if so the coefficients of linear combination. The problem is same as finding a solution to [1,−19] = r v + s w = r[1, 3] + s[−2, 5] for two real numbers r and s. It follows from comparing the components that: r − 2s = −1, 3r + 5s = 19. Eliminating the r variable gives: 0 + 11s = 22, s = 2, then r = 3. Solution is unique. 2 1.4 Matlab Exercises Here are hands-on Matlab Exercises. Exercise 1: enter vectors in Matlab: v = [1 2 3 4]</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Community Preserving Network Embedding (Xiao Wang et al., 2017)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Xiao Wang, Peng Cui, Jing Wang, J. Pei, Wenwu Zhu, Shiqiang Yang. (2017)<br><strong>Community Preserving Network Embedding</strong><br>AAAI<br><a href=https://www.semanticscholar.org/paper/d3e0d596efd9d19b93d357565a68dfa925dce2bb>Paper Link</a><br>Influential Citation Count (56), SS-ID (d3e0d596efd9d19b93d357565a68dfa925dce2bb)</p><p><strong>ABSTRACT</strong><br>Network embedding, aiming to learn the low-dimensional representations of nodes in networks, is of paramount importance in many real applications. One basic requirement of network embedding is to preserve the structure and inherent properties of the networks. While previous network embedding methods primarily preserve the microscopic structure, such as the first- and second-order proximities of nodes, the mesoscopic community structure, which is one of the most prominent feature of networks, is largely ignored. In this paper, we propose a novel Modularized Nonnegative Matrix Factorization (M-NMF) model to incorporate the community structure into network embedding. We exploit the consensus relationship between the representations of nodes and community structure, and then jointly optimize NMF based representation learning model and modularity based community detection model in a unified framework, which enables the learned representations of nodes to preserve both of the microscopic and community structures. We also provide efficient updating rules to infer the parameters of our model, together with the correctness and convergence guarantees. Extensive experimental results on a variety of real-world networks show the superior performance of the proposed method over the state-of-the-arts.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>HINE: Heterogeneous Information Network Embedding (Yuxin Chen et al., 2017)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation>Yuxin Chen, Chenguang Wang. (2017)<br><strong>HINE: Heterogeneous Information Network Embedding</strong><br>DASFAA<br><a href=https://www.semanticscholar.org/paper/d8fbd38ec3c8aedf9964882a73e84d8a540de4b9>Paper Link</a><br>Influential Citation Count (0), SS-ID (d8fbd38ec3c8aedf9964882a73e84d8a540de4b9)</p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Multiplicative latent factor models for description and prediction of social networks (Peter D. Hoff, 2009)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation>Peter D. Hoff. (2009)<br><strong>Multiplicative latent factor models for description and prediction of social networks</strong><br>Comput. Math. Organ. Theory<br><a href=https://www.semanticscholar.org/paper/dbe30a96b7db2df4e8f6c3492e2092c68feedcd6>Paper Link</a><br>Influential Citation Count (19), SS-ID (dbe30a96b7db2df4e8f6c3492e2092c68feedcd6)</p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>Fast Eigen-Functions Tracking on Dynamic Graphs (C. Chen et al., 2015)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>C. Chen, Hanghang Tong. (2015)<br><strong>Fast Eigen-Functions Tracking on Dynamic Graphs</strong><br>SDM<br><a href=https://www.semanticscholar.org/paper/ddc3dc43095173a6b5f0a6f5fbdde79e1abac34d>Paper Link</a><br>Influential Citation Count (3), SS-ID (ddc3dc43095173a6b5f0a6f5fbdde79e1abac34d)</p><p><strong>ABSTRACT</strong><br>Many important graph parameters can be expressed as eigenfunctions of its adjacency matrix. Examples include epidemic threshold, graph robustness, etc. It is often of key importance to accurately monitor these parameters. For example, knowing that Ebola virus has already been brought to the US continent, to avoid the virus from spreading away, it is important to know which emerging connections among related people would cause great reduction on the epidemic threshold of the network. However, most, if not all, of the existing algorithms computing these measures assume that the input graph is static, despite the fact that almost all real graphs are evolving over time. In this paper, we propose two online algorithms to track the eigen-functions of a dynamic graph with linear complexity wrt the number of nodes and number of changed edges in the graph. The key idea is to leverage matrix perturbation theory to efficiently update the top eigen-pairs of the underlying graph without recomputing them from scratch at each time stamp. Experiment results demonstrate that our methods can reach up to 20× speedup with precision more than 80% for fairly long period of time.</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>A new status index derived from sociometric analysis (L. Katz, 1953)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>L. Katz. (1953)<br><strong>A new status index derived from sociometric analysis</strong></p><p><a href=https://www.semanticscholar.org/paper/f1d40a639d7f83d373f07b2bf4a96f0313b584d8>Paper Link</a><br>Influential Citation Count (240), SS-ID (f1d40a639d7f83d373f07b2bf4a96f0313b584d8)</p></p class="details-citation"></div></div></details><style>div.details-citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}div.details-citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}div.details-citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}div.details-citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}div.details-citation cite{display:block;text-align:right;color:#888;font-size:.9em}summary.details-summary{cursor:pointer;transition:.2s}summary:hover{cursor:pointer;background-color:#ebf0f5}details.details-summary{margin-top:.25rem}details.details-summary[open] .details-summary-content{animation:fadeIn .5s ease}@keyframes fadeIn{0%{opacity:0;transform:translateY(-10px)}100%{opacity:1;transform:none}}</style><details class=details-summary><summary class=details-summary>DeepWalk: online learning of social representations (Bryan Perozzi et al., 2014)</summary><div class=details-summary-content><div class=details-citation><p class=details-citation><p>Bryan Perozzi, Rami Al-Rfou, S. Skiena. (2014)<br><strong>DeepWalk: online learning of social representations</strong><br>KDD<br><a href=https://www.semanticscholar.org/paper/fff114cbba4f3ba900f33da574283e3de7f26c83>Paper Link</a><br>Influential Citation Count (1396), SS-ID (fff114cbba4f3ba900f33da574283e3de7f26c83)</p><p><strong>ABSTRACT</strong><br>We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk&rsquo;s latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk&rsquo;s representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk&rsquo;s representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.</p></p class="details-citation"></div></div></details></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>CodeBERT: A Pre-Trained Model for Programming and Natural Languages</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#citation>Citation</a></li><li><a href=#abstract>Abstract</a></li><li><a href=#background--whats-new>Background & What&rsquo;s New</a></li><li><a href=#dataset>Dataset</a></li><li><a href=#model-description>Model Description</a><ul><li></li></ul></li><li><a href=#results>Results</a><ul><li><a href=#settings>Settings</a></li><li><a href=#effectiveness-of-the-static-model>Effectiveness of the Static Model</a></li><li><a href=#effectiveness-of-the-dynamic-model>Effectiveness of the Dynamic Model</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script>
<script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script>
<script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script>
<script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script>
<script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script>
<script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>