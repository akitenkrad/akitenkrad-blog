<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2023.05 on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/</link><description>Recent content in 2023.05 on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Sat, 13 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/index.xml" rel="self" type="application/rss+xml"/><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/20230513094804/</link><pubDate>Sat, 13 May 2023 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/20230513094804/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Alec, R., &amp;amp; Karthik, N. (2018). Improving Language Understanding by Generative Pre-Training. OpenAI. https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035 Abstract We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus.</description></item></channel></rss>