<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Papers on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/</link><description>Recent content in Papers on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Tue, 02 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/blog-akitenkrad/posts/papers/index.xml" rel="self" type="application/rss+xml"/><item><title>Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202208/20220802103319/</link><pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202208/20220802103319/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Xue, H., Yang, L., Jiang, W., Wei, Y., Hu, Y., &amp;amp; Lin, Y. (2021). Modeling Dynamic Heterogeneous Network for Link Prediction Using Hierarchical Attention with Temporal RNN. Springer, Cham. pp.282–298. https://doi.org/10.1007/978-3-030-67658-2_17 Abstract Network embedding aims to learn low-dimensional representations of nodes while capturing structure information of networks. It has achieved great success on many tasks of network analysis such as link prediction and node classification.</description></item><item><title>Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202207/20220727145036/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202207/20220727145036/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Yang, L., Xiao, Z., Jiang, W., Wei, Y., Hu, Y., &amp;amp; Wang, H. (2020).
Dynamic heterogeneous graph embedding using hierarchical attentions.
Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 12036 LNCS.
https://doi.org/10.1007/978-3-030-45442-5_53 Abstract Graph embedding has attracted many research interests. Existing works mainly focus on static homogeneous/heterogeneous networks or dynamic homogeneous networks.</description></item><item><title>Dynamic Network Embedding Survey</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202207/20220726163444/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202207/20220726163444/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Xue, G., Zhong, M., Li, J., Chen, J., Zhai, C., &amp;amp; Kong, R. (2022)
Dynamic network embedding survey
Neurocomputing, 472, 212–223. https://doi.org/10.1016/J.NEUCOM.2021.03.138 Abstract Since many real world networks are evolving over time, such as social networks and user-item networks, there are increasing research efforts on dynamic network embedding in recent years. They learn node representations from a sequence of evolving graphs but not only the latest network, for preserving both structural and temporal information from the dynamic networks.</description></item><item><title>High-order Proximity Preserved Embedding for Dynamic Networks</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220618223844/</link><pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220618223844/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Zhu, D., Cui, P., Zhang, Z., Pei, J., &amp;amp; Zhu, W. (2018).
High-Order Proximity Preserved Embedding for Dynamic Networks.
IEEE Transactions on Knowledge and Data Engineering, 30(11), 2134–2144.
https://doi.org/10.1109/TKDE.2018.2822283 Abstract Network embedding, aiming to embed a network into a low dimensional vector space while preserving the inherent structural properties of the network, has attracted considerable attention. However, most existing embedding methods focus on the static network while neglecting the evolving characteristic of real-world networks.</description></item><item><title>Attributed Network Embedding for Learning in a Dynamic Environment</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220612105422/</link><pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220612105422/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, and Huan Liu. 2017.
Attributed Network Embedding for Learning in a Dynamic Environment.
In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM &amp;lsquo;17). Association for Computing Machinery, New York, NY, USA, 387–396.
https://doi.org/10.1145/3132847.3132919 Abstract Network embedding leverages the node proximity manifested to learn a low-dimensional node vector representation for each node in the network.</description></item><item><title>CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220608085622/</link><pubDate>Wed, 08 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220608085622/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., &amp;amp; Zhou, M. (2020).
CodeBERT: A Pre-Trained Model for Programming and Natural Languages.
Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020, 1536–1547.
https://doi.org/10.18653/V1/2020.FINDINGS-EMNLP.139 Abstract We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL).</description></item><item><title>S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220602171700/</link><pubDate>Thu, 02 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220602171700/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du,Weifeng Lv, and Ming Zhou. 2018.
S-Net: Fromanswer extraction to answer synthesis for machinereading comprehension.
InAssociation for the Ad-vancement of Artificial Intelligence (AAAI), pages5940–5947. Abstract In this paper, we present a novel approach to machine reading comprehension forthe MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a ques-tion with exact text spans in a passage, the MS-MARCO dataset defines the taskas answering a question from multiple passages and the words in the answer arenot necessary in the passages.</description></item><item><title>Neural Machine Translation of Rare Words with Subword Units</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220530102936/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220530102936/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2015).
Neural Machine Translation of Rare Words with Subword Units.
https://doi.org/10.48550/arxiv.1508.07909 Abstract Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units.</description></item><item><title>Attention Is All You Need</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220529131339/</link><pubDate>Sun, 29 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220529131339/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp;amp; Polosukhin, I. (2017).
Attention is all you need.
Advances in Neural Information Processing Systems, 2017-Decem, 5999–6009.
http://arxiv.org/abs/1706.03762 Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism.</description></item><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220525115521/</link><pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220525115521/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Abstract Background &amp;amp; Wat&amp;rsquo;s New Dataset Model Description Training Settings Results References Semi-Supervised Text Classification Using EM (O. Chapelle et al., 2006) O. Chapelle, Bernhard SchÃ¶lkopf, A. Zien. (2006)
Semi-Supervised Text Classification Using EM
Paper Link
Influential Citation Count (2), SS-ID (03bafef700d35112a9926dd1b2be91a4aa6984a4)
ABSTRACT
This chapter contains sections titled: Introduction, A Generative Model for Text, Experimental Results with Basic EM, Using a More Expressive Generative Model, Overcoming the Challenges of Local Maxima, Conclusions and Summary</description></item><item><title>RoBERTa: A Robustly Optimized BERT Pretraining Approach</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220523223206/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220523223206/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp;amp; Stoyanov, V. (2019).
RoBERTa: A Robustly Optimized BERT Pretraining Approach.
https://doi.org/10.48550/arxiv.1907.11692 Abstract Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results.</description></item><item><title>Semi-Supervised Classification with Graph Convolutional Networks</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220520124748/</link><pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220520124748/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Kipf, T. N., &amp;amp; Welling, M. (2016).
Semi-Supervised Classification with Graph Convolutional Networks.
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2019)
https://doi.org/10.48550/arxiv.1609.02907 Abstract We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions.</description></item><item><title>A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220518224923/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220518224923/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Jeong, C., Jang, S., Shin, H., Park, E., &amp;amp; Choi, S. (2019).
A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks.
https://doi.org/10.48550/arxiv.1903.06464 Abstract With the tremendous growth in the number of scientific papers being published, searching for references while writing a scientific paper is a time-consuming process. A technique that could add a reference citation at the appropriate place in a sentence will be beneficial.</description></item><item><title>UnitedQA: A Hybrid Approach for Open Domain Question Answering</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220514151839/</link><pubDate>Sat, 14 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220514151839/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Cheng, H., Shen, Y., Liu, X., He, P., Chen, W., &amp;amp; Gao, J. (2021).
UnitedQA: A Hybrid Approach for Open Domain Question Answering.
https://doi.org/10.48550/arxiv.2101.00178 Abstract To date, most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively. In this paper, we study a hybrid approach for leveraging the strengths of both models.</description></item><item><title>Multi-Style Generative Reading Comprehension</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220511010217/</link><pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220511010217/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Nishida, K., Saito, I., Nishida, K., Shinoda, K., Otsuka, A., Asano, H., &amp;amp; Tomita, J. (2020).
Multi-style Generative Reading Comprehension.
ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 2273–2284.
https://doi.org/10.18653/v1/p19-1220 Abstract This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We proposea multi-style abstractive summarization model for question answering, called Masque.</description></item><item><title>Survey on graph embeddings and their applications to machine learning problems on graphs</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220509110738/</link><pubDate>Mon, 09 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220509110738/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Makarov, I., Kiselev, D., Nikitinsky, N., &amp;amp; Subelj, L. (2021).
Survey on graph embeddings and their applications to machine learning problems on graphs.
PeerJ Computer Science, 7, 1–62.
Paper Link Abstract Dealing with relational data always required significant computational resources,domain expertise and task-dependent feature engineering to incorporate structuralinformation into a predictive model. Nowadays, a family of automated graphfeature engineering techniques has been proposed in different streams of literature.</description></item><item><title>A Deep Cascade Model for Multi-Document Reading Comprehension</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220508162318/</link><pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220508162318/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Yan, M., Xia, J., Wu, C., Bi, B., Zhao, Z., Zhang, J., Si, L., Wang, R., Wang, W., &amp;amp; Chen, H. (2019).
A Deep Cascade Model for Multi-Document Reading Comprehension.
Proceedings of the AAAI Conference on Artificial Intelligence, 33, 7354–7361.
https://doi.org/10.1609/aaai.v33i01.33017354 Abstract A fundamental trade-off between effectiveness and efficiency needs to be balanced when designing an online question answering system. Effectiveness comes from sophisticated functions such as extractive machine reading comprehension (MRC), while efficiency is obtained from improvements in preliminary retrieval components such as candidate document selection and paragraph ranking.</description></item><item><title>A Primer in BERTology: What We Know About How BERT Works</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220506021208/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220506021208/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Rogers, A., Kovaleva, O., &amp;amp; Rumshisky, A. (2020).
A Primer in BERTology: What We Know About How BERT Works.
Transactions of the Association for Computational Linguistics, 8, 842–866.
Paper Link Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model.</description></item><item><title>Dense Passage Retrieval for Open-Domain Question Answering</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220505222900/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220505222900/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., &amp;amp; Yih, W. (2020).
Dense Passage Retrieval for Open-Domain Question Answering.
Paper Link Abstract Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework.</description></item><item><title>DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220503010000/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220503010000/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation He, P., Liu, X., Gao, J., &amp;amp; Chen, W. (2020).
DeBERTa: Decoding-enhanced BERT with Disentangled Attention
Paper Link Abstract Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques.</description></item></channel></rss>