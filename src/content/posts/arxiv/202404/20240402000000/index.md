---
draft: false
title: "arXiv @ 2024.04.02"
date: 2024-04-02
author: "akitenkrad"
description: ""
tags: ["arXiv", "Published:2024"]
menu:
  sidebar:
    name: "arXiv @ 2024.04.02"
    identifier: arxiv_20240402
    parent: 202404_arxiv
    weight: 10
math: true
---

<figure style="border:none; width:100%; display:flex; justify-content: center">
    <iframe src="pie.html" width=900 height=620 style="border:none"></iframe>
</figure>


## Primary Categories

- [cs.AI (3)](#csai-3)
- [cs.AR (1)](#csar-1)
- [cs.CL (31)](#cscl-31)
- [cs.CR (2)](#cscr-2)
- [cs.CV (39)](#cscv-39)
- [cs.CY (1)](#cscy-1)
- [cs.DB (2)](#csdb-2)
- [cs.GT (1)](#csgt-1)
- [cs.HC (2)](#cshc-2)
- [cs.IR (4)](#csir-4)
- [cs.IT (4)](#csit-4)
- [cs.LG (27)](#cslg-27)
- [cs.MA (1)](#csma-1)
- [cs.OS (1)](#csos-1)
- [cs.RO (6)](#csro-6)
- [cs.SD (3)](#cssd-3)
- [cs.SE (5)](#csse-5)
- [cs.SI (1)](#cssi-1)
- [econ.GN (1)](#econgn-1)
- [eess.AS (1)](#eessas-1)
- [eess.IV (3)](#eessiv-3)
- [eess.SY (4)](#eesssy-4)
- [math.OC (1)](#mathoc-1)
- [physics.optics (1)](#physicsoptics-1)

## Keywords

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>keyword</th>
      <th>cs.CL</th>
      <th>cs.CV</th>
      <th>cs.LG</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Adversarial Attack</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Anomaly Detection</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Autoencoder</td>
      <td></td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Automatic Evaluation</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Automatic Speech Recognition</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Bandit Algorithm</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Benchmarking</td>
      <td>7</td>
      <td>12</td>
      <td>8</td>
    </tr>
    <tr>
      <td>Claude</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Clustering</td>
      <td></td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Code Generation</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Continual Learning</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Contrastive Learning</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>ControlNet</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Convolution</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Convolutional Neural Network</td>
      <td>2</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Coreference Resolution</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Curriculum Learning</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Data Augmentation</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Dense Retrieval</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Diffusion Model</td>
      <td></td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Direct Preference Optimization</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Disambiguation</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Distribution Shift</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Domain Adaptation</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Emotion Recognition</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Face Recognition</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Fairness</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fake News Detection</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Few-shot</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Fine-tuning</td>
      <td>7</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Foundation Model</td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>GLUE</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT</td>
      <td>5</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-3</td>
      <td>3</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-3.5</td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-4</td>
      <td>5</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Generative AI</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Generative Adversarial Network</td>
      <td></td>
      <td></td>
      <td>3</td>
    </tr>
    <tr>
      <td>Geometry</td>
      <td></td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Graph</td>
      <td></td>
      <td>2</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Graph Attention Networks</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Classification</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Graph Embedding</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Graph Neural Network</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Grounding</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Image2text</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>In-context Learning</td>
      <td>7</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Information Retrieval</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Instruction Tuning</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Key Point Analysis</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Distillation</td>
      <td></td>
      <td>7</td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Graph</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Knowledge Transfer</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>LLaMA</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>LSTM</td>
      <td>3</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Large Language Model</td>
      <td>28</td>
      <td>7</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Lemmatization</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Low-Resource</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>MNIST</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Machine Unlearning</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Meta Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Multi-modal</td>
      <td>6</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Node Classification</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td></td>
      <td>5</td>
      <td></td>
    </tr>
    <tr>
      <td>Out-of-distribution</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Out-of-domain</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Perplexity</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Pre-trained Language Model</td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Prompt</td>
      <td>3</td>
      <td>8</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Prompt Learning</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Quantization</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Question Answering</td>
      <td>4</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Reasoning</td>
      <td>3</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Reconstruction Loss</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Reinforcement Learning</td>
      <td>1</td>
      <td></td>
      <td>5</td>
    </tr>
    <tr>
      <td>Reinforcement Learning from Human Feedback</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Representation Learning</td>
      <td></td>
      <td></td>
      <td>3</td>
    </tr>
    <tr>
      <td>Retrieval-Augmented Generation</td>
      <td>3</td>
      <td></td>
      <td>3</td>
    </tr>
    <tr>
      <td>Sample Size</td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Self-Attention</td>
      <td></td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Self-supervised Learning</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Semi-Supervised Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Simulation</td>
      <td></td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Simulator</td>
      <td></td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td>5</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Supervised Learning</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Text Generation</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text-to-speech</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text2image</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Tokenization</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Transfer Learning</td>
      <td></td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Transformer</td>
      <td></td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Unsupervised Learning</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Variational Autoencoder</td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Vision Transformer</td>
      <td></td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Vision-and-Language</td>
      <td></td>
      <td>4</td>
      <td></td>
    </tr>
    <tr>
      <td>Visual Question Answering</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly Supervised Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly-supervised Learning</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Zero-shot</td>
      <td>2</td>
      <td>3</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<script>
$(function() {
  $("table").addClass("keyword-table table-bordered border-success");
  $("table thead").addClass("sticky-top");
  $("table tbody td").css("text-align", "");
});
</script>


## cs.CL (31)



### (1/31 | 1/145) EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories (Jia Li et al., 2024)

{{<citation>}}

Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, Zhi Jin. (2024)  
**EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories**
<br/>
<button class="copy-to-clipboard" title="EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories" index=1>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-SE, cs.CL  
Keyword Score: 93  
Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4, Code Generation, Large Language Model, Large Language Model, Prompt, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00599v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00599v1.pdf" filename="2404.00599v1.pdf">Download PDF</button>

---


**ABSTRACT**  
How to evaluate <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in <b>code</b> <b>generation</b> is an open question. Existing <b>benchmarks</b> demonstrate poor alignment with real-world <b>code</b> <b>repositories</b> and are insufficient to evaluate the coding abilities of <b>LLMs.</b> This paper proposes a new <b>benchmark</b> - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., <b>code</b> <b>distributions</b> and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference <b>code,</b> <b>and</b> reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving <b>benchmark</b> to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level <b>code</b> <b>generation</b> and evaluate 10 popular <b>LLMs</b> (e.g., <b>gpt-4,</b> <b>gpt-3.5,</b> DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these <b>LLMs</b> in real-world repositories. For example, the highest Pass@1 of <b>gpt-4</b> only is 20.73% in our experiments. We also analyze failed cases and <b>summarize</b> the shortcomings of existing <b>LLMs</b> in EvoCodeBench. We release EvoCodeBench, all <b>prompts,</b> and <b>LLMs'</b> completions for further community analysis.

{{</citation>}}


### (2/31 | 2/145) Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent Units and Deep Learning Techniques (Utsha Roy et al., 2024)

{{<citation>}}

Utsha Roy, Mst. Sazia Tahosin, Md. Mahedi Hassan, Taminul Islam, Fahim Imtiaz, Md Rezwane Sadik, Yassine Maleh, Rejwan Bin Sulaiman, Md. Simul Hasan Talukder. (2024)  
**Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent Units and Deep Learning Techniques**
<br/>
<button class="copy-to-clipboard" title="Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent Units and Deep Learning Techniques" index=2>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-LG, cs.CL  
Keyword Score: 90  
Keywords: Graph Attention Networks, Convolution, Convolutional Neural Network, Convolutional Neural Network, LSTM, LSTM, LSTM, Fake News Detection, Fake News Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.01345v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.01345v1.pdf" filename="2404.01345v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rise of <b>fake</b> <b>news</b> <b>has</b> made the need for effective detection methods, including in languages other than English, increasingly important. The study aims to address the challenges of Bangla which is considered a less important language. To this end, a complete dataset containing about 50,000 news items is proposed. Several deep learning models have been tested on this dataset, including the bidirectional <b>gated</b> recurrent unit (GRU), the <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM),</b> the 1D <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN),</b> and hybrid architectures. For this research, we assessed the efficacy of the model utilizing a range of useful measures, including recall, precision, F1 score, and accuracy. This was done by employing a big application. We carry out comprehensive trials to show the effectiveness of these models in identifying bogus news in Bangla, with the Bidirectional GRU model having a stunning accuracy of 99.16%. Our analysis highlights the importance of dataset balance and the need for continual improvement efforts to a substantial degree. This study makes a major contribution to the creation of Bangla <b>fake</b> <b>news</b> <b>detecting</b> systems with limited resources, thereby setting the stage for future improvements in the detection process.

{{</citation>}}


### (3/31 | 3/145) RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation (Chi-Min Chan et al., 2024)

{{<citation>}}

Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, Jie Fu. (2024)  
**RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation**
<br/>
<button class="copy-to-clipboard" title="RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation" index=3>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 90  
Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Disambiguation, Question Answering, In-context Learning, In-context Learning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00610v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00610v1.pdf" filename="2404.00610v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside <b>LLMs'</b> <b>in-context</b> <b>learning</b> abilities. However, existing <b>RAG</b> implementations primarily focus on initial input for context <b>retrieval,</b> <b>overlooking</b> <b>the</b> nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and <b>disambiguation.</b> Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\% across three single-hop <b>QA</b> datasets, and also demonstrates enhanced performance in handling complex, multi-hop <b>QA</b> datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.

{{</citation>}}


### (4/31 | 4/145) CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs (Jingzhe Shi et al., 2024)

{{<citation>}}

Jingzhe Shi, Jialuo Li, Qinwei Ma, Zaiwen Yang, Huan Ma, Lei Li. (2024)  
**CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs**
<br/>
<button class="copy-to-clipboard" title="CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs" index=4>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 90  
Keywords: GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Question Answering, Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.01343v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.01343v1.pdf" filename="2404.01343v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Businesses and software platforms are increasingly turning to <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> such as <b>GPT-3.5,</b> <b>GPT-4,</b> GLM-3, and <b>LLaMa-2</b> for chat assistance with file access or as <b>reasoning</b> agents for customer service. However, current <b>LLM-based</b> customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an <b>LLM</b> agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverage a combination of small and <b>large</b> <b>LLMs</b> <b>to</b> achieve satisfying performance at a reasonable inference cost. We introduce a practical dataset, the CPHOS-dataset, which includes a database, guiding files, and <b>QA</b> pairs collected from CPHOS, an online platform that facilitates the organization of simulated Physics Olympiads for high school teachers and students. We have conducted extensive experiments to validate the performance of our proposed CHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how <b>LLMs</b> can enhance or serve as alternatives to human customer service. Our code and dataset will be open-sourced soon.

{{</citation>}}


### (5/31 | 5/145) Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods (Yujuan Fu et al., 2024)

{{<citation>}}

Yujuan Fu, Giridhar Kaushik Ramachandran, Nicholas J Dobbins, Namu Park, Michael Leu, Abby R. Rosenberg, Kevin Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen. (2024)  
**Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods**
<br/>
<button class="copy-to-clipboard" title="Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods" index=5>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 80  
Keywords: Fine-tuning, Fine-tuning, GPT, GPT-4, In-context Learning, In-context Learning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00826v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00826v1.pdf" filename="2404.00826v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using <b>fine-tuned</b> and <b>in-context</b> <b>learning</b> methods with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with an overall annotator agreement of 81.9 F1. Our proposed <b>fine-tuning</b> <b>LLM-based</b> extractors achieve high performance at 78.4 F1 for event arguments. <b>In-context</b> <b>learning</b> approaches with <b>GPT-4</b> demonstrate promise for reliable SDoH extraction with limited annotated examples, with extraction performance at 82.3 F1 for event triggers.

{{</citation>}}


### (6/31 | 6/145) Extensive Self-Contrast Enables Feedback-Free Language Model Alignment (Xiao Liu et al., 2024)

{{<citation>}}

Xiao Liu, Xixuan Song, Yuxiao Dong, Jie Tang. (2024)  
**Extensive Self-Contrast Enables Feedback-Free Language Model Alignment**
<br/>
<button class="copy-to-clipboard" title="Extensive Self-Contrast Enables Feedback-Free Language Model Alignment" index=6>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 80  
Keywords: Direct Preference Optimization, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Supervised Learning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00604v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00604v1.pdf" filename="2404.00604v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> has been a central technique for recent <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> alignment. However, its heavy dependence on costly human or <b>LLM-as-Judge</b> preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free <b>large</b> <b>language</b> <b>model</b> alignment method via exploiting extensive self-generated negatives. With only <b>supervised</b> <b>fine-tuning</b> (SFT) targets, Self-Contrast leverages the <b>LLM</b> itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with <b>direct</b> <b>preference</b> <b>optimization</b> (DPO) on three datasets show that, Self-Contrast could consistently outperform SFT and standard DPO training by <b>large</b> <b>margins.</b> <b>And</b> as the number of self-generated negatives increases, the performance of Self-Contrast continues to grow. Code and data are available at https://github.com/THUDM/Self-Contrast.

{{</citation>}}


### (7/31 | 7/145) How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library (Mathieu Ravaut et al., 2024)

{{<citation>}}

Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq Joty. (2024)  
**How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library**
<br/>
<button class="copy-to-clipboard" title="How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library" index=7>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 63  
Keywords: Benchmarking, Claude, GPT, GPT-4, Question Answering, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00699v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00699v1.pdf" filename="2404.00699v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the rise of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular <b>question-answering</b> <b>benchmarks</b> could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that <b>LLMs</b> have seen; if not impossible with closed-source models like <b>GPT-4</b> and <b>Claude-3</b> not divulging any information on the training set. As a result, contamination becomes a critical issue: <b>LLMs'</b> performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address contamination, or a clear consensus on prevention, mitigation and classification of contamination. In this paper, we survey all recent work on contamination with <b>LLMs,</b> and help the community track contamination levels of <b>LLMs</b> by releasing an open-source Python library named LLMSanitize implementing major contamination detection algorithms, which link is: https://github.com/ntunlp/LLMSanitize.

{{</citation>}}


### (8/31 | 8/145) CoUDA: Coherence Evaluation via Unified Data Augmentation (Dawei Zhu et al., 2024)

{{<citation>}}

Dawei Zhu, Wenhao Wu, Yifan Song, Fangwei Zhu, Ziqiang Cao, Sujian Li. (2024)  
**CoUDA: Coherence Evaluation via Unified Data Augmentation**
<br/>
<button class="copy-to-clipboard" title="CoUDA: Coherence Evaluation via Unified Data Augmentation" index=8>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Data Augmentation, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00681v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00681v1.pdf" filename="2404.00681v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of <b>large</b> <b>language</b> <b>models.</b> Due to the scarcity of annotated <b>data,</b> <b>data</b> <b>augmentation</b> is commonly used for training coherence evaluation models. However, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance. In this paper, we take inspiration from linguistic theory of discourse structure, and propose a <b>data</b> <b>augmentation</b> framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively. Especially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. During inference, CoUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse. Extensive experiments in coherence evaluation show that, with only 233M parameters, CoUDA achieves state-of-the-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent <b>GPT-3.5</b> and <b>GPT-4</b> based metrics.

{{</citation>}}


### (9/31 | 9/145) ParaICL: Towards Robust Parallel In-Context Learning (Xingxuan Li et al., 2024)

{{<citation>}}

Xingxuan Li, Xuan-Phi Nguyen, Shafiq Joty, Lidong Bing. (2024)  
**ParaICL: Towards Robust Parallel In-Context Learning**
<br/>
<button class="copy-to-clipboard" title="ParaICL: Towards Robust Parallel In-Context Learning" index=9>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 60  
Keywords: Few-shot, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00570v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00570v1.pdf" filename="2404.00570v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become the norm in natural language processing (NLP), excelling in <b>few-shot</b> <b>in-context</b> <b>learning</b> <b>(ICL)</b> with their remarkable abilities. Nonetheless, the success of <b>ICL</b> largely hinges on the choice of <b>few-shot</b> demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve <b>ICL</b> performances. However, our preliminary experiments indicate that the effectiveness of <b>ICL</b> is limited by the length of the input context. Moreover, varying combinations of <b>few-shot</b> demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel <b>in-context</b> <b>learning</b> (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstration examples into different batches according to the semantic similarities of the questions in the demonstrations to the test question. It then computes normalized batch semantic scores for each batch. A weighted average semantic objective, constrained by adaptive plausibility, is applied to select the most appropriate tokens. Through extensive experiments, we validate the effectiveness of ParaICL and conduct ablation studies to underscore its design rationale. We further demonstrate that ParaICL can seamlessly integrate with existing methods.

{{</citation>}}


### (10/31 | 10/145) WavLLM: Towards Robust and Adaptive Speech Large Language Model (Shujie Hu et al., 2024)

{{<citation>}}

Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, Furu Wei. (2024)  
**WavLLM: Towards Robust and Adaptive Speech Large Language Model**
<br/>
<button class="copy-to-clipboard" title="WavLLM: Towards Robust and Adaptive Speech Large Language Model" index=10>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-SD, cs.CL, eess-AS  
Keyword Score: 59  
Keywords: Benchmarking, Curriculum Learning, Multi-modal, Multi-modal, Automatic Speech Recognition, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00656v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00656v1.pdf" filename="2404.00656v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have revolutionized the field of natural language processing, progressively broadening their scope to <b>multimodal</b> perception and generation. However, effectively integrating listening capabilities into <b>LLMs</b> poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech <b>large</b> <b>language</b> <b>model</b> with dual encoders, and a <b>prompt-aware</b> LoRA weight adapter, optimized by a two-stage <b>curriculum</b> <b>learning</b> approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the <b>curriculum</b> <b>learning</b> framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a <b>prompt-aware</b> LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech <b>benchmarks</b> including tasks such as <b>ASR,</b> ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \url{aka.ms/wavllm}.

{{</citation>}}


### (11/31 | 11/145) Query-driven Relevant Paragraph Extraction from Legal Judgments (T. Y. S. S Santosh et al., 2024)

{{<citation>}}

T. Y. S. S Santosh, Elvin Quero Hernandez, Matthias Grabmair. (2024)  
**Query-driven Relevant Paragraph Extraction from Legal Judgments**
<br/>
<button class="copy-to-clipboard" title="Query-driven Relevant Paragraph Extraction from Legal Judgments" index=11>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-IR, cs.CL  
Keyword Score: 53  
Keywords: Benchmarking, Distribution Shift, Distribution Shift, Fine-tuning, Fine-tuning, Zero-shot, Information Retrieval  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00595v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00595v1.pdf" filename="2404.00595v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Legal professionals often grapple with navigating lengthy legal judgements to pinpoint <b>information</b> <b>that</b> directly address their queries. This paper focus on this task of extracting relevant paragraphs from legal judgements based on the query. We construct a specialized dataset for this task from the European Court of Human Rights (ECtHR) using the case law guides. We assess the performance of current retrieval models in a <b>zero-shot</b> way and also establish <b>fine-tuning</b> <b>benchmarks</b> using various models. The results highlight the significant gap between <b>fine-tuned</b> and <b>zero-shot</b> performance, emphasizing the challenge of handling <b>distribution</b> <b>shift</b> in the legal domain. We notice that the legal pre-training handles <b>distribution</b> <b>shift</b> on the corpus side but still struggles on query side <b>distribution</b> <b>shift,</b> with unseen legal queries. We also explore various Parameter Efficient <b>Fine-Tuning</b> (PEFT) methods to evaluate their practicality within the context of <b>information</b> <b>retrieval,</b> shedding light on the effectiveness of different PEFT methods across diverse configurations with pre-training and model architectures influencing the choice of PEFT method.

{{</citation>}}


### (12/31 | 12/145) Fairness in Large Language Models: A Taxonomic Survey (Zhibo Chu et al., 2024)

{{<citation>}}

Zhibo Chu, Zichong Wang, Wenbin Zhang. (2024)  
**Fairness in Large Language Models: A Taxonomic Survey**
<br/>
<button class="copy-to-clipboard" title="Fairness in Large Language Models: A Taxonomic Survey" index=12>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Fairness, Large Language Model, Large Language Model, Prompt, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.01349v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.01349v1.pdf" filename="2404.01349v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack <b>fairness</b> considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, <b>prompting</b> extensive study in fair <b>LLMs.</b> On the other hand, <b>fairness</b> in <b>LLMs,</b> in contrast to <b>fairness</b> in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair <b>LLMs.</b> Specifically, a brief introduction to <b>LLMs</b> is provided, followed by an analysis of factors contributing to bias in <b>LLMs.</b> Additionally, the concept of <b>fairness</b> in <b>LLMs</b> is discussed categorically, summarizing metrics for evaluating bias in <b>LLMs</b> and existing algorithms for promoting <b>fairness.</b> Furthermore, resources for evaluating bias in <b>LLMs,</b> including toolkits and datasets, are <b>summarized.</b> Finally, existing research challenges and open questions are discussed.

{{</citation>}}


### (13/31 | 13/145) From Robustness to Improved Generalization and Calibration in Pre-trained Language Models (Josip Jukić et al., 2024)

{{<citation>}}

Josip Jukić, Jan Šnajder. (2024)  
**From Robustness to Improved Generalization and Calibration in Pre-trained Language Models**
<br/>
<button class="copy-to-clipboard" title="From Robustness to Improved Generalization and Calibration in Pre-trained Language Models" index=13>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 43  
Keywords: Benchmarking, Fine-tuning, GLUE, Pre-trained Language Model, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00758v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00758v1.pdf" filename="2404.00758v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Enhancing generalization and uncertainty quantification in <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing <b>PLM</b> performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where <b>PLM</b> inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within <b>PLM</b> intermediate representations relative to their inputs. Our evaluation using the <b>GLUE</b> <b>benchmark</b> demonstrates that JacHess significantly improves in-domain generalization and calibration in <b>PLMs,</b> outperforming unregularized <b>fine-tuning</b> and other similar regularization methods.

{{</citation>}}


### (14/31 | 14/145) A Controlled Reevaluation of Coreference Resolution Models (Ian Porada et al., 2024)

{{<citation>}}

Ian Porada, Xiyuan Zou, Jackie Chi Kit Cheung. (2024)  
**A Controlled Reevaluation of Coreference Resolution Models**
<br/>
<button class="copy-to-clipboard" title="A Controlled Reevaluation of Coreference Resolution Models" index=14>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 40  
Keywords: Fine-tuning, Out-of-domain, Coreference Resolution, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00727v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00727v1.pdf" filename="2404.00727v1.pdf">Download PDF</button>

---


**ABSTRACT**  
All state-of-the-art <b>coreference</b> <b>resolution</b> (CR) models involve <b>finetuning</b> a <b>pretrained</b> <b>language</b> <b>model.</b> Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the <b>pretrained</b> <b>language</b> <b>model</b> used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to <b>out-of-domain</b> textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of the increase in F1 score reported in the past five years.

{{</citation>}}


### (15/31 | 15/145) Learning to Plan for Language Modeling from Unlabeled Data (Nathan Cornille et al., 2024)

{{<citation>}}

Nathan Cornille, Marie-Francine Moens, Florian Mai. (2024)  
**Learning to Plan for Language Modeling from Unlabeled Data**
<br/>
<button class="copy-to-clipboard" title="Learning to Plan for Language Modeling from Unlabeled Data" index=15>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 40  
Keywords: Self-supervised Learning, Self-supervised Learning, Unsupervised Learning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00614v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00614v1.pdf" filename="2404.00614v1.pdf">Download PDF</button>

---


**ABSTRACT**  
By training to predict the next token in an unlabeled corpus, <b>large</b> <b>language</b> <b>models</b> learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a <b>self-supervised</b> <b>learning</b> objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an <b>unsupervised</b> way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is <b>unsupervised</b> and external to the language model, new planner modules can be trained at <b>large</b> <b>scale</b> <b>and</b> easily be shared with the community.

{{</citation>}}


### (16/31 | 16/145) DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model (Lirui Zhao et al., 2024)

{{<citation>}}

Lirui Zhao, Yue Yang, Kaipeng Zhang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, Rongrong Ji. (2024)  
**DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model**
<br/>
<button class="copy-to-clipboard" title="DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model" index=16>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Text2image, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.01342v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.01342v1.pdf" filename="2404.01342v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Text-to-image</b> (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> we introduce DiffAgent, an <b>LLM</b> agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent's capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. Our evaluations reveal that DiffAgent not only excels in identifying the appropriate T2I API but also underscores the effectiveness of the SFTA training framework. Codes are available at https://github.com/OpenGVLab/DiffAgent.

{{</citation>}}


### (17/31 | 17/145) Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization (Hritik Bansal et al., 2024)

{{<citation>}}

Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, Aditya Grover. (2024)  
**Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization**
<br/>
<button class="copy-to-clipboard" title="Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization" index=17>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 30  
Keywords: Large Language Model, Large Language Model, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00530v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00530v1.pdf" filename="2404.00530v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A common technique for aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the <b>LLM</b> trained with joint instruction-response preference data using DOVE outperforms the <b>LLM</b> trained with DPO by 5.2% and 3.3% win-rate for the <b>summarization</b> and open-ended dialogue datasets, respectively. Our findings reveal that joint preferences over instruction and response pairs can significantly enhance the alignment of <b>LLMs</b> by tapping into a broader spectrum of human preference elicitation. The data and code is available at https://github.com/Hritikbansal/dove.

{{</citation>}}


### (18/31 | 18/145) Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation (Rohan Chaudhury et al., 2024)

{{<citation>}}

Rohan Chaudhury, Mihir Godbole, Aakash Garg, Jinsil Hwaryoung Seo. (2024)  
**Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation**
<br/>
<button class="copy-to-clipboard" title="Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation" index=18>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-HC, cs.CL  
Keyword Score: 30  
Keywords: Zero-shot, Text Generation, Text-to-speech  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.01339v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.01339v1.pdf" filename="2404.01339v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Contemporary conversational systems often present a significant limitation: their responses lack the emotional depth and disfluent characteristic of human interactions. This absence becomes particularly noticeable when users seek more personalized and empathetic interactions. Consequently, this makes them seem mechanical and less relatable to human users. Recognizing this gap, we embarked on a journey to humanize machine communication, to ensure AI systems not only comprehend but also resonate. To address this shortcoming, we have designed an innovative speech synthesis pipeline. Within this framework, a cutting-edge language model introduces both human-like emotion and disfluencies in a <b>zero-shot</b> setting. These intricacies are seamlessly integrated into the generated <b>text</b> <b>by</b> the language model during <b>text</b> <b>generation,</b> allowing the system to mirror human speech patterns better, promoting more intuitive and natural user interactions. These generated elements are then adeptly transformed into corresponding speech patterns and emotive sounds using a rule-based approach during the <b>text-to-speech</b> <b>phase.</b> Based on our experiments, our novel system produces synthesized speech that's almost indistinguishable from genuine human communication, making each interaction feel more personal and authentic.

{{</citation>}}


### (19/31 | 19/145) Benchmark Transparency: Measuring the Impact of Data on Evaluation (Venelin Kovatchev et al., 2024)

{{<citation>}}

Venelin Kovatchev, Matthew Lease. (2024)  
**Benchmark Transparency: Measuring the Impact of Data on Evaluation**
<br/>
<button class="copy-to-clipboard" title="Benchmark Transparency: Measuring the Impact of Data on Evaluation" index=19>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 23  
Keywords: Benchmarking, Out-of-distribution, Perplexity  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00748v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00748v1.pdf" filename="2404.00748v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and <b>perplexity.</b> We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric. In a second set of experiments, we demonstrate that the impact of data on evaluation is not just observable, but also predictable. We propose to use <b>benchmark</b> transparency as a method for comparing datasets and quantifying the similarity between them. We find that the ``dataset similarity vector'' can be used to predict how well a model generalizes out of distribution.

{{</citation>}}


### (20/31 | 20/145) ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights (T. Y. S. S Santosh et al., 2024)

{{<citation>}}

T. Y. S. S Santosh, Rashid Gustav Haddad, Matthias Grabmair. (2024)  
**ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights**
<br/>
<button class="copy-to-clipboard" title="ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case Retrieval in the European Court of Human Rights" index=20>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-IR, cs.CL  
Keyword Score: 23  
Keywords: Benchmarking, Dense Retrieval, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00596v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00596v1.pdf" filename="2404.00596v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In common law jurisdictions, legal practitioners rely on precedents to construct arguments, in line with the doctrine of \emph{stare decisis}. As the number of cases grow over the years, prior case retrieval (PCR) has garnered significant attention. Besides lacking real-world scale, existing PCR datasets do not simulate a realistic setting, because their queries use complete case documents while only masking references to prior cases. The query is thereby exposed to legal <b>reasoning</b> not yet available when constructing an argument for an undecided case as well as spurious patterns left behind by citation masks, potentially short-circuiting a comprehensive understanding of case facts and legal principles. To address these limitations, we introduce a PCR dataset based on judgements from the European Court of Human Rights (ECtHR), which explicitly separate facts from arguments and exhibit precedential practices, aiding us to develop this PCR dataset to foster systems' comprehensive understanding. We <b>benchmark</b> different lexical and <b>dense</b> <b>retrieval</b> approaches with various negative sampling strategies, adapting them to deal with long text sequences using hierarchical variants. We found that difficulty-based negative sampling strategies were not effective for the PCR task, highlighting the need for investigation into domain-specific difficulty criteria. Furthermore, we observe performance of the <b>dense</b> <b>models</b> degrade with time and calls for further research into temporal adaptation of retrieval models. Additionally, we assess the influence of different views , Halsbury's and Goodhart's, in practice in ECtHR jurisdiction using PCR task.

{{</citation>}}


### (21/31 | 21/145) On the True Distribution Approximation of Minimum Bayes-Risk Decoding (Atsumoto Ohashi et al., 2024)

{{<citation>}}

Atsumoto Ohashi, Ukyo Honda, Tetsuro Morimura, Yuu Jinnai. (2024)  
**On the True Distribution Approximation of Minimum Bayes-Risk Decoding**
<br/>
<button class="copy-to-clipboard" title="On the True Distribution Approximation of Minimum Bayes-Risk Decoding" index=21>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Anomaly Detection, Text Generation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00752v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00752v1.pdf" filename="2404.00752v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in <b>text</b> <b>generation.</b> MBR decoding considers <b>texts</b> <b>sampled</b> from a model as pseudo-references and selects the <b>text</b> <b>with</b> the highest similarity to the others. Therefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods. From a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references. However, this approximation has not been the subject of in-depth study. In this study, we propose using <b>anomaly</b> <b>detection</b> to measure the degree of approximation. We first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced <b>anomaly</b> <b>scores</b> do. The results are the first to empirically support the link between the performance and the core assumption of MBR decoding.

{{</citation>}}


### (22/31 | 22/145) Can Language Models Recognize Convincing Arguments? (Paula Rescala et al., 2024)

{{<citation>}}

Paula Rescala, Manoel Horta Ribeiro, Tiancheng Hu, Robert West. (2024)  
**Can Language Models Recognize Convincing Arguments?**
<br/>
<button class="copy-to-clipboard" title="Can Language Models Recognize Convincing Arguments?" index=22>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-CY, cs.CL  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00750v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00750v1.pdf" filename="2404.00750v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The remarkable and ever-increasing capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. To gain insights into <b>LLMs'</b> persuasive capabilities without directly engaging in experimentation with humans, we propose studying their performance on the related task of detecting convincing arguments. We extend a dataset by Durmus & Cardie (2018) with debates, votes, and user traits and propose tasks measuring <b>LLMs'</b> ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that <b>LLMs</b> perform on par with humans in these tasks and that combining predictions from different <b>LLMs</b> yields significant performance gains, even surpassing human performance. The data and code released with this paper contribute to the crucial ongoing effort of continuously evaluating and monitoring the rapidly evolving capabilities and potential impact of <b>LLMs.</b>

{{</citation>}}


### (23/31 | 23/145) Opera Graeca Adnotata: Building a 34M+ Token Multilayer Corpus for Ancient Greek (Giuseppe G. A. Celano, 2024)

{{<citation>}}

Giuseppe G. A. Celano. (2024)  
**Opera Graeca Adnotata: Building a 34M+ Token Multilayer Corpus for Ancient Greek**
<br/>
<button class="copy-to-clipboard" title="Opera Graeca Adnotata: Building a 34M+ Token Multilayer Corpus for Ancient Greek" index=23>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Lemmatization, Tokenization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00739v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00739v1.pdf" filename="2404.00739v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this article, the beta version 0.1.0 of Opera Graeca Adnotata (OGA), the largest open-access multilayer corpus for Ancient Greek (AG) is presented. OGA consists of 1,687 literary works and 34M+ tokens coming from the PerseusDL and OpenGreekAndLatin GitHub repositories, which host AG texts ranging from about 800 BCE to about 250 CE. The texts have been enriched with seven annotation layers: (i) <b>tokenization</b> layer; (ii) sentence segmentation layer; (iii) <b>lemmatization</b> layer; (iv) morphological layer; (v) dependency layer; (vi) dependency function layer; (vii) Canonical Text Services (CTS) citation layer. The creation of each layer is described by highlighting the main technical and annotation-related issues encountered. <b>Tokenization,</b> sentence segmentation, and CTS citation are performed by rule-based algorithms, while morphosyntactic annotation is the output of the COMBO parser trained on the data of the Ancient Greek Dependency Treebank. For the sake of scalability and reusability, the corpus is released in the standoff formats PAULA XML and its offspring LAULA XML.

{{</citation>}}


### (24/31 | 24/145) Explainable Multi-hop Question Generation: An End-to-End Approach without Intermediate Question Labeling (Seonjeong Hwang et al., 2024)

{{<citation>}}

Seonjeong Hwang, Yunsu Kim, Gary Geunbae Lee. (2024)  
**Explainable Multi-hop Question Generation: An End-to-End Approach without Intermediate Question Labeling**
<br/>
<button class="copy-to-clipboard" title="Explainable Multi-hop Question Generation: An End-to-End Approach without Intermediate Question Labeling" index=24>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Question Answering, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00571v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00571v1.pdf" filename="2404.00571v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In response to the increasing use of interactive artificial intelligence, the demand for the capacity to handle complex <b>questions</b> <b>has</b> increased. Multi-hop <b>question</b> <b>generation</b> aims to generate complex <b>questions</b> <b>that</b> requires multi-step <b>reasoning</b> over several documents. Previous studies have predominantly utilized end-to-end models, wherein <b>questions</b> <b>are</b> decoded based on the representation of context documents. However, these approaches lack the ability to explain the <b>reasoning</b> process behind the generated multi-hop <b>questions.</b> <b>Additionally,</b> the <b>question</b> <b>rewriting</b> approach, which incrementally increases the <b>question</b> <b>complexity,</b> also has limitations due to the requirement of labeling data for intermediate-stage <b>questions.</b> <b>In</b> this paper, we introduce an end-to-end <b>question</b> <b>rewriting</b> model that increases <b>question</b> <b>complexity</b> through sequential rewriting. The proposed model has the advantage of training with only the final multi-hop <b>questions,</b> <b>without</b> intermediate <b>questions.</b> <b>Experimental</b> results demonstrate the effectiveness of our model in generating complex <b>questions,</b> <b>particularly</b> 3- and 4-hop <b>questions,</b> <b>which</b> are appropriately paired with input answers. We also prove that our model logically and incrementally increases the complexity of <b>questions,</b> <b>and</b> the generated multi-hop <b>questions</b> <b>are</b> also beneficial for training <b>question</b> <b>answering</b> models.

{{</citation>}}


### (25/31 | 25/145) MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models (Zebang Cheng et al., 2024)

{{<citation>}}

Zebang Cheng, Fuqiang Niu, Yuxiang Lin, Zhi-Qi Cheng, Bowen Zhang, Xiaojiang Peng. (2024)  
**MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models**
<br/>
<button class="copy-to-clipboard" title="MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models" index=25>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-CV, cs-MM, cs.CL  
Keyword Score: 16  
Keywords: Multi-modal, Multi-modal, Emotion Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00511v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00511v2.pdf" filename="2404.00511v2.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on <b>multimodal</b> <b>emotion</b> <b>cause</b> analysis in conversations. We propose a novel <b>Multimodal</b> <b>Emotion</b> <b>Recognition</b> and <b>Multimodal</b> <b>Emotion</b> <b>Cause</b> Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized <b>emotion</b> <b>encoders.</b> Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced <b>emotion</b> <b>understanding</b> and causality inference. Experimental evaluation demonstrates the advantages of our <b>multimodal</b> approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git

{{</citation>}}


### (26/31 | 26/145) PID Control-Based Self-Healing to Improve the Robustness of Large Language Models (Zhuotong Chen et al., 2024)

{{<citation>}}

Zhuotong Chen, Zihu Wang, Yifan Yang, Qianxiao Li, Zheng Zhang. (2024)  
**PID Control-Based Self-Healing to Improve the Robustness of Large Language Models**
<br/>
<button class="copy-to-clipboard" title="PID Control-Based Self-Healing to Improve the Robustness of Large Language Models" index=26>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00828v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00828v1.pdf" filename="2404.00828v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite the effectiveness of deep neural networks in numerous natural language processing applications, recent findings have exposed the vulnerability of these language models when minor perturbations are introduced. While appearing semantically indistinguishable to humans, these perturbations can significantly reduce the performance of well-trained language models, raising concerns about the reliability of deploying them in safe-critical situations. In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data. This is formulated as a trajectory optimization problem in which the internal states of the neural network layers are automatically corrected using a PID (Proportional-Integral-Derivative) control mechanism. The P controller targets immediate state adjustments, while the I and D controllers consider past states and future dynamical trends, respectively. We leverage the geometrical properties of the training data to design effective linear PID controllers. This approach reduces the computational cost to that of using just the P controller, instead of the full PID control. Further, we introduce an analytical method for approximating the optimal control solutions, enhancing the real-time inference capabilities of this controlled system. Moreover, we conduct a theoretical error analysis of the analytic solution in a simplified setting. The proposed PID control-based self-healing is a low cost framework that improves the robustness of pre-trained <b>large</b> <b>language</b> <b>models,</b> whether standard or robustly trained, against a wide range of perturbations. A detailed implementation can be found in:https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models.

{{</citation>}}


### (27/31 | 27/145) Mind Your Neighbours: Leveraging Analogous Instances for Rhetorical Role Labeling for Legal Documents (T. Y. S. S Santosh et al., 2024)

{{<citation>}}

T. Y. S. S Santosh, Hassan Sarwat, Ahmed Abdou, Matthias Grabmair. (2024)  
**Mind Your Neighbours: Leveraging Analogous Instances for Rhetorical Role Labeling for Legal Documents**
<br/>
<button class="copy-to-clipboard" title="Mind Your Neighbours: Leveraging Analogous Instances for Rhetorical Role Labeling for Legal Documents" index=27>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.01344v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.01344v1.pdf" filename="2404.01344v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Rhetorical Role Labeling (RRL) of legal judgments is essential for various tasks, such as case <b>summarization,</b> semantic search and argument mining. However, it presents challenges such as inferring sentence roles from context, interrelated roles, limited annotated data, and label imbalance. This study introduces novel techniques to enhance RRL performance by leveraging knowledge from semantically similar instances (neighbours). We explore inference-based and training-based approaches, achieving remarkable improvements in challenging macro-F1 scores. For inference-based methods, we explore interpolation techniques that bolster label predictions without re-training. While in training-based methods, we integrate prototypical learning with our novel discourse-aware contrastive method that work directly on embedding spaces. Additionally, we assess the cross-domain applicability of our methods, demonstrating their effectiveness in transferring knowledge across diverse legal domains.

{{</citation>}}


### (28/31 | 28/145) LexAbSumm: Aspect-based Summarization of Legal Decisions (T. Y. S. S Santosh et al., 2024)

{{<citation>}}

T. Y. S. S Santosh, Mahmoud Aly, Matthias Grabmair. (2024)  
**LexAbSumm: Aspect-based Summarization of Legal Decisions**
<br/>
<button class="copy-to-clipboard" title="LexAbSumm: Aspect-based Summarization of Legal Decisions" index=28>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00594v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00594v1.pdf" filename="2404.00594v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Legal professionals frequently encounter long legal judgments that hold critical insights for their work. While recent advances have led to automated <b>summarization</b> solutions for legal documents, they typically provide generic summaries, which may not meet the diverse information needs of users. To address this gap, we introduce LexAbSumm, a novel dataset designed for aspect-based <b>summarization</b> of legal case decisions, sourced from the European Court of Human Rights jurisdiction. We evaluate several abstractive <b>summarization</b> models tailored for longer documents on LexAbSumm, revealing a challenge in conditioning these models to produce aspect-specific summaries. We release LexAbSum to facilitate research in aspect-based <b>summarization</b> for legal domain.

{{</citation>}}


### (29/31 | 29/145) Leveraging Corpus Metadata to Detect Template-based Translation: An Exploratory Case Study of the Egyptian Arabic Wikipedia Edition (Saied Alshahrani et al., 2024)

{{<citation>}}

Saied Alshahrani, Hesham Haroon, Ali Elfilali, Mariama Njie, Jeanna Matthews. (2024)  
**Leveraging Corpus Metadata to Detect Template-based Translation: An Exploratory Case Study of the Egyptian Arabic Wikipedia Edition**
<br/>
<button class="copy-to-clipboard" title="Leveraging Corpus Metadata to Detect Template-based Translation: An Exploratory Case Study of the Egyptian Arabic Wikipedia Edition" index=29>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Low-Resource  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00565v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00565v1.pdf" filename="2404.00565v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Wikipedia articles (content pages) are commonly used corpora in Natural Language Processing (NLP) research, especially in <b>low-resource</b> languages other than English. Yet, a few research studies have studied the three Arabic Wikipedia editions, Arabic Wikipedia (AR), Egyptian Arabic Wikipedia (ARZ), and Moroccan Arabic Wikipedia (ARY), and documented issues in the Egyptian Arabic Wikipedia edition regarding the massive automatic creation of its articles using template-based translation from English to Arabic without human involvement, overwhelming the Egyptian Arabic Wikipedia with articles that do not only have low-quality content but also with articles that do not represent the Egyptian people, their culture, and their dialect. In this paper, we aim to mitigate the problem of template translation that occurred in the Egyptian Arabic Wikipedia by identifying these template-translated articles and their characteristics through exploratory analysis and building automatic detection systems. We first explore the content of the three Arabic Wikipedia editions in terms of density, quality, and human contributions and utilize the resulting insights to build multivariate machine learning classifiers leveraging articles' metadata to detect the template-translated articles automatically. We then publicly deploy and host the best-performing classifier, XGBoost, as an online application called EGYPTIAN WIKIPEDIA SCANNER and release the extracted, filtered, and labeled datasets to the research community to benefit from our datasets and the online, web-based detection system.

{{</citation>}}


### (30/31 | 30/145) DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations (Weihao Zeng et al., 2024)

{{<citation>}}

Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang, Yukai Xu, Weiran Xu. (2024)  
**DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations**
<br/>
<button class="copy-to-clipboard" title="DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations" index=30>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00557v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00557v1.pdf" filename="2404.00557v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with <b>LLMs</b> to learn diverse task-oriented dialogue representations. DivTOD guides <b>LLMs</b> in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.

{{</citation>}}


### (31/31 | 31/145) Against The Achilles' Heel: A Survey on Red Teaming for Generative Models (Lizhi Lin et al., 2024)

{{<citation>}}

Lizhi Lin, Honglin Mu, Zenan Zhai, Minghan Wang, Yuxia Wang, Renxi Wang, Junjie Gao, Yixuan Zhang, Wanxiang Che, Timothy Baldwin, Xudong Han, Haonan Li. (2024)  
**Against The Achilles' Heel: A Survey on Red Teaming for Generative Models**
<br/>
<button class="copy-to-clipboard" title="Against The Achilles' Heel: A Survey on Red Teaming for Generative Models" index=31>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 6  
Keywords: Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00629v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00629v1.pdf" filename="2404.00629v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safety issues as various vulnerabilities are exposed. Faced with the problem, the field of red teaming is experiencing fast-paced growth, which highlights the need for a comprehensive organization covering the entire pipeline and addressing emerging topics for the community. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework that unifies various automatic red teaming approaches. Moreover, our survey covers novel areas including <b>multimodal</b> attacks and defenses, risks around multilingual models, overkill of harmless queries, and safety of downstream applications. We hope this survey can provide a systematic perspective on the field and unlock new areas of research.

{{</citation>}}


## cs.CV (39)



### (1/39 | 32/145) Training-Free Semantic Segmentation via LLM-Supervision (Wenfang Sun et al., 2024)

{{<citation>}}

Wenfang Sun, Yingjun Du, Gaowen Liu, Ramana Kompella, Cees G. M. Snoek. (2024)  
**Training-Free Semantic Segmentation via LLM-Supervision**
<br/>
<button class="copy-to-clipboard" title="Training-Free Semantic Segmentation via LLM-Supervision" index=32>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 83  
Keywords: Benchmarking, Fine-tuning, Zero-shot, GPT, GPT-3, Large Language Model, Large Language Model, Prompt, Prompt Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00701v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00701v1.pdf" filename="2404.00701v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in open vocabulary models, like CLIP, have notably advanced <b>zero-shot</b> classification and segmentation by utilizing natural language for class-specific embeddings. However, most research has focused on improving model accuracy through <b>prompt</b> <b>engineering,</b> <b>prompt</b> <b>learning,</b> or <b>fine-tuning</b> with limited labeled data, thereby overlooking the importance of refining the class descriptors. This paper introduces a new approach to text-supervised semantic segmentation using supervision by a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> that does not require extra training. Our method starts from an <b>LLM,</b> like <b>GPT-3,</b> to generate a detailed set of subclasses for more accurate class representation. We then employ an advanced text-supervised semantic segmentation model to apply the generated subclasses as target labels, resulting in diverse segmentation results tailored to each subclass's unique characteristics. Additionally, we propose an assembly that merges the segmentation maps from the various subclass descriptors to ensure a more comprehensive representation of the different aspects in the test images. Through comprehensive experiments on three standard <b>benchmarks,</b> our method outperforms traditional text-supervised semantic segmentation methods by a marked margin.

{{</citation>}}


### (2/39 | 33/145) DMSSN: Distilled Mixed Spectral-Spatial Network for Hyperspectral Salient Object Detection (Haolin Qin et al., 2024)

{{<citation>}}

Haolin Qin, Tingfa Xu, Peifu Liu, Jingxuan Xu, Jianan Li. (2024)  
**DMSSN: Distilled Mixed Spectral-Spatial Network for Hyperspectral Salient Object Detection**
<br/>
<button class="copy-to-clipboard" title="DMSSN: Distilled Mixed Spectral-Spatial Network for Hyperspectral Salient Object Detection" index=33>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 60  
Keywords: Object Detection, Autoencoder, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00694v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00694v1.pdf" filename="2404.00694v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Hyperspectral salient <b>object</b> <b>detection</b> (HSOD) has exhibited remarkable promise across various applications, particularly in intricate scenarios where conventional RGB-based approaches fall short. Despite the considerable progress in HSOD method advancements, two critical challenges require immediate attention. Firstly, existing hyperspectral data dimension reduction techniques incur a loss of spectral information, which adversely affects detection accuracy. Secondly, previous methods insufficiently harness the inherent distinctive attributes of hyperspectral images (HSIs) during the feature extraction process. To address these challenges, we propose a novel approach termed the <b>Distilled</b> Mixed Spectral-Spatial Network (DMSSN), comprising a <b>Distilled</b> Spectral Encoding process and a Mixed Spectral-Spatial <b>Transformer</b> (MSST) feature extraction network. The encoding process utilizes <b>knowledge</b> <b>distillation</b> to construct a lightweight <b>autoencoder</b> for dimension reduction, striking a balance between robust encoding capabilities and low computational costs. The MSST extracts spectral-spatial features through multiple attention head groups, collaboratively enhancing its resistance to intricate scenarios. Moreover, we have created a large-scale HSOD dataset, HSOD-BIT, to tackle the issue of data scarcity in this field and meet the fundamental data requirements of deep network training. Extensive experiments demonstrate that our proposed DMSSN achieves state-of-the-art performance on multiple datasets. We will soon make the code and dataset publicly available on https://github.com/anonymous0519/HSOD-BIT.

{{</citation>}}


### (3/39 | 34/145) Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning (Kun Ding et al., 2024)

{{<citation>}}

Kun Ding, Haojian Zhang, Qiang Yu, Ying Wang, Shiming Xiang, Chunhong Pan. (2024)  
**Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning**
<br/>
<button class="copy-to-clipboard" title="Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning" index=34>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 60  
Keywords: Few-shot, Fine-tuning, Out-of-distribution, Zero-shot, Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00603v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00603v1.pdf" filename="2404.00603v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose a generalized method for boosting the generalization ability of pre-trained <b>vision-language</b> models (VLMs) while <b>fine-tuning</b> on downstream <b>few-shot</b> tasks. The idea is realized by exploiting <b>out-of-distribution</b> (OOD) detection to predict whether a sample belongs to a base distribution or a novel distribution and then using the score generated by a dedicated competition based scoring function to fuse the <b>zero-shot</b> and <b>few-shot</b> classifier. The fused classifier is dynamic, which will bias towards the <b>zero-shot</b> classifier if a sample is more likely from the distribution pre-trained on, leading to improved base-to-novel generalization ability. Our method is performed only in test stage, which is applicable to boost existing methods without time-consuming re-training. Extensive experiments show that even weak distribution detectors can still improve VLMs' generalization ability. Specifically, with the help of OOD detectors, the harmonic mean of CoOp and ProGrad increase by 2.6 and 1.5 percentage points over 11 recognition datasets in the base-to-novel setting.

{{</citation>}}


### (4/39 | 35/145) M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language Models (Fan Bai et al., 2024)

{{<citation>}}

Fan Bai, Yuxin Du, Tiejun Huang, Max Q. -H. Meng, Bo Zhao. (2024)  
**M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language Models**
<br/>
<button class="copy-to-clipboard" title="M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language Models" index=35>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 56  
Keywords: Automatic Evaluation, Benchmarking, Multi-modal, Image2text, Question Answering, Visual Question Answering, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00578v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00578v1.pdf" filename="2404.00578v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Medical image analysis is essential to clinical diagnosis and treatment, which is increasingly supported by <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs). However, previous research has primarily focused on 2D medical images, leaving 3D images under-explored, despite their richer spatial information. This paper aims to advance 3D medical image analysis with MLLMs. To this end, we present a <b>large-scale</b> <b>3D</b> <b>multi-modal</b> medical dataset, M3D-Data, comprising 120K <b>image-text</b> pairs and 662K instruction-response pairs specifically tailored for various 3D medical tasks, such as <b>image-text</b> retrieval, report generation, <b>visual</b> <b>question</b> <b>answering,</b> positioning, and segmentation. Additionally, we propose M3D-LaMed, a versatile <b>multi-modal</b> <b>large</b> <b>language</b> <b>model</b> for 3D medical image analysis. Furthermore, we introduce a new 3D <b>multi-modal</b> medical <b>benchmark,</b> M3D-Bench, which facilitates <b>automatic</b> <b>evaluation</b> across eight tasks. Through comprehensive evaluation, our method proves to be a robust model for 3D medical image analysis, outperforming existing solutions. All code, data, and models are publicly available at: https://github.com/BAAI-DCAI/M3D.

{{</citation>}}


### (5/39 | 36/145) Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open Domain Generalization (Mainak Singha et al., 2024)

{{<citation>}}

Mainak Singha, Ankit Jha, Shirsha Bose, Ashwin Nair, Moloud Abdar, Biplab Banerjee. (2024)  
**Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open Domain Generalization**
<br/>
<button class="copy-to-clipboard" title="Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open Domain Generalization" index=36>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Diffusion Model, Convolutional Neural Network, Prompt, Prompt Learning, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00710v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00710v1.pdf" filename="2404.00710v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We delve into Open Domain Generalization (ODG), marked by domain and category shifts between training's labeled source and testing's unlabeled target domains. Existing solutions to ODG face limitations due to constrained generalizations of traditional <b>CNN</b> backbones and errors in detecting target open samples in the absence of prior knowledge. Addressing these pitfalls, we introduce ODG-CLIP, harnessing the semantic prowess of the <b>vision-language</b> model, CLIP. Our framework brings forth three primary innovations: Firstly, distinct from prevailing paradigms, we conceptualize ODG as a multi-class classification challenge encompassing both known and novel categories. Central to our approach is modeling a unique <b>prompt</b> <b>tailored</b> for detecting unknown class samples, and to train this, we employ a readily accessible stable <b>diffusion</b> <b>model,</b> elegantly generating proxy images for the open class. Secondly, aiming for domain-tailored classification <b>(prompt)</b> <b>weights</b> while ensuring a balance of precision and simplicity, we devise a novel visual stylecentric <b>prompt</b> <b>learning</b> mechanism. Finally, we infuse images with class-discriminative knowledge derived from the <b>prompt</b> <b>space</b> to augment the fidelity of CLIP's visual embeddings. We introduce a novel objective to safeguard the continuity of this infused semantic intel across domains, especially for the shared classes. Through rigorous testing on diverse datasets, covering closed and open-set DG contexts, ODG-CLIP demonstrates clear supremacy, consistently outpacing peers with performance boosts between 8%-16%. Code will be available at https://github.com/mainaksingha01/ODG-CLIP.

{{</citation>}}


### (6/39 | 37/145) Weak-to-Strong 3D Object Detection with X-Ray Distillation (Alexander Gambashidze et al., 2024)

{{<citation>}}

Alexander Gambashidze, Aleksandr Dadukin, Maksim Golyadkin, Maria Razzhivina, Ilya Makarov. (2024)  
**Weak-to-Strong 3D Object Detection with X-Ray Distillation**
<br/>
<button class="copy-to-clipboard" title="Weak-to-Strong 3D Object Detection with X-Ray Distillation" index=37>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Object Detection, Knowledge Distillation, Knowledge Distillation, Semi-Supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00679v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00679v1.pdf" filename="2404.00679v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper addresses the critical challenges of sparsity and occlusion in LiDAR-based 3D <b>object</b> <b>detection.</b> Current methods often rely on supplementary modules or specific architectural designs, potentially limiting their applicability to new and evolving architectures. To our <b>knowledge,</b> <b>we</b> are the first to propose a versatile technique that seamlessly integrates into any existing framework for 3D <b>Object</b> <b>Detection,</b> marking the first instance of Weak-to-Strong generalization in 3D computer vision. We introduce a novel framework, X-Ray <b>Distillation</b> with <b>Object-Complete</b> <b>Frames,</b> suitable for both <b>supervised</b> and <b>semi-supervised</b> <b>settings,</b> that leverages the temporal aspect of point cloud sequences. This method extracts crucial information from both previous and subsequent LiDAR frames, creating <b>Object-Complete</b> <b>frames</b> that represent <b>objects</b> <b>from</b> multiple viewpoints, thus addressing occlusion and sparsity. Given the limitation of not being able to generate <b>Object-Complete</b> <b>frames</b> during online inference, we utilize <b>Knowledge</b> <b>Distillation</b> within a Teacher-Student framework. This technique encourages the strong Student model to emulate the behavior of the weaker Teacher, which processes simple and informative <b>Object-Complete</b> <b>frames,</b> effectively offering a comprehensive view of <b>objects</b> <b>as</b> if seen through X-ray vision. Our proposed methods surpass state-of-the-art in <b>semi-supervised</b> <b>learning</b> by 1-1.5 mAP and enhance the performance of five established <b>supervised</b> models by 1-2 mAP on standard autonomous driving datasets, even with default hyperparameters. Code for <b>Object-Complete</b> <b>frames</b> is available here: https://github.com/sakharok13/X-Ray-Teacher-Patching-Tools.

{{</citation>}}


### (7/39 | 38/145) Weakly-Supervised Cross-Domain Segmentation of Electron Microscopy with Sparse Point Annotation (Dafei Qiu et al., 2024)

{{<citation>}}

Dafei Qiu, Shan Xiong, Jiajin Yi, Jialin Peng. (2024)  
**Weakly-Supervised Cross-Domain Segmentation of Electron Microscopy with Sparse Point Annotation**
<br/>
<button class="copy-to-clipboard" title="Weakly-Supervised Cross-Domain Segmentation of Electron Microscopy with Sparse Point Annotation" index=38>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Supervised Learning, Unsupervised Learning, Weakly-supervised Learning, Domain Adaptation, Weakly Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00667v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00667v1.pdf" filename="2404.00667v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Accurate segmentation of organelle instances from electron microscopy (EM) images plays an essential role in many neuroscience researches. However, practical scenarios usually suffer from high annotation costs, label scarcity, and large <b>domain</b> <b>diversity.</b> While <b>unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA) that assumes no annotation effort on the target data is promising to alleviate these challenges, its performance on complicated segmentation tasks is still far from practical usage. To address these issues, we investigate a highly annotation-efficient <b>weak</b> <b>supervision,</b> which assumes only sparse center-points on a small subset of object instances in the target training images. To achieve accurate segmentation with partial point annotations, we introduce instance counting and center detection as auxiliary tasks and design a multitask learning framework to leverage correlations among the counting, detection, and segmentation, which are all tasks with partial or no supervision. Building upon the different <b>domain-invariances</b> <b>of</b> the three tasks, we enforce counting estimation with a novel soft consistency loss as a global prior for center detection, which further guides the per-pixel segmentation. To further compensate for annotation sparsity, we develop a cross-position cut-and-paste for label augmentation and an entropy-based pseudo-label selection. The experimental results highlight that, by simply using extremely <b>weak</b> <b>annotation,</b> e.g., 15\% sparse points, for model training, the proposed model is capable of significantly outperforming UDA methods and produces comparable performance as the <b>supervised</b> counterpart. The high robustness of our model shown in the validations and the low requirement of expert knowledge for sparse point annotation further improve the potential application value of our model.

{{</citation>}}


### (8/39 | 39/145) $R^2$-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding (Ye Liu et al., 2024)

{{<citation>}}

Ye Liu, Jixuan He, Wanhua Li, Junsik Kim, Donglai Wei, Hanspeter Pfister, Chang Wen Chen. (2024)  
**$R^2$-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding**
<br/>
<button class="copy-to-clipboard" title="$R^2$-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding" index=39>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 43  
Keywords: Benchmarking, Transfer Learning, Grounding, Reasoning, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00801v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00801v1.pdf" filename="2404.00801v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Video temporal <b>grounding</b> (VTG) is a fine-grained video understanding problem that aims to ground relevant clips in untrimmed videos given natural language queries. Most existing VTG models are built upon frame-wise final-layer CLIP features, aided by additional temporal backbones (e.g., SlowFast) with sophisticated temporal <b>reasoning</b> mechanisms. In this work, we claim that CLIP itself already shows great potential for fine-grained spatial-temporal modeling, as each layer offers distinct yet useful information under different granularity levels. Motivated by this, we propose Reversed Recurrent Tuning ($R^2$-Tuning), a parameter- and memory-efficient <b>transfer</b> <b>learning</b> framework for video temporal <b>grounding.</b> Our method learns a lightweight $R^2$ Block containing only 1.5% of the total parameters to perform progressive spatial-temporal modeling. Starting from the last layer of CLIP, $R^2$ Block recurrently aggregates spatial features from earlier layers, then refines temporal correlation conditioning on the given query, resulting in a coarse-to-fine scheme. $R^2$-Tuning achieves state-of-the-art performance across three VTG tasks (i.e., moment retrieval, highlight detection, and video <b>summarization)</b> on six public <b>benchmarks</b> (i.e., QVHighlights, Charades-STA, Ego4D-NLQ, TACoS, YouTube Highlights, and TVSum) even without the additional backbone, demonstrating the significance and effectiveness of the proposed scheme. Our code is available at https://github.com/yeliudev/R2-Tuning.

{{</citation>}}


### (9/39 | 40/145) Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning (Jakaria Rabbi et al., 2024)

{{<citation>}}

Jakaria Rabbi, Johannes Kiechle, Christian Beaulieu, Nilanjan Ray, Dana Cobzas. (2024)  
**Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning**
<br/>
<button class="copy-to-clipboard" title="Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning" index=40>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV, q-bio-NC  
Keyword Score: 43  
Keywords: Graph, Autoencoder, Contrastive Learning, Supervised Learning, Variational Autoencoder  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00785v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00785v1.pdf" filename="2404.00785v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a <b>Graph</b> <b>Variational</b> <b>Autoencoder</b> (VAE) enhanced with <b>Supervised</b> <b>Contrastive</b> <b>Learning,</b> our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and <b>contrastive</b> <b>loss</b> functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our <b>supervised</b> disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in patients with Multiple Sclerosis (MS) using the hippocampus data. Our <b>Graph</b> VAE with <b>Supervised</b> <b>Contrastive</b> <b>Learning</b> shows the volume changes of the hippocampus of MS populations at different ages, and the result is consistent with the current neuroimaging literature. This research provides valuable insights into the relationship between neurological disorder and hippocampal shape changes in different age groups of MS populations using a <b>Graph</b> VAE with <b>Supervised</b> <b>Contrastive</b> <b>loss.</b>

{{</citation>}}


### (10/39 | 41/145) LLM meets Vision-Language Models for Zero-Shot One-Class Classification (Yassir Bendou et al., 2024)

{{<citation>}}

Yassir Bendou, Giulia Lioi, Bastien Pasdeloup, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux, Vincent Gripon. (2024)  
**LLM meets Vision-Language Models for Zero-Shot One-Class Classification**
<br/>
<button class="copy-to-clipboard" title="LLM meets Vision-Language Models for Zero-Shot One-Class Classification" index=41>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 43  
Keywords: Benchmarking, Zero-shot, Large Language Model, Large Language Model, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00675v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00675v2.pdf" filename="2404.00675v2.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider the problem of <b>zero-shot</b> one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries <b>large</b> <b>language</b> <b>models</b> for visually confusing objects and then relies on <b>vision-language</b> pre-trained models (e.g., CLIP) to perform classification. By adapting <b>large-scale</b> <b>vision</b> <b>benchmarks,</b> we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic <b>benchmark</b> where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to discriminate between a single category and other semantically related ones using only its label

{{</citation>}}


### (11/39 | 42/145) DeeDSR: Towards Real-World Image Super-Resolution via Degradation-Aware Stable Diffusion (Chunyang Bi et al., 2024)

{{<citation>}}

Chunyang Bi, Xin Luo, Sheng Shen, Mengxi Zhang, Huanjing Yue, Jingyu Yang. (2024)  
**DeeDSR: Towards Real-World Image Super-Resolution via Degradation-Aware Stable Diffusion**
<br/>
<button class="copy-to-clipboard" title="DeeDSR: Towards Real-World Image Super-Resolution via Degradation-Aware Stable Diffusion" index=42>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 43  
Keywords: ControlNet, Diffusion Model, Benchmarking, Contrastive Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00661v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00661v1.pdf" filename="2404.00661v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Diffusion</b> <b>models,</b> known for their powerful generative capabilities, play a crucial role in addressing real-world super-resolution challenges. However, these models often focus on improving local textures while neglecting the impacts of global degradation, which can significantly reduce semantic fidelity and lead to inaccurate reconstructions and suboptimal super-resolution performance. To address this issue, we introduce a novel two-stage, degradation-aware framework that enhances the <b>diffusion</b> <b>model's</b> ability to recognize content and degradation in low-resolution images. In the first stage, we employ <b>unsupervised</b> <b>contrastive</b> <b>learning</b> to obtain representations of image degradations. In the second stage, we integrate a degradation-aware module into a simplified <b>ControlNet,</b> enabling flexible adaptation to various degradations based on the learned representations. Furthermore, we decompose the degradation-aware features into global semantics and local details branches, which are then injected into the <b>diffusion</b> <b>denoising</b> module to modulate the target generation. Our method effectively recovers semantically precise and photorealistic details, particularly under significant degradation conditions, demonstrating state-of-the-art performance across various <b>benchmarks.</b> Codes will be released at https://github.com/bichunyang419/DeeDSR.

{{</citation>}}


### (12/39 | 43/145) DRCT: Saving Image Super-resolution away from Information Bottleneck (Chih-Chung Hsu et al., 2024)

{{<citation>}}

Chih-Chung Hsu, Chia-Ming Lee, Yi-Shiuan Chou. (2024)  
**DRCT: Saving Image Super-resolution away from Information Bottleneck**
<br/>
<button class="copy-to-clipboard" title="DRCT: Saving Image Super-resolution away from Information Bottleneck" index=43>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Vision Transformer, Convolutional Neural Network, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00722v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00722v2.pdf" filename="2404.00722v2.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, <b>Vision</b> <b>Transformer-based</b> applications to low-level <b>vision</b> <b>tasks</b> have achieved widespread success. Unlike <b>CNN-based</b> models, <b>Transformers</b> are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To address this, we propose the Dense-residual-connected <b>Transformer</b> (DRCT), aimed at mitigating the loss of spatial information through dense-residual connections between layers, thereby unleashing the model's potential and enhancing performance. Experiment results indicate that our approach is not only straightforward but also achieves remarkable efficiency, surpassing state-of-the-art methods and performing commendably at NTIRE2024.

{{</citation>}}


### (13/39 | 44/145) SpiralMLP: A Lightweight Vision MLP Architecture (Haojie Mu et al., 2024)

{{<citation>}}

Haojie Mu, Burhan Ul Tayyab, Nicholas Chua. (2024)  
**SpiralMLP: A Lightweight Vision MLP Architecture**
<br/>
<button class="copy-to-clipboard" title="SpiralMLP: A Lightweight Vision MLP Architecture" index=44>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 36  
Keywords: Benchmarking, Benchmarking, Convolution, Convolutional Neural Network, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00648v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00648v1.pdf" filename="2404.00648v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present SpiralMLP, a novel architecture that introduces a Spiral FC layer as a replacement for the conventional Token Mixing approach. Differing from several existing MLP-based models that primarily emphasize axes, our Spiral FC layer is designed as a deformable <b>convolution</b> layer with spiral-like offsets. We further adapt Spiral FC into two variants: Self-Spiral FC and Cross-Spiral FC, which enable both local and global feature integration seamlessly, eliminating the need for additional processing steps. To thoroughly investigate the effectiveness of the spiral-like offsets and validate our design, we conduct ablation studies and explore optimal configurations. In empirical tests, SpiralMLP reaches state-of-the-art performance, similar to <b>Transformers,</b> <b>CNNs,</b> and other MLPs, <b>benchmarking</b> on ImageNet-1k, COCO and ADE20K. SpiralMLP still maintains linear computational complexity O(HW) and is compatible with varying input image resolutions. Our study reveals that targeting the full receptive field is not essential for achieving high performance, instead, adopting a refined approach offers better results.

{{</citation>}}


### (14/39 | 45/145) Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction (Junuk Cha et al., 2024)

{{<citation>}}

Junuk Cha, Jihyeon Kim, Jae Shin Yoon, Seungryul Baek. (2024)  
**Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction**
<br/>
<button class="copy-to-clipboard" title="Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction" index=45>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 35  
Keywords: Diffusion Model, Geometry, Transformer, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00562v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00562v2.pdf" filename="2404.00562v2.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category, which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g., contacts and semantics) from text <b>prompts.</b> To address this challenge, we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation, a VAE-based network takes as input a text and an object mesh, and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local <b>geometry</b> structure of diverse objects that is independent of the objects' category, and thus, it is applicable to general objects. For motion generation, a <b>Transformer-based</b> <b>diffusion</b> <b>model</b> utilizes this 3D contact map as a strong prior for generating physically plausible hand-object motion as a function of text <b>prompts</b> by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally, we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the object-hand contacts and to suppress the penetration artifacts. In the experiments, we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research. Codes and data are available in: https://github.com/JunukCha/Text2HOI.

{{</citation>}}


### (15/39 | 46/145) KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced Transformer for 3D Human Pose Estimation (Jihua Peng et al., 2024)

{{<citation>}}

Jihua Peng, Yanghong Zhou, P. Y. Mok. (2024)  
**KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced Transformer for 3D Human Pose Estimation**
<br/>
<button class="copy-to-clipboard" title="KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced Transformer for 3D Human Pose Estimation" index=46>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 33  
Keywords: Benchmarking, Transformer, Key Point Analysis, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00658v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00658v2.pdf" filename="2404.00658v2.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a novel Kinematics and Trajectory Prior Knowledge-Enhanced <b>Transformer</b> (KTPFormer), which overcomes the weakness in existing <b>transformer-based</b> methods for 3D human pose estimation that the derivation of Q, K, V vectors in their <b>self-attention</b> mechanisms are all based on simple linear mapping. We propose two prior attention modules, namely Kinematics Prior Attention <b>(KPA)</b> and Trajectory Prior Attention (TPA) to take advantage of the known anatomical structure of the human body and motion trajectory information, to facilitate effective learning of global dependencies and features in the multi-head <b>self-attention.</b> <b>KPA</b> models kinematic relationships in the human body by constructing a topology of kinematics, while TPA builds a trajectory topology to learn the information of joint motion trajectory across frames. Yielding Q, K, V vectors with prior knowledge, the two modules enable KTPFormer to model both spatial and temporal correlations simultaneously. Extensive experiments on three <b>benchmarks</b> (Human3.6M, MPI-INF-3DHP and HumanEva) show that KTPFormer achieves superior performance in comparison to state-of-the-art methods. More importantly, our <b>KPA</b> and TPA modules have lightweight plug-and-play designs and can be integrated into various <b>transformer-based</b> networks (i.e., diffusion-based) to improve the performance with only a very small increase in the computational overhead. The code is available at: https://github.com/JihuaPeng/KTPFormer.

{{</citation>}}


### (16/39 | 47/145) Deep Instruction Tuning for Segment Anything Model (Xiaorui Huang et al., 2024)

{{<citation>}}

Xiaorui Huang, Gen Luo, Chaoyang Zhu, Bo Tong, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji. (2024)  
**Deep Instruction Tuning for Segment Anything Model**
<br/>
<button class="copy-to-clipboard" title="Deep Instruction Tuning for Segment Anything Model" index=47>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 33  
Keywords: Benchmarking, Instruction Tuning, Prompt, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00650v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00650v1.pdf" filename="2404.00650v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Segment Anything Model (SAM) exhibits powerful yet versatile capabilities on (un) conditional image segmentation tasks recently. Although SAM can support various segmentation <b>prompts,</b> we note that, compared to point- and box-guided segmentation, it performs much worse on text-instructed tasks. We argue that deep text <b>instruction</b> <b>tuning</b> is key to mitigate such shortcoming caused by the shallow fusion scheme in its default light-weight mask decoder. In this paper, two \emph{deep <b>instruction</b> <b>tuning}</b> (DIT) methods are proposed, one is end-to-end and the other is layer-wise. With these tuning methods, we can regard the image encoder of SAM as a stand-alone <b>vision-language</b> learner in contrast to building another deep fusion branch. Extensive experiments on three highly competitive <b>benchmark</b> datasets of referring image segmentation show that a simple end-to-end DIT improves SAM by a large margin, with layer-wise DIT further boosts the performance to state-of-the-art. Our code is anonymously released at: https://github.com/wysnzzzz/DIT.

{{</citation>}}


### (17/39 | 48/145) GAMA-IR: Global Additive Multidimensional Averaging for Fast Image Restoration (Youssef Mansour et al., 2024)

{{<citation>}}

Youssef Mansour, Reinhard Heckel. (2024)  
**GAMA-IR: Global Additive Multidimensional Averaging for Fast Image Restoration**
<br/>
<button class="copy-to-clipboard" title="GAMA-IR: Global Additive Multidimensional Averaging for Fast Image Restoration" index=48>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV, eess-IV  
Keyword Score: 30  
Keywords: Convolution, Convolutional Neural Network, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00807v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00807v1.pdf" filename="2404.00807v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep learning-based methods have shown remarkable success for various image restoration tasks such as denoising and deblurring. The current state-of-the-art networks are relatively deep and utilize (variants of) self attention mechanisms. Those networks are significantly slower than shallow <b>convolutional</b> <b>networks,</b> which however perform worse. In this paper, we introduce an image restoration network that is both fast and yields excellent image quality. The network is designed to minimize the latency and memory consumption when executed on a standard GPU, while maintaining state-of-the-art performance. The network is a simple shallow network with an efficient block that implements global additive multidimensional averaging operations. This block can capture global information and enable a large receptive field even when used in shallow networks with minimal computational overhead. Through extensive experiments and evaluations on diverse tasks, we demonstrate that our network achieves comparable or even superior results to existing state-of-the-art image restoration networks with less latency. For instance, we exceed the state-of-the-art result on real-world SIDD denoising by 0.11dB, while being 2 to 10 times faster.

{{</citation>}}


### (18/39 | 49/145) Domain Generalizable Person Search Using Unreal Dataset (Minyoung Oh et al., 2024)

{{<citation>}}

Minyoung Oh, Duhyun Kim, Jae-Young Sim. (2024)  
**Domain Generalizable Person Search Using Unreal Dataset**
<br/>
<button class="copy-to-clipboard" title="Domain Generalizable Person Search Using Unreal Dataset" index=49>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Unsupervised Learning, Weakly-supervised Learning, Domain Adaptation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00626v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00626v1.pdf" filename="2404.00626v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Collecting and labeling real datasets to train the person search networks not only requires a lot of time and effort, but also accompanies privacy issues. The <b>weakly-supervised</b> and <b>unsupervised</b> <b>domain</b> <b>adaptation</b> methods have been proposed to alleviate the labeling burden for target datasets, however, their generalization capability is limited. We introduce a novel person search method based on the <b>domain</b> <b>generalization</b> framework, that uses an automatically labeled unreal dataset only for training but is applicable to arbitrary unseen real datasets. To alleviate the <b>domain</b> <b>gaps</b> when transferring the knowledge from the unreal source dataset to the real target datasets, we estimate the fidelity of person instances which is then used to train the end-to-end network adaptively. Moreover, we devise a <b>domain-invariant</b> <b>feature</b> learning scheme to encourage the network to suppress the <b>domain-related</b> <b>features.</b> Experimental results demonstrate that the proposed method provides the competitive performance to existing person search methods even though it is applicable to arbitrary unseen datasets without any prior knowledge and re-training burdens.

{{</citation>}}


### (19/39 | 50/145) Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches (Lingxuan Wu et al., 2024)

{{<citation>}}

Lingxuan Wu, Xiao Yang, Yinpeng Dong, Liuwei Xie, Hang Su, Jun Zhu. (2024)  
**Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches**
<br/>
<button class="copy-to-clipboard" title="Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches" index=50>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Face Recognition, Object Detection, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00540v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00540v1.pdf" filename="2404.00540v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The vulnerability of deep neural networks to <b>adversarial</b> <b>patches</b> has motivated numerous defense strategies for boosting model robustness. However, the prevailing defenses depend on single observation or pre-established adversary information to counter <b>adversarial</b> <b>patches,</b> often failing to be confronted with unseen or adaptive <b>adversarial</b> <b>attacks</b> and easily exhibiting unsatisfying performance in dynamic 3D environments. Inspired by active human perception and recurrent feedback mechanisms, we develop Embodied Active Defense (EAD), a proactive defensive strategy that actively contextualizes environmental information to address misaligned <b>adversarial</b> <b>patches</b> in 3D real-world settings. To achieve this, EAD develops two central recurrent sub-modules, i.e., a perception module and a policy module, to implement two critical functions of active vision. These models recurrently process a series of beliefs and observations, facilitating progressive refinement of their comprehension of the target <b>object</b> <b>and</b> enabling the development of strategic actions to counter <b>adversarial</b> <b>patches</b> in 3D environments. To optimize learning efficiency, we incorporate a differentiable approximation of environmental dynamics and deploy patches that are agnostic to the adversary strategies. Extensive experiments demonstrate that EAD substantially enhances robustness against a variety of patches within just a few steps through its action policy in safety-critical tasks (e.g., <b>face</b> <b>recognition</b> and <b>object</b> <b>detection),</b> without compromising standard accuracy. Furthermore, due to the attack-agnostic characteristic, EAD facilitates excellent generalization to unseen attacks, diminishing the averaged attack success rate by 95 percent across a range of unseen <b>adversarial</b> <b>attacks.</b>

{{</citation>}}


### (20/39 | 51/145) Transformer based Pluralistic Image Completion with Reduced Information Loss (Qiankun Liu et al., 2024)

{{<citation>}}

Qiankun Liu, Yuqi Jiang, Zhentao Tan, Dongdong Chen, Ying Fu, Qi Chu, Gang Hua, Nenghai Yu. (2024)  
**Transformer based Pluralistic Image Completion with Reduced Information Loss**
<br/>
<button class="copy-to-clipboard" title="Transformer based Pluralistic Image Completion with Reduced Information Loss" index=51>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Quantization, Quantization, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00513v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00513v1.pdf" filename="2404.00513v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Transformer</b> based methods have achieved great success in image inpainting recently. However, we find that these solutions regard each pixel as a token, thus suffering from an information loss issue from two aspects: 1) They downsample the input image into much lower resolutions for efficiency consideration. 2) They <b>quantize</b> $256^3$ RGB values to a small number (such as 512) of <b>quantized</b> color values. The indices of <b>quantized</b> pixels are used as tokens for the inputs and prediction targets of the <b>transformer.</b> To mitigate these issues, we propose a new <b>transformer</b> based framework called "PUT". Specifically, to avoid input downsampling while maintaining computation efficiency, we design a patch-based auto-encoder P-VQVAE. The encoder converts the masked image into non-overlapped patch tokens and the decoder recovers the masked regions from the inpainted tokens while keeping the unmasked regions unchanged. To eliminate the information loss caused by input <b>quantization,</b> an Un-quantized <b>Transformer</b> is applied. It directly takes features from the P-VQVAE encoder as input without any <b>quantization</b> and only regards the <b>quantized</b> tokens as prediction targets. Furthermore, to make the inpainting process more controllable, we introduce semantic and structural conditions as extra guidance. Extensive experiments show that our method greatly outperforms existing <b>transformer</b> based methods on image fidelity and achieves much higher diversity and better fidelity than state-of-the-art pluralistic inpainting methods on complex large-scale datasets (e.g., ImageNet). Codes are available at https://github.com/liuqk3/PUT.

{{</citation>}}


### (21/39 | 52/145) Towards Realistic Scene Generation with LiDAR Diffusion Models (Haoxi Ran et al., 2024)

{{<citation>}}

Haoxi Ran, Vitor Guizilini, Yue Wang. (2024)  
**Towards Realistic Scene Generation with LiDAR Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Towards Realistic Scene Generation with LiDAR Diffusion Models" index=52>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-RO, cs.CV  
Keyword Score: 25  
Keywords: Diffusion Model, Geometry, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00815v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00815v1.pdf" filename="2404.00815v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Diffusion</b> <b>models</b> (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle. This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D <b>geometry</b> of LiDAR scenes, which consumes much of their representation power. In this paper, we propose LiDAR <b>Diffusion</b> <b>Models</b> (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline. Our method targets three major desiderata: pattern realism, <b>geometry</b> realism, and object realism. Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene <b>geometry,</b> and patch-wise encoding for a full 3D object context. With these three core designs, our method achieves competitive performance on unconditional LiDAR generation in 64-beam scenario and state of the art on conditional LiDAR generation, while maintaining high efficiency compared to point-based DMs (up to 107$\times$ faster). Furthermore, by compressing LiDAR scenes into a latent space, we enable the controllability of DMs with various conditions such as semantic maps, camera views, and text <b>prompts.</b> Our code and pretrained weights are available at https://github.com/hancyran/LiDAR-Diffusion.

{{</citation>}}


### (22/39 | 53/145) Absolute-Unified Multi-Class Anomaly Detection via Class-Agnostic Distribution Alignment (Jia Guo et al., 2024)

{{<citation>}}

Jia Guo, Shuai Lu, Weihang Zhang, Huiqi Li. (2024)  
**Absolute-Unified Multi-Class Anomaly Detection via Class-Agnostic Distribution Alignment**
<br/>
<button class="copy-to-clipboard" title="Absolute-Unified Multi-Class Anomaly Detection via Class-Agnostic Distribution Alignment" index=53>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Anomaly Detection, Benchmarking, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00724v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00724v1.pdf" filename="2404.00724v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Conventional <b>unsupervised</b> <b>anomaly</b> <b>detection</b> (UAD) methods build separate models for each object category. Recent studies have proposed to train a unified model for multiple classes, namely model-unified UAD. However, such methods still implement the unified model separately on each class during inference with respective <b>anomaly</b> <b>decision</b> thresholds, which hinders their application when the image categories are entirely unavailable. In this work, we present a simple yet powerful method to address multi-class <b>anomaly</b> <b>detection</b> without any class information, namely \textit{absolute-unified} UAD. We target the crux of prior works in this challenging setting: different objects have mismatched <b>anomaly</b> <b>score</b> distributions. We propose Class-Agnostic Distribution Alignment (CADA) to align the mismatched score distribution of each implicit class without knowing class information, which enables unified <b>anomaly</b> <b>detection</b> for all classes and samples. The essence of CADA is to predict each class's score distribution of normal samples given any image, normal or anomalous, of this class. As a general component, CADA can activate the potential of nearly all UAD methods under absolute-unified setting. Our approach is extensively evaluated under the proposed setting on two popular UAD <b>benchmark</b> datasets, MVTec AD and VisA, where we exceed previous state-of-the-art by a large margin.

{{</citation>}}


### (23/39 | 54/145) Privacy-preserving Optics for Enhancing Protection in Face De-identification (Jhon Lopez et al., 2024)

{{<citation>}}

Jhon Lopez, Carlos Hinojosa, Henry Arguello, Bernard Ghanem. (2024)  
**Privacy-preserving Optics for Enhancing Protection in Face De-identification**
<br/>
<button class="copy-to-clipboard" title="Privacy-preserving Optics for Enhancing Protection in Face De-identification" index=54>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs.CV, eess-IV  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00777v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00777v1.pdf" filename="2404.00777v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The modern surge in camera usage alongside widespread computer vision technology applications poses significant privacy and security concerns. Current artificial intelligence (AI) technologies aid in recognizing relevant events and assisting in daily tasks in homes, offices, hospitals, etc. The need to access or process personal information for these purposes raises privacy concerns. While software-level solutions like face de-identification provide a good privacy/utility trade-off, they present vulnerabilities to sniffing attacks. In this paper, we propose a hardware-level face de-identification method to solve this vulnerability. Specifically, our approach first learns an optical encoder along with a regression model to obtain a face heatmap while hiding the face identity from the source image. We also propose an anonymization framework that generates a new face using the privacy-preserving image, face heatmap, and a reference face image from a public dataset as input. We validate our approach with extensive <b>simulations</b> and hardware experiments.

{{</citation>}}


### (24/39 | 55/145) Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts (Qin Liu et al., 2024)

{{<citation>}}

Qin Liu, Jaemin Cho, Mohit Bansal, Marc Niethammer. (2024)  
**Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts**
<br/>
<button class="copy-to-clipboard" title="Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts" index=55>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Foundation Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00741v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00741v1.pdf" filename="2404.00741v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The goal of interactive image segmentation is to delineate specific regions within an image via visual or language <b>prompts.</b> Low-latency and high-quality interactive segmentation with diverse <b>prompts</b> remain challenging for existing specialist and generalist models. Specialist models, with their limited <b>prompts</b> and task-specific designs, experience high latency because the image must be recomputed every time the <b>prompt</b> is updated, due to the joint encoding of image and visual <b>prompts.</b> Generalist models, exemplified by the Segment Anything Model (SAM), have recently excelled in <b>prompt</b> diversity and efficiency, lifting image segmentation to the <b>foundation</b> <b>model</b> era. However, for high-quality segmentations, SAM still lags behind state-of-the-art specialist models despite SAM being trained with x100 more segmentation masks. In this work, we delve deep into the architectural differences between the two types of models. We observe that dense representation and fusion of visual <b>prompts</b> are the key design choices contributing to the high segmentation quality of specialist models. In light of this, we reintroduce this dense design into the generalist models, to facilitate the development of generalist models with high segmentation quality. To densely represent diverse visual <b>prompts,</b> we propose to use a dense map to capture five types: clicks, boxes, polygons, scribbles, and masks. Thus, we propose SegNext, a next-generation interactive segmentation approach offering low latency, high quality, and diverse <b>prompt</b> support. Our method outperforms current state-of-the-art methods on HQSeg-44K and DAVIS, both quantitatively and qualitatively.

{{</citation>}}


### (25/39 | 56/145) Learning to Rank Patches for Unbiased Image Redundancy Reduction (Yang Luo et al., 2024)

{{<citation>}}

Yang Luo, Zhineng Chen, Peng Zhou, Zuxuan Wu, Xieping Gao, Yu-Gang Jiang. (2024)  
**Learning to Rank Patches for Unbiased Image Redundancy Reduction**
<br/>
<button class="copy-to-clipboard" title="Learning to Rank Patches for Unbiased Image Redundancy Reduction" index=56>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Self-supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00680v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00680v1.pdf" filename="2404.00680v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Images suffer from heavy spatial redundancy because pixels in neighboring regions are spatially correlated. Existing approaches strive to overcome this limitation by reducing less meaningful image regions. However, current leading methods rely on supervisory signals. They may compel models to preserve content that aligns with labeled categories and discard content belonging to unlabeled categories. This categorical inductive bias makes these methods less effective in real-world scenarios. To address this issue, we propose a <b>self-supervised</b> framework for image redundancy reduction called Learning to Rank Patches (LTRP). We observe that image reconstruction of masked image modeling models is sensitive to the removal of visible patches when the masking ratio is high (e.g., 90\%). Building upon it, we implement LTRP via two steps: inferring the semantic density score of each patch by quantifying variation between reconstructions with and without this patch, and learning to rank the patches with the pseudo score. The entire process is <b>self-supervised,</b> thus getting out of the dilemma of categorical inductive bias. We design extensive experiments on different datasets and tasks. The results demonstrate that LTRP outperforms both <b>supervised</b> and other <b>self-supervised</b> methods due to the fair assessment of image content.

{{</citation>}}


### (26/39 | 57/145) Attire-Based Anomaly Detection in Restricted Areas Using YOLOv8 for Enhanced CCTV Security (Abdul Aziz A. B et al., 2024)

{{<citation>}}

Abdul Aziz A. B, Aindri Bajpai. (2024)  
**Attire-Based Anomaly Detection in Restricted Areas Using YOLOv8 for Enhanced CCTV Security**
<br/>
<button class="copy-to-clipboard" title="Attire-Based Anomaly Detection in Restricted Areas Using YOLOv8 for Enhanced CCTV Security" index=57>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: 68T40, 68T45, 68T05, 68U20, cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Object Detection, Anomaly Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00645v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00645v1.pdf" filename="2404.00645v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This research introduces an innovative security enhancement approach, employing advanced image analysis and soft computing. The focus is on an intelligent surveillance system that detects unauthorized individuals in restricted areas by analyzing attire. Traditional security measures face challenges in monitoring unauthorized access. Leveraging YOLOv8, an advanced <b>object</b> <b>detection</b> algorithm, our system identifies authorized personnel based on their attire in CCTV footage. The methodology involves training the YOLOv8 model on a comprehensive dataset of uniform patterns, ensuring precise recognition in specific regions. Soft computing techniques enhance adaptability to dynamic environments and varying lighting conditions. This research contributes to image analysis and soft computing, providing a sophisticated security solution. Emphasizing uniform-based <b>anomaly</b> <b>detection,</b> it establishes a foundation for robust security systems in restricted areas. The outcomes highlight the potential of YOLOv8-based surveillance in ensuring safety in sensitive locations.

{{</citation>}}


### (27/39 | 58/145) IPT-V2: Efficient Image Processing Transformer using Hierarchical Attentions (Zhijun Tu et al., 2024)

{{<citation>}}

Zhijun Tu, Kunpeng Du, Hanting Chen, Hailing Wang, Wei Li, Jie Hu, Yunhe Wang. (2024)  
**IPT-V2: Efficient Image Processing Transformer using Hierarchical Attentions**
<br/>
<button class="copy-to-clipboard" title="IPT-V2: Efficient Image Processing Transformer using Hierarchical Attentions" index=58>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Transformer, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00633v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00633v1.pdf" filename="2404.00633v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advances have demonstrated the powerful capability of <b>transformer</b> architecture in image restoration. However, our analysis indicates that existing transformerbased methods can not establish both exact global and local dependencies simultaneously, which are much critical to restore the details and missing content of degraded images. To this end, we present an efficient image processing <b>transformer</b> architecture with hierarchical attentions, called IPTV2, adopting a focal context <b>self-attention</b> (FCSA) and a global grid <b>self-attention</b> (GGSA) to obtain adequate token interactions in local and global receptive fields. Specifically, FCSA applies the shifted window mechanism into the channel <b>self-attention,</b> helps capture the local context and mutual interaction across channels. And GGSA constructs long-range dependencies in the cross-window grid, aggregates global information in spatial dimension. Moreover, we introduce structural re-parameterization technique to feed-forward network to further improve the model capability. Extensive experiments demonstrate that our proposed IPT-V2 achieves state-of-the-art results on various image processing tasks, covering denoising, deblurring, deraining and obtains much better trade-off for performance and computational complexity than previous methods. Besides, we extend our method to image generation as latent diffusion backbone, and significantly outperforms DiTs.

{{</citation>}}


### (28/39 | 59/145) Denoising Distillation Makes Event-Frame Transformers as Accurate Gaze Trackers (Jiading Li et al., 2024)

{{<citation>}}

Jiading Li, Zhiyu Zhu, Jinhui Hou, Junhui Hou, Jinjian Wu. (2024)  
**Denoising Distillation Makes Event-Frame Transformers as Accurate Gaze Trackers**
<br/>
<button class="copy-to-clipboard" title="Denoising Distillation Makes Event-Frame Transformers as Accurate Gaze Trackers" index=59>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Knowledge Distillation, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00548v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00548v1.pdf" filename="2404.00548v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper tackles the problem of passive gaze estimation using both event and frame data. Considering inherently different physiological structures, it's intractable to accurately estimate purely based on a given state. Thus, we reformulate the gaze estimation as the quantification of state transitions from the current state to several prior registered anchor states. Technically, we propose a two-stage learning-based gaze estimation framework to divide the whole gaze estimation process into a coarse-to-fine process of anchor state selection and final gaze location. Moreover, to improve generalization ability, we align a group of local experts with a student network, where a novel denoising <b>distillation</b> algorithm is introduced to utilize denoising diffusion technique to iteratively remove inherent noise of event data. Extensive experiments demonstrate the effectiveness of the proposed method, which greatly surpasses state-of-the-art methods by a large extent of 15$\%$. The code will be publicly available at https://github.com/jdjdli/Denoise_distill_EF_gazetracker.

{{</citation>}}


### (29/39 | 60/145) LLMs are Good Action Recognizers (Haoxuan Qu et al., 2024)

{{<citation>}}

Haoxuan Qu, Yujun Cai, Jun Liu. (2024)  
**LLMs are Good Action Recognizers**
<br/>
<button class="copy-to-clipboard" title="LLMs are Good Action Recognizers" index=60>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00532v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00532v1.pdf" filename="2404.00532v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Skeleton-based action recognition has attracted lots of research attention. Recently, to build an accurate skeleton-based action recognizer, a variety of works have been proposed. Among them, some works use <b>large</b> <b>model</b> <b>architectures</b> as backbones of their recognizers to boost the skeleton data representation capability, while some other works pre-train their recognizers on external data to enrich the knowledge. In this work, we observe that <b>large</b> <b>language</b> <b>models</b> which have been extensively used in various natural language processing tasks generally hold both <b>large</b> <b>model</b> <b>architectures</b> and rich implicit knowledge. Motivated by this, we propose a novel <b>LLM-AR</b> framework, in which we investigate treating the <b>Large</b> <b>Language</b> <b>Model</b> as an Action Recognizer. In our framework, we propose a linguistic projection process to project each input action signal (i.e., each skeleton sequence) into its ``sentence format'' (i.e., an ``action sentence''). Moreover, we also incorporate our framework with several designs to further facilitate this linguistic projection process. Extensive experiments demonstrate the efficacy of our proposed framework.

{{</citation>}}


### (30/39 | 61/145) Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation (Wenxiao Deng et al., 2024)

{{<citation>}}

Wenxiao Deng, Wenbin Li, Tianyu Ding, Lei Wang, Hongguang Zhang, Kuihua Huang, Jing Huo, Yang Gao. (2024)  
**Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation**
<br/>
<button class="copy-to-clipboard" title="Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation" index=61>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 16  
Keywords: Clustering, Knowledge Distillation, Sample Size  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00563v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00563v1.pdf" filename="2404.00563v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Dataset <b>distillation</b> has emerged as a promising approach in deep learning, enabling efficient training with small synthetic datasets derived from larger real ones. Particularly, distribution matching-based <b>distillation</b> methods attract attention thanks to its effectiveness and low computational cost. However, these methods face two primary limitations: the dispersed feature distribution within the same class in synthetic datasets, reducing class discrimination, and an exclusive focus on mean feature consistency, lacking precision and comprehensiveness. To address these challenges, we introduce two novel constraints: a class centralization constraint and a covariance matching constraint. The class centralization constraint aims to enhance class discrimination by more closely <b>clustering</b> <b>samples</b> <b>within</b> classes. The covariance matching constraint seeks to achieve more accurate feature distribution matching between real and synthetic datasets through local feature covariance matrices, particularly beneficial when <b>sample</b> <b>sizes</b> are much smaller than the number of features. Experiments demonstrate notable improvements with these constraints, yielding performance boosts of up to 6.6% on CIFAR10, 2.9% on SVHN, 2.5% on CIFAR100, and 2.5% on TinyImageNet, compared to the state-of-the-art relevant methods. In addition, our method maintains robust performance in cross-architecture settings, with a maximum performance drop of 1.7% on four architectures. Code is available at https://github.com/VincenDen/IID.

{{</citation>}}


### (31/39 | 62/145) Dual DETRs for Multi-Label Temporal Action Detection (Yuhan Zhu et al., 2024)

{{<citation>}}

Yuhan Zhu, Guozhen Zhang, Jing Tan, Gangshan Wu, Limin Wang. (2024)  
**Dual DETRs for Multi-Label Temporal Action Detection**
<br/>
<button class="copy-to-clipboard" title="Dual DETRs for Multi-Label Temporal Action Detection" index=62>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Object Detection, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00653v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00653v1.pdf" filename="2404.00653v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Temporal Action Detection (TAD) aims to identify the action boundaries and the corresponding category within untrimmed videos. Inspired by the success of DETR in <b>object</b> <b>detection,</b> several methods have adapted the query-based framework to the TAD task. However, these approaches primarily followed DETR to predict actions at the instance level (i.e., identify each action by its center point), leading to sub-optimal boundary localization. To address this issue, we propose a new Dual-level query-based TAD framework, namely DualDETR, to detect actions from both instance-level and boundary-level. Decoding at different levels requires semantics of different granularity, therefore we introduce a two-branch decoding structure. This structure builds distinctive decoding processes for different levels, facilitating explicit capture of temporal cues and semantics at each level. On top of the two-branch design, we present a joint query initialization strategy to align queries from both levels. Specifically, we leverage encoder proposals to match queries from each level in a one-to-one manner. Then, the matched queries are initialized using position and content prior from the matched action proposal. The aligned dual-level queries can refine the matched proposal with complementary cues during subsequent decoding. We evaluate DualDETR on three challenging multi-label TAD <b>benchmarks.</b> The experimental results demonstrate the superior performance of DualDETR to the existing state-of-the-art methods, achieving a substantial improvement under det-mAP and delivering impressive results under seg-mAP.

{{</citation>}}


### (32/39 | 63/145) Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated Objects (Wenxiao Cai et al., 2024)

{{<citation>}}

Wenxiao Cai, Xinyue Leiınst, Xinyu He, Junming Leo Chen, Yangang Wang. (2024)  
**Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated Objects**
<br/>
<button class="copy-to-clipboard" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated Objects" index=63>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Few-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00674v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00674v1.pdf" filename="2404.00674v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present Knowledge NeRF to synthesize novel views for dynamic scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited.To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at https://github.com/RussRobin/Knowledge_NeRF.

{{</citation>}}


### (33/39 | 64/145) Statistical Analysis by Semiparametric Additive Regression and LSTM-FCN Based Hierarchical Classification for Computer Vision Quantification of Parkinsonian Bradykinesia (Youngseo Cho et al., 2024)

{{<citation>}}

Youngseo Cho, In Hee Kwak, Dohyeon Kim, Jinhee Na, Hanjoo Sung, Jeongjae Lee, Young Eun Kim, Hyeo-il Ma. (2024)  
**Statistical Analysis by Semiparametric Additive Regression and LSTM-FCN Based Hierarchical Classification for Computer Vision Quantification of Parkinsonian Bradykinesia**
<br/>
<button class="copy-to-clipboard" title="Statistical Analysis by Semiparametric Additive Regression and LSTM-FCN Based Hierarchical Classification for Computer Vision Quantification of Parkinsonian Bradykinesia" index=64>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV, q-bio-QM, stat-AP  
Keyword Score: 10  
Keywords: LSTM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00670v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00670v1.pdf" filename="2404.00670v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Bradykinesia, characterized by involuntary slowing or decrement of movement, is a fundamental symptom of Parkinson's Disease (PD) and is vital for its clinical diagnosis. Despite various methodologies explored to quantify bradykinesia, computer vision-based approaches have shown promising results. However, these methods often fall short in adequately addressing key bradykinesia characteristics in repetitive limb movements: "occasional arrest" and "decrement in amplitude." This research advances vision-based quantification of bradykinesia by introducing nuanced numerical analysis to capture decrement in amplitudes and employing a simple deep learning technique, <b>LSTM-FCN,</b> for precise classification of occasional arrests. Our approach structures the classification process hierarchically, tailoring it to the unique dynamics of bradykinesia in PD. Statistical analysis of the extracted features, including those representing arrest and fatigue, has demonstrated their statistical significance in most cases. This finding underscores the importance of considering "occasional arrest" and "decrement in amplitude" in bradykinesia quantification of limb movement. Our enhanced diagnostic tool has been rigorously tested on an extensive dataset comprising 1396 motion videos from 310 PD patients, achieving an accuracy of 80.3%. The results confirm the robustness and reliability of our method.

{{</citation>}}


### (34/39 | 65/145) LAESI: Leaf Area Estimation with Synthetic Imagery (Jacek Kałużny et al., 2024)

{{<citation>}}

Jacek Kałużny, Yannik Schreckenberg, Karol Cyganik, Peter Annighöfer, Sören Pirk, Dominik L. Michels, Mikolaj Cieslak, Farhah Assaad-Gerbert, Bedrich Benes, Wojciech Pałubicki. (2024)  
**LAESI: Leaf Area Estimation with Synthetic Imagery**
<br/>
<button class="copy-to-clipboard" title="LAESI: Leaf Area Estimation with Synthetic Imagery" index=65>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: 68T07, 68T45, I-2-10; I-4-6, cs-AI, cs-CV, cs-GR, cs-LG, cs.CV  
Keyword Score: 10  
Keywords: Generative AI  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00593v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00593v1.pdf" filename="2404.00593v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images on millimeter paper, each with semantic masks and surface area labels. This dataset provides a resource for leaf morphology analysis primarily aimed at beech and oak leaves. We evaluate the applicability of the dataset by training machine learning models for leaf surface area prediction and semantic segmentation, using real images for validation. Our validation shows that these models can be trained to predict leaf surface area with a relative error not greater than an average human annotator. LAESI also provides an efficient framework based on 3D procedural models and <b>generative</b> <b>AI</b> for the large-scale, controllable generation of data with potential further applications in agriculture and biology. We evaluate the inclusion of <b>generative</b> <b>AI</b> in our procedural data generation pipeline and show how data filtering based on annotation consistency results in datasets which allow training the highest performing vision models.

{{</citation>}}


### (35/39 | 66/145) Memory-based Cross-modal Semantic Alignment Network for Radiology Report Generation (Yitian Tao et al., 2024)

{{<citation>}}

Yitian Tao, Liyan Ma, Jing Yu, Han Zhang. (2024)  
**Memory-based Cross-modal Semantic Alignment Network for Radiology Report Generation**
<br/>
<button class="copy-to-clipboard" title="Memory-based Cross-modal Semantic Alignment Network for Radiology Report Generation" index=66>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00588v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00588v1.pdf" filename="2404.00588v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Generating radiology reports automatically reduces the workload of radiologists and helps the diagnoses of specific diseases. Many existing methods take this task as modality transfer process. However, since the key information related to disease accounts for a small proportion in both image and report, it is hard for the model to learn the latent relation between the radiology image and its report, thus failing to generate fluent and accurate radiology reports. To tackle this problem, we propose a memory-based cross-modal semantic alignment model (MCSAM) following an encoder-decoder paradigm. MCSAM includes a well initialized long-term clinical memory bank to learn disease-related representations as well as prior knowledge for different modalities to retrieve and use the retrieved memory to perform feature consolidation. To ensure the semantic consistency of the retrieved cross modal prior knowledge, a cross-modal semantic alignment module (SAM) is proposed. SAM is also able to generate semantic visual feature embeddings which can be added to the decoder and benefits report generation. More importantly, to memorize the state and additional information while generating reports with the decoder, we use learnable memory tokens which can be seen as <b>prompts.</b> Extensive experiments demonstrate the promising performance of our proposed method which generates state-of-the-art performance on the MIMIC-CXR dataset.

{{</citation>}}


### (36/39 | 67/145) Comparison of Methods in Human Skin Decomposition (Hao Gong et al., 2024)

{{<citation>}}

Hao Gong, Michel Desvignes. (2024)  
**Comparison of Methods in Human Skin Decomposition**
<br/>
<button class="copy-to-clipboard" title="Comparison of Methods in Human Skin Decomposition" index=67>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: In-context Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00552v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00552v1.pdf" filename="2404.00552v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Decomposition of skin pigment plays an important role in medical fields. Human skin can be decomposed into two primitive components, hemoglobin and melanin. It is our goal to apply these results for diagnosis of skin cancer. In this paper, various methods for skin pigment decomposition are reviewed comparatively and the performance of each method is evaluated both theoretically and experimentally. In addition, isometric feature mapping (Isomap) is introduced in order to improve the dimensionality reduction performance in context of skin decomposition.

{{</citation>}}


### (37/39 | 68/145) OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees (Hakyeong Kim et al., 2024)

{{<citation>}}

Hakyeong Kim, Andreas Meuleman, Hyeonjoong Jang, James Tompkin, Min H. Kim. (2024)  
**OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees**
<br/>
<button class="copy-to-clipboard" title="OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees" index=68>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-GR, cs.CV  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00678v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00678v1.pdf" filename="2404.00678v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a method to reconstruct indoor and outdoor static scene <b>geometry</b> and appearance from an omnidirectional video moving in a small circular sweep. This setting is challenging because of the small baseline and large depth ranges, making it difficult to find ray crossings. To better constrain the optimization, we estimate <b>geometry</b> as a signed distance field within a spherical binoctree data structure and use a complementary efficient tree traversal strategy based on a breadth-first search for sampling. Unlike regular grids or trees, the shape of this structure well-matches the camera setting, creating a better memory-quality trade-off. From an initial depth estimate, the binoctree is adaptively subdivided throughout the optimization; previous methods use a fixed depth that leaves the scene undersampled. In comparison with three neural optimization methods and two non-neural methods, ours shows decreased <b>geometry</b> error on average, especially in a detailed scene, while significantly reducing the required number of voxels to represent such details.

{{</citation>}}


### (38/39 | 69/145) Deep Extrinsic Manifold Representation for Vision Tasks (Tongtong Zhang et al., 2024)

{{<citation>}}

Tongtong Zhang, Xian Wei, Yuanxiang Li. (2024)  
**Deep Extrinsic Manifold Representation for Vision Tasks**
<br/>
<button class="copy-to-clipboard" title="Deep Extrinsic Manifold Representation for Vision Tasks" index=69>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00544v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00544v1.pdf" filename="2404.00544v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Non-Euclidean data is frequently encountered across different fields, yet there is limited literature that addresses the fundamental challenge of training neural networks with manifold representations as outputs. We introduce the trick named Deep Extrinsic Manifold Representation (DEMR) for visual tasks in this context. DEMR incorporates extrinsic manifold embedding into deep neural networks, which helps generate manifold representations. The DEMR approach does not directly optimize the complex geodesic loss. Instead, it focuses on optimizing the computation <b>graph</b> within the embedded Euclidean space, allowing for adaptability to various architectural requirements. We provide empirical evidence supporting the proposed concept on two types of manifolds, $SE(3)$ and its associated quotient manifolds. This evidence offers theoretical assurances regarding feasibility, asymptotic properties, and generalization capability. The experimental results show that DEMR effectively adapts to point cloud alignment, producing outputs in $ SE(3) $, as well as in illumination subspace learning with outputs on the Grassmann manifold.

{{</citation>}}


### (39/39 | 70/145) NYC-Indoor-VPR: A Long-Term Indoor Visual Place Recognition Dataset with Semi-Automatic Annotation (Diwei Sheng et al., 2024)

{{<citation>}}

Diwei Sheng, Anbang Yang, John-Ross Rizzo, Chen Feng. (2024)  
**NYC-Indoor-VPR: A Long-Term Indoor Visual Place Recognition Dataset with Semi-Automatic Annotation**
<br/>
<button class="copy-to-clipboard" title="NYC-Indoor-VPR: A Long-Term Indoor Visual Place Recognition Dataset with Semi-Automatic Annotation" index=70>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00504v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00504v1.pdf" filename="2404.00504v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Visual Place Recognition (VPR) in indoor environments is beneficial to humans and robots for better localization and navigation. It is challenging due to appearance changes at various frequencies, and difficulties of obtaining ground truth metric trajectories for training and evaluation. This paper introduces the NYC-Indoor-VPR dataset, a unique and rich collection of over 36,000 images compiled from 13 distinct crowded scenes in New York City taken under varying lighting conditions with appearance changes. Each scene has multiple revisits across a year. To establish the ground truth for VPR, we propose a semiautomatic annotation approach that computes the positional information of each image. Our method specifically takes pairs of videos as input and yields matched pairs of images along with their estimated relative locations. The accuracy of this matching is refined by human annotators, who utilize our annotation software to correlate the selected keyframes. Finally, we present a <b>benchmark</b> evaluation of several state-of-the-art VPR algorithms using our annotated dataset, revealing its challenge and thus value for VPR research.

{{</citation>}}


## cs.LG (27)



### (1/27 | 71/145) Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing (Zhenyu Qian et al., 2024)

{{<citation>}}

Zhenyu Qian, Yiming Qian, Yuting Song, Fei Gao, Hai Jin, Chen Yu, Xia Xie. (2024)  
**Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing**
<br/>
<button class="copy-to-clipboard" title="Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing" index=71>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG  
Keyword Score: 66  
Keywords: Graph Classification, Graph, Benchmarking, Few-shot, Fine-tuning, Geometry, Knowledge Graph, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00589v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00589v1.pdf" filename="2404.00589v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Handling <b>graph</b> <b>data</b> is one of the most difficult tasks. Traditional techniques, such as those based on <b>geometry</b> and matrix factorization, rely on assumptions about the data relations that become inadequate when handling <b>large</b> <b>and</b> <b>complex</b> <b>graph</b> <b>data.</b> On the other hand, deep learning approaches demonstrate promising results in handling <b>large</b> <b>graph</b> <b>data,</b> but they often fall short of providing interpretable explanations. To equip the <b>graph</b> <b>processing</b> with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two <b>graph</b> <b>processing</b> tasks: <b>few-shot</b> <b>knowledge</b> <b>graph</b> <b>completion</b> and <b>graph</b> <b>classification.</b> Our results demonstrate that through parameter efficient <b>fine-tuning,</b> the <b>LLM</b> surpasses state-of-the-art algorithms by a substantial margin across ten diverse <b>benchmark</b> datasets. Moreover, to address the challenge of explainability, we propose an uncertainty estimation based on perturbation, along with a calibration scheme to quantify the confidence scores of the generated answers. Our confidence measure achieves an AUC of 0.8 or higher on seven out of the ten datasets in predicting the correctness of the answer generated by <b>LLM.</b>

{{</citation>}}


### (2/27 | 72/145) Variational Autoencoders for exteroceptive perception in reinforcement learning-based collision avoidance (Thomas Nakken Larsen et al., 2024)

{{<citation>}}

Thomas Nakken Larsen, Eirik Runde Barlaug, Adil Rasheed. (2024)  
**Variational Autoencoders for exteroceptive perception in reinforcement learning-based collision avoidance**
<br/>
<button class="copy-to-clipboard" title="Variational Autoencoders for exteroceptive perception in reinforcement learning-based collision avoidance" index=72>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-RO, cs.LG  
Keyword Score: 50  
Keywords: Autoencoder, Reinforcement Learning, Simulation, Simulator, Variational Autoencoder  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00623v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00623v1.pdf" filename="2404.00623v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Modern control systems are increasingly turning to machine learning algorithms to augment their performance and adaptability. Within this context, Deep <b>Reinforcement</b> <b>Learning</b> (DRL) has emerged as a promising control framework, particularly in the domain of marine transportation. Its potential for autonomous marine applications lies in its ability to seamlessly combine path-following and collision avoidance with an arbitrary number of obstacles. However, current DRL algorithms require disproportionally large computational resources to find near-optimal policies compared to the posed control problem when the searchable parameter space becomes large. To combat this, our work delves into the application of <b>Variational</b> <b>AutoEncoders</b> (VAEs) to acquire a generalized, low-dimensional latent encoding of a high-fidelity range-finding sensor, which serves as the exteroceptive input to a DRL agent. The agent's performance, encompassing path-following and collision avoidance, is systematically tested and evaluated within a stochastic <b>simulation</b> environment, presenting a comprehensive exploration of our proposed approach in maritime control systems.

{{</citation>}}


### (3/27 | 73/145) Transfer Learning with Reconstruction Loss (Wei Cui et al., 2024)

{{<citation>}}

Wei Cui, Wei Yu. (2024)  
**Transfer Learning with Reconstruction Loss**
<br/>
<button class="copy-to-clipboard" title="Transfer Learning with Reconstruction Loss" index=73>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs-NI, cs.LG, stat-ML  
Keyword Score: 50  
Keywords: MNIST, Reconstruction Loss, Simulation, Simulator, Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00505v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00505v1.pdf" filename="2404.00505v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional <b>reconstruction</b> <b>stage</b> associated with a new <b>reconstruction</b> <b>loss.</b> This loss is for reconstructing the common information starting from a selected hidden layer in the model. The proposed approach encourages the learned features to be general and transferable, and therefore can be readily used for efficient <b>transfer</b> <b>learning.</b> For numerical <b>simulations,</b> three applications are studied: <b>transfer</b> <b>learning</b> on classifying <b>MNIST</b> handwritten digits, the device-to-device wireless network power allocation, and the multiple-input-single-output network downlink beamforming and localization. <b>Simulation</b> results suggest that the proposed approach is highly efficient in data and model complexity, is resilient to over-fitting, and has competitive performances.

{{</citation>}}


### (4/27 | 74/145) PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning (Weihua Hu et al., 2024)

{{<citation>}}

Weihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan, Jure Leskovec, Matthias Fey. (2024)  
**PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning**
<br/>
<button class="copy-to-clipboard" title="PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning" index=74>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-DB, cs-LG, cs.LG, stat-ML  
Keyword Score: 46  
Keywords: Graph, Graph Neural Network, Graph Neural Network, Foundation Model, Multi-modal, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00776v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00776v1.pdf" filename="2404.00776v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present PyTorch Frame, a PyTorch-based framework for deep learning over <b>multi-modal</b> tabular data. PyTorch Frame makes tabular deep learning easy by providing a PyTorch-based data structure to handle complex tabular data, introducing a model abstraction to enable modular implementation of tabular models, and allowing external <b>foundation</b> <b>models</b> to be incorporated to handle complex columns (e.g., <b>LLMs</b> for text columns). We demonstrate the usefulness of PyTorch Frame by implementing diverse tabular models in a modular way, successfully applying these models to complex <b>multi-modal</b> tabular data, and integrating our framework with PyTorch Geometric, a PyTorch library for <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> to perform end-to-end learning over relational databases.

{{</citation>}}


### (5/27 | 75/145) A General and Efficient Training for Transformer via Token Expansion (Wenxuan Huang et al., 2024)

{{<citation>}}

Wenxuan Huang, Yunhang Shen, Jiao Xie, Baochang Zhang, Gaoqi He, Ke Li, Xing Sun, Shaohui Lin. (2024)  
**A General and Efficient Training for Transformer via Token Expansion**
<br/>
<button class="copy-to-clipboard" title="A General and Efficient Training for Transformer via Token Expansion" index=75>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG  
Keyword Score: 40  
Keywords: Vision Transformer, Fine-tuning, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00672v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00672v1.pdf" filename="2404.00672v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The remarkable performance of <b>Vision</b> <b>Transformers</b> (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original <b>transformers,</b> including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different <b>Transformer</b> networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original <b>transformers,</b> preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and <b>fine-tuning</b> process of <b>transformers</b> (e.g., DeiT and LV-ViT), but also effective for efficient training frameworks (e.g., EfficientTrain), without twisting the original training hyper-parameters, architecture, and introducing additional training strategies. Extensive experiments demonstrate that ToE achieves about 1.3x faster for the training of ViTs in a lossless manner, or even with performance gains over the full-token training baselines. Code is available at https://github.com/Osilly/TokenExpansion .

{{</citation>}}


### (6/27 | 76/145) HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs (Yue Zhang et al., 2024)

{{<citation>}}

Yue Zhang, Yuntian He, Saket Gurukar, Srinivasan Parthasarathy. (2024)  
**HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs**
<br/>
<button class="copy-to-clipboard" title="HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs" index=76>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 38  
Keywords: Node Classification, Graph, Graph Embedding, Convolution, Representation Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00816v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00816v1.pdf" filename="2404.00816v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Heterogeneous <b>graphs</b> <b>are</b> ubiquitous in real-world applications because they can represent various relationships between different types of entities. Therefore, learning embeddings in such <b>graphs</b> <b>is</b> a critical problem in <b>graph</b> <b>machine</b> learning. However, existing solutions for this problem fail to scale to large heterogeneous <b>graphs</b> <b>due</b> to their high computational complexity. To address this issue, we propose a Multi-Level Embedding framework of <b>nodes</b> <b>on</b> a heterogeneous <b>graph</b> <b>(HeteroMILE)</b> - a generic methodology that allows contemporary <b>graph</b> <b>embedding</b> methods to scale to large <b>graphs.</b> <b>HeteroMILE</b> repeatedly coarsens the large sized <b>graph</b> <b>into</b> a smaller size while preserving the backbone structure of the <b>graph</b> <b>before</b> embedding it, effectively reducing the computational cost by avoiding time-consuming processing operations. It then refines the coarsened embedding to the original <b>graph</b> <b>using</b> a heterogeneous <b>graph</b> <b>convolution</b> neural network. We evaluate our approach using several popular heterogeneous <b>graph</b> <b>datasets.</b> The experimental results show that HeteroMILE can substantially reduce computational time (approximately 20x speedup) and generate an embedding of better quality for link prediction and <b>node</b> <b>classification.</b>

{{</citation>}}


### (7/27 | 77/145) HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs (Sunwoo Kim et al., 2024)

{{<citation>}}

Sunwoo Kim, Shinhwan Kang, Fanchen Bu, Soo Yong Lee, Jaemin Yoo, Kijung Shin. (2024)  
**HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs**
<br/>
<button class="copy-to-clipboard" title="HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs" index=77>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 38  
Keywords: Node Classification, Benchmarking, Representation Learning, Self-supervised Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00638v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00638v1.pdf" filename="2404.00638v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple <b>nodes</b> <b>with</b> hyperedges, and better capturing the topology is essential for effective <b>representation</b> <b>learning.</b> Recent advances in generative <b>self-supervised</b> <b>learning</b> (SSL) suggest that hypergraph neural networks learned from generative self supervision have the potential to effectively encode the complex hypergraph topology. Designing a generative SSL strategy for hypergraphs, however, is not straightforward. Questions remain with regard to its generative SSL task, connection to downstream tasks, and empirical properties of learned <b>representations.</b> <b>In</b> light of the promises and challenges, we propose a novel generative SSL strategy for hypergraphs. We first formulate a generative SSL task on hypergraphs, hyperedge filling, and highlight its theoretical connection to <b>node</b> <b>classification.</b> Based on the generative SSL task, we propose a hypergraph SSL method, HypeBoy. HypeBoy learns effective general-purpose hypergraph <b>representations,</b> <b>outperforming</b> 16 baseline methods across 11 <b>benchmark</b> datasets.

{{</citation>}}


### (8/27 | 78/145) Observations on Building RAG Systems for Technical Documents (Sumit Soman et al., 2024)

{{<citation>}}

Sumit Soman, Sujoy Roychowdhury. (2024)  
**Observations on Building RAG Systems for Technical Documents**
<br/>
<button class="copy-to-clipboard" title="Observations on Building RAG Systems for Technical Documents" index=78>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: I-2-7, cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00657v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00657v1.pdf" filename="2404.00657v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting <b>RAG</b> and perform experiments to highlight best practices and potential challenges to build <b>RAG</b> systems for technical documents.

{{</citation>}}


### (9/27 | 79/145) CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization (Yao Ni et al., 2024)

{{<citation>}}

Yao Ni, Piotr Koniusz. (2024)  
**CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization**
<br/>
<button class="copy-to-clipboard" title="CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization" index=79>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Few-shot, Generative Adversarial Network, Generative Adversarial Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00521v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00521v2.pdf" filename="2404.00521v2.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, <b>GANs</b> often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient <b>GANs.</b> Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances <b>GAN</b> training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN's effectiveness in reducing gradients in latent features and weights, improving stability and generalization in <b>GAN</b> training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven high-resolution <b>few-shot</b> image datasets.

{{</citation>}}


### (10/27 | 80/145) DailyMAE: Towards Pretraining Masked Autoencoders in One Day (Jiantao Wu et al., 2024)

{{<citation>}}

Jiantao Wu, Shentong Mo, Sara Atito, Zhenhua Feng, Josef Kittler, Muhammad Awais. (2024)  
**DailyMAE: Towards Pretraining Masked Autoencoders in One Day**
<br/>
<button class="copy-to-clipboard" title="DailyMAE: Towards Pretraining Masked Autoencoders in One Day" index=80>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Autoencoder, Self-supervised Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00509v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00509v1.pdf" filename="2404.00509v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, masked image modeling (MIM), an important <b>self-supervised</b> <b>learning</b> (SSL) method, has drawn attention for its effectiveness in learning data representation from unlabeled data. Numerous studies underscore the advantages of MIM, highlighting how models pretrained on extensive datasets can enhance the performance of downstream tasks. However, the high computational demands of pretraining pose significant challenges, particularly within academic environments, thereby impeding the SSL research progress. In this study, we propose efficient training recipes for MIM based SSL that focuses on mitigating data loading bottlenecks and employing progressive training techniques and other tricks to closely maintain pretraining performance. Our library enables the training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs within just 18 hours, using a single machine equipped with 8 A100 GPUs. By achieving speed gains of up to 5.8 times, this work not only demonstrates the feasibility of conducting high-efficiency SSL training but also paves the way for broader accessibility and promotes advancement in SSL research particularly for prototyping and initial testing of SSL ideas. The code is available in https://github.com/erow/FastSSL.

{{</citation>}}


### (11/27 | 81/145) Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models (Shaofei Shen et al., 2024)

{{<citation>}}

Shaofei Shen, Chenhao Zhang, Yawen Zhao, Alina Bialkowski, Weitong Chen, Miao Xu. (2024)  
**Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models**
<br/>
<button class="copy-to-clipboard" title="Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models" index=81>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Machine Unlearning, Supervised Learning, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00506v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00506v1.pdf" filename="2404.00506v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Machine</b> <b>unlearning</b> aims to remove information derived from forgotten data while preserving that of the remaining dataset in a well-trained model. With the increasing emphasis on data privacy, several approaches to <b>machine</b> <b>unlearning</b> have emerged. However, these methods typically rely on complete supervision throughout the unlearning process. Unfortunately, obtaining such supervision, whether for the forgetting or remaining data, can be impractical due to the substantial cost associated with annotating real-world datasets. This challenge <b>prompts</b> us to propose a supervision-free unlearning approach that operates without the need for labels during the unlearning process. Specifically, we introduce a variational approach to approximate the distribution of representations for the remaining data. Leveraging this approximation, we adapt the original model to eliminate information from the forgotten data at the representation level. To further address the issue of lacking supervision information, which hinders alignment with ground truth, we introduce a contrastive loss to facilitate the matching of representations between the remaining data and those of the original model, thus preserving predictive performance. Experimental results across various unlearning tasks demonstrate the effectiveness of our proposed method, Label-Agnostic Forgetting (LAF) without using any labels, which achieves comparable performance to state-of-the-art methods that rely on full supervision information. Furthermore, our approach excels in semi-supervised scenarios, leveraging limited supervision information to outperform fully <b>supervised</b> baselines. This work not only showcases the viability of supervision-free unlearning in deep models but also opens up a new possibility for future research in unlearning at the representation level.

{{</citation>}}


### (12/27 | 82/145) Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning (Mohamed Elsayed et al., 2024)

{{<citation>}}

Mohamed Elsayed, A. Rupam Mahmood. (2024)  
**Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning**
<br/>
<button class="copy-to-clipboard" title="Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning" index=82>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 25  
Keywords: Continual Learning, Reinforcement Learning, Representation Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00781v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00781v1.pdf" filename="2404.00781v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep <b>representation</b> <b>learning</b> methods struggle with <b>continual</b> <b>learning,</b> suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the <b>continual</b> <b>learning</b> of <b>representations.</b> <b>UPGD</b> combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where <b>continual</b> <b>learning</b> problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended <b>reinforcement</b> <b>learning</b> experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both <b>continual</b> <b>learning</b> issues.

{{</citation>}}


### (13/27 | 83/145) Rehearsal-Free Modular and Compositional Continual Learning for Language Models (Mingyang Wang et al., 2024)

{{<citation>}}

Mingyang Wang, Heike Adel, Lukas Lange, Jannik Strötgen, Hinrich Schütze. (2024)  
**Rehearsal-Free Modular and Compositional Continual Learning for Language Models**
<br/>
<button class="copy-to-clipboard" title="Rehearsal-Free Modular and Compositional Continual Learning for Language Models" index=83>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG  
Keyword Score: 23  
Keywords: Benchmarking, Continual Learning, Knowledge Transfer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00790v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00790v1.pdf" filename="2404.00790v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Continual</b> <b>learning</b> aims at incrementally acquiring new <b>knowledge</b> <b>while</b> not forgetting existing <b>knowledge.</b> <b>To</b> overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation <b>continual</b> <b>learning</b> does not consider interaction between tasks, thus hindering <b>knowledge</b> <b>transfer.</b> In this work, we propose MoCL, a rehearsal-free Modular and Compositional <b>Continual</b> <b>Learning</b> framework which continually adds new modules to language models and composes them with existing modules. Experiments on various <b>benchmarks</b> show that MoCL outperforms state of the art and effectively facilitates <b>knowledge</b> <b>transfer.</b>

{{</citation>}}


### (14/27 | 84/145) Meta Learning in Bandits within Shared Affine Subspaces (Steven Bilaj et al., 2024)

{{<citation>}}

Steven Bilaj, Sofien Dhouib, Setareh Maghsudi. (2024)  
**Meta Learning in Bandits within Shared Affine Subspaces**
<br/>
<button class="copy-to-clipboard" title="Meta Learning in Bandits within Shared Affine Subspaces" index=84>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 20  
Keywords: Bandit Algorithm, Meta Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00688v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00688v1.pdf" filename="2404.00688v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We study the problem of <b>meta-learning</b> <b>several</b> contextual stochastic <b>bandits</b> tasks by leveraging their concentration around a low-dimensional affine subspace, which we learn via online principal component analysis to reduce the expected regret over the encountered <b>bandits.</b> We propose and theoretically analyze two strategies that solve the problem: One based on the principle of optimism in the face of uncertainty and the other via Thompson sampling. Our framework is generic and includes previously proposed approaches as special cases. Besides, the empirical results show that our methods significantly reduce the regret on several <b>bandit</b> tasks.

{{</citation>}}


### (15/27 | 85/145) Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification (PoTsang B. Huang et al., 2024)

{{<citation>}}

PoTsang B. Huang, Muhammad Rizwan, Mehboob Ali. (2024)  
**Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification**
<br/>
<button class="copy-to-clipboard" title="Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification" index=85>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CV, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Convolutional Neural Network, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00576v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00576v1.pdf" filename="2404.00576v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The uncontrolled and unstructured growth of brain cells is known as brain tumor, which has one of the highest mortality rates among diseases from all types of cancers. Due to limited diagnostic and treatment capabilities, they pose significant challenges, especially in third-world countries. Early diagnosis plays a vital role in effectively managing brain tumors and reducing mortality rates. However, the availability of diagnostic methods is hindered by various limitations, including high costs and lengthy result acquisition times, impeding early detection of the disease. In this study, we present two cutting-edge bi-fold weighted voting ensemble models that aim to boost the effectiveness of weighted ensemble methods. These two proposed methods combine the classification outcomes from multiple classifiers and determine the optimal result by selecting the one with the highest probability in the first approach, and the highest weighted prediction in the second technique. These approaches significantly improve the overall performance of weighted ensemble techniques. In the first proposed method, we improve the soft voting technique (SVT) by introducing a novel <b>unsupervised</b> weight calculating schema (UWCS) to enhance its weight assigning capability, known as the extended soft voting technique (ESVT). Secondly, we propose a novel weighted method (NWM) by using the proposed UWCS. Both of our approaches incorporate three distinct models: a custom-built <b>CNN,</b> VGG-16, and InceptionResNetV2 which has been trained on publicly available datasets. The effectiveness of our proposed systems is evaluated through blind testing, where exceptional results are achieved. We then establish a comparative analysis of the performance of our proposed methods with that of SVT to show their superiority and effectiveness.

{{</citation>}}


### (16/27 | 86/145) Generative weather for improved crop model simulations (Yuji Saikai, 2024)

{{<citation>}}

Yuji Saikai. (2024)  
**Generative weather for improved crop model simulations**
<br/>
<button class="copy-to-clipboard" title="Generative weather for improved crop model simulations" index=86>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00528v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00528v1.pdf" filename="2404.00528v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Accurate and precise crop yield prediction is invaluable for decision making at both farm levels and regional levels. To make yield prediction, crop models are widely used for their capability to simulate hypothetical scenarios. While accuracy and precision of yield prediction critically depend on weather inputs to <b>simulations,</b> surprisingly little attention has been paid to preparing weather inputs. We propose a new method to construct generative models for long-term weather forecasts and ultimately improve crop yield prediction. We demonstrate use of the method in two representative scenarios -- single-year production of wheat, barley and canola and three-year production using rotations of these crops. Results show significant improvement from the conventional method, measured in terms of mean and standard deviation of prediction errors. Our method outperformed the conventional method in every one of 18 metrics for the first scenario and in 29 out of 36 metrics for the second scenario. For individual crop modellers to start applying the method to their problems, technical details are carefully explained, and all the code, trained PyTorch models, APSIM <b>simulation</b> files and result data are made available.

{{</citation>}}


### (17/27 | 87/145) Creating synthetic energy meter data using conditional diffusion and building metadata (Chun Fu et al., 2024)

{{<citation>}}

Chun Fu, Hussain Kazmi, Matias Quintana, Clayton Miller. (2024)  
**Creating synthetic energy meter data using conditional diffusion and building metadata**
<br/>
<button class="copy-to-clipboard" title="Creating synthetic energy meter data using conditional diffusion and building metadata" index=87>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-SY, cs.LG, eess-SY  
Keyword Score: 20  
Keywords: Diffusion Model, Generative Adversarial Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00525v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00525v1.pdf" filename="2404.00525v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Advances in machine learning and increased computational power have driven progress in energy-related research. However, limited access to private energy data from buildings hinders traditional regression models relying on historical data. While <b>generative</b> <b>models</b> <b>offer</b> a solution, previous studies have primarily focused on short-term generation periods (e.g., daily profiles) and a limited number of meters. Thus, the study proposes a conditional <b>diffusion</b> <b>model</b> for generating high-quality synthetic energy data using relevant metadata. Using a dataset comprising 1,828 power meters from various buildings and countries, this model is compared with traditional methods like Conditional <b>Generative</b> <b>Adversarial</b> <b>Networks</b> (CGAN) and Conditional Variational Auto-Encoders (CVAE). It explicitly handles long-term annual consumption profiles, harnessing metadata such as location, weather, building, and meter type to produce coherent synthetic data that closely resembles real-world energy consumption patterns. The results demonstrate the proposed <b>diffusion</b> <b>model's</b> superior performance, with a 36% reduction in Frechet Inception Distance (FID) score and a 13% decrease in Kullback-Leibler divergence (KL divergence) compared to the following best method. The proposed method successfully generates high-quality energy data through metadata, and its code will be open-sourced, establishing a foundation for a broader array of energy data generation models in the future.

{{</citation>}}


### (18/27 | 88/145) Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement Learning (Satoko Iida et al., 2024)

{{<citation>}}

Satoko Iida, Ryota Yasudo. (2024)  
**Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement Learning" index=88>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 16  
Keywords: Graph, Benchmarking, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00539v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00539v1.pdf" filename="2404.00539v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Quadratic Assignment Problem (QAP) is a practical combinatorial optimization problems that has been studied for several years. Since it is NP-hard, solving large problem instances of QAP is challenging. Although heuristics can find semi-optimal solutions, the execution time significantly increases as the problem size increases. Recently, solving combinatorial optimization problems by deep learning has been attracting attention as a faster solver than heuristics. Even with deep learning, however, solving large QAP is still challenging. In this paper, we propose the deep <b>reinforcement</b> <b>learning</b> model called the two-stage <b>graph</b> pointer network (GPN) for solving QAP. Two-stage GPN relies on GPN, which has been proposed for Euclidean Traveling Salesman Problem (TSP). First, we extend GPN for general TSP, and then we add new algorithms to that model for solving QAP. Our experimental results show that our two-stage GPN provides semi-optimal solutions for <b>benchmark</b> problem instances from TSPlib and QAPLIB.

{{</citation>}}


### (19/27 | 89/145) Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning (Srinjoy Roy et al., 2024)

{{<citation>}}

Srinjoy Roy, Swagatam Das. (2024)  
**Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning" index=89>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 13  
Keywords: Benchmarking, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00686v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00686v2.pdf" filename="2404.00686v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Accounting for the uncertainty of value functions boosts exploration in <b>Reinforcement</b> <b>Learning</b> (RL). Our work introduces Maximum Mean Discrepancy Q-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty propagation during Temporal Difference (TD) updates. MMD-QL uses the MMD barycenter for this purpose, as MMD provides a tighter estimate of closeness between probability measures than the Wasserstein distance. Firstly, we establish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under the average loss metric. Concerning the accumulated rewards, experiments on tabular environments show that MMD-QL outperforms WQL and other algorithms. Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network (MMD-QN). Making reasonable assumptions, we analyze the convergence rates of MMD-QN using function approximation. Empirical results on challenging Atari games demonstrate that MMD-QN performs well compared to <b>benchmark</b> deep RL algorithms, highlighting its effectiveness in handling large state-action spaces.

{{</citation>}}


### (20/27 | 90/145) From Similarity to Superiority: Channel Clustering for Time Series Forecasting (Jialin Chen et al., 2024)

{{<citation>}}

Jialin Chen, Jan Eric Lenssen, Aosong Feng, Weihua Hu, Matthias Fey, Leandros Tassiulas, Jure Leskovec, Rex Ying. (2024)  
**From Similarity to Superiority: Channel Clustering for Time Series Forecasting**
<br/>
<button class="copy-to-clipboard" title="From Similarity to Superiority: Channel Clustering for Time Series Forecasting" index=90>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 13  
Keywords: Clustering, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.01340v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.01340v1.pdf" filename="2404.01340v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Time series forecasting has attracted significant attention in recent decades. Previous studies have demonstrated that the Channel-Independent (CI) strategy improves forecasting performance by treating different channels individually, while it leads to poor generalization on unseen instances and ignores potentially necessary interactions between channels. Conversely, the Channel-Dependent (CD) strategy mixes all channels with even irrelevant and indiscriminate information, which, however, results in oversmoothing issues and limits forecasting accuracy. There is a lack of channel strategy that effectively balances individual channel treatment for improved forecasting performance without overlooking essential interactions between channels. Motivated by our observation of a correlation between the time series model's performance boost against channel mixing and the intrinsic similarity on a pair of channels, we developed a novel and adaptable Channel <b>Clustering</b> Module (CCM). CCM dynamically groups channels characterized by intrinsic similarities and leverages cluster identity instead of channel identity, combining the best of CD and CI worlds. Extensive experiments on real-world datasets demonstrate that CCM can (1) boost the performance of CI and CD models by an average margin of 2.4% and 7.2% on long-term and short-term forecasting, respectively; (2) enable <b>zero-shot</b> forecasting with mainstream time series forecasting models; (3) uncover intrinsic time series patterns among channels and improve interpretability of complex time series models.

{{</citation>}}


### (21/27 | 91/145) Minimum-Norm Interpolation Under Covariate Shift (Neil Mallinar et al., 2024)

{{<citation>}}

Neil Mallinar, Austin Zane, Spencer Frei, Bin Yu. (2024)  
**Minimum-Norm Interpolation Under Covariate Shift**
<br/>
<button class="copy-to-clipboard" title="Minimum-Norm Interpolation Under Covariate Shift" index=91>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 13  
Keywords: Sample Size, Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00522v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00522v1.pdf" filename="2404.00522v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Transfer</b> <b>learning</b> is a critical part of real-world machine learning deployments and has been extensively studied in experimental works with overparameterized neural networks. However, even in the simplest setting of linear regression a notable gap still exists in the theoretical understanding of <b>transfer</b> <b>learning.</b> In-distribution research on high-dimensional linear regression has led to the identification of a phenomenon known as \textit{benign overfitting}, in which linear interpolators overfit to noisy training labels and yet still generalize well. This behavior occurs under specific conditions on the source covariance matrix and input data dimension. Therefore, it is natural to wonder how such high-dimensional linear models behave under <b>transfer</b> <b>learning.</b> We prove the first non-asymptotic excess risk bounds for benignly-overfit linear interpolators in the <b>transfer</b> <b>learning</b> setting. From our analysis, we propose a taxonomy of \textit{beneficial} and \textit{malignant} covariate shifts based on the degree of overparameterization. We follow our analysis with empirical studies that show these beneficial and malignant covariate shifts for linear interpolators on real image data, and for fully-connected neural networks in settings where the input data dimension is larger than the training <b>sample</b> <b>size.</b>

{{</citation>}}


### (22/27 | 92/145) On Difficulties of Attention Factorization through Shared Memory (Uladzislau Yorsh et al., 2024)

{{<citation>}}

Uladzislau Yorsh, Martin Holeňa, Ondřej Bojar, David Herel. (2024)  
**On Difficulties of Attention Factorization through Shared Memory**
<br/>
<button class="copy-to-clipboard" title="On Difficulties of Attention Factorization through Shared Memory" index=92>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00798v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00798v1.pdf" filename="2404.00798v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Transformers</b> have revolutionized deep learning in numerous fields, including natural language processing, computer vision, and audio processing. Their strength lies in their attention mechanism, which allows for the discovering of complex input relationships. However, this mechanism's quadratic time and memory complexity pose challenges for larger inputs. Researchers are now investigating models like Linear Unified Nested Attention (Luna) or Memory Augmented <b>Transformer,</b> which leverage external learnable memory to either reduce the attention computation complexity down to linear, or to propagate information between chunks in chunk-wise processing. Our findings challenge the conventional thinking on these models, revealing that interfacing with the memory directly through an attention operation is suboptimal, and that the performance may be considerably improved by filtering the input signal before communicating with memory.

{{</citation>}}


### (23/27 | 93/145) Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration (Yibo Wang et al., 2024)

{{<citation>}}

Yibo Wang, Jiang Zhao. (2024)  
**Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration**
<br/>
<button class="copy-to-clipboard" title="Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration" index=93>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00651v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00651v1.pdf" filename="2404.00651v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements in deep <b>reinforcement</b> <b>learning</b> (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive experiments, our method shows competitive or even superior performance compared to prior works, especially the sparse reward cases.

{{</citation>}}


### (24/27 | 94/145) ADs: Active Data-sharing for Data Quality Assurance in Advanced Manufacturing Systems (Yue Zhao et al., 2024)

{{<citation>}}

Yue Zhao, Yuxuan Li, Chenang Liu, Yinan Wang. (2024)  
**ADs: Active Data-sharing for Data Quality Assurance in Advanced Manufacturing Systems**
<br/>
<button class="copy-to-clipboard" title="ADs: Active Data-sharing for Data Quality Assurance in Advanced Manufacturing Systems" index=94>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Anomaly Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00572v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00572v1.pdf" filename="2404.00572v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Machine learning (ML) methods are widely used in industrial applications, which usually require a large amount of training data. However, data collection needs extensive time costs and investments in the manufacturing system, and data scarcity commonly exists. Therefore, data-sharing is widely enabled among multiple machines with similar functionality to augment the dataset for building ML methods. However, distribution mismatch inevitably exists in their data due to different working conditions, while the ML methods are assumed to be built and tested on the dataset following the same distribution. Thus, an Active Data-sharing (ADs) framework is proposed to ensure the quality of the shared data among multiple machines. It is designed to simultaneously select the most informative data points benefiting the downstream tasks and mitigate the distribution mismatch among all selected data points. The proposed method is validated on <b>anomaly</b> <b>detection</b> on in-situ monitoring data from three additive manufacturing processes.

{{</citation>}}


### (25/27 | 95/145) Block-Diagonal Guided DBSCAN Clustering (Zheng Xing et al., 2024)

{{<citation>}}

Zheng Xing, Weibing Zhao. (2024)  
**Block-Diagonal Guided DBSCAN Clustering**
<br/>
<button class="copy-to-clipboard" title="Block-Diagonal Guided DBSCAN Clustering" index=95>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-DS, cs-LG, cs.LG  
Keyword Score: 9  
Keywords: Graph, Benchmarking, Clustering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.01341v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.01341v1.pdf" filename="2404.01341v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Cluster analysis plays a crucial role in database mining, and one of the most widely used algorithms in this field is DBSCAN. However, DBSCAN has several limitations, such as difficulty in handling high-dimensional large-scale data, sensitivity to input parameters, and lack of robustness in producing <b>clustering</b> results. This paper introduces an improved version of DBSCAN that leverages the block-diagonal property of the similarity <b>graph</b> to guide the <b>clustering</b> procedure of DBSCAN. The key idea is to construct a <b>graph</b> that measures the similarity between high-dimensional large-scale data points and has the potential to be transformed into a block-diagonal form through an unknown permutation, followed by a cluster-ordering procedure to generate the desired permutation. The <b>clustering</b> structure can be easily determined by identifying the diagonal blocks in the permuted <b>graph.</b> We propose a gradient descent-based method to solve the proposed problem. Additionally, we develop a DBSCAN-based points traversal algorithm that identifies clusters with high densities in the <b>graph</b> and generates an augmented ordering of clusters. The block-diagonal structure of the <b>graph</b> is then achieved through permutation based on the traversal order, providing a flexible foundation for both automatic and interactive cluster analysis. We introduce a split-and-refine algorithm to automatically search for all diagonal blocks in the permuted <b>graph</b> with theoretically optimal guarantees under specific cases. We extensively evaluate our proposed approach on twelve challenging real-world <b>benchmark</b> <b>clustering</b> datasets and demonstrate its superior performance compared to the state-of-the-art <b>clustering</b> method on every dataset.

{{</citation>}}


### (26/27 | 96/145) SOAR: Improved Indexing for Approximate Nearest Neighbor Search (Philip Sun et al., 2024)

{{<citation>}}

Philip Sun, David Simcha, Dave Dopson, Ruiqi Guo, Sanjiv Kumar. (2024)  
**SOAR: Improved Indexing for Approximate Nearest Neighbor Search**
<br/>
<button class="copy-to-clipboard" title="SOAR: Improved Indexing for Approximate Nearest Neighbor Search" index=96>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00774v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00774v1.pdf" filename="2404.00774v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces SOAR: Spilling with Orthogonality-Amplified Residuals, a novel data indexing technique for approximate nearest neighbor (ANN) search. SOAR extends upon previous approaches to ANN search, such as spill trees, that utilize multiple redundant representations while partitioning the data to reduce the probability of missing a nearest neighbor during search. Rather than training and computing these redundant representations independently, however, SOAR uses an orthogonality-amplified residual loss, which optimizes each representation to compensate for cases where other representations perform poorly. This drastically improves the overall index quality, resulting in state-of-the-art ANN <b>benchmark</b> performance while maintaining fast indexing times and low memory consumption.

{{</citation>}}


### (27/27 | 97/145) Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in Quantifying Uncertainty Propagation (Minglei Yang et al., 2024)

{{<citation>}}

Minglei Yang, Pengjun Wang, Ming Fan, Dan Lu, Yanzhao Cao, Guannan Zhang. (2024)  
**Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in Quantifying Uncertainty Propagation**
<br/>
<button class="copy-to-clipboard" title="Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in Quantifying Uncertainty Propagation" index=97>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-NA, cs.LG, math-NA  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00502v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00502v1.pdf" filename="2404.00502v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We introduce a conditional pseudo-reversible normalizing flow for constructing surrogate models of a physical model polluted by additive noise to efficiently quantify forward and inverse uncertainty propagation. Existing surrogate modeling approaches usually focus on approximating the deterministic component of physical model. However, this strategy necessitates knowledge of noise and resorts to auxiliary sampling methods for quantifying inverse uncertainty propagation. In this work, we develop the conditional pseudo-reversible normalizing flow model to directly learn and efficiently generate samples from the conditional probability density functions. The training process utilizes dataset consisting of input-output pairs without requiring prior knowledge about the noise and the function. Our model, once trained, can generate samples from any conditional probability density functions whose high probability regions are covered by the training set. Moreover, the pseudo-reversibility feature allows for the use of fully-connected neural network architectures, which simplifies the implementation and enables theoretical analysis. We provide a rigorous convergence analysis of the conditional pseudo-reversible normalizing flow model, showing its ability to converge to the target conditional probability density function using the Kullback-Leibler divergence. To demonstrate the effectiveness of our method, we apply it to several <b>benchmark</b> tests and a real-world geologic carbon storage problem.

{{</citation>}}


## cs.IR (4)



### (1/4 | 98/145) Tired of Plugins? Large Language Models Can Be End-To-End Recommenders (Wenlin Zhang et al., 2024)

{{<citation>}}

Wenlin Zhang, Chuhan, Wu, Xiangyang Li, Yuhao Wang, Kuicai Dong, Yichao Wang, Xinyi Dai, Xiangyu Zhao, Huifeng Guo, Ruiming Tang. (2024)  
**Tired of Plugins? Large Language Models Can Be End-To-End Recommenders**
<br/>
<button class="copy-to-clipboard" title="Tired of Plugins? Large Language Models Can Be End-To-End Recommenders" index=98>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs.IR  
Keyword Score: 60  
Keywords: Recommendation, Recommender System, Supervised Learning, Zero-shot, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00702v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00702v1.pdf" filename="2404.00702v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Recommender</b> <b>systems</b> aim to predict user interest based on historical behavioral data. They are mainly designed in sequential pipelines, requiring lots of data to train different sub-systems, and are hard to scale to new domains. Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable generalized capabilities, enabling a singular model to tackle diverse <b>recommendation</b> tasks across various scenarios. Nonetheless, existing <b>LLM-based</b> <b>recommendation</b> systems utilize <b>LLM</b> purely for a single task of the <b>recommendation</b> pipeline. Besides, these systems face challenges in presenting <b>large-scale</b> <b>item</b> <b>sets</b> to <b>LLMs</b> in natural language format, due to the constraint of input length. To address these challenges, we introduce an <b>LLM-based</b> end-to-end <b>recommendation</b> framework: UniLLMRec. Specifically, UniLLMRec integrates multi-stage tasks (e.g. recall, ranking, re-ranking) via chain-of-recommendations. To deal with <b>large-scale</b> <b>items,</b> <b>we</b> propose a novel strategy to structure all items into an item tree, which can be dynamically updated and effectively retrieved. UniLLMRec shows promising <b>zero-shot</b> results in comparison with conventional <b>supervised</b> models. Additionally, it boasts high efficiency, reducing the input token need by 86% compared to existing <b>LLM-based</b> models. Such efficiency not only accelerates task completion but also optimizes resource utilization. To facilitate model understanding and to ensure reproducibility, we have made our code publicly available.

{{</citation>}}


### (2/4 | 99/145) A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys) (Yashar Deldjoo et al., 2024)

{{<citation>}}

Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, René Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, Silvia Milano. (2024)  
**A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)**
<br/>
<button class="copy-to-clipboard" title="A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)" index=99>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-AI, cs-IR, cs.IR  
Keyword Score: 46  
Keywords: Multi-modal, Multi-modal, Recommendation, Recommender System, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00579v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00579v1.pdf" filename="2404.00579v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Traditional <b>recommender</b> <b>systems</b> (RS) have used user-item rating histories as their primary data source, with collaborative filtering being one of the principal methods. However, generative models have recently developed abilities to model and sample from complex data distributions, including not only user-item interaction histories but also text, images, and videos - unlocking this rich data for novel <b>recommendation</b> tasks. Through this comprehensive and multi-disciplinary survey, we aim to connect the key advancements in RS using Generative Models (Gen-RecSys), encompassing: a foundational overview of interaction-driven generative models; the application of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> for generative <b>recommendation,</b> retrieval, and conversational <b>recommendation;</b> and the integration of <b>multimodal</b> models for processing and generating image and video content in RS. Our holistic perspective allows us to highlight necessary paradigms for evaluating the impact and harm of Gen-RecSys and identify open challenges. A more up-to-date version of the papers is maintained at: https://github.com/yasdel/LLM-RecSys.

{{</citation>}}


### (3/4 | 100/145) Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey (Qijiong Liu et al., 2024)

{{<citation>}}

Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai, Zhaocheng Du, Xiao-Ming Wu, Zhou Zhao, Rui Zhang, Zhenhua Dong. (2024)  
**Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey**
<br/>
<button class="copy-to-clipboard" title="Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey" index=100>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs-MM, cs.IR  
Keyword Score: 26  
Keywords: Multi-modal, Multi-modal, Recommendation, Recommender System  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00621v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00621v1.pdf" filename="2404.00621v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Personalized <b>recommendation</b> serves as a ubiquitous channel for users to discover information or items tailored to their interests. However, traditional <b>recommendation</b> models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of <b>multimodal</b> data poses a limitation to <b>recommender</b> <b>systems,</b> especially in multimedia services like news, music, and short-video platforms. The recent advancements in pretrained <b>multimodal</b> models offer new opportunities and challenges in developing content-aware <b>recommender</b> <b>systems.</b> This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in <b>multimodal</b> pretraining, adaptation, and generation techniques, as well as their applications to <b>recommender</b> <b>systems.</b> Furthermore, we discuss open challenges and opportunities for future research in this domain. We hope that this survey, along with our tutorial materials, will inspire further research efforts to advance this evolving landscape.

{{</citation>}}


### (4/4 | 101/145) Generative Retrieval as Multi-Vector Dense Retrieval (Shiguang Wu et al., 2024)

{{<citation>}}

Shiguang Wu, Wenda Wei, Mengqi Zhang, Zhumin Chen, Jun Ma, Zhaochun Ren, Maarten de Rijke, Pengjie Ren. (2024)  
**Generative Retrieval as Multi-Vector Dense Retrieval**
<br/>
<button class="copy-to-clipboard" title="Generative Retrieval as Multi-Vector Dense Retrieval" index=101>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-AI, cs-IR, cs.IR  
Keyword Score: 10  
Keywords: Dense Retrieval  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00684v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00684v1.pdf" filename="2404.00684v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Generative retrieval generates identifiers of relevant documents in an end-to-end manner using a sequence-to-sequence architecture for a given query. The relation between generative retrieval and other retrieval methods, especially those based on matching within <b>dense</b> <b>retrieval</b> models, is not yet fully comprehended. Prior work has demonstrated that generative retrieval with atomic identifiers is equivalent to single-vector <b>dense</b> <b>retrieval.</b> Accordingly, generative retrieval exhibits behavior analogous to hierarchical search within a tree index in <b>dense</b> <b>retrieval</b> when using hierarchical semantic identifiers. However, prior work focuses solely on the retrieval stage without considering the deep interactions within the decoder of generative retrieval. In this paper, we fill this gap by demonstrating that generative retrieval and multi-vector <b>dense</b> <b>retrieval</b> share the same framework for measuring the relevance to a query of a document. Specifically, we examine the attention layer and prediction head of generative retrieval, revealing that generative retrieval can be understood as a special case of multi-vector <b>dense</b> <b>retrieval.</b> Both methods compute relevance as a sum of products of query and document vectors and an alignment matrix. We then explore how generative retrieval applies this framework, employing distinct strategies for computing document token vectors and the alignment matrix. We have conducted experiments to verify our conclusions and show that both paradigms exhibit commonalities of term matching in their alignment matrix.

{{</citation>}}


## cs.SD (3)



### (1/3 | 102/145) CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models (Xiang Li et al., 2024)

{{<citation>}}

Xiang Li, Fan Bu, Ambuj Mehrish, Yingting Li, Jiale Han, Bo Cheng, Soujanya Poria. (2024)  
**CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models**
<br/>
<button class="copy-to-clipboard" title="CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models" index=102>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-CL, cs-SD, cs.SD, eess-AS  
Keyword Score: 60  
Keywords: Diffusion Model, Adversarial Learning, Continuous Time, Continuous Time, Generative Adversarial Network, Text-to-speech, Text-to-speech  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00569v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00569v1.pdf" filename="2404.00569v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Neural <b>Text-to-Speech</b> <b>(TTS)</b> systems find broad applications in voice assistants, e-learning, and audiobook creation. The pursuit of modern models, like <b>Diffusion</b> <b>Models</b> (DMs), holds promise for achieving high-fidelity, real-time speech synthesis. Yet, the efficiency of multi-step sampling in <b>Diffusion</b> <b>Models</b> presents challenges. Efforts have been made to integrate <b>GANs</b> with DMs, speeding up inference by approximating denoising distributions, but this introduces issues with model convergence due to <b>adversarial</b> <b>training.</b> To overcome this, we introduce CM-TTS, a novel architecture grounded in consistency models (CMs). Drawing inspiration from <b>continuous-time</b> <b>diffusion</b> <b>models,</b> CM-TTS achieves top-quality speech synthesis in fewer steps without <b>adversarial</b> <b>training</b> or pre-trained model dependencies. We further design weighted samplers to incorporate different sampling positions into model training with dynamic probabilities, ensuring unbiased learning throughout the entire training process. We present a real-time mel-spectrogram generation consistency model, validated through comprehensive evaluations. Experimental results underscore CM-TTS's superiority over existing single-step speech synthesis systems, representing a significant advancement in the field.

{{</citation>}}


### (2/3 | 103/145) Personalized Neural Speech Codec (Inseon Jang et al., 2024)

{{<citation>}}

Inseon Jang, Haici Yang, Wootaek Lim, Seungkwon Beack, Minje Kim. (2024)  
**Personalized Neural Speech Codec**
<br/>
<button class="copy-to-clipboard" title="Personalized Neural Speech Codec" index=103>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-SD, cs.SD, eess-AS  
Keyword Score: 10  
Keywords: Model Compression  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00791v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00791v1.pdf" filename="2404.00791v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we propose a personalized neural speech codec, envisioning that personalization can reduce the <b>model</b> <b>complexity</b> or improve perceptual speech quality. Despite the common usage of speech codecs where only a single talker is involved on each side of the communication, personalizing a codec for the specific user has rarely been explored in the literature. First, we assume speakers can be grouped into smaller subsets based on their perceptual similarity. Then, we also postulate that a group-specific codec can focus on the group's speech characteristics to improve its perceptual quality and computational efficiency. To this end, we first develop a Siamese network that learns the speaker embeddings from the LibriSpeech dataset, which are then grouped into underlying speaker clusters. Finally, we retrain the LPCNet-based speech codec baselines on each of the speaker clusters. Subjective listening tests show that the proposed personalization scheme introduces <b>model</b> <b>compression</b> while maintaining speech quality. In other words, with the same <b>model</b> <b>complexity,</b> personalized codecs produce better speech quality.

{{</citation>}}


### (3/3 | 104/145) Measuring audio prompt adherence with distribution-based embedding distances (Maarten Grachten, 2024)

{{<citation>}}

Maarten Grachten. (2024)  
**Measuring audio prompt adherence with distribution-based embedding distances**
<br/>
<button class="copy-to-clipboard" title="Measuring audio prompt adherence with distribution-based embedding distances" index=104>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SD  
Categories: cs-SD, cs.SD, eess-AS  
Keyword Score: 10  
Keywords: Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00775v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00775v1.pdf" filename="2404.00775v1.pdf">Download PDF</button>

---


**ABSTRACT**  
An increasing number of generative music models can be conditioned on an audio <b>prompt</b> that serves as musical context for which the model is to create an accompaniment (often further specified using a text <b>prompt).</b> Evaluation of how well model outputs adhere to the audio <b>prompt</b> is often done in a model or problem specific manner, presumably because no generic evaluation method for audio <b>prompt</b> adherence has emerged. Such a method could be useful both in the development and training of new models, and to make performance comparable across models. In this paper we investigate whether commonly used distribution-based distances like Fr\'echet Audio Distance (FAD), can be used to measure audio <b>prompt</b> adherence. We propose a simple procedure based on a small number of constituents (an embedding model, a projection, an embedding distance, and a data fusion method), that we systematically assess using a baseline validation. In a follow-up experiment we test the sensitivity of the proposed audio adherence measure to pitch and time shift perturbations. The results show that the proposed measure is sensitive to such perturbations, even when the reference and candidate distributions are from different music collections. Although more experimentation is needed to answer unaddressed questions like the robustness of the measure to acoustic artifacts that do not affect the audio <b>prompt</b> adherence, the current results suggest that distribution-based embedding distances provide a viable way of measuring audio <b>prompt</b> adherence. An python/pytorch implementation of the proposed measure is publicly available as a github repository.

{{</citation>}}


## econ.GN (1)



### (1/1 | 105/145) Algorithmic Collusion by Large Language Models (Sara Fish et al., 2024)

{{<citation>}}

Sara Fish, Yannai A. Gonczarowski, Ran I. Shorrer. (2024)  
**Algorithmic Collusion by Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Algorithmic Collusion by Large Language Models" index=105>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: econ.GN  
Categories: cs-AI, cs-GT, econ-GN, econ.GN, q-fin-EC  
Keyword Score: 50  
Keywords: GPT, GPT-4, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00806v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00806v1.pdf" filename="2404.00806v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> and specifically <b>GPT-4.</b> We find that (1) <b>LLM-based</b> agents are adept at pricing tasks, (2) <b>LLM-based</b> pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in <b>LLM</b> instructions <b>("prompts")</b> may increase collusion. These results extend to auction settings. Our findings underscore the need for antitrust regulation regarding algorithmic pricing, and uncover regulatory challenges unique to <b>LLM-based</b> pricing agents.

{{</citation>}}


## eess.SY (4)



### (1/4 | 106/145) Nonparametric End-to-End Probabilistic Forecasting of Distributed Generation Outputs Considering Missing Data Imputation (Minghui Chen et al., 2024)

{{<citation>}}

Minghui Chen, Zichao Meng, Yanping Liu, Longbo Luo, Ye Guo, Kang Wang. (2024)  
**Nonparametric End-to-End Probabilistic Forecasting of Distributed Generation Outputs Considering Missing Data Imputation**
<br/>
<button class="copy-to-clipboard" title="Nonparametric End-to-End Probabilistic Forecasting of Distributed Generation Outputs Considering Missing Data Imputation" index=106>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-LG, cs-SY, eess-SY, eess.SY  
Keyword Score: 50  
Keywords: Simulation, Simulator, LSTM, LSTM, LSTM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00729v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00729v1.pdf" filename="2404.00729v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we introduce a nonparametric end-to-end method for probabilistic forecasting of distributed renewable generation outputs while including missing data imputation. Firstly, we employ a nonparametric probabilistic forecast model utilizing the <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM)</b> network to model the probability distributions of distributed renewable generations' outputs. Secondly, we design an end-to-end training process that includes missing data imputation through iterative imputation and iterative loss-based training procedures. This two-step modeling approach effectively combines the strengths of the nonparametric method with the end-to-end approach. Consequently, our approach demonstrates exceptional capabilities in probabilistic forecasting for the outputs of distributed renewable generations while effectively handling missing values. <b>Simulation</b> results confirm the superior performance of our approach compared to existing alternatives.

{{</citation>}}


### (2/4 | 107/145) Hierarchical Climate Control Strategy for Electric Vehicles with Door-Opening Consideration (Sanghyeon Nam et al., 2024)

{{<citation>}}

Sanghyeon Nam, Hyejin Lee, Youngki Kim, Kyoung hyun Kwak, Kyoungseok Han. (2024)  
**Hierarchical Climate Control Strategy for Electric Vehicles with Door-Opening Consideration**
<br/>
<button class="copy-to-clipboard" title="Hierarchical Climate Control Strategy for Electric Vehicles with Door-Opening Consideration" index=107>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00559v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00559v1.pdf" filename="2404.00559v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This study proposes a novel climate control strategy for electric vehicles (EVs) by addressing door-opening interruptions, an overlooked aspect in EV thermal management. We create and validate an EV <b>simulation</b> model that incorporates door-opening scenarios. Three controllers are compared using the <b>simulation</b> model: (i) a hierarchical non-linear model predictive control (NMPC) with a unique coolant dividing layer and a component for cabin air inflow regulation based on door-opening signals; (ii) a single MPC controller; and (iii) a rule-based controller. The hierarchical controller outperforms, reducing door-opening temperature drops by 46.96% and 51.33% compared to single layer MPC and rule-based methods in the relevant section. Additionally, our strategy minimizes the maximum temperature gaps between the sections during recovery by 86.4% and 78.7%, surpassing single layer MPC and rule-based approaches, respectively. We believe that this result opens up future possibilities for incorporating the thermal comfort of passengers across all sections within the vehicle.

{{</citation>}}


### (3/4 | 108/145) Scalable second-order consensus of hierarchical groups (Jiamin Wang et al., 2024)

{{<citation>}}

Jiamin Wang, Jian Liu, Feng Xiao, Ning Xi, Yuanshi Zheng. (2024)  
**Scalable second-order consensus of hierarchical groups**
<br/>
<button class="copy-to-clipboard" title="Scalable second-order consensus of hierarchical groups" index=108>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00625v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00625v1.pdf" filename="2404.00625v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Motivated by widespread dominance hierarchy, growth of group sizes, and feedback mechanisms in social species, we are devoted to exploring the scalable second-order consensus of hierarchical groups. More specifically, a hierarchical group consists of a collection of agents with double-integrator dynamics on a directed acyclic <b>graph</b> with additional reverse edges, which characterize feedback mechanisms across hierarchical layers. As the group size grows and the reverse edges appear, we investigate whether the absolute velocity protocol and the relative velocity protocol can preserve the system consensus property without tuning the control gains. It is rigorously proved that the absolute velocity protocol is able to achieve completely scalable second-order consensus but the relative velocity protocol cannot. This result theoretically reveals how the scalable coordination behavior in hierarchical groups is determined by local interaction rules. Moreover, we develop a hierarchical structure in order to achieve scalable second-order consensus for networks of any size and with any number of reverse edges.

{{</citation>}}


### (4/4 | 109/145) Reduced-order Koopman modeling and predictive control of nonlinear processes (Xuewen Zhang et al., 2024)

{{<citation>}}

Xuewen Zhang, Minghao Han, Xunyuan Yin. (2024)  
**Reduced-order Koopman modeling and predictive control of nonlinear processes**
<br/>
<button class="copy-to-clipboard" title="Reduced-order Koopman modeling and predictive control of nonlinear processes" index=109>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00553v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00553v1.pdf" filename="2404.00553v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we propose an efficient data-driven predictive control approach for general nonlinear processes based on a reduced-order Koopman operator. A Kalman-based sparse identification of nonlinear dynamics method is employed to select lifting functions for Koopman identification. The selected lifting functions are used to project the original nonlinear state-space into a higher-dimensional linear function space, in which Koopman-based linear models can be constructed for the underlying nonlinear process. To curb the significant increase in the dimensionality of the resulting full-order Koopman models caused by the use of lifting functions, we propose a reduced-order Koopman modeling approach based on proper orthogonal decomposition. A computationally efficient linear robust predictive control scheme is established based on the reduced-order Koopman model. A case study on a <b>benchmark</b> chemical process is conducted to illustrate the effectiveness of the proposed method. Comprehensive comparisons are conducted to demonstrate the advantage of the proposed method.

{{</citation>}}


## eess.IV (3)



### (1/3 | 110/145) MugenNet: A Novel Combined Convolution Neural Network and Transformer Network with its Application for Colonic Polyp Image Segmentation (Chen Peng et al., 2024)

{{<citation>}}

Chen Peng, Zhiqin Qian, Kunyu Wang, Qi Luo, Zhuming Bi, Wenjun Zhang. (2024)  
**MugenNet: A Novel Combined Convolution Neural Network and Transformer Network with its Application for Colonic Polyp Image Segmentation**
<br/>
<button class="copy-to-clipboard" title="MugenNet: A Novel Combined Convolution Neural Network and Transformer Network with its Application for Colonic Polyp Image Segmentation" index=110>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 50  
Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00726v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00726v1.pdf" filename="2404.00726v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Biomedical image segmentation is a very important part in disease diagnosis. The term "colonic polyps" refers to polypoid lesions that occur on the surface of the colonic mucosa within the intestinal lumen. In clinical practice, early detection of polyps is conducted through colonoscopy examinations and biomedical image processing. Therefore, the accurate polyp image segmentation is of great significance in colonoscopy examinations. <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> is a common automatic segmentation method, but its main disadvantage is the long training time. <b>Transformer</b> utilizes a <b>self-attention</b> mechanism, which essentially assigns different importance weights to each piece of information, thus achieving high computational efficiency during segmentation. However, a potential drawback is the risk of information loss. In the study reported in this paper, based on the well-known hybridization principle, we proposed a method to combine <b>CNN</b> and <b>Transformer</b> to retain the strengths of both, and we applied this method to build a system called MugenNet for colonic polyp image segmentation. We conducted a comprehensive experiment to compare MugenNet with other <b>CNN</b> models on five publicly available datasets. The ablation experiment on MugentNet was conducted as well. The experimental results show that MugenNet achieves significantly higher processing speed and accuracy compared with <b>CNN</b> alone. The generalized implication with our work is a method to optimally combine two complimentary methods of machine learning.

{{</citation>}}


### (2/3 | 111/145) Pneumonia App: a mobile application for efficient pediatric pneumonia diagnosis using explainable convolutional neural networks (CNN) (Jiaming Deng et al., 2024)

{{<citation>}}

Jiaming Deng, Zhenglin Chen, Minjiang Chen, Lulu Xu, Jiaqi Yang, Zhendong Luo, Peiwu Qin. (2024)  
**Pneumonia App: a mobile application for efficient pediatric pneumonia diagnosis using explainable convolutional neural networks (CNN)**
<br/>
<button class="copy-to-clipboard" title="Pneumonia App: a mobile application for efficient pediatric pneumonia diagnosis using explainable convolutional neural networks (CNN)" index=111>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: 68, J-3, cs-CV, eess-IV, eess.IV  
Keyword Score: 30  
Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00549v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00549v1.pdf" filename="2404.00549v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Mycoplasma pneumoniae pneumonia (MPP) poses significant diagnostic challenges in pediatric healthcare, especially in regions like China where it's prevalent. We introduce PneumoniaAPP, a mobile application leveraging deep learning techniques for rapid MPP detection. Our approach capitalizes on <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> trained on a comprehensive dataset comprising 3345 chest X-ray (CXR) images, which includes 833 CXR images revealing MPP and additionally augmented with samples from a public dataset. The <b>CNN</b> model achieved an accuracy of 88.20% and an AUROC of 0.9218 across all classes, with a specific accuracy of 97.64% for the mycoplasma class, as demonstrated on the testing dataset. Furthermore, we integrated explainability techniques into PneumoniaAPP to aid respiratory physicians in lung opacity localization. Our contribution extends beyond existing research by targeting pediatric MPP, emphasizing the age group of 0-12 years, and prioritizing deployment on mobile devices. This work signifies a significant advancement in pediatric pneumonia diagnosis, offering a reliable and accessible tool to alleviate diagnostic burdens in healthcare settings.

{{</citation>}}


### (3/3 | 112/145) GAN with Skip Patch Discriminator for Biological Electron Microscopy Image Generation (Nishith Ranjon Roy et al., 2024)

{{<citation>}}

Nishith Ranjon Roy, Nailah Rawnaq, Tulin Kaman. (2024)  
**GAN with Skip Patch Discriminator for Biological Electron Microscopy Image Generation**
<br/>
<button class="copy-to-clipboard" title="GAN with Skip Patch Discriminator for Biological Electron Microscopy Image Generation" index=112>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: 92B20, cs-CV, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Generative Adversarial Network, Generative Adversarial Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00558v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00558v1.pdf" filename="2404.00558v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Generating realistic electron microscopy (EM) images has been a challenging problem due to their complex global and local structures. Isola et al. proposed pix2pix, a conditional <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN),</b> for the general purpose of image-to-image translation; which fails to generate realistic EM images. We propose a new architecture for the discriminator in the <b>GAN</b> providing access to multiple patch sizes using skip patches and generating realistic EM images.

{{</citation>}}


## cs.AI (3)



### (1/3 | 113/145) RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning (Ao Lv et al., 2024)

{{<citation>}}

Ao Lv, Yongzhong Huang, Guige Ouyang, Yue Chen, Haoran Xie. (2024)  
**RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning**
<br/>
<button class="copy-to-clipboard" title="RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning" index=113>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 41  
Keywords: Graph, Benchmarking, Knowledge Graph, Reasoning, Temporal Knowledge Graph, Temporal Knowledge Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00586v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00586v1.pdf" filename="2404.00586v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Temporal</b> <b>Knowledge</b> <b>Graph</b> <b>(TKG)</b> <b>reasoning</b> is based on historical information to predict the future. Therefore, parsing and mining historical information is key to predicting the future. Most existing methods fail to concurrently address and comprehend historical information from both global and local perspectives. Neglecting the global view might result in overlooking macroscopic trends and patterns, while ignoring the local view can lead to missing critical detailed information. Additionally, some methods do not focus on learning from high-frequency repeating events, which means they may not fully grasp frequently occurring historical events. To this end, we propose the \textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work(RLGNet). We utilize a global history encoder to capture the overarching nature of historical information. Subsequently, the local history encoder provides information related to the query timestamp. Finally, we employ the repeating history encoder to identify and learn from frequently occurring historical events. In the evaluation on six <b>benchmark</b> datasets, our approach generally outperforms existing <b>TKG</b> <b>reasoning</b> models in multi-step and single-step <b>reasoning</b> tasks.

{{</citation>}}


### (2/3 | 114/145) A Theory for Length Generalization in Learning to Reason (Changnan Xiao et al., 2024)

{{<citation>}}

Changnan Xiao, Bing Liu. (2024)  
**A Theory for Length Generalization in Learning to Reason**
<br/>
<button class="copy-to-clipboard" title="A Theory for Length Generalization in Learning to Reason" index=114>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 23  
Keywords: Graph, Transformer, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00560v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00560v1.pdf" filename="2404.00560v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Length generalization (LG) is a challenging problem in learning to reason. It refers to the phenomenon that when trained on <b>reasoning</b> problems of smaller lengths or sizes, the resulting model struggles with problems of larger sizes or lengths. Although LG has been studied by many researchers, the challenge remains. This paper proposes a theoretical study of LG for problems whose <b>reasoning</b> processes can be modeled as DAGs (directed acyclic <b>graphs).</b> The paper first identifies and proves the conditions under which LG can be achieved in learning to reason. It then designs problem representations based on the theory to learn to solve challenging <b>reasoning</b> problems like parity, addition, and multiplication, using a <b>Transformer</b> to achieve perfect LG.

{{</citation>}}


### (3/3 | 115/145) Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery (Cristina Cornelio et al., 2024)

{{<citation>}}

Cristina Cornelio, Mohammed Diab. (2024)  
**Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery**
<br/>
<button class="copy-to-clipboard" title="Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery" index=115>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-LG, cs-LO, cs-RO, cs.AI  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00756v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00756v1.pdf" filename="2404.00756v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and <b>LLM-based</b> planners, Recover exploits symbolic information to enhance the ability of <b>LLMs</b> to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on <b>LLMs.</b>

{{</citation>}}


## cs.SE (5)



### (1/5 | 116/145) CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks (Yiqing Xie et al., 2024)

{{<citation>}}

Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, Carolyn Rose. (2024)  
**CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks**
<br/>
<button class="copy-to-clipboard" title="CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks" index=116>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-CL, cs-SE, cs.SE  
Keyword Score: 33  
Keywords: Benchmarking, Code Generation, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00566v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00566v1.pdf" filename="2404.00566v1.pdf">Download PDF</button>

---


**ABSTRACT**  
To facilitate evaluation of <b>code</b> <b>generation</b> systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based <b>benchmarks</b> that only requires light guidance from humans. Specifically, we leverage a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to convert an arbitrary piece of <b>code</b> <b>into</b> an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from <b>code</b> <b>in</b> 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct <b>code</b> <b>generation</b> experiments on open-source and proprietary models and analyze the performance of both humans and models. We will release the <b>code</b> <b>of</b> both the framework and the dataset upon acceptance.

{{</citation>}}


### (2/5 | 117/145) The Larger the Better? Improved LLM Code-Generation via Budget Reallocation (Michael Hassid et al., 2024)

{{<citation>}}

Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, Yossi Adi. (2024)  
**The Larger the Better? Improved LLM Code-Generation via Budget Reallocation**
<br/>
<button class="copy-to-clipboard" title="The Larger the Better? Improved LLM Code-Generation via Budget Reallocation" index=117>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-AI, cs-CL, cs-LG, cs-SE, cs.SE  
Keyword Score: 30  
Keywords: Code Generation, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00725v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00725v1.pdf" filename="2404.00725v1.pdf">Download PDF</button>

---


**ABSTRACT**  
It is a common belief that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze <b>code</b> <b>generation</b> <b>LLMs</b> of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking <b>LLM</b> outputs.

{{</citation>}}


### (3/5 | 118/145) Enchanting Program Specification Synthesis by Large Language Models using Static Analysis and Program Verification (Cheng Wen et al., 2024)

{{<citation>}}

Cheng Wen, Jialun Cao, Jie Su, Zhiwu Xu, Shengchao Qin, Mengda He, Haokun Li, Shing-Chi Cheung, Cong Tian. (2024)  
**Enchanting Program Specification Synthesis by Large Language Models using Static Analysis and Program Verification**
<br/>
<button class="copy-to-clipboard" title="Enchanting Program Specification Synthesis by Large Language Models using Static Analysis and Program Verification" index=118>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00762v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00762v2.pdf" filename="2404.00762v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Formal verification provides a rigorous and systematic approach to ensure the correctness and reliability of software systems. Yet, constructing specifications for the full proof relies on domain expertise and non-trivial manpower. In view of such needs, an automated approach for specification synthesis is desired. While existing automated approaches are limited in their versatility, i.e., they either focus only on synthesizing loop invariants for numerical programs, or are tailored for specific types of programs or invariants. Programs involving multiple complicated data types (e.g., arrays, pointers) and code structures (e.g., nested loops, function calls) are often beyond their capabilities. To help bridge this gap, we present AutoSpec, an automated approach to synthesize specifications for automated program verification. It overcomes the shortcomings of existing work in specification versatility, synthesizing satisfiable and adequate specifications for full proof. It is driven by static analysis and program verification, and is empowered by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> AutoSpec addresses the practical challenges in three ways: (1) driving \name by static analysis and program verification, <b>LLMs</b> serve as generators to generate candidate specifications, (2) programs are decomposed to direct the attention of <b>LLMs,</b> and (3) candidate specifications are validated in each round to avoid error accumulation during the interaction with <b>LLMs.</b> In this way, AutoSpec can incrementally and iteratively generate satisfiable and adequate specifications. The evaluation shows its effectiveness and usefulness, as it outperforms existing works by successfully verifying 79% of programs through automatic specification synthesis, a significant improvement of 1.592x. It can also be successfully applied to verify the programs in a real-world X509-parser project.

{{</citation>}}


### (4/5 | 119/145) Towards Practical Requirement Analysis and Verification: A Case Study on Software IP Components in Aerospace Embedded Systems (Zhi Ma et al., 2024)

{{<citation>}}

Zhi Ma, Cheng Wen, Jie Su, Ming Zhao, Bin Yu, Xu Lu, Cong Tian. (2024)  
**Towards Practical Requirement Analysis and Verification: A Case Study on Software IP Components in Aerospace Embedded Systems**
<br/>
<button class="copy-to-clipboard" title="Towards Practical Requirement Analysis and Verification: A Case Study on Software IP Components in Aerospace Embedded Systems" index=119>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00795v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00795v1.pdf" filename="2404.00795v1.pdf">Download PDF</button>

---


**ABSTRACT**  
IP-based software design is a crucial research field that aims to improve efficiency and reliability by reusing complex software components known as intellectual property (IP) components. To ensure the reusability of these components, particularly in security-sensitive software systems, it is necessary to analyze the requirements and perform formal verification for each IP component. However, converting the requirements of IP components from natural language descriptions to temporal logic and subsequently conducting formal verification demands domain expertise and non-trivial manpower. This paper presents a case study on software IP components derived from aerospace embedded systems, with the objective of automating the requirement analysis and verification process. The study begins by employing <b>Large</b> <b>Language</b> <b>Models</b> to convert unstructured natural language into formal specifications. Subsequently, three distinct verification techniques are employed to ascertain whether the source code meets the extracted temporal logic properties. By doing so, five real-world IP components from the China Academy of Space Technology (CAST) have been successfully verified.

{{</citation>}}


### (5/5 | 120/145) Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs (Shiwen Shan et al., 2024)

{{<citation>}}

Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li, Zibin Zheng. (2024)  
**Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs**
<br/>
<button class="copy-to-clipboard" title="Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs" index=120>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-LG, cs-SE, cs.SE  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00640v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00640v2.pdf" filename="2404.00640v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an <b>LLM-based</b> two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis. To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models~(LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.

{{</citation>}}


## cs.RO (6)



### (1/6 | 121/145) CARL: Congestion-Aware Reinforcement Learning for Imitation-based Perturbations in Mixed Traffic Control (Bibek Poudel et al., 2024)

{{<citation>}}

Bibek Poudel, Weizi Li. (2024)  
**CARL: Congestion-Aware Reinforcement Learning for Imitation-based Perturbations in Mixed Traffic Control**
<br/>
<button class="copy-to-clipboard" title="CARL: Congestion-Aware Reinforcement Learning for Imitation-based Perturbations in Mixed Traffic Control" index=121>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 30  
Keywords: Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00796v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00796v1.pdf" filename="2404.00796v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Human-driven vehicles (HVs) exhibit complex and diverse behaviors. Accurately modeling such behavior is crucial for validating Robot Vehicles (RVs) in <b>simulation</b> and realizing the potential of mixed traffic control. However, existing approaches like parameterized models and data-driven techniques struggle to capture the full complexity and diversity. To address this, in this work, we introduce CARL, a hybrid technique combining imitation learning for close proximity car-following and probabilistic sampling for larger headways. We also propose two classes of RL-based RVs: a safety RV focused on maximizing safety and an efficiency RV focused on maximizing efficiency. Our experiments show that the safety RV increases Time-to-Collision above the critical 4 second threshold and reduces Deceleration Rate to Avoid a Crash by up to 80%, while the efficiency RV achieves improvements in throughput of up to 49%. These results demonstrate the effectiveness of CARL in enhancing both safety and efficiency in mixed traffic.

{{</citation>}}


### (2/6 | 122/145) Using Explainable AI and Hierarchical Planning for Outreach with Robots (Daksh Dobhal et al., 2024)

{{<citation>}}

Daksh Dobhal, Jayesh Nagpal, Rushang Karia, Pulkit Verma, Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava. (2024)  
**Using Explainable AI and Hierarchical Planning for Outreach with Robots**
<br/>
<button class="copy-to-clipboard" title="Using Explainable AI and Hierarchical Planning for Outreach with Robots" index=122>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Explainable AI, Natural Language Explanation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00808v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00808v1.pdf" filename="2404.00808v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Understanding how robots plan and execute tasks is crucial in today's world, where they are becoming more prevalent in our daily lives. However, teaching non-experts the complexities of robot planning can be challenging. This work presents an open-source platform that simplifies the process using a visual interface that completely abstracts the complex internals of hierarchical planning that robots use for performing task and motion planning. Using the principles developed in the field of <b>explainable</b> <b>AI,</b> this intuitive platform enables users to create plans for robots to complete tasks, and provides helpful hints and <b>natural</b> <b>language</b> <b>explanations</b> for errors. The platform also has a built-in simulator to demonstrate how robots execute submitted plans. This platform's efficacy was tested in a user study on university students with little to no computer science background. Our results show that this platform is highly effective in teaching novice users the intuitions of robot task planning.

{{</citation>}}


### (3/6 | 123/145) Task-Space Riccati Feedback based Whole Body Control for Underactuated Legged Locomotion (Shunpeng Yang et al., 2024)

{{<citation>}}

Shunpeng Yang, Zejun Hong, Sen Li, Patrick Wensing, Wei Zhang, Hua Chen. (2024)  
**Task-Space Riccati Feedback based Whole Body Control for Underactuated Legged Locomotion**
<br/>
<button class="copy-to-clipboard" title="Task-Space Riccati Feedback based Whole Body Control for Underactuated Legged Locomotion" index=123>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00591v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00591v1.pdf" filename="2404.00591v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This manuscript primarily aims to enhance the performance of whole-body controllers(WBC) for underactuated legged locomotion. We introduce a systematic parameter design mechanism for the floating-base feedback control within the WBC. The proposed approach involves utilizing the linearized model of unactuated dynamics to formulate a Linear Quadratic Regulator(LQR) and solving a Riccati gain while accounting for potential physical constraints through a second-order approximation of the log-barrier function. And then the user-tuned feedback gain for the floating base task is replaced by a new one constructed from the solved Riccati gain. Extensive <b>simulations</b> conducted in MuJoCo with a point bipedal robot, as well as real-world experiments performed on a quadruped robot, demonstrate the effectiveness of the proposed method. In the different bipedal locomotion tasks, compared with the user-tuned method, the proposed approach is at least 12% better and up to 50% better at linear velocity tracking, and at least 7% better and up to 47% better at angular velocity tracking. In the quadruped experiment, linear velocity tracking is improved by at least 3% and angular velocity tracking is improved by at least 23% using the proposed method.

{{</citation>}}


### (4/6 | 124/145) Competition-Aware Decision-Making Approach for Mobile Robots in Racing Scenarios (Kyoungtae Ji et al., 2024)

{{<citation>}}

Kyoungtae Ji, Sangjae Bae, Nan Li, Kyoungseok Han. (2024)  
**Competition-Aware Decision-Making Approach for Mobile Robots in Racing Scenarios**
<br/>
<button class="copy-to-clipboard" title="Competition-Aware Decision-Making Approach for Mobile Robots in Racing Scenarios" index=124>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO, math-OC  
Keyword Score: 20  
Keywords: human-in-the-loop, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00520v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00520v1.pdf" filename="2404.00520v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a game-theoretic strategy for racing, where the autonomous ego agent seeks to block a racing opponent that aims to overtake the ego agent. After a library of trajectory candidates and an associated reward matrix are constructed, the optimal trajectory in terms of maximizing the cumulative reward over the planning horizon is determined based on the level-K <b>reasoning</b> framework. In particular, the level of the opponent is estimated online according to its behavior over a past window and is then used to determine the trajectory for the ego agent. Taking into account that the opponent may change its level and strategy during the decision process of the ego agent, we introduce a trajectory mixing strategy that blends the level-K optimal trajectory with a fail-safe trajectory. The overall algorithm was tested and evaluated in various simulated racing scenarios, which also includes <b>human-in-the-loop</b> experiments. Comparative analysis against the conventional level-K framework demonstrates the superiority of our proposed approach in terms of overtake-blocking success rates.

{{</citation>}}


### (5/6 | 125/145) Graph-Based vs. Error State Kalman Filter-Based Fusion Of 5G And Inertial Data For MAV Indoor Pose Estimation (Meisam Kabiri et al., 2024)

{{<citation>}}

Meisam Kabiri, Claudio Cimarelli, Hriday Bavle, Jose Luis Sanchez-Lopez, Holger Voos. (2024)  
**Graph-Based vs. Error State Kalman Filter-Based Fusion Of 5G And Inertial Data For MAV Indoor Pose Estimation**
<br/>
<button class="copy-to-clipboard" title="Graph-Based vs. Error State Kalman Filter-Based Fusion Of 5G And Inertial Data For MAV Indoor Pose Estimation" index=125>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 6  
Keywords: Graph, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00691v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00691v1.pdf" filename="2404.00691v1.pdf">Download PDF</button>

---


**ABSTRACT**  
5G New Radio Time of Arrival (ToA) data has the potential to revolutionize indoor localization for micro aerial vehicles (MAVs). However, its performance under varying network setups, especially when combined with IMU data for real-time localization, has not been fully explored so far. In this study, we develop an error state Kalman filter (ESKF) and a pose <b>graph</b> optimization (PGO) approach to address this gap. We systematically evaluate the performance of the derived approaches for real-time MAV localization in realistic scenarios with 5G base stations in Line-Of-Sight (LOS), demonstrating the potential of 5G technologies in this domain. In order to experimentally test and compare our localization approaches, we augment the EuRoC MAV <b>benchmark</b> dataset for visual-inertial odometry with simulated yet highly realistic 5G ToA measurements. Our experimental results comprehensively assess the impact of varying network setups, including varying base station numbers and network configurations, on ToA-based MAV localization performance. The findings show promising results for seamless and robust localization using 5G ToA measurements, achieving an accuracy of 15 cm throughout the entire trajectory within a <b>graph-based</b> framework with five 5G base stations, and an accuracy of up to 34 cm in the case of ESKF-based localization. Additionally, we measure the run time of both algorithms and show that they are both fast enough for real-time implementation.

{{</citation>}}


### (6/6 | 126/145) End-to-End Autonomous Driving through V2X Cooperation (Haibao Yu et al., 2024)

{{<citation>}}

Haibao Yu, Wenxian Yang, Jiaru Zhong, Zhenwei Yang, Siqi Fan, Ping Luo, Zaiqing Nie. (2024)  
**End-to-End Autonomous Driving through V2X Cooperation**
<br/>
<button class="copy-to-clipboard" title="End-to-End Autonomous Driving through V2X Cooperation" index=126>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-CV, cs-MA, cs-RO, cs.RO  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00717v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00717v1.pdf" filename="2404.00717v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Cooperatively utilizing both ego-vehicle and infrastructure sensor data via V2X communication has emerged as a promising approach for advanced autonomous driving. However, current research mainly focuses on improving individual modules, rather than taking end-to-end learning to optimize final planning performance, resulting in underutilized data potential. In this paper, we introduce UniV2X, a pioneering cooperative autonomous driving framework that seamlessly integrates all key driving modules across diverse views into a unified network. We propose a sparse-dense hybrid data transmission and fusion mechanism for effective vehicle-infrastructure cooperation, offering three advantages: 1) Effective for simultaneously enhancing agent perception, online mapping, and occupancy prediction, ultimately improving planning performance. 2) Transmission-friendly for practical and limited communication conditions. 3) Reliable data fusion with interpretability of this hybrid data. We implement UniV2X, as well as reproducing several <b>benchmark</b> methods, on the challenging DAIR-V2X, the real-world cooperative driving dataset. Experimental results demonstrate the effectiveness of UniV2X in significantly enhancing planning performance, as well as all intermediate output performance. Code is at https://github.com/AIR-THU/UniV2X.

{{</citation>}}


## eess.AS (1)



### (1/1 | 127/145) Scaling Properties of Speech Language Models (Santiago Cuervo et al., 2024)

{{<citation>}}

Santiago Cuervo, Ricard Marxer. (2024)  
**Scaling Properties of Speech Language Models**
<br/>
<button class="copy-to-clipboard" title="Scaling Properties of Speech Language Models" index=127>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.AS  
Categories: cs-AI, cs-CL, cs-NE, eess-AS, eess.AS  
Keyword Score: 30  
Keywords: Tokenization, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00685v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00685v1.pdf" filename="2404.00685v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and <b>LLMs,</b> which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based <b>LLMs.</b> Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech <b>tokenization.</b>

{{</citation>}}


## cs.AR (1)



### (1/1 | 128/145) RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning (Dongsheng Zuo et al., 2024)

{{<citation>}}

Dongsheng Zuo, Jiadong Zhu, Yikang Ouyang, Yuzhe Ma. (2024)  
**RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning" index=128>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AR  
Categories: cs-AR, cs-LG, cs.AR  
Keyword Score: 30  
Keywords: Convolution, Convolutional Neural Network, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00639v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00639v1.pdf" filename="2404.00639v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Multiplication is a fundamental operation in many applications, and multipliers are widely adopted in various circuits. However, optimizing multipliers is challenging and non-trivial due to the huge design space. In this paper, we propose RL-MUL, a multiplier design optimization framework based on <b>reinforcement</b> <b>learning.</b> Specifically, we utilize matrix and tensor representations for the compressor tree of a multiplier, based on which the <b>convolutional</b> <b>neural</b> <b>networks</b> can be seamlessly incorporated as the agent network. The agent can learn to optimize the multiplier structure based on a Pareto-driven reward which is customized to accommodate the trade-off between area and delay. Additionally, the capability of RL-MUL is extended to optimize the fused multiply-accumulator (MAC) designs. Experiments are conducted on different bit widths of multipliers. The results demonstrate that the multipliers produced by RL-MUL can dominate all baseline designs in terms of area and delay. The performance gain of RL-MUL is further validated by comparing the area and delay of processing element arrays using multipliers from RL-MUL and baseline approaches.

{{</citation>}}


## cs.IT (4)



### (1/4 | 129/145) Network-Assisted Full-Duplex Cell-Free mmWave Networks: Hybrid MIMO Processing and Multi-Agent DRL-Based Power Allocation (Qingrui Fan et al., 2024)

{{<citation>}}

Qingrui Fan, Yu Zhang, Jiamin Li, Dongming Wang, Hongbiao Zhang, Xiaohu You. (2024)  
**Network-Assisted Full-Duplex Cell-Free mmWave Networks: Hybrid MIMO Processing and Multi-Agent DRL-Based Power Allocation**
<br/>
<button class="copy-to-clipboard" title="Network-Assisted Full-Duplex Cell-Free mmWave Networks: Hybrid MIMO Processing and Multi-Agent DRL-Based Power Allocation" index=129>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs-MA, cs.IT, eess-SP, math-IT  
Keyword Score: 30  
Keywords: Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00631v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00631v1.pdf" filename="2404.00631v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper investigates the network-assisted full-duplex (NAFD) cell-free millimeter-wave (mmWave) networks, where the distribution of the transmitting access points (T-APs) and receiving access points (R-APs) across distinct geographical locations mitigates cross-link interference, facilitating the attainment of a truly flexible duplex mode. To curtail deployment expenses and power consumption for mmWave band operations, each AP incorporates a hybrid digital-analog structure encompassing precoder/combiner functions. However, this incorporation introduces processing intricacies within channel estimation and precoding/combining design. In this paper, we first present a hybrid multiple-input multiple-output (MIMO) processing framework and derive explicit expressions for both uplink and downlink achievable rates. Then we formulate a power allocation problem to maximize the weighted bidirectional sum rates. To tackle this non-convex problem, we develop a collaborative multi-agent deep <b>reinforcement</b> <b>learning</b> (MADRL) algorithm called multi-agent twin delayed deep deterministic policy gradient (MATD3) for NAFD cell-free mmWave networks. Specifically, given the tightly coupled nature of both uplink and downlink power coefficients in NAFD cell-free mmWave networks, the MATD3 algorithm resolves such coupled conflicts through an interactive learning process between agents and the environment. Finally, the <b>simulation</b> results validate the effectiveness of the proposed channel estimation methods within our hybrid MIMO processing paradigm, and demonstrate that our MATD3 algorithm outperforms both multi-agent deep deterministic policy gradient (MADDPG) and conventional power allocation strategies.

{{</citation>}}


### (2/4 | 130/145) Robust Beamforming Design and Antenna Selection for Dynamic HRIS-aided Massive MIMO Systems (Jintao Wang et al., 2024)

{{<citation>}}

Jintao Wang, Binggui Zhou, Chengzhi Ma, Shiqi Gong, Guanghua Yang, Shaodan Ma. (2024)  
**Robust Beamforming Design and Antenna Selection for Dynamic HRIS-aided Massive MIMO Systems**
<br/>
<button class="copy-to-clipboard" title="Robust Beamforming Design and Antenna Selection for Dynamic HRIS-aided Massive MIMO Systems" index=130>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 23  
Keywords: Benchmarking, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00598v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00598v1.pdf" filename="2404.00598v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, a dynamic hybrid active-passive reconfigurable intelligent surface (HRIS) is proposed to further enhance the massive multiple-input-multiple-output (MIMO) system, since it supports the dynamic placement of active and passive elements. Specifically, considering the impact of the hardware impairments (HWIs), we investigate the channel-aware configuration of the receive antennas at the base station (BS) and the active/passive elements at the HRIS to improve the reliability of system. To this end, we investigate the average mean-square-error (MSE) minimization problem for the HRIS-aided massive MIMO system by jointly optimizing the BS receive antenna selection matrix, the reflection phase coefficients, the reflection amplitude matrix, and the mode selection matrix of the HRIS under the power budget of the HRIS. To tackle the non-convexity and intractability of this problem, we first transform the binary and discrete variables into continuous ones, and then propose a penalty-based exact block coordinate descent (BCD) algorithm to solve these subproblems alternately. Numerical <b>simulations</b> demonstrate the great superiority of the proposed scheme over the conventional <b>benchmark</b> schemes.

{{</citation>}}


### (3/4 | 131/145) Fluid Antenna Relay Assisted Communication Systems Through Antenna Location Optimization (Ruopeng Xu et al., 2024)

{{<citation>}}

Ruopeng Xu, Yixuan Chen, Jiawen Kang, Minrui Xu, Zhaohui Yang, Chongwen Huang, Niyato Dusit. (2024)  
**Fluid Antenna Relay Assisted Communication Systems Through Antenna Location Optimization**
<br/>
<button class="copy-to-clipboard" title="Fluid Antenna Relay Assisted Communication Systems Through Antenna Location Optimization" index=131>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00628v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00628v1.pdf" filename="2404.00628v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we investigate the problem of resource allocation for fluid antenna relay (FAR) system with antenna location optimization. In the considered model, each user transmits information to a base station (BS) with help of FAR. The antenna location of the FAR is flexible and can be adapted to dynamic location distribution of the users. We formulate a sum rate maximization problem through jointly optimizing the antenna location and bandwidth allocation with meeting the minimum rate requirements, total bandwidth budget, and feasible antenna region constraints. To solve this problem, we obtain the optimal bandwidth in closed form. Based on the optimal bandwidth, the original problem is reduced to the antenna location optimization problem and an alternating algorithm is proposed. <b>Simulation</b> results verify the effectiveness of the proposed algorithm and the sum rate can be increased by up to 125% compared to the conventional schemes.

{{</citation>}}


### (4/4 | 132/145) Resource Allocation for Green Probabilistic Semantic Communication with Rate Splitting (Ruopeng Xu et al., 2024)

{{<citation>}}

Ruopeng Xu, Zhaohui Yang, Zhouxiang Zhao, Qianqian Yang, Zhaoyang Zhang. (2024)  
**Resource Allocation for Green Probabilistic Semantic Communication with Rate Splitting**
<br/>
<button class="copy-to-clipboard" title="Resource Allocation for Green Probabilistic Semantic Communication with Rate Splitting" index=132>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00612v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00612v1.pdf" filename="2404.00612v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, the energy efficient design for probabilistic semantic communication (PSC) system with rate splitting multiple access (RSMA) is investigated. Basic principles are first reviewed to show how the PSC system works to extract, compress and transmit the semantic information in a task-oriented transmission. Subsequently, the process of how multiuser semantic information can be represented, compressed and transmitted with RSMA is presented, during which the semantic compression ratio (SCR) is introduced to directly measure the computation overhead in a transmission task, and communication overhead is indirectly described as well. Hence, the problem of wireless resource allocation jointly considering the computation and communication consumption for the PSC system with RSMA is investigated. Both conventional wireless resource constraints and unique constraints on semantic communication are considered to maximize the energy efficiency (EE). <b>Simulation</b> results verify the effectiveness of the proposed scheme.

{{</citation>}}


## math.OC (1)



### (1/1 | 133/145) Dynamic Transfer Policies for Parallel Queues (Timothy C. Y. Chan et al., 2024)

{{<citation>}}

Timothy C. Y. Chan, Jangwon Park, Vahid Sarhangian. (2024)  
**Dynamic Transfer Policies for Parallel Queues**
<br/>
<button class="copy-to-clipboard" title="Dynamic Transfer Policies for Parallel Queues" index=133>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.OC  
Categories: cs-SY, eess-SY, math-OC, math.OC  
Keyword Score: 30  
Keywords: Continuous Time, Continuous Time, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00543v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00543v1.pdf" filename="2404.00543v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider the problem of load balancing in parallel queues by transferring customers between them at discrete points in time. Holding costs accrue as customers wait in the queue, while transfer decisions incur both fixed (setup) and variable costs proportional to the number and direction of transfers. Our work is primarily motivated by inter-facility patient transfers between hospitals during a surge in demand for hospitalization (e.g., during a pandemic). By analyzing an associated fluid control problem, we show that under fairly general assumptions including time-varying arrivals and convex increasing holding costs, the optimal policy in each period partitions the state-space into a well-defined $\textit{no-transfer region}$ and its complement, such that transferring is optimal if and only if the system is sufficiently imbalanced. In the absence of fixed transfer costs, an optimal policy moves the state to the no-transfer region's boundary; in contrast, with fixed costs, the state is moved to the no-transfer region's relative interior. We further leverage the fluid control problem to provide insights on the trade-off between holding and transfer costs, emphasizing the importance of preventing excessive idleness when transfers are not feasible in <b>continuous-time.</b> <b>Using</b> <b>simulation</b> experiments, we investigate the performance and robustness of the fluid policy for the stochastic system. In particular, our case study calibrated using data during the pandemic in the Greater Toronto Area demonstrates that transferring patients between hospitals could result in up to 27.7% reduction in total cost with relatively few transfers.

{{</citation>}}


## cs.CR (2)



### (1/2 | 134/145) Privacy Re-identification Attacks on Tabular GANs (Abdallah Alshantti et al., 2024)

{{<citation>}}

Abdallah Alshantti, Adil Rasheed, Frank Westad. (2024)  
**Privacy Re-identification Attacks on Tabular GANs**
<br/>
<button class="copy-to-clipboard" title="Privacy Re-identification Attacks on Tabular GANs" index=134>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-LG, cs.CR  
Keyword Score: 25  
Keywords: Black Box, Generative Adversarial Network, Generative Adversarial Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00696v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00696v1.pdf" filename="2404.00696v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Generative</b> <b>models</b> <b>are</b> subject to overfitting and thus may potentially leak sensitive information from the training data. In this work. we investigate the privacy risks that can potentially arise from the use of <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> for creating tabular synthetic datasets. For the purpose, we analyse the effects of re-identification attacks on synthetic data, i.e., attacks which aim at selecting samples that are predicted to correspond to memorised training samples based on their proximity to the nearest synthetic records. We thus consider multiple settings where different attackers might have different access levels or knowledge of the <b>generative</b> <b>model</b> <b>and</b> predictive, and assess which information is potentially most useful for launching more successful re-identification attacks. In doing so we also consider the situation for which re-identification attacks are formulated as reconstruction attacks, i.e., the situation where an attacker uses evolutionary multi-objective optimisation for perturbing synthetic samples closer to the training space. The results indicate that attackers can indeed pose major privacy risks by selecting synthetic samples that are likely representative of memorised training samples. In addition, we notice that privacy threats considerably increase when the attacker either has knowledge or has <b>black-box</b> <b>access</b> to the <b>generative</b> <b>models.</b> <b>We</b> also find that reconstruction attacks through multi-objective optimisation even increase the risk of identifying confidential samples.

{{</citation>}}


### (2/2 | 135/145) A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures (Thanh Tam Nguyen et al., 2024)

{{<citation>}}

Thanh Tam Nguyen, Thanh Trung Huynh, Zhao Ren, Thanh Toan Nguyen, Phi Le Nguyen, Hongzhi Yin, Quoc Viet Hung Nguyen. (2024)  
**A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures**
<br/>
<button class="copy-to-clipboard" title="A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures" index=135>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-AI, cs-CR, cs-CY, cs-LG, cs.CR  
Keyword Score: 10  
Keywords: Explainable AI  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00673v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00673v1.pdf" filename="2404.00673v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As the adoption of <b>explainable</b> <b>AI</b> (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorisation of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have established an online resource repository, which will be continuously updated with new and relevant findings. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-privex.

{{</citation>}}


## cs.MA (1)



### (1/1 | 136/145) OpenMines: A Light and Comprehensive Mining Simulation Environment for Truck Dispatching (Shi Meng et al., 2024)

{{<citation>}}

Shi Meng, Bin Tian, Xiaotong Zhang, Shuangying Qi, Caiji Zhang, Qiang Zhang. (2024)  
**OpenMines: A Light and Comprehensive Mining Simulation Environment for Truck Dispatching**
<br/>
<button class="copy-to-clipboard" title="OpenMines: A Light and Comprehensive Mining Simulation Environment for Truck Dispatching" index=136>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.MA  
Categories: cs-MA, cs-SY, cs.MA, eess-SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00622v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00622v1.pdf" filename="2404.00622v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Mine fleet management algorithms can significantly reduce operational costs and enhance productivity in mining systems. Most current fleet management algorithms are evaluated based on self-implemented or proprietary <b>simulation</b> environments, posing challenges for replication and comparison. This paper models the <b>simulation</b> environment for mine fleet management from a complex systems perspective. Building upon previous work, we introduce probabilistic, user-defined events for random event <b>simulation</b> and implement various evaluation metrics and baselines, effectively reflecting the robustness of fleet management algorithms against unforeseen incidents. We present ``OpenMines'', an open-source framework encompassing the entire process of mine system modeling, algorithm development, and evaluation, facilitating future algorithm comparison and replication in the field. Code is available in https://github.com/370025263/openmines.

{{</citation>}}


## cs.CY (1)



### (1/1 | 137/145) AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight (Nicola Fabiano, 2024)

{{<citation>}}

Nicola Fabiano. (2024)  
**AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight**
<br/>
<button class="copy-to-clipboard" title="AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight" index=137>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: cs-AI, cs-CL, cs-CY, cs.CY  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00600v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00600v2.pdf" filename="2404.00600v2.pdf">Download PDF</button>

---


**ABSTRACT**  
The imposing evolution of artificial intelligence systems and, specifically, of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.

{{</citation>}}


## cs.HC (2)



### (1/2 | 138/145) 'My agent understands me better': Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents (Yuki Hou et al., 2024)

{{<citation>}}

Yuki Hou, Haruki Tamoto, Homei Miyashita. (2024)  
**'My agent understands me better': Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents**
<br/>
<button class="copy-to-clipboard" title="'My agent understands me better': Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents" index=138>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: I-2-4; H-3-3, cs-HC, cs.HC  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00573v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00573v1.pdf" filename="2404.00573v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of <b>large</b> <b>language</b> <b>model</b> based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of <b>LLMs.</b> We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user's interaction history in a database that encapsulates each memory's content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences.

{{</citation>}}


### (2/2 | 139/145) Designing Human-AI Systems: Anthropomorphism and Framing Bias on Human-AI Collaboration (Samuel Aleksander Sánchez Olszewski, 2024)

{{<citation>}}

Samuel Aleksander Sánchez Olszewski. (2024)  
**Designing Human-AI Systems: Anthropomorphism and Framing Bias on Human-AI Collaboration**
<br/>
<button class="copy-to-clipboard" title="Designing Human-AI Systems: Anthropomorphism and Framing Bias on Human-AI Collaboration" index=139>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 10  
Keywords: Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00634v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00634v1.pdf" filename="2404.00634v1.pdf">Download PDF</button>

---


**ABSTRACT**  
AI is redefining how humans interact with technology, leading to a synergetic collaboration between the two. Nevertheless, the effects of human cognition on this collaboration remain unclear. This study investigates the implications of two cognitive biases, anthropomorphism and framing effect, on human-AI collaboration within a hiring setting. Subjects were asked to select job candidates with the help of an AI-powered <b>recommendation</b> tool. The tool was manipulated to have either human-like or robot-like characteristics and presented its <b>recommendations</b> in either positive or negative frames. The results revealed that the framing of AI's <b>recommendations</b> had no significant influence on subjects' decisions. In contrast, anthropomorphism significantly affected subjects' agreement with AI <b>recommendations.</b> Contrary to expectations, subjects were less likely to agree with the AI if it had human-like characteristics. These findings demonstrate that cognitive biases can impact human-AI collaboration and highlight the need for tailored approaches to AI product design, rather than a single, universal solution.

{{</citation>}}


## physics.optics (1)



### (1/1 | 140/145) Unified, Verifiable Neural Simulators for Electromagnetic Wave Inverse Problems (Charles Dove et al., 2024)

{{<citation>}}

Charles Dove, Jatearoon Boondicharern, Laura Waller. (2024)  
**Unified, Verifiable Neural Simulators for Electromagnetic Wave Inverse Problems**
<br/>
<button class="copy-to-clipboard" title="Unified, Verifiable Neural Simulators for Electromagnetic Wave Inverse Problems" index=140>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.optics  
Categories: cs-LG, eess-IV, physics-optics, physics.optics  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00545v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00545v1.pdf" filename="2404.00545v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Simulators based on neural networks offer a path to orders-of-magnitude faster electromagnetic wave <b>simulations.</b> Existing models, however, only address narrowly tailored classes of problems and only scale to systems of a few dozen degrees of freedom (DoFs). Here, we demonstrate a single, unified model capable of addressing scattering <b>simulations</b> with thousands of DoFs, of any wavelength, any illumination wavefront, and freeform materials, within broad configurable bounds. Based on an attentional multi-conditioning strategy, our method also allows non-recurrent supervision on and prediction of intermediate physical states, which provides improved generalization with no additional data-generation cost. Using this O(1)-time intermediate prediction capability, we propose and prove a rigorous, efficiently computable upper bound on prediction error, allowing accuracy guarantees at inference time for all predictions. After training solely on randomized systems, we demonstrate the unified model across a suite of challenging multi-disciplinary inverse problems, finding strong efficacy and speed improvements up to 96% for problems in optical tomography, beam shaping through volumetric random media, and freeform photonic inverse design, with no problem-specific training. Our findings demonstrate a path to universal, verifiably accurate neural surrogates for existing scattering simulators, and our conditioning and training methods are directly applicable to any PDE admitting a time-domain iterative solver.

{{</citation>}}


## cs.DB (2)



### (1/2 | 141/145) Mining Sequential Patterns in Uncertain Databases Using Hierarchical Index Structure (Kashob Kumar Roy et al., 2024)

{{<citation>}}

Kashob Kumar Roy, Md Hasibul Haque Moon, Md Mahmudur Rahman, Chowdhury Farhan Ahmed, Carson K. Leung. (2024)  
**Mining Sequential Patterns in Uncertain Databases Using Hierarchical Index Structure**
<br/>
<button class="copy-to-clipboard" title="Mining Sequential Patterns in Uncertain Databases Using Hierarchical Index Structure" index=141>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DB  
Categories: cs-DB, cs.DB  
Keyword Score: 10  
Keywords: Pruning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.01347v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.01347v1.pdf" filename="2404.01347v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this uncertain world, data uncertainty is inherent in many applications and its importance is growing drastically due to the rapid development of modern technologies. Nowadays, researchers have paid more attention to mine patterns in uncertain databases. A few recent works attempt to mine frequent uncertain sequential patterns. Despite their success, they are incompetent to reduce the number of false-positive pattern generation in their mining process and maintain the patterns efficiently. In this paper, we propose multiple theoretically tightened <b>pruning</b> upper bounds that remarkably reduce the mining space. A novel hierarchical structure is introduced to maintain the patterns in a space-efficient way. Afterward, we develop a versatile framework for mining uncertain sequential patterns that can effectively handle weight constraints as well. Besides, with the advent of incremental uncertain databases, existing works are not scalable. There exist several incremental sequential pattern mining algorithms, but they are limited to mine in precise databases. Therefore, we propose a new technique to adapt our framework to mine patterns when the database is incremental. Finally, we conduct extensive experiments on several real-life datasets and show the efficacy of our framework in different applications.

{{</citation>}}


### (2/2 | 142/145) SoK: The Faults in our Graph Benchmarks (Puneet Mehrotra et al., 2024)

{{<citation>}}

Puneet Mehrotra, Vaastav Anand, Daniel Margo, Milad Rezaei Hajidehi, Margo Seltzer. (2024)  
**SoK: The Faults in our Graph Benchmarks**
<br/>
<button class="copy-to-clipboard" title="SoK: The Faults in our Graph Benchmarks" index=142>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DB  
Categories: cs-DB, cs.DB  
Keyword Score: 9  
Keywords: Graph, Benchmarking, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00766v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00766v1.pdf" filename="2404.00766v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Graph-structured</b> data is prevalent in domains such as social networks, financial transactions, brain networks, and protein interactions. As a result, the research community has produced new databases and analytics engines to process such data. Unfortunately, there is not yet widespread <b>benchmark</b> standardization in <b>graph</b> processing, and the heterogeneity of evaluations found in the literature can lead researchers astray. Evaluations frequently ignore datasets' statistical idiosyncrasies, which significantly affect system performance. Scalability studies often use datasets that fit easily in memory on a modest desktop. Some studies rely on synthetic <b>graph</b> generators, but these generators produce <b>graphs</b> with unnatural characteristics that also affect performance, producing misleading results. Currently, the community has no consistent and principled manner with which to compare systems and provide guidance to developers who wish to select the system most suited to their application. We provide three different systematizations of <b>benchmarking</b> practices. First, we present a 12-year literary review of <b>graph</b> processing <b>benchmarking,</b> including a summary of the prevalence of specific datasets and <b>benchmarks</b> used in these papers. Second, we demonstrate the impact of two statistical properties of datasets that drastically affect <b>benchmark</b> performance. We show how different assignments of IDs to vertices, called vertex orderings, dramatically alter <b>benchmark</b> performance due to the caching behavior they induce. We also show the impact of zero-degree vertices on the runtime of <b>benchmarks</b> such as breadth-first search and single-source shortest path. We show that these issues can cause performance to change by as much as 38% on several popular <b>graph</b> processing systems. Finally, we suggest best practices to account for these issues when evaluating <b>graph</b> systems.

{{</citation>}}


## cs.GT (1)



### (1/1 | 143/145) An Abundance of Katherines: The Game Theory of Baby Naming (Katy Blumer et al., 2024)

{{<citation>}}

Katy Blumer, Kate Donahue, Katie Fritz, Kate Ivanovich, Katherine Lee, Katie Luo, Cathy Meng, Katie Van Koevering. (2024)  
**An Abundance of Katherines: The Game Theory of Baby Naming**
<br/>
<button class="copy-to-clipboard" title="An Abundance of Katherines: The Game Theory of Baby Naming" index=143>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.GT  
Categories: cs-CY, cs-GT, cs.GT  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00732v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00732v2.pdf" filename="2404.00732v2.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we study the highly competitive arena of baby naming. Through making several Extremely Reasonable Assumptions (namely, that parents are myopic, perfectly knowledgeable agents who pick a name based solely on its uniquness), we create a model which is not only tractable and clean, but also perfectly captures the real world. We then extend our investigation with numerical experiments, as well as analysis of <b>large</b> <b>language</b> <b>model</b> tools. We conclude by discussing avenues for future research.

{{</citation>}}


## cs.OS (1)



### (1/1 | 144/145) THEMIS: Time, Heterogeneity, and Energy Minded Scheduling for Fair Multi-Tenant Use in FPGAs (Emre Karabulut et al., 2024)

{{<citation>}}

Emre Karabulut, Arsalan Ali Malik, Amro Awad, Aydin Aysu. (2024)  
**THEMIS: Time, Heterogeneity, and Energy Minded Scheduling for Fair Multi-Tenant Use in FPGAs**
<br/>
<button class="copy-to-clipboard" title="THEMIS: Time, Heterogeneity, and Energy Minded Scheduling for Fair Multi-Tenant Use in FPGAs" index=144>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.OS  
Categories: cs-DC, cs-OS, cs.OS  
Keyword Score: 10  
Keywords: Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00507v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00507v1.pdf" filename="2404.00507v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Using correct design metrics and understanding the limitations of the underlying technology is critical to developing effective scheduling algorithms. Unfortunately, existing scheduling techniques used \emph{incorrect} metrics and had \emph{unrealistic} assumptions for fair scheduling of multi-tenant FPGAs where each tenant is aimed to share approximately the same number of resources both spatially and temporally. This paper introduces an enhanced fair scheduling algorithm for multi-tenant FPGA use, addressing previous metric and assumption issues, with three specific improvements claimed First, our method ensures spatiotemporal <b>fairness</b> by considering both spatial and temporal aspects, addressing the limitation of prior work that assumed uniform task latency. Second, we incorporate energy considerations into <b>fairness</b> by adjusting scheduling intervals and accounting for energy overhead, thereby balancing energy efficiency with <b>fairness.</b> Third, we acknowledge overlooked aspects of FPGA multi-tenancy, including heterogeneous regions and the constraints on dynamically merging/splitting partially reconfigurable regions. We develop and evaluate our improved fair scheduling algorithm with these three enhancements. Inspired by the Greek goddess of law and personification of justice, we name our fair scheduling solution THEMIS: \underline{T}ime, \underline{H}eterogeneity, and \underline{E}nergy \underline{Mi}nded \underline{S}cheduling. We used the Xilinx Zedboard XC7Z020 to quantify our approach's savings. Compared to previous algorithms, our improved scheduling algorithm enhances <b>fairness</b> between 24.2--98.4\% and allows a trade-off between 55.3$\times$ in energy vs. 69.3$\times$ in <b>fairness.</b> The paper thus informs cloud providers about future scheduling optimizations for <b>fairness</b> with related challenges and opportunities.

{{</citation>}}


## cs.SI (1)



### (1/1 | 145/145) Learning the mechanisms of network growth (Lourens Touwen et al., 2024)

{{<citation>}}

Lourens Touwen, Doina Bucur, Remco van der Hofstad, Alessandro Garavaglia, Nelly Litvak. (2024)  
**Learning the mechanisms of network growth**
<br/>
<button class="copy-to-clipboard" title="Learning the mechanisms of network growth" index=145>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-SI, cs.SI, math-PR, stat-ML  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.00793v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.00793v1.pdf" filename="2404.00793v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We propose a novel model-selection method for dynamic real-life networks. Our approach involves training a classifier on a large body of synthetic network data. The data is generated by simulating nine state-of-the-art random <b>graph</b> models for dynamic networks, with parameter range chosen to ensure exponential growth of the network size in time. We design a conceptually novel type of dynamic features that count new links received by a group of vertices in a particular time interval. The proposed features are easy to compute, analytically tractable, and interpretable. Our approach achieves a near-perfect classification of synthetic networks, exceeding the state-of-the-art by a large margin. Applying our classification method to real-world citation networks gives credibility to the claims in the literature that models with preferential attachment, fitness and aging fit real-world citation networks best, although sometimes, the predicted model does not involve vertex fitness.

{{</citation>}}
