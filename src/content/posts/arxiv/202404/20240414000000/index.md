---
draft: false
title: "arXiv @ 2024.04.14"
date: 2024-04-14
author: "akitenkrad"
description: ""
tags: ["arXiv", "Published:2024"]
menu:
  sidebar:
    name: "arXiv @ 2024.04.14"
    identifier: arxiv_20240414
    parent: 202404_arxiv
    weight: 10
math: true
---

<figure style="border:none; width:100%; display:flex; justify-content: center">
    <iframe src="pie.html" width=900 height=620 style="border:none"></iframe>
</figure>


## Primary Categories

- [cs.AI (5)](#csai-5)
- [cs.CE (3)](#csce-3)
- [cs.CL (26)](#cscl-26)
- [cs.CR (12)](#cscr-12)
- [cs.CV (61)](#cscv-61)
- [cs.CY (1)](#cscy-1)
- [cs.DB (1)](#csdb-1)
- [cs.DC (4)](#csdc-4)
- [cs.DL (1)](#csdl-1)
- [cs.DS (1)](#csds-1)
- [cs.ET (1)](#cset-1)
- [cs.GT (2)](#csgt-2)
- [cs.HC (5)](#cshc-5)
- [cs.IR (7)](#csir-7)
- [cs.IT (2)](#csit-2)
- [cs.LG (36)](#cslg-36)
- [cs.LO (1)](#cslo-1)
- [cs.MM (1)](#csmm-1)
- [cs.NE (4)](#csne-4)
- [cs.NI (1)](#csni-1)
- [cs.PF (1)](#cspf-1)
- [cs.RO (13)](#csro-13)
- [cs.SE (3)](#csse-3)
- [cs.SI (2)](#cssi-2)
- [eess.IV (9)](#eessiv-9)
- [eess.SP (4)](#eesssp-4)
- [eess.SY (6)](#eesssy-6)
- [math.CO (4)](#mathco-4)
- [math.NA (4)](#mathna-4)
- [physics.ao-ph (2)](#physicsao-ph-2)
- [physics.chem-ph (1)](#physicschem-ph-1)
- [physics.flu-dyn (1)](#physicsflu-dyn-1)
- [physics.soc-ph (1)](#physicssoc-ph-1)
- [q-bio.QM (1)](#q-bioqm-1)
- [stat.ME (1)](#statme-1)
- [stat.ML (7)](#statml-7)

## Keywords

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>keyword</th>
      <th>cs.CL</th>
      <th>cs.CR</th>
      <th>cs.CV</th>
      <th>cs.LG</th>
      <th>cs.RO</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Adversarial Attack</td>
      <td></td>
      <td></td>
      <td>5</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Adversarial Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Anomaly Detection</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Augmented Reality (AR)</td>
      <td></td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Autoencoder</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Automatic Speech Recognition</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
      <td>3</td>
    </tr>
    <tr>
      <td>BERT</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>BLEU</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Bag-of-Words</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Benchmarking</td>
      <td>5</td>
      <td>1</td>
      <td>19</td>
      <td>6</td>
      <td></td>
    </tr>
    <tr>
      <td>Black Box</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>ChatGPT</td>
      <td>1</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Clustering</td>
      <td>1</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Code Generation</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Continual Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Contrastive Learning</td>
      <td></td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Convolution</td>
      <td></td>
      <td>1</td>
      <td>6</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Convolutional Neural Network</td>
      <td></td>
      <td>4</td>
      <td>21</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Counter-factual</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Data Augmentation</td>
      <td>1</td>
      <td></td>
      <td>4</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Deep Neural Network</td>
      <td></td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Dialogue State Tracking</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Diffusion Model</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Distribution Shift</td>
      <td></td>
      <td></td>
      <td>4</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Domain Adaptation</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Event Argument Extraction</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Event Detection</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Face Recognition</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fairness</td>
      <td></td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Federated Learning</td>
      <td></td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Few-shot</td>
      <td></td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Few-shot Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Fine-tuning</td>
      <td>6</td>
      <td></td>
      <td>11</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Foundation Model</td>
      <td></td>
      <td>1</td>
      <td>6</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT</td>
      <td>5</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>GPT-2</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-3</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-3.5</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>GPT-4</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Generative AI</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Generative Adversarial Network</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Geometry</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Graph</td>
      <td>2</td>
      <td>1</td>
      <td></td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Graph Attention Networks</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Classification</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Graph Neural Network</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>High-Resource</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Image2text</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Information Retrieval</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Graph</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Transfer</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>LLaMA</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Large Language Model</td>
      <td>14</td>
      <td>4</td>
      <td>5</td>
      <td>15</td>
      <td>8</td>
    </tr>
    <tr>
      <td>Low-Resource</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Malware</td>
      <td></td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Masked Language Model</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Massive Multitask Language Understanding (MMLU)</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Mistral</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Multi-modal</td>
      <td></td>
      <td></td>
      <td>7</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Mutual Information</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Neural Machine Translation</td>
      <td>2</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Node Embedding</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td></td>
      <td></td>
      <td>5</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Out-of-domain</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>PaLM</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Pre-trained Language Model</td>
      <td>3</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Prompt</td>
      <td>6</td>
      <td>2</td>
      <td>5</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Question Answering</td>
      <td>8</td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Reasoning</td>
      <td></td>
      <td></td>
      <td>3</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Recommendation</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Reconstruction Loss</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Recurrent Neural Network</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Reinforcement Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>7</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Reinforcement Learning from Human Feedback</td>
      <td></td>
      <td></td>
      <td></td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Representation Learning</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Retrieval-Augmented Generation</td>
      <td></td>
      <td></td>
      <td></td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>RoBERTa</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Scaling Law</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Security</td>
      <td></td>
      <td>10</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Self-Attention</td>
      <td>1</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Self-Distillation</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Self-adaptive Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Self-supervised Learning</td>
      <td></td>
      <td></td>
      <td>7</td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Semantic Parsing</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Simulation</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td>9</td>
    </tr>
    <tr>
      <td>Simulator</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td>9</td>
    </tr>
    <tr>
      <td>Spoofing</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Stemming</td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Stochastic Gradient Descent</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Supervised Learning</td>
      <td></td>
      <td>2</td>
      <td>4</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>TF-IDF</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Classification</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Text Embedding</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Generation</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Tokenization</td>
      <td>1</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Transfer Learning</td>
      <td>1</td>
      <td></td>
      <td>2</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Transformer</td>
      <td>5</td>
      <td></td>
      <td>9</td>
      <td>5</td>
      <td></td>
    </tr>
    <tr>
      <td>Unsupervised Learning</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Variational Autoencoder</td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Virtual Reality (VR)</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Vision Transformer</td>
      <td></td>
      <td></td>
      <td>6</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Vision-and-Language</td>
      <td></td>
      <td></td>
      <td>3</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Visual Question Answering</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Weakly-supervised Learning</td>
      <td></td>
      <td></td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Zero-shot</td>
      <td>3</td>
      <td></td>
      <td>6</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Zero-shot Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>human-in-the-loop</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<script>
$(function() {
  $("table").addClass("keyword-table table-bordered border-success");
  $("table thead").addClass("sticky-top");
  $("table tbody td").css("text-align", "");
});
</script>


## cs.CV (61)



### (1/61 | 1/235) Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts (Övgü Özdemir et al., 2024)

{{<citation>}}

Övgü Özdemir, Erdem Akagündüz. (2024)  
**Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts**
<br/>
<button class="copy-to-clipboard" title="Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts" index=1>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 90  
Keywords: Zero-shot, Question Answering, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08589v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08589v1.pdf" filename="2404.08589v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Visual</b> <b>question</b> <b>answering</b> <b>(VQA)</b> is known as an AI-complete task as it requires understanding, <b>reasoning,</b> and inferring about the vision and the language content. Over the past few years, numerous neural architectures have been suggested for the <b>VQA</b> problem. However, achieving success in <b>zero-shot</b> <b>VQA</b> remains a challenge due to its requirement for advanced generalization and <b>reasoning</b> skills. This study explores the impact of incorporating image captioning as an intermediary process within the <b>VQA</b> pipeline. Specifically, we explore the efficacy of utilizing image captions instead of images and leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to establish a <b>zero-shot</b> setting. Since image captioning is the most crucial step in this process, we compare the impact of state-of-the-art image captioning models on <b>VQA</b> performance across various <b>question</b> <b>types</b> in terms of structure and semantics. We propose a straightforward and efficient <b>question-driven</b> <b>image</b> captioning approach within this pipeline to transfer contextual information into the <b>question-answering</b> <b>(QA)</b> model. This method involves extracting keywords from the <b>question,</b> <b>generating</b> a caption for each image-question pair using the keywords, and incorporating the <b>question-driven</b> <b>caption</b> into the <b>LLM</b> <b>prompt.</b> We evaluate the efficacy of using general-purpose and <b>question-driven</b> <b>image</b> captions in the <b>VQA</b> pipeline. Our study highlights the potential of employing image captions and harnessing the capabilities of <b>LLMs</b> to achieve competitive performance on GQA under the <b>zero-shot</b> setting. Our code is available at \url{https://github.com/ovguyo/captions-in-VQA}.

{{</citation>}}


### (2/61 | 2/235) Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation (Sina Hajimiri et al., 2024)

{{<citation>}}

Sina Hajimiri, Ismail Ben Ayed, Jose Dolz. (2024)  
**Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation**
<br/>
<button class="copy-to-clipboard" title="Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation" index=2>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 83  
Keywords: Vision Transformer, Benchmarking, Foundation Model, Supervised Learning, Zero-shot, Transformer, Self-Attention, Vision Transformer, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08181v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08181v1.pdf" filename="2404.08181v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite the significant progress in deep learning for dense visual recognition problems, such as semantic segmentation, traditional methods are constrained by fixed class sets. Meanwhile, <b>vision-language</b> <b>foundation</b> <b>models,</b> such as CLIP, have showcased remarkable effectiveness in numerous <b>zero-shot</b> image-level tasks, owing to their robust generalizability. Recently, a body of work has investigated utilizing these models in open-vocabulary semantic segmentation (OVSS). However, existing approaches often rely on impractical <b>supervised</b> pre-training or access to additional pre-trained networks. In this work, we propose a strong baseline for training-free OVSS, termed Neighbour-Aware CLIP (NACLIP), representing a straightforward adaptation of CLIP tailored for this scenario. Our method enforces localization of patches in the <b>self-attention</b> of CLIP's <b>vision</b> <b>transformer</b> which, despite being crucial for dense prediction tasks, has been overlooked in the OVSS literature. By incorporating design choices favouring segmentation, our approach significantly improves performance without requiring additional data, auxiliary pre-trained networks, or extensive hyperparameter tuning, making it highly practical for real-world applications. Experiments are performed on 8 popular semantic segmentation <b>benchmarks,</b> yielding state-of-the-art performance on most scenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP .

{{</citation>}}


### (3/61 | 3/235) Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection (Zhiwei Yang et al., 2024)

{{<citation>}}

Zhiwei Yang, Jing Liu, Peng Wu. (2024)  
**Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection**
<br/>
<button class="copy-to-clipboard" title="Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection" index=3>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 73  
Keywords: Anomaly Detection, Benchmarking, Fine-tuning, Self-adaptive Learning, Supervised Learning, Weakly-supervised Learning, Domain Adaptation, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08531v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08531v1.pdf" filename="2404.08531v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Weakly <b>supervised</b> video <b>anomaly</b> <b>detection</b> (WSVAD) is a challenging task. Generating fine-grained pseudo-labels based on weak-label and then self-training a classifier is currently a promising solution. However, since the existing methods use only RGB visual modality and the utilization of category text information is neglected, thus limiting the generation of more accurate pseudo-labels and affecting the performance of self-training. Inspired by the manual labeling process based on the event description, in this paper, we propose a novel pseudo-label generation and self-training framework based on Text <b>Prompt</b> with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer the rich language-visual knowledge of the contrastive language-image pre-training (CLIP) model for aligning the video event description text and corresponding video frames to generate pseudo-labels. Specifically, We first <b>fine-tune</b> the CLIP for <b>domain</b> <b>adaptation</b> by designing two ranking losses and a distributional inconsistency loss. Further, we propose a learnable text <b>prompt</b> mechanism with the assist of a normality visual <b>prompt</b> to further improve the matching accuracy of video event description text and video frames. Then, we design a pseudo-label generation module based on the normality guidance to infer reliable frame-level pseudo-labels. Finally, we introduce a temporal context <b>self-adaptive</b> <b>learning</b> module to learn the temporal dependencies of different video events more flexibly and accurately. Extensive experiments show that our method achieves state-of-the-art performance on two <b>benchmark</b> datasets, UCF-Crime and XD-Viole

{{</citation>}}


### (4/61 | 4/235) Single-image driven 3d viewpoint training data augmentation for effective wine label recognition (Yueh-Cheng Huang et al., 2024)

{{<citation>}}

Yueh-Cheng Huang, Hsin-Yi Chen, Cheng-Jui Hung, Jen-Hui Chuang, Jenq-Neng Hwang. (2024)  
**Single-image driven 3d viewpoint training data augmentation for effective wine label recognition**
<br/>
<button class="copy-to-clipboard" title="Single-image driven 3d viewpoint training data augmentation for effective wine label recognition" index=4>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 60  
Keywords: Vision Transformer, Data Augmentation, Generative Adversarial Network, Generative Adversarial Network, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08820v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08820v1.pdf" filename="2404.08820v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Confronting the critical challenge of insufficient training <b>data</b> <b>in</b> the field of complex image recognition, this paper introduces a novel 3D viewpoint augmentation technique specifically tailored for wine label recognition. This method enhances deep learning model performance by generating visually realistic training samples from a single real-world wine label image, overcoming the challenges posed by the intricate combinations of text and logos. Classical <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)</b> methods fall short in synthesizing such intricate content combination. Our proposed solution leverages time-tested computer <b>vision</b> <b>and</b> image processing strategies to expand our training dataset, thereby broadening the range of training samples for deep learning applications. This innovative approach to <b>data</b> <b>augmentation</b> circumvents the constraints of limited training resources. Using the augmented training images through batch-all triplet metric learning on a <b>Vision</b> <b>Transformer</b> (ViT) architecture, we can get the most discriminative embedding features for every wine label, enabling us to perform one-shot recognition of existing wine labels in the training classes or future newly collected wine labels unavailable in the training. Experimental results show a significant increase in recognition accuracy over conventional 2D <b>data</b> <b>augmentation</b> techniques.

{{</citation>}}


### (5/61 | 5/235) Advanced wood species identification based on multiple anatomical sections and using deep feature transfer and fusion (Kallil M. Zielinski et al., 2024)

{{<citation>}}

Kallil M. Zielinski, Leonardo Scabini, Lucas C. Ribas, Núbia R. da Silva, Hans Beeckman, Jan Verwaeren, Odemir M. Bruno, Bernard De Baets. (2024)  
**Advanced wood species identification based on multiple anatomical sections and using deep feature transfer and fusion**
<br/>
<button class="copy-to-clipboard" title="Advanced wood species identification based on multiple anatomical sections and using deep feature transfer and fusion" index=5>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 50  
Keywords: Convolutional Neural Network, Convolutional Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08585v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08585v1.pdf" filename="2404.08585v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, we have seen many advancements in wood species identification. Methods like DNA analysis, Near Infrared (NIR) spectroscopy, and Direct Analysis in Real Time (DART) mass spectrometry complement the long-established wood anatomical assessment of cell and tissue morphology. However, most of these methods have some limitations such as high costs, the need for skilled experts for data interpretation, and the lack of good datasets for professional reference. Therefore, most of these methods, and certainly the wood anatomical assessment, may benefit from tools based on Artificial Intelligence. In this paper, we apply two <b>transfer</b> <b>learning</b> techniques with <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> to a multi-view Congolese wood species dataset including sections from different orientations and viewed at different microscopic magnifications. We explore two feature extraction methods in detail, namely Global Average Pooling (GAP) and Random Encoding of Aggregated Deep Activation Maps (RADAM), for efficient and accurate wood species identification. Our results indicate superior accuracy on diverse datasets and anatomical sections, surpassing the results of other methods. Our proposal represents a significant advancement in wood species identification, offering a robust tool to support the conservation of forest ecosystems and promote sustainable forestry practices.

{{</citation>}}


### (6/61 | 6/235) MambaDFuse: A Mamba-based Dual-phase Model for Multi-modality Image Fusion (Zhe Li et al., 2024)

{{<citation>}}

Zhe Li, Haiwei Pan, Kejia Zhang, Yuhua Wang, Fengming Yu. (2024)  
**MambaDFuse: A Mamba-based Dual-phase Model for Multi-modality Image Fusion**
<br/>
<button class="copy-to-clipboard" title="MambaDFuse: A Mamba-based Dual-phase Model for Multi-modality Image Fusion" index=6>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 46  
Keywords: Convolutional Neural Network, Object Detection, Benchmarking, Convolutional Neural Network, Deep Neural Network, Multi-modal, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08406v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08406v1.pdf" filename="2404.08406v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Multi-modality image fusion (MMIF) aims to integrate complementary information from different modalities into a single fused image to represent the imaging scene and facilitate downstream visual tasks comprehensively. In recent years, significant progress has been made in MMIF tasks due to advances in <b>deep</b> <b>neural</b> <b>networks.</b> However, existing methods cannot effectively and efficiently extract modality-specific and modality-fused features constrained by the inherent local reductive bias <b>(CNN)</b> or quadratic computational complexity <b>(Transformers).</b> To overcome this issue, we propose a Mamba-based Dual-phase Fusion (MambaDFuse) model. Firstly, a dual-level feature extractor is designed to capture long-range features from single-modality images by extracting low and high-level features from <b>CNN</b> and Mamba blocks. Then, a dual-phase feature fusion module is proposed to obtain fusion features that combine complementary information from different modalities. It uses the channel exchange method for shallow fusion and the enhanced <b>Multi-modal</b> Mamba (M3) blocks for <b>deep</b> <b>fusion.</b> <b>Finally,</b> the fused image reconstruction module utilizes the inverse transformation of the feature extraction to generate the fused result. Through extensive experiments, our approach achieves promising fusion results in infrared-visible image fusion and medical image fusion. Additionally, in a unified <b>benchmark,</b> MambaDFuse has also demonstrated improved performance in downstream tasks such as <b>object</b> <b>detection.</b> Code with checkpoints will be available after the peer-review process.

{{</citation>}}


### (7/61 | 7/235) MoE-FFD: Mixture of Experts for Generalized and Parameter-Efficient Face Forgery Detection (Chenqi Kong et al., 2024)

{{<citation>}}

Chenqi Kong, Anwei Luo, Song Xia, Yi Yu, Haoliang Li, Alex C. Kot. (2024)  
**MoE-FFD: Mixture of Experts for Generalized and Parameter-Efficient Face Forgery Detection**
<br/>
<button class="copy-to-clipboard" title="MoE-FFD: Mixture of Experts for Generalized and Parameter-Efficient Face Forgery Detection" index=7>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 45  
Keywords: Convolutional Neural Network, Convolutional Neural Network, Fine-tuning, Transformer, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08452v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08452v1.pdf" filename="2404.08452v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deepfakes have recently raised significant trust issues and <b>security</b> concerns among the public. Compared to <b>CNN</b> face forgery detectors, ViT-based methods take advantage of the expressivity of <b>transformers,</b> achieving superior detection performance. However, these approaches still exhibit the following limitations: (1). Fully <b>fine-tuning</b> ViT-based models from ImageNet weights demands substantial computational and storage resources; (2). ViT-based methods struggle to capture local forgery clues, leading to model bias and limited generalizability. To tackle these challenges, this work introduces Mixture-of-Experts modules for Face Forgery Detection (MoE-FFD), a generalized yet parameter-efficient ViT-based approach. MoE-FFD only updates lightweight Low-Rank Adaptation (LoRA) and Adapter layers while keeping the ViT backbone frozen, thereby achieving parameter-efficient training. Moreover, MoE-FFD leverages the expressivity of <b>transformers</b> and local priors of <b>CNNs</b> to simultaneously extract global and local forgery clues. Additionally, novel MoE modules are designed to scale the model's capacity and select optimal forgery experts, further enhancing forgery detection performance. The proposed MoE learning scheme can be seamlessly adapted to various <b>transformer</b> backbones in a plug-and-play manner. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art face forgery detection performance with reduced parameter overhead. The code will be released upon acceptance.

{{</citation>}}


### (8/61 | 8/235) Improving Continuous Sign Language Recognition with Adapted Image Models (Lianyu Hu et al., 2024)

{{<citation>}}

Lianyu Hu, Tongkai Shi, Liqing Gao, Zekang Liu, Wei Feng. (2024)  
**Improving Continuous Sign Language Recognition with Adapted Image Models**
<br/>
<button class="copy-to-clipboard" title="Improving Continuous Sign Language Recognition with Adapted Image Models" index=8>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 43  
Keywords: Benchmarking, Fine-tuning, Fine-tuning, Image2text, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08226v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08226v1.pdf" filename="2404.08226v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The increase of web-scale weakly labelled <b>image-text</b> pairs have greatly facilitated the development of large-scale <b>vision-language</b> models (e.g., CLIP), which have shown impressive generalization performance over a series of downstream tasks. However, the massive model size and scarcity of available data limit their applications to <b>fine-tune</b> the whole model in downstream tasks. Besides, fully <b>fine-tuning</b> the model easily forgets the generic essential knowledge acquired in the pretraining stage and overfits the downstream data. To enable high efficiency when adapting these large <b>vision-language</b> models (e.g., CLIP) to performing continuous sign language recognition (CSLR) while preserving their generalizability, we propose a novel strategy (AdaptSign). Especially, CLIP is adopted as the visual backbone to extract frame-wise features whose parameters are fixed, and a set of learnable modules are introduced to model spatial sign variations or capture temporal sign movements. The introduced additional modules are quite lightweight, only owning 3.2% extra computations with high efficiency. The generic knowledge acquired in the pretraining stage is well-preserved in the frozen CLIP backbone in this process. Extensive experiments show that despite being efficient, AdaptSign is able to demonstrate superior performance across a series of CSLR <b>benchmarks</b> including PHOENIX14, PHOENIX14-T, CSL-Daily and CSL compared to existing methods. Visualizations show that AdaptSign could learn to dynamically pay major attention to the informative spatial regions and cross-frame trajectories in sign videos.

{{</citation>}}


### (9/61 | 9/235) Uncertainty Quantification in Detecting Choroidal Metastases on MRI via Evolutionary Strategies (Bala McRae-Posani et al., 2024)

{{<citation>}}

Bala McRae-Posani, Andrei Holodny, Hrithwik Shalu, Joseph N Stember. (2024)  
**Uncertainty Quantification in Detecting Choroidal Metastases on MRI via Evolutionary Strategies**
<br/>
<button class="copy-to-clipboard" title="Uncertainty Quantification in Detecting Choroidal Metastases on MRI via Evolutionary Strategies" index=9>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs-NE, cs.CV  
Keyword Score: 40  
Keywords: Convolutional Neural Network, Convolutional Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08853v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08853v1.pdf" filename="2404.08853v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Uncertainty quantification plays a vital role in facilitating the practical implementation of AI in radiology by addressing growing concerns around trustworthiness. Given the challenges associated with acquiring large, annotated datasets in this field, there is a need for methods that enable uncertainty quantification in small data AI approaches tailored to radiology images. In this study, we focused on uncertainty quantification within the context of the small data evolutionary strategies-based technique of deep neuroevolution (DNE). Specifically, we employed DNE to train a simple <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> with MRI images of the eyes for binary classification. The goal was to distinguish between normal eyes and those with metastatic tumors called choroidal metastases. The training set comprised 18 images with choroidal metastases and 18 without tumors, while the testing set contained a tumor-to-normal ratio of 15:15. We trained <b>CNN</b> model weights via DNE for approximately 40,000 episodes, ultimately reaching a convergence of 100% accuracy on the training set. We saved all models that achieved maximal training set accuracy. Then, by applying these models to the testing set, we established an ensemble method for uncertainty quantification.The saved set of models produced distributions for each testing set image between the two classes of normal and tumor-containing. The relative frequencies permitted uncertainty quantification of model predictions. Intriguingly, we found that subjective features appreciated by human radiologists explained images for which uncertainty was high, highlighting the significance of uncertainty quantification in AI-driven radiological analyses.

{{</citation>}}


### (10/61 | 10/235) Pathological Primitive Segmentation Based on Visual Foundation Model with Zero-Shot Mask Generation (Abu Bakor Hayat Arnob et al., 2024)

{{<citation>}}

Abu Bakor Hayat Arnob, Xiangxue Wang, Yiping Jiao, Xiao Gan, Wenlong Ming, Jun Xu. (2024)  
**Pathological Primitive Segmentation Based on Visual Foundation Model with Zero-Shot Mask Generation**
<br/>
<button class="copy-to-clipboard" title="Pathological Primitive Segmentation Based on Visual Foundation Model with Zero-Shot Mask Generation" index=10>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: I-4-6; I-2, cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Fine-tuning, Foundation Model, Zero-shot, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08584v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08584v1.pdf" filename="2404.08584v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Medical image processing usually requires a model trained with carefully crafted datasets due to unique image characteristics and domain-specific challenges, especially in pathology. Primitive detection and segmentation in digitized tissue samples are essential for objective and automated diagnosis and prognosis of cancer. SAM (Segment Anything Model) has recently been developed to segment general objects from natural images with high accuracy, but it requires human <b>prompts</b> to generate masks. In this work, we present a novel approach that adapts pre-trained natural image encoders of SAM for detection-based region proposals. Regions proposed by a pre-trained encoder are sent to cascaded feature propagation layers for projection. Then, local semantic and global context is aggregated from multi-scale for bounding box localization and classification. Finally, the SAM decoder uses the identified bounding boxes as essential <b>prompts</b> to generate a comprehensive primitive segmentation map. The entire base framework, SAM, requires no additional training or <b>fine-tuning</b> but could produce an end-to-end result for two fundamental segmentation tasks in pathology. Our method compares with state-of-the-art models in F1 score for nuclei detection and binary/multiclass panoptic(bPQ/mPQ) and mask quality(dice) for segmentation quality on the PanNuke dataset while offering end-to-end efficiency. Our model also achieves remarkable Average Precision (+4.5%) on the secondary dataset (HuBMAP Kidney) compared to Faster RCNN. The code is publicly available at https://github.com/learner-codec/autoprom_sam.

{{</citation>}}


### (11/61 | 11/235) ChatGPT and general-purpose AI count fruits in pictures surprisingly well (Konlavach Mengsuwan et al., 2024)

{{<citation>}}

Konlavach Mengsuwan, Juan Camilo Rivera Palacio, Masahiro Ryo. (2024)  
**ChatGPT and general-purpose AI count fruits in pictures surprisingly well**
<br/>
<button class="copy-to-clipboard" title="ChatGPT and general-purpose AI count fruits in pictures surprisingly well" index=11>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV, eess-IV  
Keyword Score: 40  
Keywords: Few-shot, Few-shot Learning, Foundation Model, ChatGPT  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08515v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08515v1.pdf" filename="2404.08515v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Object counting is a popular task in deep learning applications in various domains, including agriculture. A conventional deep learning approach requires a large amount of training data, often a logistic problem in a real-world application. To address this issue, we examined how well <b>ChatGPT</b> (GPT4V) and a general-purpose AI <b>(foundation</b> <b>model</b> for object counting, T-Rex) can count the number of fruit bodies (coffee cherries) in 100 images. The <b>foundation</b> <b>model</b> with <b>few-shot</b> <b>learning</b> outperformed the trained YOLOv8 model (R2 = 0.923 and 0.900, respectively). <b>ChatGPT</b> also showed some interesting potential, especially when <b>few-shot</b> <b>learning</b> with human feedback was applied (R2 = 0.360 and 0.460, respectively). Moreover, we examined the time required for implementation as a practical question. Obtaining the results with the <b>foundation</b> <b>model</b> and <b>ChatGPT</b> were much shorter than the YOLOv8 model (0.83 hrs, 1.75 hrs, and 161 hrs). We interpret these results as two surprises for deep learning users in applied domains: a <b>foundation</b> <b>model</b> with <b>few-shot</b> <b>domain-specific</b> learning can drastically save time and effort compared to the conventional approach, and <b>ChatGPT</b> can reveal a relatively good performance. Both approaches do not need coding skills, which can foster AI education and dissemination.

{{</citation>}}


### (12/61 | 12/235) TDANet: Target-Directed Attention Network For Object-Goal Visual Navigation With Zero-Shot Ability (Shiwei Lian et al., 2024)

{{<citation>}}

Shiwei Lian, Feitian Zhang. (2024)  
**TDANet: Target-Directed Attention Network For Object-Goal Visual Navigation With Zero-Shot Ability**
<br/>
<button class="copy-to-clipboard" title="TDANet: Target-Directed Attention Network For Object-Goal Visual Navigation With Zero-Shot Ability" index=12>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-RO, cs.CV  
Keyword Score: 40  
Keywords: Reinforcement Learning, Simulation, Simulator, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08353v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08353v1.pdf" filename="2404.08353v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The generalization of the end-to-end deep <b>reinforcement</b> <b>learning</b> (DRL) for object-goal visual navigation is a long-standing challenge since object classes and placements vary in new test environments. Learning domain-independent visual representation is critical for enabling the trained DRL agent with the ability to generalize to unseen scenes and objects. In this letter, a target-directed attention network (TDANet) is proposed to learn the end-to-end object-goal visual navigation policy with <b>zero-shot</b> ability. TDANet features a novel target attention (TA) module that learns both the spatial and semantic relationships among objects to help TDANet focus on the most relevant observed objects to the target. With the Siamese architecture (SA) design, TDANet distinguishes the difference between the current and target states and generates the domain-independent visual representation. To evaluate the navigation performance of TDANet, extensive experiments are conducted in the AI2-THOR embodied AI environment. The <b>simulation</b> results demonstrate a strong generalization ability of TDANet to unseen scenes and target objects, with higher navigation success rate (SR) and success weighted by length (SPL) than other state-of-the-art models.

{{</citation>}}


### (13/61 | 13/235) Emerging Property of Masked Token for Effective Pre-training (Hyesong Choi et al., 2024)

{{<citation>}}

Hyesong Choi, Hunsang Lee, Seyoung Joung, Hyejin Park, Jiyeong Kim, Dongbo Min. (2024)  
**Emerging Property of Masked Token for Effective Pre-training**
<br/>
<button class="copy-to-clipboard" title="Emerging Property of Masked Token for Effective Pre-training" index=13>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 40  
Keywords: Self-supervised Learning, Self-supervised Learning, Masked Language Model, Masked Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08330v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08330v1.pdf" filename="2404.08330v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Driven by the success of <b>Masked</b> <b>Language</b> <b>Modeling</b> <b>(MLM),</b> the realm of <b>self-supervised</b> <b>learning</b> for computer vision has been invigorated by the central role of <b>Masked</b> <b>Image</b> <b>Modeling</b> (MIM) in driving recent breakthroughs. Notwithstanding the achievements of MIM across various downstream tasks, its overall efficiency is occasionally hampered by the lengthy duration of the pre-training phase. This paper presents a perspective that the optimization of <b>masked</b> <b>tokens</b> <b>as</b> a means of addressing the prevailing issue. Initially, we delve into an exploration of the inherent properties that a <b>masked</b> <b>token</b> <b>ought</b> to possess. Within the properties, we principally dedicated to articulating and emphasizing the `data singularity' attribute inherent in <b>masked</b> <b>tokens.</b> <b>Through</b> a comprehensive analysis of the heterogeneity between <b>masked</b> <b>tokens</b> <b>and</b> visible tokens within pre-trained models, we propose a novel approach termed <b>masked</b> <b>token</b> <b>optimization</b> (MTO), specifically designed to improve model efficiency through weight recalibration and the enhancement of the key property of <b>masked</b> <b>tokens.</b> <b>The</b> proposed method serves as an adaptable solution that seamlessly integrates into any MIM approach that leverages <b>masked</b> <b>tokens.</b> <b>As</b> a result, MTO achieves a considerable improvement in pre-training efficiency, resulting in an approximately 50% reduction in pre-training epochs required to attain converged performance of the recent approaches.

{{</citation>}}


### (14/61 | 14/235) SpectralMamba: Efficient Mamba for Hyperspectral Image Classification (Jing Yao et al., 2024)

{{<citation>}}

Jing Yao, Danfeng Hong, Chenyu Li, Jocelyn Chanussot. (2024)  
**SpectralMamba: Efficient Mamba for Hyperspectral Image Classification**
<br/>
<button class="copy-to-clipboard" title="SpectralMamba: Efficient Mamba for Hyperspectral Image Classification" index=14>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 38  
Keywords: Benchmarking, Convolution, Representation Learning, Recurrent Neural Network, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08489v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08489v1.pdf" filename="2404.08489v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Recurrent</b> <b>neural</b> <b>networks</b> and <b>Transformers</b> have recently dominated most applications in hyperspectral (HS) imaging, owing to their capability to capture long-range dependencies from spectrum sequences. However, despite the success of these sequential architectures, the non-ignorable inefficiency caused by either difficulty in parallelization or computationally prohibitive attention still hinders their practicality, especially for large-scale observation in remote sensing scenarios. To address this issue, we herein propose SpectralMamba -- a novel state space model incorporated efficient deep learning framework for HS image classification. SpectralMamba features the simplified but adequate modeling of HS data dynamics at two levels. First, in spatial-spectral space, a dynamical mask is learned by efficient <b>convolutions</b> to simultaneously encode spatial regularity and spectral peculiarity, thus attenuating the spectral variability and confusion in discriminative <b>representation</b> <b>learning.</b> Second, the merged spectrum can then be efficiently operated in the hidden state space with all parameters learned input-dependent, yielding selectively focused responses without reliance on redundant attention or imparallelizable recurrence. To explore the room for further computational downsizing, a piece-wise scanning mechanism is employed in-between, transferring approximately continuous spectrum into sequences with squeezed length while maintaining short- and long-term contextual profiles among hundreds of bands. Through extensive experiments on four <b>benchmark</b> HS datasets acquired by satellite-, aircraft-, and UAV-borne imagers, SpectralMamba surprisingly creates promising win-wins from both performance and efficiency perspectives.

{{</citation>}}


### (15/61 | 15/235) Masked Image Modeling as a Framework for Self-Supervised Learning across Eye Movements (Robin Weiler et al., 2024)

{{<citation>}}

Robin Weiler, Matthias Brucklacher, Cyriel M. A. Pennartz, Sander M. Bohté. (2024)  
**Masked Image Modeling as a Framework for Self-Supervised Learning across Eye Movements**
<br/>
<button class="copy-to-clipboard" title="Masked Image Modeling as a Framework for Self-Supervised Learning across Eye Movements" index=15>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 35  
Keywords: Data Augmentation, Representation Learning, Self-supervised Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08526v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08526v1.pdf" filename="2404.08526v1.pdf">Download PDF</button>

---


**ABSTRACT**  
To make sense of their surroundings, intelligent systems must transform complex sensory inputs to structured codes that are reduced to task-relevant information such as object category. Biological agents achieve this in a largely autonomous manner, presumably via self-\allowbreak super-\allowbreak vised learning. Whereas previous attempts to model the underlying mechanisms were largely discriminative in nature, there is ample evidence that the brain employs a generative model of the world. Here, we propose that eye movements, in combination with the focused nature of primate vision, constitute a generative, <b>self-supervised</b> <b>task</b> of predicting and revealing visual information. We construct a proof-of-principle model starting from the framework of masked image modeling (MIM), a common approach in deep <b>representation</b> <b>learning.</b> To do so, we analyze how core components of MIM such as masking technique and <b>data</b> <b>augmentation</b> influence the formation of category-specific <b>representations.</b> <b>This</b> allows us not only to better understand the principles behind MIM, but to then reassemble a MIM more in line with the focused nature of biological perception. From a theoretical angle, we find that MIM disentangles neurons in latent space, a property that has been suggested to structure visual <b>representations</b> <b>in</b> primates, without explicit regulation. Together with previous findings of invariance learning, this highlights an interesting connection of MIM to latent regularization approaches for <b>self-supervised</b> <b>learning.</b> The source code is available under https://github.com/RobinWeiler/FocusMIM

{{</citation>}}


### (16/61 | 16/235) Adapting CNNs for Fisheye Cameras without Retraining (Ryan Griffiths et al., 2024)

{{<citation>}}

Ryan Griffiths, Donald G. Dansereau. (2024)  
**Adapting CNNs for Fisheye Cameras without Retraining**
<br/>
<button class="copy-to-clipboard" title="Adapting CNNs for Fisheye Cameras without Retraining" index=16>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-RO, cs.CV  
Keyword Score: 35  
Keywords: Convolutional Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08187v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08187v1.pdf" filename="2404.08187v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The majority of image processing approaches assume images are in or can be rectified to a perspective projection. However, in many applications it is beneficial to use non conventional cameras, such as fisheye cameras, that have a larger field of view (FOV). The issue arises that these large-FOV images can't be rectified to a perspective projection without significant cropping of the original image. To address this issue we propose Rectified <b>Convolutions</b> (RectConv); a new approach for adapting pre-trained <b>convolutional</b> <b>networks</b> to operate with new non-perspective images, without any retraining. Replacing the <b>convolutional</b> <b>layers</b> of the network with RectConv layers allows the network to see both rectified patches and the entire FOV. We demonstrate RectConv adapting multiple pre-trained networks to perform segmentation and detection on fisheye imagery from two publicly available datasets. Our approach requires no additional data or training, and operates directly on the native image as captured from the camera. We believe this work is a step toward adapting the vast resources available for perspective images to operate across a broad range of camera geometries.

{{</citation>}}


### (17/61 | 17/235) LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning (Junchi Wang et al., 2024)

{{<citation>}}

Junchi Wang, Lei Ke. (2024)  
**LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning**
<br/>
<button class="copy-to-clipboard" title="LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning" index=17>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 33  
Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08767v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08767v1.pdf" filename="2404.08767v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Understanding human instructions to identify the target objects is vital for perception systems. In recent years, the advancements of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have introduced new possibilities for image segmentation. In this work, we delve into <b>reasoning</b> segmentation, a novel task that enables segmentation system to reason and interpret implicit user intention via <b>large</b> <b>language</b> <b>model</b> <b>reasoning</b> and then segment the corresponding target. Our work on <b>reasoning</b> segmentation contributes on both the methodological design and dataset labeling. For the model, we propose a new framework named <b>LLM-Seg.</b> <b>LLM-Seg</b> effectively connects the current foundational Segmentation Anything Model and the <b>LLM</b> by mask proposals selection. For the dataset, we propose an automatic data generation pipeline and construct a new <b>reasoning</b> segmentation dataset named <b>LLM-Seg40K.</b> Experiments demonstrate that our <b>LLM-Seg</b> exhibits competitive performance compared with existing methods. Furthermore, our proposed pipeline can efficiently produce high-quality <b>reasoning</b> segmentation datasets. The <b>LLM-Seg40K</b> dataset, developed through this pipeline, serves as a new <b>benchmark</b> for training and evaluating various <b>reasoning</b> segmentation approaches. Our code, models and dataset are at https://github.com/wangjunchi/LLMSeg.

{{</citation>}}


### (18/61 | 18/235) `Eyes of a Hawk and Ears of a Fox': Part Prototype Network for Generalized Zero-Shot Learning (Joshua Feinglass et al., 2024)

{{<citation>}}

Joshua Feinglass, Jayaraman J. Thiagarajan, Rushil Anirudh, T. S. Jayram, Yezhou Yang. (2024)  
**`Eyes of a Hawk and Ears of a Fox': Part Prototype Network for Generalized Zero-Shot Learning**
<br/>
<button class="copy-to-clipboard" title="`Eyes of a Hawk and Ears of a Fox': Part Prototype Network for Generalized Zero-Shot Learning" index=18>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 33  
Keywords: Benchmarking, Zero-shot, Vision-and-Language, Zero-shot Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08761v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08761v1.pdf" filename="2404.08761v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Current approaches in Generalized <b>Zero-Shot</b> <b>Learning</b> (GZSL) are built upon base models which consider only a single class attribute vector representation over the entire image. This is an oversimplification of the process of novel category recognition, where different regions of the image may have properties from different seen classes and thus have different predominant attributes. With this in mind, we take a fundamentally different approach: a pre-trained <b>Vision-Language</b> detector (VINVL) sensitive to attribute information is employed to efficiently obtain region features. A learned function maps the region features to region-specific attribute attention used to construct class part prototypes. We conduct experiments on a popular GZSL <b>benchmark</b> consisting of the CUB, SUN, and AWA2 datasets where our proposed Part Prototype Network (PPN) achieves promising results when compared with other popular base models. Corresponding ablation studies and analysis show that our approach is highly practical and has a distinct advantage over global attribute attention when localized proposals are available.

{{</citation>}}


### (19/61 | 19/235) FaceFilterSense: A Filter-Resistant Face Recognition and Facial Attribute Analysis Framework (Shubham Tiwari et al., 2024)

{{<citation>}}

Shubham Tiwari, Yash Sethia, Ritesh Kumar, Ashwani Tanwar, Rudresh Dwivedi. (2024)  
**FaceFilterSense: A Filter-Resistant Face Recognition and Facial Attribute Analysis Framework**
<br/>
<button class="copy-to-clipboard" title="FaceFilterSense: A Filter-Resistant Face Recognition and Facial Attribute Analysis Framework" index=19>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 33  
Keywords: Face Recognition, Augmented Reality (AR), Augmented Reality (AR), Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08277v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08277v1.pdf" filename="2404.08277v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the advent of social media, fun selfie filters have come into tremendous mainstream use affecting the functioning of facial biometric systems as well as image recognition systems. These filters vary from beautification filters and <b>Augmented</b> <b>Reality</b> <b>(AR)-based</b> filters to filters that modify facial landmarks. Hence, there is a need to assess the impact of such filters on the performance of existing <b>face</b> <b>recognition</b> systems. The limitation associated with existing solutions is that these solutions focus more on the beautification filters. However, the current <b>AR-based</b> filters and filters which distort facial key points are in vogue recently and make the <b>faces</b> <b>highly</b> unrecognizable even to the naked eye. Also, the filters considered are mostly obsolete with limited variations. To mitigate these limitations, we aim to perform a holistic impact analysis of the latest filters and propose an user recognition model with the filtered images. We have utilized a <b>benchmark</b> dataset for baseline images, and applied the latest filters over them to generate a beautified/filtered dataset. Next, we have introduced a model FaceFilterNet for beautified user recognition. In this framework, we also utilize our model to comment on various attributes of the person including age, gender, and ethnicity. In addition, we have also presented a filter-wise impact analysis on <b>face</b> <b>recognition,</b> age estimation, gender, and ethnicity prediction. The proposed method affirms the efficacy of our dataset with an accuracy of 87.25% and an optimal accuracy for facial attribute analysis.

{{</citation>}}


### (20/61 | 20/235) Towards Sim-to-Real Industrial Parts Classification with Synthetic Dataset (Xiaomeng Zhu et al., 2024)

{{<citation>}}

Xiaomeng Zhu, Talha Bilal, Pär Mårtensson, Lars Hanson, Mårten Björkman, Atsuto Maki. (2024)  
**Towards Sim-to-Real Industrial Parts Classification with Synthetic Dataset**
<br/>
<button class="copy-to-clipboard" title="Towards Sim-to-Real Industrial Parts Classification with Synthetic Dataset" index=20>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 31  
Keywords: Benchmarking, Benchmarking, Deep Neural Network, Self-supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08778v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08778v1.pdf" filename="2404.08778v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper is about effectively utilizing synthetic data for training <b>deep</b> <b>neural</b> <b>networks</b> for industrial parts classification, in particular, by taking into account the domain gap against real-world images. To this end, we introduce a synthetic dataset that may serve as a preliminary testbed for the Sim-to-Real challenge; it contains 17 objects of six industrial use cases, including isolated and assembled parts. A few subsets of objects exhibit large similarities in shape and albedo for reflecting challenging cases of industrial parts. All the sample images come with and without random backgrounds and post-processing for evaluating the importance of domain randomization. We call it Synthetic Industrial Parts dataset (SIP-17). We study the usefulness of SIP-17 through <b>benchmarking</b> the performance of five state-of-the-art <b>deep</b> <b>network</b> <b>models,</b> <b>supervised</b> and <b>self-supervised,</b> trained only on the synthetic data while testing them on real data. By analyzing the results, we deduce some insights on the feasibility and challenges of using synthetic data for industrial parts classification and for further developing larger-scale synthetic datasets. Our dataset and code are publicly available.

{{</citation>}}


### (21/61 | 21/235) E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors to New Generators Using Limited Data (Aref Azizpour et al., 2024)

{{<citation>}}

Aref Azizpour, Tai D. Nguyen, Manil Shrestha, Kaidi Xu, Edward Kim, Matthew C. Stamm. (2024)  
**E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors to New Generators Using Limited Data**
<br/>
<button class="copy-to-clipboard" title="E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors to New Generators Using Limited Data" index=21>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 30  
Keywords: Continual Learning, Generative AI, Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08814v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08814v2.pdf" filename="2404.08814v2.pdf">Download PDF</button>

---


**ABSTRACT**  
As <b>generative</b> <b>AI</b> progresses rapidly, new synthetic image generators continue to emerge at a swift pace. Traditional detection methods face two main challenges in adapting to these generators: the forensic traces of synthetic images from new techniques can vastly differ from those learned during training, and access to data for these new generators is often limited. To address these issues, we introduce the Ensemble of Expert Embedders (E3), a novel <b>continual</b> <b>learning</b> framework for updating synthetic image detectors. E3 enables the accurate detection of images from newly emerged generators using minimal training data. Our approach does this by first employing <b>transfer</b> <b>learning</b> to develop a suite of expert embedders, each specializing in the forensic traces of a specific generator. Then, all embeddings are jointly analyzed by an Expert Knowledge Fusion Network to produce accurate and reliable detection decisions. Our experiments demonstrate that E3 outperforms existing <b>continual</b> <b>learning</b> methods, including those developed specifically for synthetic image detection.

{{</citation>}}


### (22/61 | 22/235) FashionFail: Addressing Failure Cases in Fashion Object Detection and Segmentation (Riza Velioglu et al., 2024)

{{<citation>}}

Riza Velioglu, Robin Chan, Barbara Hammer. (2024)  
**FashionFail: Addressing Failure Cases in Fashion Object Detection and Segmentation**
<br/>
<button class="copy-to-clipboard" title="FashionFail: Addressing Failure Cases in Fashion Object Detection and Segmentation" index=22>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Object Detection, Data Augmentation, Foundation Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08582v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08582v1.pdf" filename="2404.08582v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In the realm of fashion <b>object</b> <b>detection</b> and segmentation for online shopping images, existing state-of-the-art fashion parsing models encounter limitations, particularly when exposed to non-model-worn apparel and close-up shots. To address these failures, we introduce FashionFail; a new fashion dataset with e-commerce images for <b>object</b> <b>detection</b> and segmentation. The dataset is efficiently curated using our novel annotation tool that leverages recent <b>foundation</b> <b>models.</b> The primary objective of FashionFail is to serve as a test bed for evaluating the robustness of models. Our analysis reveals the shortcomings of leading models, such as Attribute-Mask R-CNN and Fashionformer. Additionally, we propose a baseline approach using naive <b>data</b> <b>augmentation</b> to mitigate common failure cases and improve model robustness. Through this work, we aim to inspire and support further research in fashion item detection and segmentation for industrial applications. The dataset, annotation tool, code, and models are available at \url{https://rizavelioglu.github.io/fashionfail/}.

{{</citation>}}


### (23/61 | 23/235) Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing Clues (Xianhua He et al., 2024)

{{<citation>}}

Xianhua He, Dashuang Liang, Song Yang, Zhanlong Hao, Hui Ma, Binjie Mao, Xi Li, Yao Wang, Pengfei Yan, Ajian Liu. (2024)  
**Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing Clues**
<br/>
<button class="copy-to-clipboard" title="Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing Clues" index=23>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Face Recognition, Data Augmentation, Spoofing  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08450v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08450v1.pdf" filename="2404.08450v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Face</b> <b>recognition</b> systems are frequently subjected to a variety of physical and digital attacks of different types. Previous methods have achieved satisfactory performance in scenarios that address physical attacks and digital attacks, respectively. However, few methods are considered to integrate a model that simultaneously addresses both physical and digital attacks, implying the necessity to develop and maintain multiple models. To jointly detect physical and digital attacks within a single model, we propose an innovative approach that can adapt to any network architecture. Our approach mainly contains two types of <b>data</b> <b>augmentation,</b> which we call Simulated Physical <b>Spoofing</b> Clues augmentation (SPSC) and Simulated Digital <b>Spoofing</b> Clues augmentation (SDSC). SPSC and SDSC augment live samples into simulated attack samples by simulating <b>spoofing</b> clues of physical and digital attacks, respectively, which significantly improve the capability of the model to detect "unseen" attack types. Extensive experiments show that SPSC and SDSC can achieve state-of-the-art generalization in Protocols 2.1 and 2.2 of the UniAttackData dataset, respectively. Our method won first place in "Unified Physical-Digital <b>Face</b> <b>Attack</b> Detection" of the 5th <b>Face</b> <b>Anti-spoofing</b> Challenge@CVPR2024. Our final submission obtains 3.75% APCER, 0.93% BPCER, and 2.34% ACER, respectively. Our code is available at https://github.com/Xianhua-He/cvpr2024-face-anti-spoofing-challenge.

{{</citation>}}


### (24/61 | 24/235) Struggle with Adversarial Defense? Try Diffusion (Yujie Li et al., 2024)

{{<citation>}}

Yujie Li, Yanbin Wang, Haitao xu, Bin Liu, Jianguo Sun, Zhenhao Guo, Wenrui Ma. (2024)  
**Struggle with Adversarial Defense? Try Diffusion**
<br/>
<button class="copy-to-clipboard" title="Struggle with Adversarial Defense? Try Diffusion" index=24>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CR, cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Diffusion Model, Adversarial Learning, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08273v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08273v1.pdf" filename="2404.08273v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Adversarial</b> <b>attacks</b> induce misclassification by introducing subtle perturbations. Recently, <b>diffusion</b> <b>models</b> are applied to the image classifiers to improve <b>adversarial</b> <b>robustness</b> through <b>adversarial</b> <b>training</b> or by purifying <b>adversarial</b> <b>noise.</b> However, <b>diffusion-based</b> <b>adversarial</b> <b>training</b> often encounters convergence challenges and high computational expenses. Additionally, <b>diffusion-based</b> <b>purification</b> inevitably causes data shift and is deemed susceptible to stronger adaptive attacks. To tackle these issues, we propose the Truth Maximization <b>Diffusion</b> <b>Classifier</b> (TMDC), a generative Bayesian classifier that builds upon pre-trained <b>diffusion</b> <b>models</b> and the Bayesian theorem. Unlike data-driven classifiers, TMDC, guided by Bayesian principles, utilizes the conditional likelihood from <b>diffusion</b> <b>models</b> to determine the class probabilities of input images, thereby insulating against the influences of data shift and the limitations of <b>adversarial</b> <b>training.</b> Moreover, to enhance TMDC's resilience against more potent <b>adversarial</b> <b>attacks,</b> we propose an optimization strategy for <b>diffusion</b> <b>classifiers.</b> This strategy involves post-training the <b>diffusion</b> <b>model</b> on perturbed datasets with ground-truth labels as conditions, guiding the <b>diffusion</b> <b>model</b> to learn the data distribution and maximizing the likelihood under the ground-truth labels. The proposed method achieves state-of-the-art performance on the CIFAR10 dataset against heavy white-box attacks and strong adaptive attacks. Specifically, TMDC achieves robust accuracies of 82.81% against $l_{\infty}$ norm-bounded perturbations and 86.05% against $l_{2}$ norm-bounded perturbations, respectively, with $\epsilon=0.05$.

{{</citation>}}


### (25/61 | 25/235) IFViT: Interpretable Fixed-Length Representation for Fingerprint Matching via Vision Transformer (Yuhang Qiu et al., 2024)

{{<citation>}}

Yuhang Qiu, Honghui Chen, Xingbo Dong, Zheng Lin, Iman Yi Liao, Massimo Tistarelli, Zhe Jin. (2024)  
**IFViT: Interpretable Fixed-Length Representation for Fingerprint Matching via Vision Transformer**
<br/>
<button class="copy-to-clipboard" title="IFViT: Interpretable Fixed-Length Representation for Fingerprint Matching via Vision Transformer" index=25>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Vision Transformer, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08237v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08237v1.pdf" filename="2404.08237v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Determining dense feature points on fingerprints used in constructing deep fixed-length representations for accurate matching, particularly at the pixel level, is of significant interest. To explore the interpretability of fingerprint matching, we propose a multi-stage interpretable fingerprint matching network, namely Interpretable Fixed-length Representation for Fingerprint Matching via <b>Vision</b> <b>Transformer</b> (IFViT), which consists of two primary modules. The first module, an interpretable dense registration module, establishes a <b>Vision</b> <b>Transformer</b> (ViT)-based Siamese Network to capture long-range dependencies and the global context in fingerprint pairs. It provides interpretable dense pixel-wise correspondences of feature points for fingerprint alignment and enhances the interpretability in the subsequent matching stage. The second module takes into account both local and global representations of the aligned fingerprint pair to achieve an interpretable fixed-length representation extraction and matching. It employs the ViTs trained in the first module with the additional fully connected layer and retrains them to simultaneously produce the discriminative fixed-length representation and interpretable dense pixel-wise correspondences of feature points. Extensive experimental results on diverse publicly available fingerprint databases demonstrate that the proposed framework not only exhibits superior performance on dense registration and matching but also significantly promotes the interpretability in deep fixed-length representations-based fingerprint matching.

{{</citation>}}


### (26/61 | 26/235) Improving Referring Image Segmentation using Vision-Aware Text Features (Hai Nguyen-Truong et al., 2024)

{{<citation>}}

Hai Nguyen-Truong, E-Ro Nguyen, Tuan-Anh Vu, Minh-Triet Tran, Binh-Son Hua, Sai-Kit Yeung. (2024)  
**Improving Referring Image Segmentation using Vision-Aware Text Features**
<br/>
<button class="copy-to-clipboard" title="Improving Referring Image Segmentation using Vision-Aware Text Features" index=26>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 29  
Keywords: Benchmarking, Multi-modal, Multi-modal, Prompt, Text Embedding  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08590v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08590v1.pdf" filename="2404.08590v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Referring image segmentation is a challenging task that involves generating pixel-wise segmentation masks based on natural language descriptions. Existing methods have relied mostly on visual features to generate the segmentation masks while treating <b>text</b> <b>features</b> as supporting components. This over-reliance on visual features can lead to suboptimal results, especially in complex scenarios where <b>text</b> <b>prompts</b> are ambiguous or context-dependent. To overcome these challenges, we present a novel framework VATEX to improve referring image segmentation by enhancing object and context understanding with Vision-Aware <b>Text</b> <b>Feature.</b> Our method involves using CLIP to derive a CLIP Prior that integrates an object-centric visual heatmap with <b>text</b> <b>description,</b> which can be used as the initial query in DETR-based architecture for the segmentation task. Furthermore, by observing that there are multiple ways to describe an instance in an image, we enforce feature similarity between <b>text</b> <b>variations</b> referring to the same visual input by two components: a novel Contextual <b>Multimodal</b> Decoder that turns <b>text</b> <b>embeddings</b> into vision-aware <b>text</b> <b>features,</b> and a Meaning Consistency Constraint to ensure further the coherent and consistent interpretation of language expressions with the context understanding obtained from the image. Our method achieves a significant performance improvement on three <b>benchmark</b> datasets RefCOCO, RefCOCO+ and G-Ref. Code is available at: https://nero1342.github.io/VATEX\_RIS.

{{</citation>}}


### (27/61 | 27/235) OmniSat: Self-Supervised Modality Fusion for Earth Observation (Guillaume Astruc et al., 2024)

{{<citation>}}

Guillaume Astruc, Nicolas Gonthier, Clement Mallet, Loic Landrieu. (2024)  
**OmniSat: Self-Supervised Modality Fusion for Earth Observation**
<br/>
<button class="copy-to-clipboard" title="OmniSat: Self-Supervised Modality Fusion for Earth Observation" index=27>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 26  
Keywords: Multi-modal, Multi-modal, Self-supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08351v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08351v1.pdf" filename="2404.08351v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The field of Earth Observations (EO) offers a wealth of data from diverse sensors, presenting a great opportunity for advancing <b>self-supervised</b> <b>multimodal</b> learning. However, current <b>multimodal</b> EO datasets and models focus on a single data type, either mono-date images or time series, which limits their expressivity. We introduce OmniSat, a novel architecture that exploits the spatial alignment between multiple EO modalities to learn expressive <b>multimodal</b> representations without labels. To demonstrate the advantages of combining modalities of different natures, we augment two existing datasets with new modalities. As demonstrated on three downstream tasks: forestry, land cover classification, and crop mapping. OmniSat can learn rich representations in an <b>unsupervised</b> manner, leading to improved performance in the semi- and fully-supervised settings, even when only one modality is available for inference. The code and dataset are available at github.com/gastruc/OmniSat.

{{</citation>}}


### (28/61 | 28/235) MSSTNet: A Multi-Scale Spatio-Temporal CNN-Transformer Network for Dynamic Facial Expression Recognition (Linhuang Wang et al., 2024)

{{<citation>}}

Linhuang Wang, Xin Kang, Fei Ding, Satoshi Nakagawa, Fuji Ren. (2024)  
**MSSTNet: A Multi-Scale Spatio-Temporal CNN-Transformer Network for Dynamic Facial Expression Recognition**
<br/>
<button class="copy-to-clipboard" title="MSSTNet: A Multi-Scale Spatio-Temporal CNN-Transformer Network for Dynamic Facial Expression Recognition" index=28>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 25  
Keywords: Convolutional Neural Network, Convolutional Neural Network, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08433v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08433v1.pdf" filename="2404.08433v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Unlike typical video action recognition, Dynamic Facial Expression Recognition (DFER) does not involve distinct moving targets but relies on localized changes in facial muscles. Addressing this distinctive attribute, we propose a Multi-Scale Spatio-temporal <b>CNN-Transformer</b> network (MSSTNet). Our approach takes spatial features of different scales extracted by <b>CNN</b> and feeds them into a Multi-scale Embedding Layer (MELayer). The MELayer extracts multi-scale spatial information and encodes these features before sending them into a Temporal <b>Transformer</b> (T-Former). The T-Former simultaneously extracts temporal information while continually integrating multi-scale spatial information. This process culminates in the generation of multi-scale spatio-temporal features that are utilized for the final classification. Our method achieves state-of-the-art results on two in-the-wild datasets. Furthermore, a series of ablation experiments and visualizations provide further validation of our approach's proficiency in leveraging spatio-temporal information within DFER.

{{</citation>}}


### (29/61 | 29/235) Counterfactual Explanations for Face Forgery Detection via Adversarial Removal of Artifacts (Yang Li et al., 2024)

{{<citation>}}

Yang Li, Songlin Yang, Wei Wang, Ziwen He, Bo Peng, Jing Dong. (2024)  
**Counterfactual Explanations for Face Forgery Detection via Adversarial Removal of Artifacts**
<br/>
<button class="copy-to-clipboard" title="Counterfactual Explanations for Face Forgery Detection via Adversarial Removal of Artifacts" index=29>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 25  
Keywords: Counter-factual, Deep Neural Network, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08341v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08341v1.pdf" filename="2404.08341v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Highly realistic AI generated face forgeries known as deepfakes have raised serious social concerns. Although <b>DNN-based</b> face forgery detection models have achieved good performance, they are vulnerable to latest generative methods that have less forgery traces and <b>adversarial</b> <b>attacks.</b> This limitation of generalization and robustness hinders the credibility of detection results and requires more explanations. In this work, we provide <b>counterfactual</b> explanations for face forgery detection from an artifact removal perspective. Specifically, we first invert the forgery images into the StyleGAN latent space, and then adversarially optimize their latent representations with the discrimination supervision from the target detection model. We verify the effectiveness of the proposed explanations from two aspects: (1) <b>Counterfactual</b> Trace Visualization: the enhanced forgery images are useful to reveal artifacts by visually contrasting the original images and two different visualization methods; (2) Transferable <b>Adversarial</b> <b>Attacks:</b> the <b>adversarial</b> <b>forgery</b> images generated by attacking the detection model are able to mislead other detection models, implying the removed artifacts are general. Extensive experiments demonstrate that our method achieves over 90% attack success rate and superior attack transferability. Compared with naive <b>adversarial</b> <b>noise</b> methods, our method adopts both generative and discriminative model priors, and optimize the latent representations in a synthesis-by-analysis way, which forces the search of <b>counterfactual</b> explanations on the natural face manifold. Thus, more general <b>counterfactual</b> traces can be found and better <b>adversarial</b> <b>attack</b> transferability can be achieved.

{{</citation>}}


### (30/61 | 30/235) On Input Formats for Radar Micro-Doppler Signature Processing by Convolutional Neural Networks (Mikolaj Czerkawski et al., 2024)

{{<citation>}}

Mikolaj Czerkawski, Carmine Clemente, Craig Michie, Christos Tachtatzis. (2024)  
**On Input Formats for Radar Micro-Doppler Signature Processing by Convolutional Neural Networks**
<br/>
<button class="copy-to-clipboard" title="On Input Formats for Radar Micro-Doppler Signature Processing by Convolutional Neural Networks" index=30>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 25  
Keywords: Convolutional Neural Network, Convolution, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08291v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08291v1.pdf" filename="2404.08291v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Convolutional</b> <b>neural</b> <b>networks</b> have often been proposed for processing radar Micro-Doppler signatures, most commonly with the goal of classifying the signals. The majority of works tend to disregard phase information from the complex time-frequency representation. Here, the utility of the phase information, as well as the optimal format of the Doppler-time input for a <b>convolutional</b> <b>neural</b> <b>network,</b> is analysed. It is found that the performance achieved by <b>convolutional</b> <b>neural</b> <b>network</b> classifiers is heavily influenced by the type of input representation, even across formats with equivalent information. Furthermore, it is demonstrated that the phase component of the Doppler-time representation contains rich information useful for classification and that unwrapping the phase in the temporal dimension can improve the results compared to a magnitude-only solution, improving accuracy from 0.920 to 0.938 on the tested human activity dataset. Further improvement of 0.947 is achieved by training a linear classifier on embeddings from multiple-formats.

{{</citation>}}


### (31/61 | 31/235) Practical Region-level Attack against Segment Anything Models (Yifan Shen et al., 2024)

{{<citation>}}

Yifan Shen, Zhengyuan Li, Gang Wang. (2024)  
**Practical Region-level Attack against Segment Anything Models**
<br/>
<button class="copy-to-clipboard" title="Practical Region-level Attack against Segment Anything Models" index=31>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CR, cs-CV, cs.CV  
Keyword Score: 25  
Keywords: Black Box, Prompt, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08255v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08255v1.pdf" filename="2404.08255v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Segment Anything Models (SAM) have made significant advancements in image segmentation, allowing users to segment target portions of an image with a single click (i.e., user <b>prompt).</b> Given its broad applications, the robustness of SAM against <b>adversarial</b> <b>attacks</b> is a critical concern. While recent works have explored <b>adversarial</b> <b>attacks</b> against a pre-defined prompt/click, their threat model is not yet realistic: (1) they often assume the user-click position is known to the attacker (point-based attack), and (2) they often operate under a white-box setting with limited transferability. In this paper, we propose a more practical region-level attack where attackers do not need to know the precise user <b>prompt.</b> The attack remains effective as the user clicks on any point on the target object in the image, hiding the object from SAM. Also, by adapting a spectrum transformation method, we make the attack more transferable under a <b>black-box</b> <b>setting.</b> Both control experiments and testing against real-world SAM services confirm its effectiveness.

{{</citation>}}


### (32/61 | 32/235) Into the Fog: Evaluating Multiple Object Tracking Robustness (Nadezda Kirillova et al., 2024)

{{<citation>}}

Nadezda Kirillova, M. Jehanzeb Mirza, Horst Possegger, Horst Bischof. (2024)  
**Into the Fog: Evaluating Multiple Object Tracking Robustness**
<br/>
<button class="copy-to-clipboard" title="Into the Fog: Evaluating Multiple Object Tracking Robustness" index=32>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.10534v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.10534v1.pdf" filename="2404.10534v1.pdf">Download PDF</button>

---


**ABSTRACT**  
State-of-the-art (SOTA) trackers have shown remarkable Multiple Object Tracking (MOT) performance when trained and evaluated on current <b>benchmarks.</b> However, these <b>benchmarks</b> primarily consist of clear scenarios, overlooking adverse atmospheric conditions such as fog, haze, smoke and dust. As a result, the robustness of SOTA trackers remains underexplored. To address these limitations, we propose a pipeline for physic-based volumetric fog <b>simulation</b> in arbitrary real-world MOT dataset utilizing frame-by-frame monocular depth estimation and a fog formation optical model. Moreover, we enhance our <b>simulation</b> by rendering of both homogeneous and heterogeneous fog effects. We propose to use the dark channel prior method to estimate fog (smoke) color, which shows promising results even in night and indoor scenes. We present the leading tracking <b>benchmark</b> MOTChallenge (MOT17 dataset) overlaid by fog (smoke for indoor scenes) of various intensity levels and conduct a comprehensive evaluation of SOTA MOT methods, revealing their limitations under fog and fog-similar challenges.

{{</citation>}}


### (33/61 | 33/235) On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation (Agneet Chatterjee et al., 2024)

{{<citation>}}

Agneet Chatterjee, Tejas Gokhale, Chitta Baral, Yezhou Yang. (2024)  
**On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation**
<br/>
<button class="copy-to-clipboard" title="On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation" index=33>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 23  
Keywords: Benchmarking, Distribution Shift, Distribution Shift, Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08540v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08540v1.pdf" filename="2404.08540v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advances in monocular depth estimation have been made by incorporating natural language as additional guidance. Although yielding impressive results, the impact of the language prior, particularly in terms of generalization and robustness, remains unexplored. In this paper, we address this gap by quantifying the impact of this prior and introduce methods to <b>benchmark</b> its effectiveness across various settings. We generate "low-level" sentences that convey object-centric, three-dimensional spatial relationships, incorporate them as additional language priors and evaluate their downstream impact on depth estimation. Our key finding is that current language-guided depth estimators perform optimally only with scene-level descriptions and counter-intuitively fare worse with low level descriptions. Despite leveraging additional data, these methods are not robust to directed <b>adversarial</b> <b>attacks</b> and decline in performance with an increase in <b>distribution</b> <b>shift.</b> Finally, to provide a foundation for future research, we identify points of failures and offer insights to better understand these shortcomings. With an increasing number of methods using language for depth estimation, our findings highlight the opportunities and pitfalls that require careful consideration for effective deployment in real-world settings

{{</citation>}}


### (34/61 | 34/235) Calibration & Reconstruction: Deep Integrated Language for Referring Image Segmentation (Yichen Yan et al., 2024)

{{<citation>}}

Yichen Yan, Xingjian He, Sihan Chen, Jing Liu. (2024)  
**Calibration & Reconstruction: Deep Integrated Language for Referring Image Segmentation**
<br/>
<button class="copy-to-clipboard" title="Calibration & Reconstruction: Deep Integrated Language for Referring Image Segmentation" index=34>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-MM, cs.CV  
Keyword Score: 23  
Keywords: Multi-modal, Reconstruction Loss, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08281v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08281v1.pdf" filename="2404.08281v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Referring image segmentation aims to segment an object referred to by natural language expression from an image. The primary challenge lies in the efficient propagation of fine-grained semantic information from textual features to visual features. Many recent works utilize a <b>Transformer</b> to address this challenge. However, conventional <b>transformer</b> decoders can distort linguistic information with deeper layers, leading to suboptimal results. In this paper, we introduce CRFormer, a model that iteratively calibrates <b>multi-modal</b> features in the <b>transformer</b> decoder. We start by generating language queries using vision features, emphasizing different aspects of the input language. Then, we propose a novel Calibration Decoder (CDec) wherein the <b>multi-modal</b> features can iteratively calibrated by the input language features. In the Calibration Decoder, we use the output of each decoder layer and the original language features to generate new queries for continuous calibration, which gradually updates the language features. Based on CDec, we introduce a Language <b>Reconstruction</b> <b>Module</b> and a <b>reconstruction</b> <b>loss.</b> This module leverages queries from the final layer of the decoder to reconstruct the input language and compute the <b>reconstruction</b> <b>loss.</b> This can further prevent the language information from being lost or distorted. Our experiments consistently show the superior performance of our approach across RefCOCO, RefCOCO+, and G-Ref datasets compared to state-of-the-art methods.

{{</citation>}}


### (35/61 | 35/235) Semantic Approach to Quantifying the Consistency of Diffusion Model Image Generation (Brinnae Bent, 2024)

{{<citation>}}

Brinnae Bent. (2024)  
**Semantic Approach to Quantifying the Consistency of Diffusion Model Image Generation**
<br/>
<button class="copy-to-clipboard" title="Semantic Approach to Quantifying the Consistency of Diffusion Model Image Generation" index=35>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-HC, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Diffusion Model, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08799v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08799v1.pdf" filename="2404.08799v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this study, we identify the need for an interpretable, quantitative score of the repeatability, or consistency, of image generation in <b>diffusion</b> <b>models.</b> We propose a semantic approach, using a pairwise mean CLIP (Contrastive Language-Image Pretraining) score as our semantic consistency score. We applied this metric to compare two state-of-the-art open-source image generation <b>diffusion</b> <b>models,</b> Stable <b>Diffusion</b> <b>XL</b> and PixArt-{\alpha}, and we found statistically significant differences between the semantic consistency scores for the models. Agreement between the Semantic Consistency Score selected model and aggregated human annotations was 94%. We also explored the consistency of SDXL and a LoRA-fine-tuned version of SDXL and found that the <b>fine-tuned</b> model had significantly higher semantic consistency in generated images. The Semantic Consistency Score proposed here offers a measure of image generation alignment, facilitating the evaluation of model architectures for specific tasks and aiding in informed decision-making regarding model selection.

{{</citation>}}


### (36/61 | 36/235) Detecting AI-Generated Images via CLIP (A. G. Moskowitz et al., 2024)

{{<citation>}}

A. G. Moskowitz, T. Gaona, J. Peterson. (2024)  
**Detecting AI-Generated Images via CLIP**
<br/>
<button class="copy-to-clipboard" title="Detecting AI-Generated Images via CLIP" index=36>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Fine-tuning, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08788v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08788v1.pdf" filename="2404.08788v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As AI-generated image (AIGI) methods become more powerful and accessible, it has become a critical task to determine if an image is real or AI-generated. Because AIGI lack the signatures of photographs and have their own unique patterns, new models are needed to determine if an image is AI-generated. In this paper, we investigate the ability of the Contrastive Language-Image Pre-training (CLIP) architecture, pre-trained on massive internet-scale data sets, to perform this differentiation. We <b>fine-tune</b> CLIP on real images and AIGI from several generative models, enabling CLIP to determine if an image is AI-generated and, if so, determine what generation method was used to create it. We show that the <b>fine-tuned</b> CLIP architecture is able to differentiate AIGI as well or better than models whose architecture is specifically designed to detect AIGI. Our method will significantly increase access to AIGI-detecting tools and reduce the negative effects of AIGI on society, as our CLIP <b>fine-tuning</b> procedures require no architecture changes from publicly available model repositories and consume significantly less GPU resources than other AIGI detection models.

{{</citation>}}


### (37/61 | 37/235) Probing the 3D Awareness of Visual Foundation Models (Mohamed El Banani et al., 2024)

{{<citation>}}

Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, Varun Jampani. (2024)  
**Probing the 3D Awareness of Visual Foundation Models**
<br/>
<button class="copy-to-clipboard" title="Probing the 3D Awareness of Visual Foundation Models" index=37>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Foundation Model, Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08636v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08636v1.pdf" filename="2404.08636v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advances in large-scale pretraining have yielded visual <b>foundation</b> <b>models</b> with strong capabilities. Not only can recent models generalize to arbitrary images for their training task, their intermediate representations are useful for other visual tasks such as detection and segmentation. Given that such models can classify, delineate, and localize objects in 2D, we ask whether they also represent their 3D structure? In this work, we analyze the 3D awareness of visual <b>foundation</b> <b>models.</b> We posit that 3D awareness implies that representations (1) encode the 3D structure of the scene and (2) consistently represent the surface across views. We conduct a series of experiments using task-specific probes and <b>zero-shot</b> inference procedures on frozen features. Our experiments reveal several limitations of the current models. Our code and analysis can be found at https://github.com/mbanani/probe3d.

{{</citation>}}


### (38/61 | 38/235) Scalability in Building Component Data Annotation: Enhancing Facade Material Classification with Synthetic Data (Josie Harrison et al., 2024)

{{<citation>}}

Josie Harrison, Alexander Hollberg, Yinan Yu. (2024)  
**Scalability in Building Component Data Annotation: Enhancing Facade Material Classification with Synthetic Data**
<br/>
<button class="copy-to-clipboard" title="Scalability in Building Component Data Annotation: Enhancing Facade Material Classification with Synthetic Data" index=38>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Fine-tuning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08557v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08557v1.pdf" filename="2404.08557v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Computer vision models trained on Google Street View images can create material cadastres. However, current approaches need manually annotated datasets that are difficult to obtain and often have class imbalance. To address these challenges, this paper <b>fine-tuned</b> a Swin <b>Transformer</b> model on a synthetic dataset generated with DALL-E and compared the performance to a similar manually annotated dataset. Although manual annotation remains the gold standard, the synthetic dataset performance demonstrates a reasonable alternative. The findings will ease annotation needed to develop material cadastres, offering architects insights into opportunities for material reuse, thus contributing to the reduction of demolition waste.

{{</citation>}}


### (39/61 | 39/235) LaSagnA: Language-based Segmentation Assistant for Complex Queries (Cong Wei et al., 2024)

{{<citation>}}

Cong Wei, Haoxian Tan, Yujie Zhong, Yujiu Yang, Lin Ma. (2024)  
**LaSagnA: Language-based Segmentation Assistant for Complex Queries**
<br/>
<button class="copy-to-clipboard" title="LaSagnA: Language-based Segmentation Assistant for Complex Queries" index=39>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Reasoning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08506v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08506v1.pdf" filename="2404.08506v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent advancements have empowered <b>Large</b> <b>Language</b> <b>Models</b> for Vision (vLLMs) to generate detailed perceptual outcomes, including bounding boxes and masks. Nonetheless, there are two constraints that restrict the further application of these vLLMs: the incapability of handling multiple targets per query and the failure to identify the absence of query objects in the image. In this study, we acknowledge that the main cause of these problems is the insufficient complexity of training queries. Consequently, we define the general sequence format for complex queries. Then we incorporate a semantic segmentation task in the current pipeline to fulfill the requirements of training data. Furthermore, we present three novel strategies to effectively handle the challenges arising from the direct integration of the proposed format. The effectiveness of our model in processing complex queries is validated by the comparable results with conventional methods on both close-set and open-set semantic segmentation datasets. Additionally, we outperform a series of vLLMs in <b>reasoning</b> and referring segmentation, showcasing our model's remarkable capabilities. We release the code at https://github.com/congvvc/LaSagnA.

{{</citation>}}


### (40/61 | 40/235) Mitigating Challenges of the Space Environment for Onboard Artificial Intelligence: Design Overview of the Imaging Payload on SpIRIT (Miguel Ortiz del Castillo et al., 2024)

{{<citation>}}

Miguel Ortiz del Castillo, Jonathan Morgan, Jack McRobbie, Clint Therakam, Zaher Joukhadar, Robert Mearns, Simon Barraclough, Richard Sinnott, Andrew Woods, Chris Bayliss, Kris Ehinger, Ben Rubinstein, James Bailey, Airlie Chapman, Michele Trenti. (2024)  
**Mitigating Challenges of the Space Environment for Onboard Artificial Intelligence: Design Overview of the Imaging Payload on SpIRIT**
<br/>
<button class="copy-to-clipboard" title="Mitigating Challenges of the Space Environment for Onboard Artificial Intelligence: Design Overview of the Imaging Payload on SpIRIT" index=40>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Distribution Shift, Distribution Shift, Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08399v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08399v1.pdf" filename="2404.08399v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Artificial intelligence (AI) and autonomous edge computing in space are emerging areas of interest to augment capabilities of nanosatellites, where modern sensors generate orders of magnitude more data than can typically be transmitted to mission control. Here, we present the hardware and software design of an onboard AI subsystem hosted on SpIRIT. The system is optimised for on-board computer vision experiments based on visible light and long wave infrared cameras. This paper highlights the key design choices made to maximise the robustness of the system in harsh space conditions, and their motivation relative to key mission requirements, such as limited compute resources, resilience to cosmic radiation, extreme temperature variations, <b>distribution</b> <b>shifts,</b> and very low transmission bandwidths. The payload, called Loris, consists of six visible light cameras, three infrared cameras, a camera control board and a Graphics Processing Unit (GPU) system-on-module. Loris enables the execution of AI models with on-orbit <b>fine-tuning</b> as well as a next-generation image compression algorithm, including progressive coding. This innovative approach not only enhances the data processing capabilities of nanosatellites but also lays the groundwork for broader applications to remote sensing from space.

{{</citation>}}


### (41/61 | 41/235) A Survey of Neural Network Robustness Assessment in Image Recognition (Jie Wang et al., 2024)

{{<citation>}}

Jie Wang, Jun Ai, Minyan Lu, Haoran Su, Dan Yu, Yutao Zhang, Junda Zhu, Jingyu Liu. (2024)  
**A Survey of Neural Network Robustness Assessment in Image Recognition**
<br/>
<button class="copy-to-clipboard" title="A Survey of Neural Network Robustness Assessment in Image Recognition" index=41>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-SY, cs.CV, eess-SY  
Keyword Score: 20  
Keywords: Augmented Reality (AR), Adversarial Attack  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08285v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08285v2.pdf" filename="2404.08285v2.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, there has been significant attention given to the robustness assessment of neural networks. Robustness plays a critical role in ensuring reliable operation of artificial intelligence (AI) systems in complex and uncertain environments. Deep learning's robustness problem is particularly significant, highlighted by the discovery of <b>adversarial</b> <b>attacks</b> on image classification models. Researchers have dedicated efforts to evaluate robustness in diverse perturbation conditions for image recognition tasks. Robustness assessment encompasses two main techniques: robustness verification/ certification for deliberate <b>adversarial</b> <b>attacks</b> and robustness testing for random data corruptions. In this survey, we present a detailed examination of both <b>adversarial</b> <b>robustness</b> <b>(AR)</b> and corruption robustness (CR) in neural network assessment. Analyzing current research papers and standards, we provide an extensive overview of robustness assessment in image recognition. Three essential aspects are analyzed: concepts, metrics, and assessment methods. We investigate the perturbation metrics and range representations used to measure the degree of perturbations on images, as well as the robustness metrics specifically for the robustness conditions of classification models. The strengths and limitations of the existing methods are also discussed, and some potential directions for future research are provided.

{{</citation>}}


### (42/61 | 42/235) Tackling Ambiguity from Perspective of Uncertainty Inference and Affinity Diversification for Weakly Supervised Semantic Segmentation (Zhiwei Yang et al., 2024)

{{<citation>}}

Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song. (2024)  
**Tackling Ambiguity from Perspective of Uncertainty Inference and Affinity Diversification for Weakly Supervised Semantic Segmentation**
<br/>
<button class="copy-to-clipboard" title="Tackling Ambiguity from Perspective of Uncertainty Inference and Affinity Diversification for Weakly Supervised Semantic Segmentation" index=42>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Supervised Learning, Weakly-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08195v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08195v1.pdf" filename="2404.08195v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Weakly <b>supervised</b> semantic segmentation (WSSS) with image-level labels intends to achieve dense tasks without laborious annotations. However, due to the ambiguous contexts and fuzzy regions, the performance of WSSS, especially the stages of generating Class Activation Maps (CAMs) and refining pseudo masks, widely suffers from ambiguity while being barely noticed by previous literature. In this work, we propose UniA, a unified single-staged WSSS framework, to efficiently tackle this issue from the perspective of uncertainty inference and affinity diversification, respectively. When activating class objects, we argue that the false activation stems from the bias to the ambiguous regions during the feature extraction. Therefore, we design a more robust feature representation with a probabilistic Gaussian distribution and introduce the uncertainty estimation to avoid the bias. A distribution loss is particularly proposed to supervise the process, which effectively captures the ambiguity and models the complex dependencies among features. When refining pseudo labels, we observe that the affinity from the prevailing refinement methods intends to be similar among ambiguities. To this end, an affinity diversification module is proposed to promote diversity among semantics. A mutual complementing refinement is proposed to initially rectify the ambiguous affinity with multiple inferred pseudo labels. More importantly, a contrastive affinity loss is further designed to diversify the relations among unrelated semantics, which reliably propagates the diversity into the whole feature representations and helps generate better pseudo masks. Extensive experiments are conducted on PASCAL VOC, MS COCO, and medical ACDC datasets, which validate the efficiency of UniA tackling ambiguity and the superiority over recent single-staged or even most multi-staged competitors.

{{</citation>}}


### (43/61 | 43/235) GPN: Generative Point-based NeRF (Haipeng Wang, 2024)

{{<citation>}}

Haipeng Wang. (2024)  
**GPN: Generative Point-based NeRF**
<br/>
<button class="copy-to-clipboard" title="GPN: Generative Point-based NeRF" index=43>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-GR, cs.CV  
Keyword Score: 15  
Keywords: Fine-tuning, Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08312v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08312v1.pdf" filename="2404.08312v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Scanning real-life scenes with modern registration devices typically gives incomplete point cloud representations, primarily due to the limitations of partial scanning, 3D occlusions, and dynamic light conditions. Recent works on processing incomplete point clouds have always focused on point cloud completion. However, these approaches do not ensure consistency between the completed point cloud and the captured images regarding color and <b>geometry.</b> We propose using Generative Point-based NeRF (GPN) to reconstruct and repair a partial cloud by fully utilizing the scanning images and the corresponding reconstructed cloud. The repaired point cloud can achieve multi-view consistency with the captured images at high spatial resolution. For the <b>finetunes</b> of a single scene, we optimize the global latent condition by incorporating an Auto-Decoder architecture while retaining multi-view consistency. As a result, the generated point clouds are smooth, plausible, and geometrically consistent with the partial scanning images. Extensive experiments on ShapeNet demonstrate that our works achieve competitive performances to the other state-of-the-art point cloud-based neural scene rendering and editing performances.

{{</citation>}}


### (44/61 | 44/235) Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies (Zichao Li et al., 2024)

{{<citation>}}

Zichao Li, Cihang Xie, Ekin Dogus Cubuk. (2024)  
**Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies**
<br/>
<button class="copy-to-clipboard" title="Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies" index=44>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 15  
Keywords: Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08197v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08197v2.pdf" filename="2404.08197v2.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper investigates the performance of the Contrastive Language-Image Pre-training (CLIP) when scaled down to limited computation budgets. We explore CLIP along three dimensions: data, architecture, and training strategies. With regards to data, we demonstrate the significance of high-quality training data and show that a smaller dataset of high-quality data can outperform a larger dataset with lower quality. We also examine how model performance varies with different dataset sizes, suggesting that smaller ViT models are better suited for smaller datasets, while larger models perform better on larger datasets with fixed compute. Additionally, we provide guidance on when to choose a <b>CNN-based</b> architecture or a ViT-based architecture for CLIP training. We compare four CLIP training strategies - SLIP, FLIP, CLIP, and CLIP+Data Augmentation - and show that the choice of training strategy depends on the available compute resource. Our analysis reveals that CLIP+Data Augmentation can achieve comparable performance to CLIP using only half of the training data. This work provides practical insights into how to effectively train and deploy CLIP models, making them more accessible and affordable for practical use in various applications.

{{</citation>}}


### (45/61 | 45/235) Data Limitations for Modeling Top-Down Effects on Drivers' Attention (Iuliia Kotseruba et al., 2024)

{{<citation>}}

Iuliia Kotseruba, John K. Tsotsos. (2024)  
**Data Limitations for Modeling Top-Down Effects on Drivers' Attention**
<br/>
<button class="copy-to-clipboard" title="Data Limitations for Modeling Top-Down Effects on Drivers' Attention" index=45>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Benchmarking, Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08749v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08749v1.pdf" filename="2404.08749v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Driving is a visuomotor task, i.e., there is a connection between what drivers see and what they do. While some models of drivers' gaze account for top-down effects of drivers' actions, the majority learn only bottom-up correlations between human gaze and driving footage. The crux of the problem is lack of public data with annotations that could be used to train top-down models and evaluate how well models of any kind capture effects of task on attention. As a result, top-down models are trained and evaluated on private data and public <b>benchmarks</b> measure only the overall fit to human data. In this paper, we focus on data limitations by examining four large-scale public datasets, DR(eye)VE, BDD-A, MAAD, and LBW, used to train and evaluate algorithms for drivers' gaze prediction. We define a set of driving tasks (lateral and longitudinal maneuvers) and context elements (intersections and right-of-way) known to affect drivers' attention, augment the datasets with annotations based on the said definitions, and analyze the characteristics of data recording and processing pipelines w.r.t. capturing what the drivers see and do. In sum, the contributions of this work are: 1) quantifying biases of the public datasets, 2) examining performance of the SOTA bottom-up models on subsets of the data involving non-trivial drivers' actions, 3) linking shortcomings of the bottom-up models to data limitations, and 4) <b>recommendations</b> for future data collection and processing. The new annotations and code for reproducing the results is available at https://github.com/ykotseruba/SCOUT.

{{</citation>}}


### (46/61 | 46/235) Training-free Boost for Open-Vocabulary Object Detection with Confidence Aggregation (Yanhao Zheng et al., 2024)

{{<citation>}}

Yanhao Zheng, Kai Liu. (2024)  
**Training-free Boost for Open-Vocabulary Object Detection with Confidence Aggregation**
<br/>
<button class="copy-to-clipboard" title="Training-free Boost for Open-Vocabulary Object Detection with Confidence Aggregation" index=46>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Object Detection, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08603v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08603v1.pdf" filename="2404.08603v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Open-vocabulary <b>object</b> <b>detection</b> (OVOD) aims at localizing and recognizing visual <b>objects</b> <b>from</b> novel classes unseen at the training time. Whereas, empirical studies reveal that advanced detectors generally assign lower scores to those novel instances, which are inadvertently suppressed during inference by commonly adopted greedy strategies like Non-Maximum Suppression (NMS), leading to sub-optimal detection performance for novel classes. This paper systematically investigates this problem with the commonly-adopted two-stage OVOD paradigm. Specifically, in the region-proposal stage, proposals that contain novel instances showcase lower objectness scores, since they are treated as background proposals during the training phase. Meanwhile, in the <b>object-classification</b> <b>stage,</b> novel <b>objects</b> <b>share</b> lower region-text similarities (i.e., classification scores) due to the biased visual-language alignment by seen training samples. To alleviate this problem, this paper introduces two advanced measures to adjust confidence scores and conserve erroneously dismissed <b>objects:</b> <b>(1)</b> a class-agnostic localization quality estimate via overlap degree of region/object proposals, and (2) a text-guided visual similarity estimate with proxy prototypes for novel classes. Integrated with adjusting techniques specifically designed for the region-proposal and <b>object-classification</b> <b>stages,</b> this paper derives the aggregated confidence estimate for the open-vocabulary <b>object</b> <b>detection</b> paradigm (AggDet). Our AggDet is a generic and training-free post-processing scheme, which consistently bolsters open-vocabulary detectors across model scales and architecture designs. For instance, AggDet receives 3.3% and 1.5% gains on OV-COCO and OV-LVIS <b>benchmarks</b> respectively, without any training cost.

{{</citation>}}


### (47/61 | 47/235) Let It Flow: Simultaneous Optimization of 3D Flow and Object Clustering (Patrik Vacek et al., 2024)

{{<citation>}}

Patrik Vacek, David Hurych, Tomáš Svoboda, Karel Zimmermann. (2024)  
**Let It Flow: Simultaneous Optimization of 3D Flow and Object Clustering**
<br/>
<button class="copy-to-clipboard" title="Let It Flow: Simultaneous Optimization of 3D Flow and Object Clustering" index=47>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 13  
Keywords: Clustering, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08363v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08363v1.pdf" filename="2404.08363v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We study the problem of <b>self-supervised</b> 3D scene flow estimation from real large-scale raw point cloud sequences, which is crucial to various tasks like trajectory prediction or instance segmentation. In the absence of ground truth scene flow labels, contemporary approaches concentrate on deducing optimizing flow across sequential pairs of point clouds by incorporating structure based regularization on flow and object rigidity. The rigid objects are estimated by a variety of 3D spatial <b>clustering</b> methods. While state-of-the-art methods successfully capture overall scene motion using the Neural Prior structure, they encounter challenges in discerning multi-object motions. We identified the structural constraints and the use of large and strict rigid clusters as the main pitfall of the current approaches and we propose a novel <b>clustering</b> approach that allows for combination of overlapping soft clusters as well as non-overlapping rigid clusters representation. Flow is then jointly estimated with progressively growing non-overlapping rigid clusters together with fixed size overlapping soft clusters. We evaluate our method on multiple datasets with LiDAR point clouds, demonstrating the superior performance over the <b>self-supervised</b> baselines reaching new state of the art results. Our method especially excels in resolving flow in complicated dynamic scenes with multiple independently moving objects close to each other which includes pedestrians, cyclists and other vulnerable road users. Our codes will be publicly available.

{{</citation>}}


### (48/61 | 48/235) Learning to Rebalance Multi-Modal Optimization by Adaptively Masking Subnetworks (Yang Yang et al., 2024)

{{<citation>}}

Yang Yang, Hongpeng Pan, Qing-Yuan Jiang, Yi Xu, Jinghui Tang. (2024)  
**Learning to Rebalance Multi-Modal Optimization by Adaptively Masking Subnetworks**
<br/>
<button class="copy-to-clipboard" title="Learning to Rebalance Multi-Modal Optimization by Adaptively Masking Subnetworks" index=48>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 13  
Keywords: Multi-modal, Mutual Information  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08347v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08347v1.pdf" filename="2404.08347v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Multi-modal</b> learning aims to enhance performance by unifying models from various modalities but often faces the "modality imbalance" problem in real data, leading to a bias towards dominant modalities and neglecting others, thereby limiting its overall effectiveness. To address this challenge, the core idea is to balance the optimization of each modality to achieve a joint optimum. Existing approaches often employ a modal-level control mechanism for adjusting the update of each modal parameter. However, such a global-wise updating mechanism ignores the different importance of each parameter. Inspired by subnetwork optimization, we explore a uniform sampling-based optimization strategy and find it more effective than global-wise updating. According to the findings, we further propose a novel importance sampling-based, element-wise joint optimization method, called Adaptively Mask Subnetworks Considering Modal Significance(AMSS). Specifically, we incorporate <b>mutual</b> <b>information</b> rates to determine the modal significance and employ non-uniform adaptive sampling to select foreground subnetworks from each modality for parameter updates, thereby rebalancing <b>multi-modal</b> learning. Additionally, we demonstrate the reliability of the AMSS strategy through convergence analysis. Building upon theoretical insights, we further enhance the <b>multi-modal</b> mask subnetwork strategy using unbiased estimation, referred to as AMSS+. Extensive experiments reveal the superiority of our approach over comparison methods.

{{</citation>}}


### (49/61 | 49/235) Analyzing Decades-Long Environmental Changes in Namibia Using Archival Aerial Photography and Deep Learning (Girmaw Abebe Tadesse et al., 2024)

{{<citation>}}

Girmaw Abebe Tadesse, Caleb Robinson, Gilles Quentin Hacheme, Akram Zaytar, Rahul Dodhia, Tsering Wangyal Shawa, Juan M. Lavista Ferres, Emmanuel H. Kreike. (2024)  
**Analyzing Decades-Long Environmental Changes in Namibia Using Archival Aerial Photography and Deep Learning**
<br/>
<button class="copy-to-clipboard" title="Analyzing Decades-Long Environmental Changes in Namibia Using Archival Aerial Photography and Deep Learning" index=49>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08544v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08544v1.pdf" filename="2404.08544v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This study explores <b>object</b> <b>detection</b> in historical aerial photographs of Namibia to identify long-term environmental changes. Specifically, we aim to identify key <b>objects</b> <b>--</b> \textit{Waterholes}, \textit{Omuti homesteads}, and \textit{Big trees} -- around Oshikango in Namibia using sub-meter gray-scale aerial imagery from 1943 and 1972. In this work, we propose a workflow for analyzing historical aerial imagery using a deep semantic segmentation model on sparse hand-labels. To this end, we employ a number of strategies including class-weighting, pseudo-labeling and empirical p-value-based filtering to balance skewed and sparse representations of <b>objects</b> <b>in</b> the ground truth data. Results demonstrate the benefits of these different training strategies resulting in an average $F_1=0.661$ and $F_1=0.755$ over the three <b>objects</b> <b>of</b> interest for the 1943 and 1972 imagery, respectively. We also identified that the average size of Waterhole and Big trees increased while the average size of Omutis decreased between 1943 and 1972 reflecting some of the local effects of the massive post-Second World War economic, agricultural, demographic, and environmental changes. This work also highlights the untapped potential of historical aerial photographs in understanding long-term environmental changes beyond Namibia (and Africa). With the lack of adequate satellite technology in the past, archival aerial photography offers a great alternative to uncover decades-long environmental changes.

{{</citation>}}


### (50/61 | 50/235) 3D Human Scan With A Moving Event Camera (Kai Kohyama et al., 2024)

{{<citation>}}

Kai Kohyama, Shintaro Shiba, Yoshimitsu Aoki. (2024)  
**3D Human Scan With A Moving Event Camera**
<br/>
<button class="copy-to-clipboard" title="3D Human Scan With A Moving Event Camera" index=50>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Virtual Reality (VR)  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08504v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08504v2.pdf" filename="2404.08504v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Capturing a 3D human body is one of the important tasks in computer vision with a wide range of applications such as <b>virtual</b> <b>reality</b> and sports analysis. However, conventional frame cameras are limited by their temporal resolution and dynamic range, which imposes constraints in real-world application setups. Event cameras have the advantages of high temporal resolution and high dynamic range (HDR), but the development of event-based methods is necessary to handle data with different characteristics. This paper proposes a novel event-based method for 3D pose estimation and human mesh recovery. Prior work on event-based human mesh recovery require frames (images) as well as event data. The proposed method solely relies on events; it carves 3D voxels by moving the event camera around a stationary body, reconstructs the human pose and mesh by attenuated rays, and fit statistical body models, preserving high-frequency details. The experimental results show that the proposed method outperforms conventional frame-based methods in the estimation accuracy of both pose and body mesh. We also demonstrate results in challenging situations where a conventional camera has motion blur. This is the first to demonstrate event-only human mesh recovery, and we hope that it is the first step toward achieving robust and accurate 3D human body scanning from vision sensors. https://florpeng.github.io/event-based-human-scan/

{{</citation>}}


### (51/61 | 51/235) New Efficient Visual OILU Markers (Youssef Chahir et al., 2024)

{{<citation>}}

Youssef Chahir, Messaoud Mostefai, Hamza Saida. (2024)  
**New Efficient Visual OILU Markers**
<br/>
<button class="copy-to-clipboard" title="New Efficient Visual OILU Markers" index=51>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Augmented Reality (AR)  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08477v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08477v1.pdf" filename="2404.08477v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Basic patterns are the source of a wide range of more or less complex geometric structures. We will exploit such patterns to develop new efficient visual markers. Besides being projective invariants, the proposed markers allow producing rich panel of unique identifiers, highly required for resource-intensive navigation and <b>augmented</b> <b>reality</b> applications. The spiral topology of our markers permits the validation of an accurate identification scheme, which is based on level set methods. The robustness of the markers against acquisition and geometric distortions is validated by extensive experimental tests.

{{</citation>}}


### (52/61 | 52/235) OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering (Jingrui Ye et al., 2024)

{{<citation>}}

Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu. (2024)  
**OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering**
<br/>
<button class="copy-to-clipboard" title="OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering" index=52>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Virtual Reality (VR)  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08449v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08449v2.pdf" filename="2404.08449v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Rendering dynamic 3D human from monocular videos is crucial for various applications such as <b>virtual</b> <b>reality</b> and digital entertainment. Most methods assume the people is in an unobstructed scene, while various objects may cause the occlusion of body parts in real-life scenarios. Previous method utilizing NeRF for surface rendering to recover the occluded areas, but it requiring more than one day to train and several seconds to render, failing to meet the requirements of real-time interactive applications. To address these issues, we propose OccGaussian based on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D Gaussian distributions in the canonical space, and we perform occlusion feature query at occluded regions, the aggregated pixel-align feature is extracted to compensate for the missing information. Then we use Gaussian Feature MLP to further process the feature along with the occlusion-aware loss functions to better perceive the occluded area. Extensive experiments both in simulated and real-world occlusions, demonstrate that our method achieves comparable or even superior performance compared to the state-of-the-art method. And we improving training and inference speeds by 250x and 800x, respectively. Our code will be available for research purposes.

{{</citation>}}


### (53/61 | 53/235) Adapting the Segment Anything Model During Usage in Novel Situations (Robin Schön et al., 2024)

{{<citation>}}

Robin Schön, Julian Lorenz, Katja Ludwig, Rainer Lienhart. (2024)  
**Adapting the Segment Anything Model During Usage in Novel Situations**
<br/>
<button class="copy-to-clipboard" title="Adapting the Segment Anything Model During Usage in Novel Situations" index=53>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Foundation Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08421v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08421v1.pdf" filename="2404.08421v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The interactive segmentation task consists in the creation of object segmentation masks based on user interactions. The most common way to guide a model towards producing a correct segmentation consists in clicks on the object and background. The recently published Segment Anything Model (SAM) supports a generalized version of the interactive segmentation problem and has been trained on an object segmentation dataset which contains 1.1B masks. Though being trained extensively and with the explicit purpose of serving as a <b>foundation</b> <b>model,</b> we show significant limitations of SAM when being applied for interactive segmentation on novel domains or object types. On the used datasets, SAM displays a failure rate $\text{FR}_{30}@90$ of up to $72.6 \%$. Since we still want such <b>foundation</b> <b>models</b> to be immediately applicable, we present a framework that can adapt SAM during immediate usage. For this we will leverage the user interactions and masks, which are constructed during the interactive segmentation process. We use this information to generate pseudo-labels, which we use to compute a loss function and optimize a part of the SAM model. The presented method causes a relative reduction of up to $48.1 \%$ in the $\text{FR}_{20}@85$ and $46.6 \%$ in the $\text{FR}_{30}@90$ metrics.

{{</citation>}}


### (54/61 | 54/235) NC-TTT: A Noise Contrastive Approach for Test-Time Training (David Osowiechi et al., 2024)

{{<citation>}}

David Osowiechi, Gustavo A. Vargas Hakim, Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers. (2024)  
**NC-TTT: A Noise Contrastive Approach for Test-Time Training**
<br/>
<button class="copy-to-clipboard" title="NC-TTT: A Noise Contrastive Approach for Test-Time Training" index=54>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 10  
Keywords: Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08392v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08392v1.pdf" filename="2404.08392v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite their exceptional performance in vision tasks, deep learning models often struggle when faced with domain shifts during testing. Test-Time Training (TTT) methods have recently gained popularity by their ability to enhance the robustness of models through the addition of an auxiliary objective that is jointly optimized with the main task. Being strictly <b>unsupervised,</b> this auxiliary objective is used at test time to adapt the model without any access to labels. In this work, we propose Noise-Contrastive Test-Time Training (NC-TTT), a novel <b>unsupervised</b> TTT technique based on the discrimination of noisy feature maps. By learning to classify noisy views of projected feature maps, and then adapting the model accordingly on new domains, classification performance can be recovered by an important margin. Experiments on several popular test-time adaptation baselines demonstrate the advantages of our method compared to recent approaches for this task. The code can be found at:https://github.com/GustavoVargasHakim/NCTTT.git

{{</citation>}}


### (55/61 | 55/235) Interference Motion Removal for Doppler Radar Vital Sign Detection Using Variational Encoder-Decoder Neural Network (Mikolaj Czerkawski et al., 2024)

{{<citation>}}

Mikolaj Czerkawski, Christos Ilioudis, Carmine Clemente, Craig Michie, Ivan Andonovic, Christos Tachtatzis. (2024)  
**Interference Motion Removal for Doppler Radar Vital Sign Detection Using Variational Encoder-Decoder Neural Network**
<br/>
<button class="copy-to-clipboard" title="Interference Motion Removal for Doppler Radar Vital Sign Detection Using Variational Encoder-Decoder Neural Network" index=55>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Convolution  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08298v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08298v1.pdf" filename="2404.08298v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The treatment of interfering motion contributions remains one of the key challenges in the domain of radar-based vital sign monitoring. Removal of the interference to extract the vital sign contributions is demanding due to overlapping Doppler bands, the complex structure of the interference motions and significant variations in the power levels of their contributions. A novel approach to the removal of interference through the use of a probabilistic deep learning model is presented. Results show that a <b>convolutional</b> encoder-decoder neural network with a variational objective is capable of learning a meaningful representation space of vital sign Doppler-time distribution facilitating their extraction from a mixture signal. The approach is tested on semi-experimental data containing real vital sign signatures and simulated returns from interfering body motions. The application of the proposed network enhances the extraction of the micro-Doppler frequency corresponding to the respiration rate is demonstrated.

{{</citation>}}


### (56/61 | 56/235) Overcoming Scene Context Constraints for Object Detection in wild using Defilters (Vamshi Krishna Kancharla et al., 2024)

{{<citation>}}

Vamshi Krishna Kancharla, Neelam sinha. (2024)  
**Overcoming Scene Context Constraints for Object Detection in wild using Defilters**
<br/>
<button class="copy-to-clipboard" title="Overcoming Scene Context Constraints for Object Detection in wild using Defilters" index=56>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Object Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08293v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08293v1.pdf" filename="2404.08293v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper focuses on improving <b>object</b> <b>detection</b> performance by addressing the issue of image distortions, commonly encountered in uncontrolled acquisition environments. High-level computer vision tasks such as <b>object</b> <b>detection,</b> recognition, and segmentation are particularly sensitive to image distortion. To address this issue, we propose a novel approach employing an image defilter to rectify image distortion prior to <b>object</b> <b>detection.</b> This method enhances <b>object</b> <b>detection</b> accuracy, as models perform optimally when trained on non-distorted images. Our experiments demonstrate that utilizing defiltered images significantly improves mean average precision compared to training <b>object</b> <b>detection</b> models on distorted images. Consequently, our proposed method offers considerable benefits for real-world applications plagued by image distortion. To our knowledge, the contribution lies in employing distortion-removal paradigm for <b>object</b> <b>detection</b> on images captured in natural settings. We achieved an improvement of 0.562 and 0.564 of mean Average precision on validation and test data.

{{</citation>}}


### (57/61 | 57/235) Enhancing Traffic Safety with Parallel Dense Video Captioning for End-to-End Event Analysis (Maged Shoman et al., 2024)

{{<citation>}}

Maged Shoman, Dongdong Wang, Armstrong Aboah, Mohamed Abdel-Aty. (2024)  
**Enhancing Traffic Safety with Parallel Dense Video Captioning for End-to-End Event Analysis**
<br/>
<button class="copy-to-clipboard" title="Enhancing Traffic Safety with Parallel Dense Video Captioning for End-to-End Event Analysis" index=57>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Knowledge Transfer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08229v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08229v1.pdf" filename="2404.08229v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces our solution for Track 2 in AI City Challenge 2024. The task aims to solve traffic safety description and analysis with the dataset of Woven Traffic Safety (WTS), a real-world Pedestrian-Centric Traffic Video Dataset for Fine-grained Spatial-Temporal Understanding. Our solution mainly focuses on the following points: 1) To solve dense video captioning, we leverage the framework of dense video captioning with parallel decoding (PDVC) to model visual-language sequences and generate dense caption by chapters for video. 2) Our work leverages CLIP to extract visual features to more efficiently perform cross-modality training between visual and textual representations. 3) We conduct domain-specific model adaptation to mitigate domain shift problem that poses recognition challenge in video understanding. 4) Moreover, we leverage BDD-5K captioned videos to conduct <b>knowledge</b> <b>transfer</b> for better understanding WTS videos and more accurate captioning. Our solution has yielded on the test set, achieving 6th place in the competition. The open source code will be available at https://github.com/UCF-SST-Lab/AICity2024CVPRW

{{</citation>}}


### (58/61 | 58/235) MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based Monocular Guidance (Yuqun Wu et al., 2024)

{{<citation>}}

Yuqun Wu, Jae Yong Lee, Chuhang Zou, Shenlong Wang, Derek Hoiem. (2024)  
**MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based Monocular Guidance**
<br/>
<button class="copy-to-clipboard" title="MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based Monocular Guidance" index=58>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 8  
Keywords: Benchmarking, Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08252v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08252v1.pdf" filename="2404.08252v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The latest regularized Neural Radiance Field (NeRF) approaches produce poor <b>geometry</b> and view extrapolation for multiview stereo (MVS) <b>benchmarks</b> such as ETH3D. In this paper, we aim to create 3D models that provide accurate <b>geometry</b> and view synthesis, partially closing the large geometric performance gap between NeRF and traditional MVS methods. We propose a patch-based approach that effectively leverages monocular surface normal and relative depth predictions. The patch-based ray sampling also enables the appearance regularization of normalized cross-correlation (NCC) and structural similarity (SSIM) between randomly sampled virtual and training views. We further show that "density restrictions" based on sparse structure-from-motion points can help greatly improve geometric accuracy with a slight drop in novel view synthesis metrics. Our experiments show 4x the performance of RegNeRF and 8x that of FreeNeRF on average F1@2cm for ETH3D MVS <b>benchmark,</b> suggesting a fruitful research direction to improve the geometric accuracy of NeRF-based models, and sheds light on a potential future approach to enable NeRF-based optimization to eventually outperform traditional MVS.

{{</citation>}}


### (59/61 | 59/235) SEVD: Synthetic Event-based Vision Dataset for Ego and Fixed Traffic Perception (Manideep Reddy Aliminati et al., 2024)

{{<citation>}}

Manideep Reddy Aliminati, Bharatesh Chakravarthi, Aayush Atul Verma, Arpitsinh Vaghela, Hua Wei, Xuesong Zhou, Yezhou Yang. (2024)  
**SEVD: Synthetic Event-based Vision Dataset for Ego and Fixed Traffic Perception**
<br/>
<button class="copy-to-clipboard" title="SEVD: Synthetic Event-based Vision Dataset for Ego and Fixed Traffic Perception" index=59>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.10540v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.10540v1.pdf" filename="2404.10540v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recently, event-based vision sensors have gained attention for autonomous driving applications, as conventional RGB cameras face limitations in handling challenging dynamic conditions. However, the availability of real-world and synthetic event-based vision datasets remains limited. In response to this gap, we present SEVD, a first-of-its-kind multi-view ego, and fixed perception synthetic event-based dataset using multiple dynamic vision sensors within the CARLA simulator. Data sequences are recorded across diverse lighting (noon, nighttime, twilight) and weather conditions (clear, cloudy, wet, rainy, foggy) with domain shifts (discrete and continuous). SEVD spans urban, suburban, rural, and highway scenes featuring various classes of objects (car, truck, van, bicycle, motorcycle, and pedestrian). Alongside event data, SEVD includes RGB imagery, depth maps, optical flow, semantic, and instance segmentation, facilitating a comprehensive understanding of the scene. Furthermore, we evaluate the dataset using state-of-the-art event-based (RED, RVT) and frame-based (YOLOv8) methods for traffic participant detection tasks and provide baseline <b>benchmarks</b> for assessment. Additionally, we conduct experiments to assess the synthetic event-based dataset's generalization capabilities. The dataset is available at https://eventbasedvision.github.io/SEVD

{{</citation>}}


### (60/61 | 60/235) COCONut: Modernizing COCO Segmentation (Xueqing Deng et al., 2024)

{{<citation>}}

Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, Liang-Chieh Chen. (2024)  
**COCONut: Modernizing COCO Segmentation**
<br/>
<button class="copy-to-clipboard" title="COCONut: Modernizing COCO Segmentation" index=60>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08639v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08639v1.pdf" filename="2404.08639v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent decades, the vision community has witnessed remarkable progress in visual recognition, partially owing to advancements in dataset <b>benchmarks.</b> Notably, the established COCO <b>benchmark</b> has propelled the development of modern detection and segmentation systems. However, the COCO segmentation <b>benchmark</b> has seen comparatively slow improvement over the last decade. Originally equipped with coarse polygon annotations for thing instances, it gradually incorporated coarse superpixel annotations for stuff regions, which were subsequently heuristically amalgamated to yield panoptic segmentation annotations. These annotations, executed by different groups of raters, have resulted not only in coarse segmentation masks but also in inconsistencies between segmentation types. In this study, we undertake a comprehensive reevaluation of the COCO segmentation annotations. By enhancing the annotation quality and expanding the dataset to encompass 383K images with more than 5.18M panoptic masks, we introduce COCONut, the COCO Next Universal segmenTation dataset. COCONut harmonizes segmentation annotations across semantic, instance, and panoptic segmentation with meticulously crafted high-quality masks, and establishes a robust <b>benchmark</b> for all segmentation tasks. To our knowledge, COCONut stands as the inaugural large-scale universal segmentation dataset, verified by human raters. We anticipate that the release of COCONut will significantly contribute to the community's ability to assess the progress of novel neural networks.

{{</citation>}}


### (61/61 | 61/235) NIR-Assisted Image Denoising: A Selective Fusion Approach and A Real-World Benchmark Datase (Rongjian Xu et al., 2024)

{{<citation>}}

Rongjian Xu, Zhilu Zhang, Renlong Wu, Wangmeng Zuo. (2024)  
**NIR-Assisted Image Denoising: A Selective Fusion Approach and A Real-World Benchmark Datase**
<br/>
<button class="copy-to-clipboard" title="NIR-Assisted Image Denoising: A Selective Fusion Approach and A Real-World Benchmark Datase" index=61>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08514v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08514v2.pdf" filename="2404.08514v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite the significant progress in image denoising, it is still challenging to restore fine-scale details while removing noise, especially in extremely low-light environments. Leveraging near-infrared (NIR) images to assist visible RGB image denoising shows the potential to address this issue, becoming a promising technology. Nonetheless, existing works still struggle with taking advantage of NIR information effectively for real-world image denoising, due to the content inconsistency between NIR-RGB images and the scarcity of real-world paired datasets. To alleviate the problem, we propose an efficient Selective Fusion Module (SFM), which can be plug-and-played into the advanced denoising networks to merge the deep NIR-RGB features. Specifically, we sequentially perform the global and local modulation for NIR and RGB features, and then integrate the two modulated features. Furthermore, we present a Real-world NIR-Assisted Image Denoising (Real-NAID) dataset, which covers diverse scenarios as well as various noise levels. Extensive experiments on both synthetic and our real-world datasets demonstrate that the proposed method achieves better results than state-of-the-art ones.

{{</citation>}}


## cs.CL (26)



### (1/26 | 62/235) Small Models Are (Still) Effective Cross-Domain Argument Extractors (William Gantt et al., 2024)

{{<citation>}}

William Gantt, Aaron Steven White. (2024)  
**Small Models Are (Still) Effective Cross-Domain Argument Extractors**
<br/>
<button class="copy-to-clipboard" title="Small Models Are (Still) Effective Cross-Domain Argument Extractors" index=62>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 90  
Keywords: Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, Event Argument Extraction, Question Answering, Question Answering, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08579v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08579v1.pdf" filename="2404.08579v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Effective ontology transfer has been a major goal of recent work on <b>event</b> <b>argument</b> <b>extraction</b> (EAE). Two methods in particular -- <b>question</b> <b>answering</b> <b>(QA)</b> and template infilling (TI) -- have emerged as promising approaches to this problem. However, detailed explorations of these techniques' ability to actually enable this transfer are lacking. In this work, we provide such a study, exploring <b>zero-shot</b> transfer using both techniques on six major EAE datasets at both the sentence and document levels. Further, we challenge the growing reliance on <b>LLMs</b> for <b>zero-shot</b> extraction, showing that vastly smaller models trained on an appropriate source ontology can yield <b>zero-shot</b> performance superior to that of <b>GPT-3.5</b> or <b>GPT-4.</b>

{{</citation>}}


### (2/26 | 63/235) Relational Prompt-based Pre-trained Language Models for Social Event Detection (Pu Li et al., 2024)

{{<citation>}}

Pu Li, Xiaoyan Yu, Hao Peng, Yantuan Xian, Linqin Wang, Li Sun, Jingyun Zhang, Philip S. Yu. (2024)  
**Relational Prompt-based Pre-trained Language Models for Social Event Detection**
<br/>
<button class="copy-to-clipboard" title="Relational Prompt-based Pre-trained Language Models for Social Event Detection" index=63>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs-SI, cs.CL  
Keyword Score: 86  
Keywords: Graph, Graph Neural Network, Graph Neural Network, Node Embedding, Clustering, Low-Resource, Event Detection, Pre-trained Language Model, Pre-trained Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08263v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08263v1.pdf" filename="2404.08263v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Social <b>Event</b> <b>Detection</b> (SED) aims to identify significant <b>events</b> <b>from</b> social streams, and has a wide application ranging from public opinion analysis to risk management. In recent years, <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> based solutions have achieved state-of-the-art performance. However, <b>GNN-based</b> methods often struggle with noisy and missing edges between messages, affecting the quality of learned message embedding. Moreover, these methods statically initialize <b>node</b> <b>embedding</b> before training, which, in turn, limits the ability to learn from message texts and relations simultaneously. In this paper, we approach social <b>event</b> <b>detection</b> from a new perspective based on <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs),</b> and present RPLM_SED (Relational <b>prompt-based</b> <b>Pre-trained</b> <b>Language</b> <b>Models</b> for Social <b>Event</b> <b>Detection).</b> We first propose a new pairwise message modeling strategy to construct social messages into message pairs with multi-relational sequences. Secondly, a new multi-relational <b>prompt-based</b> pairwise message learning mechanism is proposed to learn more comprehensive message representation from message pairs with multi-relational <b>prompts</b> using <b>PLMs.</b> Thirdly, we design a new <b>clustering</b> constraint to optimize the encoding process by enhancing intra-cluster compactness and inter-cluster dispersion, making the message representation more distinguishable. We evaluate the RPLM_SED on three real-world datasets, demonstrating that the RPLM_SED model achieves state-of-the-art performance in offline, online, <b>low-resource,</b> and long-tail distribution scenarios for social <b>event</b> <b>detection</b> tasks.

{{</citation>}}


### (3/26 | 64/235) CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation (Matthew DeLorenzo et al., 2024)

{{<citation>}}

Matthew DeLorenzo, Vasudev Gohil, Jeyavijayan Rajendran. (2024)  
**CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation**
<br/>
<button class="copy-to-clipboard" title="CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation" index=64>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 70  
Keywords: GPT, GPT-3, GPT-3.5, Code Generation, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08806v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08806v1.pdf" filename="2404.08806v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have proved effective and efficient in generating <b>code,</b> <b>leading</b> to their utilization within the hardware design process. Prior works evaluating <b>LLMs'</b> abilities for register transfer level <b>code</b> <b>generation</b> solely focus on functional correctness. However, the creativity associated with these <b>LLMs,</b> or the ability to generate novel and unique solutions, is a metric not as well understood, in part due to the challenge of quantifying this quality. To address this research gap, we present CreativeEval, a framework for evaluating the creativity of <b>LLMs</b> within the context of generating hardware designs. We quantify four creative sub-components, fluency, flexibility, originality, and elaboration, through various <b>prompting</b> and post-processing techniques. We then evaluate multiple popular <b>LLMs</b> (including <b>GPT</b> models, CodeLlama, and VeriGen) upon this creativity metric, with results indicating <b>GPT-3.5</b> as the most creative model in generating hardware designs.

{{</citation>}}


### (4/26 | 65/235) Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case Study (Wan-Hua Her et al., 2024)

{{<citation>}}

Wan-Hua Her, Udo Kruschwitz. (2024)  
**Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case Study**
<br/>
<button class="copy-to-clipboard" title="Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case Study" index=65>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 70  
Keywords: High-Resource, Low-Resource, Transfer Learning, Neural Machine Translation, Neural Machine Translation, BLEU, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08259v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08259v1.pdf" filename="2404.08259v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Machine</b> <b>Translation</b> has made impressive progress in recent years offering close to human-level performance on many languages, but studies have primarily focused on <b>high-resource</b> languages with broad online presence and resources. With the help of growing <b>Large</b> <b>Language</b> <b>Models,</b> more and more <b>low-resource</b> languages achieve better results through the presence of other languages. However, studies have shown that not all <b>low-resource</b> languages can benefit from multilingual systems, especially those with insufficient training and evaluation data. In this paper, we revisit state-of-the-art <b>Neural</b> <b>Machine</b> <b>Translation</b> techniques to develop automatic translation systems between German and Bavarian. We investigate conditions of <b>low-resource</b> languages such as data scarcity and parameter sensitivity and focus on refined solutions that combat <b>low-resource</b> difficulties and creative solutions such as harnessing language similarity. Our experiment entails applying Back-translation and <b>Transfer</b> <b>Learning</b> to automatically generate more training data and achieve higher translation performance. We demonstrate noisiness in the data and present our approach to carry out text preprocessing extensively. Evaluation was conducted using combined metrics: <b>BLEU,</b> chrF and TER. Statistical significance results with Bonferroni correction show surprisingly high baseline systems, and that Back-translation leads to significant improvement. Furthermore, we present a qualitative analysis of translation errors and system limitations.

{{</citation>}}


### (5/26 | 66/235) Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian (Aleksa Cvetanović et al., 2024)

{{<citation>}}

Aleksa Cvetanović, Predrag Tadić. (2024)  
**Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian**
<br/>
<button class="copy-to-clipboard" title="Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian" index=66>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 63  
Keywords: Benchmarking, Fine-tuning, Fine-tuning, Zero-shot, Transformer, Question Answering, Question Answering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08617v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08617v1.pdf" filename="2404.08617v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we focus on generating a synthetic <b>question</b> <b>answering</b> <b>(QA)</b> dataset using an adapted Translate-Align-Retrieve method. Using this method, we created the largest Serbian <b>QA</b> dataset of more than 87K samples, which we name SQuAD-sr. To acknowledge the script duality in Serbian, we generated both Cyrillic and Latin versions of the dataset. We investigate the dataset quality and use it to <b>fine-tune</b> several pre-trained <b>QA</b> models. Best results were obtained by <b>fine-tuning</b> the BERTi\'c model on our Latin SQuAD-sr dataset, achieving 73.91% Exact Match and 82.97% F1 score on the <b>benchmark</b> XQuAD dataset, which we translated into Serbian for the purpose of evaluation. The results show that our model exceeds <b>zero-shot</b> baselines, but fails to go beyond human performance. We note the advantage of using a monolingual pre-trained model over multilingual, as well as the performance increase gained by using Latin over Cyrillic. By performing additional analysis, we show that <b>questions</b> <b>about</b> numeric values or dates are more likely to be answered correctly than other types of <b>questions.</b> <b>Finally,</b> we conclude that SQuAD-sr is of sufficient quality for <b>fine-tuning</b> a Serbian <b>QA</b> model, in the absence of a manually crafted and annotated dataset.

{{</citation>}}


### (6/26 | 67/235) ASR advancements for indigenous languages: Quechua, Guarani, Bribri, Kotiria, and Wa'ikhana (Monica Romero et al., 2024)

{{<citation>}}

Monica Romero, Sandra Gomez, Iván G. Torre. (2024)  
**ASR advancements for indigenous languages: Quechua, Guarani, Bribri, Kotiria, and Wa'ikhana**
<br/>
<button class="copy-to-clipboard" title="ASR advancements for indigenous languages: Quechua, Guarani, Bribri, Kotiria, and Wa'ikhana" index=67>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 50  
Keywords: Data Augmentation, Fine-tuning, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08368v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08368v1.pdf" filename="2404.08368v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Indigenous languages are a fundamental legacy in the development of human communication, embodying the unique identity and culture of local communities of America. The Second AmericasNLP Competition Track 1 of NeurIPS 2022 proposed developing <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> systems for five indigenous languages: Quechua, Guarani, Bribri, Kotiria, and Wa'ikhana. In this paper, we propose a reliable <b>ASR</b> model for each target language by crawling <b>speech</b> <b>corpora</b> spanning diverse sources and applying <b>data</b> <b>augmentation</b> methods that resulted in the winning approach in this competition. To achieve this, we systematically investigated the impact of different hyperparameters by a Bayesian search on the performance of the language models, specifically focusing on the variants of the Wav2vec2.0 XLS-R model: 300M and 1B parameters. Moreover, we performed a global sensitivity analysis to assess the contribution of various hyperparametric configurations to the performances of our best models. Importantly, our results show that freeze <b>fine-tuning</b> updates and dropout rate are more vital parameters than the total number of epochs of lr. Additionally, we liberate our best models -- with no other <b>ASR</b> model reported until now for two Wa'ikhana and Kotiria -- and the many experiments performed to pave the way to other researchers to continue improving <b>ASR</b> in minority languages. This insight opens up interesting avenues for future work, allowing for the advancement of <b>ASR</b> techniques in the preservation of minority indigenous and acknowledging the complexities involved in this important endeavour.

{{</citation>}}


### (7/26 | 68/235) Pre-training Small Base LMs with Fewer Tokens (Sunny Sanyal et al., 2024)

{{<citation>}}

Sunny Sanyal, Sujay Sanghavi, Alexandros G. Dimakis. (2024)  
**Pre-training Small Base LMs with Fewer Tokens**
<br/>
<button class="copy-to-clipboard" title="Pre-training Small Base LMs with Fewer Tokens" index=68>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 43  
Keywords: Benchmarking, GPT, GPT-2, Transformer, Massive Multitask Language Understanding (MMLU)  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08634v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08634v1.pdf" filename="2404.08634v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We study the effectiveness of a simple approach to develop a small base language model (LM) starting from an existing large base LM: first inherit a few <b>transformer</b> blocks from the larger LM, and then train this smaller model on a very small subset (0.1\%) of the raw pretraining data of the larger model. We call our simple recipe Inheritune and first demonstrate it for building a small base LM with 1.5B parameters using 1B tokens (and a starting few layers of larger LM of 3B parameters); we do this using a single A6000 GPU for less than half a day. Across 9 diverse evaluation datasets as well as the <b>MMLU</b> <b>benchmark,</b> the resulting model compares favorably to publicly available base models of 1B-2B size, some of which have been trained using 50-1000 times more tokens. We investigate Inheritune in a slightly different setting where we train small LMs utilizing larger LMs and their full pre-training dataset. Here we show that smaller LMs trained utilizing some of the layers of <b>GPT2-medium</b> (355M) and <b>GPT-2-large</b> (770M) can effectively match the val loss of their bigger counterparts when trained from scratch for the same number of training steps on OpenWebText dataset with 9B tokens. We analyze our recipe with extensive experiments and demonstrate it efficacy on diverse settings. Our code is available at https://github.com/sanyalsunny111/LLM-Inheritune.

{{</citation>}}


### (8/26 | 69/235) Pretraining and Updating Language- and Domain-specific Large Language Model: A Case Study in Japanese Business Domain (Kosuke Takahashi et al., 2024)

{{<citation>}}

Kosuke Takahashi, Takahiro Omi, Kosuke Arima, Tatsuya Ishigaki. (2024)  
**Pretraining and Updating Language- and Domain-specific Large Language Model: A Case Study in Japanese Business Domain**
<br/>
<button class="copy-to-clipboard" title="Pretraining and Updating Language- and Domain-specific Large Language Model: A Case Study in Japanese Business Domain" index=69>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: 68T50, cs-AI, cs-CL, cs.CL  
Keyword Score: 43  
Keywords: Benchmarking, Question Answering, Question Answering, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08262v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08262v2.pdf" filename="2404.08262v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Several previous studies have considered language- and domain-specific <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as separate topics. This study explores the combination of a non-English language and a high-demand industry domain, focusing on a Japanese business-specific <b>LLM.</b> This type of a model requires expertise in the business domain, strong language skills, and regular updates of its knowledge. We trained a 13-billion-parameter <b>LLM</b> from scratch using a new dataset of business texts and patents, and continually pretrained it with the latest business documents. Further we propose a new <b>benchmark</b> for Japanese business domain <b>question</b> <b>answering</b> <b>(QA)</b> and evaluate our models on it. The results show that our pretrained model improves <b>QA</b> accuracy without losing general knowledge, and that continual pretraining enhances adaptation to new information. Our pretrained model and business domain <b>benchmark</b> are publicly available.

{{</citation>}}


### (9/26 | 70/235) BERT-LSH: Reducing Absolute Compute For Attention (Zezheng Li et al., 2024)

{{<citation>}}

Zezheng Li, Kingston Yip. (2024)  
**BERT-LSH: Reducing Absolute Compute For Attention**
<br/>
<button class="copy-to-clipboard" title="BERT-LSH: Reducing Absolute Compute For Attention" index=70>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 30  
Keywords: Fine-tuning, BERT, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08836v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08836v1.pdf" filename="2404.08836v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This study introduces a novel <b>BERT-LSH</b> model that incorporates Locality Sensitive Hashing (LSH) to approximate the attention mechanism in the <b>BERT</b> architecture. We examine the computational efficiency and performance of this model compared to a standard baseline <b>BERT</b> model. Our findings reveal that <b>BERT-LSH</b> significantly reduces computational demand for the <b>self-attention</b> layer while unexpectedly outperforming the baseline model in pretraining and <b>fine-tuning</b> tasks. These results suggest that the LSH-based attention mechanism not only offers computational advantages but also may enhance the model's ability to generalize from its training data. For more information, visit our GitHub repository: https://github.com/leo4life2/algoml-final

{{</citation>}}


### (10/26 | 71/235) Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance (Yewei Song et al., 2024)

{{<citation>}}

Yewei Song, Cedric Lothritz, Daniel Tang, Tegawendé F. Bissyandé, Jacques Klein. (2024)  
**Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance**
<br/>
<button class="copy-to-clipboard" title="Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance" index=71>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-PL, cs-SE, cs.CL  
Keyword Score: 30  
Keywords: GPT, BLEU, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08817v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08817v1.pdf" filename="2404.08817v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper revisits recent code similarity evaluation metrics, particularly focusing on the application of Abstract Syntax Tree (AST) editing distance in diverse programming languages. In particular, we explore the usefulness of these metrics and compare them to traditional sequence similarity metrics. Our experiments showcase the effectiveness of AST editing distance in capturing intricate code structures, revealing a high correlation with established metrics. Furthermore, we explore the strengths and weaknesses of AST editing distance and <b>prompt-based</b> <b>GPT</b> similarity scores in comparison to <b>BLEU</b> score, execution match, and Jaccard Similarity. We propose, optimize, and publish an adaptable metric that demonstrates effectiveness across all tested languages, representing an enhanced version of Tree Similarity of Edit Distance (TSED).

{{</citation>}}


### (11/26 | 72/235) The Generation Gap:Exploring Age Bias in Large Language Models (Siyang Liu et al., 2024)

{{<citation>}}

Siyang Liu, Trish Maturi, Siqi Shen, Rada Mihalcea. (2024)  
**The Generation Gap:Exploring Age Bias in Large Language Models**
<br/>
<button class="copy-to-clipboard" title="The Generation Gap:Exploring Age Bias in Large Language Models" index=72>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08760v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08760v1.pdf" filename="2404.08760v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we explore the alignment of values in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of <b>prompts</b> tailored to ensure response robustness, we find a general inclination of <b>LLM</b> values towards younger demographics. Additionally, we explore the impact of incorporating age identity information in <b>prompts</b> and observe challenges in mitigating value discrepancies with different age cohorts. Our findings highlight the age bias in <b>LLMs</b> and provide insights for future work.

{{</citation>}}


### (12/26 | 73/235) MoPE: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking (Tianwen Tang et al., 2024)

{{<citation>}}

Tianwen Tang, Tong Zhu, Haodong Liu, Yin Bai, Jia Cheng, Wenliang Chen. (2024)  
**MoPE: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking**
<br/>
<button class="copy-to-clipboard" title="MoPE: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking" index=73>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Stochastic Gradient Descent, Zero-shot, Dialogue State Tracking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08559v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08559v1.pdf" filename="2404.08559v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Zero-shot</b> <b>dialogue</b> <b>state</b> <b>tracking</b> (DST) transfers knowledge to unseen domains, reducing the cost of annotating new datasets. Previous <b>zero-shot</b> DST models mainly suffer from domain transferring and partial prediction problems. To address these challenges, we propose Mixture of Prefix Experts (MoPE) to establish connections between similar slots in different domains, which strengthens the model transfer performance in unseen domains. Empirical results demonstrate that MoPE-DST achieves the joint goal accuracy of 57.13% on MultiWOZ2.1 and 55.40% on <b>SGD.</b>

{{</citation>}}


### (13/26 | 74/235) VertAttack: Taking advantage of Text Classifiers' horizontal vision (Jonathan Rusert, 2024)

{{<citation>}}

Jonathan Rusert. (2024)  
**VertAttack: Taking advantage of Text Classifiers' horizontal vision**
<br/>
<button class="copy-to-clipboard" title="VertAttack: Taking advantage of Text Classifiers' horizontal vision" index=74>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: I-2-7, cs-CL, cs.CL  
Keyword Score: 30  
Keywords: RoBERTa, Transformer, Text Classification  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08538v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08538v1.pdf" filename="2404.08538v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Text</b> <b>classification</b> systems have continuously improved in performance over the years. However, nearly all current SOTA classifiers have a similar shortcoming, they process <b>text</b> <b>in</b> a horizontal manner. Vertically written words will not be recognized by a classifier. In contrast, humans are easily able to recognize and read words written both horizontally and vertically. Hence, a human adversary could write problematic words vertically and the meaning would still be preserved to other humans. We simulate such an attack, VertAttack. VertAttack identifies which words a classifier is reliant on and then rewrites those words vertically. We find that VertAttack is able to greatly drop the accuracy of 4 different <b>transformer</b> models on 5 datasets. For example, on the SST2 dataset, VertAttack is able to drop <b>RoBERTa's</b> accuracy from 94 to 13%. Furthermore, since VertAttack does not replace the word, meaning is easily preserved. We verify this via a human study and find that crowdworkers are able to correctly label 77% perturbed <b>texts</b> <b>perturbed,</b> compared to 81% of the original <b>texts.</b> <b>We</b> believe VertAttack offers a look into how humans might circumvent classifiers in the future and thus inspire a look into more robust algorithms.

{{</citation>}}


### (14/26 | 75/235) Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation (Haozhe Zhao et al., 2024)

{{<citation>}}

Haozhe Zhao, Zefan Cai, Shuzheng Si, Liang Chen, Yufeng He, Kaikai An, Baobao Chang. (2024)  
**Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation**
<br/>
<button class="copy-to-clipboard" title="Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation" index=75>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Fine-tuning, Self-Distillation, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08491v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08491v1.pdf" filename="2404.08491v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Large-scale multilingual <b>Pretrained</b> <b>Language</b> <b>Models</b> (mPLMs) yield impressive performance on cross-language tasks, yet significant performance disparities exist across different languages within the same mPLM. Previous studies endeavored to narrow these disparities by supervise <b>fine-tuning</b> the mPLMs with multilingual data. However, obtaining labeled multilingual data is time-consuming, and <b>fine-tuning</b> mPLM with limited labeled multilingual data merely encapsulates the knowledge specific to the labeled data. Therefore, we introduce ALSACE to leverage the learned knowledge from the well-performing languages to guide under-performing ones within the same mPLM, eliminating the need for additional labeled multilingual data. Experiments show that ALSACE effectively mitigates language-level performance disparity across various mPLMs while showing the competitive performance on different multilingual NLU tasks, ranging from full resource to limited resource settings. The code for our approach is available at https://github.com/pkunlp-icler/ALSACE.

{{</citation>}}


### (15/26 | 76/235) Thematic Analysis with Large Language Models: does it work with languages other than English? A targeted test in Italian (Stefano De Paoli, 2024)

{{<citation>}}

Stefano De Paoli. (2024)  
**Thematic Analysis with Large Language Models: does it work with languages other than English? A targeted test in Italian**
<br/>
<button class="copy-to-clipboard" title="Thematic Analysis with Large Language Models: does it work with languages other than English? A targeted test in Italian" index=76>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 30  
Keywords: Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08488v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08488v1.pdf" filename="2404.08488v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper proposes a test to perform Thematic Analysis (TA) with <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> on data which is in a different language than English. While there has been initial promising work on using pre-trained <b>LLMs</b> for TA on data in English, we lack any tests on whether these models can reasonably perform the same analysis with good quality in other language. In this paper a test will be proposed using an open access dataset of semi-structured interviews in Italian. The test shows that a pre-trained model can perform such a TA on the data, also using <b>prompts</b> in Italian. A comparative test shows the model capacity to produce themes which have a good resemblance with those produced independently by human researchers. The main implication of this study is that pre-trained <b>LLMs</b> may thus be suitable to support analysis in multilingual situations, so long as the language is supported by the model used.

{{</citation>}}


### (16/26 | 77/235) Learning representations of learning representations (Rita González-Márquez et al., 2024)

{{<citation>}}

Rita González-Márquez, Dmitry Kobak. (2024)  
**Learning representations of learning representations**
<br/>
<button class="copy-to-clipboard" title="Learning representations of learning representations" index=77>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-DL, cs-LG, cs.CL  
Keyword Score: 30  
Keywords: Bag-of-Words, Transformer, TF-IDF  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08403v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08403v1.pdf" filename="2404.08403v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The ICLR conference is unique among the top machine learning conferences in that all submitted papers are openly available. Here we present the ICLR dataset consisting of abstracts of all 24 thousand ICLR submissions from 2017-2024 with meta-data, decision scores, and custom keyword-based labels. We find that on this dataset, <b>bag-of-words</b> representation outperforms most dedicated sentence <b>transformer</b> models in terms of $k$NN classification accuracy, and the top performing language models barely outperform <b>TF-IDF.</b> We see this as a challenge for the NLP community. Furthermore, we use the ICLR dataset to study how the field of machine learning has changed over the last seven years, finding some improvement in gender balance. Using a 2D embedding of the abstracts' texts, we describe a shift in research topics from 2017 to 2024 and identify hedgehogs and foxes among the authors with the highest number of ICLR submissions.

{{</citation>}}


### (17/26 | 78/235) Toward a Theory of Tokenization in LLMs (Nived Rajaraman et al., 2024)

{{<citation>}}

Nived Rajaraman, Jiantao Jiao, Kannan Ramchandran. (2024)  
**Toward a Theory of Tokenization in LLMs**
<br/>
<button class="copy-to-clipboard" title="Toward a Theory of Tokenization in LLMs" index=78>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-LG, cs.CL  
Keyword Score: 30  
Keywords: Transformer, Tokenization, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08335v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08335v1.pdf" filename="2404.08335v1.pdf">Download PDF</button>

---


**ABSTRACT**  
While there has been a large body of research attempting to circumvent <b>tokenization</b> for language modeling (Clark et al., 2022; Xue et al., 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate <b>tokenization</b> from a theoretical point of view by studying the behavior of <b>transformers</b> on simple data generating processes. When trained on data drawn from certain simple $k^{\text{th}}$-order Markov processes for $k > 1$, <b>transformers</b> exhibit a surprising phenomenon - in the absence of <b>tokenization,</b> they empirically fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al., 2024). With the addition of <b>tokenization,</b> however, we empirically observe that <b>transformers</b> break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by <b>transformers</b> with and without <b>tokenization.</b> With the appropriate <b>tokenization,</b> we show that even the simplest unigram models (over tokens) learnt by <b>transformers</b> are able to model the probability of sequences drawn from $k^{\text{th}}$-order Markov sources near optimally. Our analysis provides a justification for the use of <b>tokenization</b> in practice through studying the behavior of <b>transformers</b> on Markovian data.

{{</citation>}}


### (18/26 | 79/235) Gaining More Insight into Neural Semantic Parsing with Challenging Benchmarks (Xiao Zhang et al., 2024)

{{<citation>}}

Xiao Zhang, Chunliu Wang, Rik van Noord, Johan Bos. (2024)  
**Gaining More Insight into Neural Semantic Parsing with Challenging Benchmarks**
<br/>
<button class="copy-to-clipboard" title="Gaining More Insight into Neural Semantic Parsing with Challenging Benchmarks" index=79>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 23  
Keywords: Benchmarking, Semantic Parsing, Text Generation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08354v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08354v1.pdf" filename="2404.08354v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The Parallel Meaning Bank (PMB) serves as a corpus for <b>semantic</b> <b>processing</b> with a focus on <b>semantic</b> <b>parsing</b> and <b>text</b> <b>generation.</b> Currently, we witness an excellent performance of neural parsers and generators on the PMB. This might suggest that such <b>semantic</b> <b>processing</b> tasks have by and large been solved. We argue that this is not the case and that performance scores from the past on the PMB are inflated by non-optimal data splits and test sets that are too easy. In response, we introduce several changes. First, instead of the prior random split, we propose a more systematic splitting approach to improve the reliability of the standard test data. Second, except for the standard test set, we also propose two challenge sets: one with longer <b>texts</b> <b>including</b> discourse structure, and one that addresses compositional generalization. We evaluate five neural models for <b>semantic</b> <b>parsing</b> and meaning-to-text generation. Our results show that model performance declines (in some cases dramatically) on the challenge sets, revealing the limitations of neural models when confronting such challenges.

{{</citation>}}


### (19/26 | 80/235) Constrained C-Test Generation via Mixed-Integer Programming (Ji-Ung Lee et al., 2024)

{{<citation>}}

Ji-Ung Lee, Marc E. Pfetsch, Iryna Gurevych. (2024)  
**Constrained C-Test Generation via Mixed-Integer Programming**
<br/>
<button class="copy-to-clipboard" title="Constrained C-Test Generation via Mixed-Integer Programming" index=80>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: GPT, GPT-4  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08821v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08821v1.pdf" filename="2404.08821v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This work proposes a novel method to generate C-Tests; a deviated form of cloze tests (a gap filling exercise) where only the last part of a word is turned into a gap. In contrast to previous works that only consider varying the gap size or gap placement to achieve locally optimal solutions, we propose a mixed-integer programming (MIP) approach. This allows us to consider gap size and placement simultaneously, achieving globally optimal solutions, and to directly integrate state-of-the-art models for gap difficulty prediction into the optimization problem. A user study with 40 participants across four C-Test generation strategies (including <b>GPT-4)</b> shows that our approach (MIP) significantly outperforms two of the baseline strategies (based on gap placement and <b>GPT-4);</b> and performs on-par with the third (based on gap size). Our analysis shows that <b>GPT-4</b> still struggles to fulfill explicit constraints during generation and that MIP produces C-Tests that correlate best with the perceived difficulty. We publish our code, model, and collected data consisting of 32 English C-Tests with 20 gaps each (totaling 3,200 individual gap responses) under an open source license.

{{</citation>}}


### (20/26 | 81/235) Evaluating the Quality of Answers in Political Q&A Sessions with Large Language Models (R. Michael Alvarez et al., 2024)

{{<citation>}}

R. Michael Alvarez, Jacob Morrier. (2024)  
**Evaluating the Quality of Answers in Political Q&A Sessions with Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Evaluating the Quality of Answers in Political Q&A Sessions with Large Language Models" index=81>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL, econ-EM  
Keyword Score: 20  
Keywords: Fine-tuning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08816v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08816v1.pdf" filename="2404.08816v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a new approach to evaluating the quality of answers in political question-and-answer sessions. We propose to measure an answer's quality based on the degree to which it allows us to infer the initial question accurately. This conception of answer quality inherently reflects their relevance to initial questions. Drawing parallels with semantic search, we argue that this measurement approach can be operationalized by <b>fine-tuning</b> a <b>large</b> <b>language</b> <b>model</b> on the observed corpus of questions and answers without additional labeled data. We showcase our measurement approach within the context of the Question Period in the Canadian House of Commons. Our approach yields valuable insights into the correlates of the quality of answers in the Question Period. We find that answer quality varies significantly based on the party affiliation of the members of Parliament asking the questions and uncover a meaningful correlation between answer quality and the topics of the questions.

{{</citation>}}


### (21/26 | 82/235) Is ChatGPT Transforming Academics' Writing Style? (Mingmeng Geng et al., 2024)

{{<citation>}}

Mingmeng Geng, Roberto Trotta. (2024)  
**Is ChatGPT Transforming Academics' Writing Style?**
<br/>
<button class="copy-to-clipboard" title="Is ChatGPT Transforming Academics' Writing Style?" index=82>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-DL, cs-LG, cs.CL  
Keyword Score: 20  
Keywords: ChatGPT, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08627v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08627v1.pdf" filename="2404.08627v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Based on one million arXiv papers submitted from May 2018 to January 2024, we assess the textual density of <b>ChatGPT's</b> writing style in their abstracts by means of a statistical analysis of word frequency changes. Our model is calibrated and validated on a mixture of real abstracts and <b>ChatGPT-modified</b> abstracts (simulated data) after a careful noise analysis. We find that <b>ChatGPT</b> is having an increasing impact on arXiv abstracts, especially in the field of computer science, where the fraction of <b>ChatGPT-revised</b> abstracts is estimated to be approximately 35%, if we take the output of one of the simplest <b>prompts,</b> "revise the following sentences", as a baseline. We conclude with an analysis of both positive and negative aspects of the penetration of <b>ChatGPT</b> into academics' writing style.

{{</citation>}}


### (22/26 | 83/235) Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think (Xinpeng Wang et al., 2024)

{{<citation>}}

Xinpeng Wang, Chengzhi Hu, Bolei Ma, Paul Röttger, Barbara Plank. (2024)  
**Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think**
<br/>
<button class="copy-to-clipboard" title="Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think" index=83>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08382v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08382v1.pdf" filename="2404.08382v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token prediction. An alternative way is to examine the text output. Prior work has shown that first token probabilities lack robustness to changes in MCQ phrasing, and that first token probabilities do not match text answers for instruction-tuned models. Therefore, in this paper, we investigate the robustness of text answers. We show that the text answers are more robust to question perturbations than the first token probabilities, when the first token answers mismatch the text answers. The difference in robustness increases as the mismatch rate becomes greater. As the mismatch reaches over 50\%, the text answer is more robust to option order changes than the debiased first token probabilities using state-of-the-art debiasing methods such as PriDe. Our findings provide further evidence for the benefits of text answer evaluation over first token probability evaluation.

{{</citation>}}


### (23/26 | 84/235) Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval (Juraj Vladika et al., 2024)

{{<citation>}}

Juraj Vladika, Florian Matthes. (2024)  
**Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval**
<br/>
<button class="copy-to-clipboard" title="Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval" index=84>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-IR, cs.CL  
Keyword Score: 20  
Keywords: Question Answering, Question Answering  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08359v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08359v1.pdf" filename="2404.08359v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In today's digital world, seeking answers to health <b>questions</b> <b>on</b> the Internet is a common practice. However, existing <b>question</b> <b>answering</b> <b>(QA)</b> systems often rely on using pre-selected and annotated evidence documents, thus making them inadequate for addressing novel <b>questions.</b> <b>Our</b> study focuses on the open-domain <b>QA</b> setting, where the key challenge is to first uncover relevant evidence in large knowledge bases. By utilizing the common retrieve-then-read <b>QA</b> pipeline and PubMed as a trustworthy collection of medical research documents, we answer health <b>questions</b> <b>from</b> three diverse datasets. We modify different retrieval settings to observe their influence on the <b>QA</b> pipeline's performance, including the number of retrieved documents, sentence selection process, the publication year of articles, and their number of citations. Our results reveal that cutting down on the amount of retrieved documents and favoring more recent and highly cited documents can improve the final macro F1 score up to 10%. We discuss the results, highlight interesting examples, and outline challenges for future research, like managing evidence disagreement and crafting user-friendly explanations.

{{</citation>}}


### (24/26 | 85/235) The Integration of Semantic and Structural Knowledge in Knowledge Graph Entity Typing (Muzhi Li et al., 2024)

{{<citation>}}

Muzhi Li, Minda Hu, Irwin King, Ho-fung Leung. (2024)  
**The Integration of Semantic and Structural Knowledge in Knowledge Graph Entity Typing**
<br/>
<button class="copy-to-clipboard" title="The Integration of Semantic and Structural Knowledge in Knowledge Graph Entity Typing" index=85>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 13  
Keywords: Graph, Knowledge Graph, Knowledge Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08313v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08313v1.pdf" filename="2404.08313v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The <b>Knowledge</b> <b>Graph</b> Entity Typing (KGET) task aims to predict missing type annotations for entities in <b>knowledge</b> <b>graphs.</b> Recent works only utilize the \textit{\textbf{structural knowledge}} in the local neighborhood of entities, disregarding \textit{\textbf{semantic knowledge}} in the textual representations of entities, relations, and types that are also crucial for type inference. Additionally, we observe that the interaction between semantic and structural <b>knowledge</b> <b>can</b> be utilized to address the false-negative problem. In this paper, we propose a novel \textbf{\underline{S}}emantic and \textbf{\underline{S}}tructure-aware <b>KG</b> \textbf{\underline{E}}ntity \textbf{\underline{T}}yping~{(SSET)} framework, which is composed of three modules. First, the \textit{Semantic <b>Knowledge</b> <b>Encoding}</b> module encodes factual <b>knowledge</b> <b>in</b> the <b>KG</b> with a Masked Entity Typing task. Then, the \textit{Structural <b>Knowledge</b> <b>Aggregation}</b> module aggregates <b>knowledge</b> <b>from</b> the multi-hop neighborhood of entities to infer missing types. Finally, the \textit{Unsupervised Type Re-ranking} module utilizes the inference results from the two models above to generate type predictions that are robust to false-negative samples. Extensive experiments show that SSET significantly outperforms existing state-of-the-art methods.

{{</citation>}}


### (25/26 | 86/235) Measuring Cross-lingual Transfer in Bytes (Leandro Rodrigues de Souza et al., 2024)

{{<citation>}}

Leandro Rodrigues de Souza, Thales Sales Almeida, Roberto Lotufo, Rodrigo Nogueira. (2024)  
**Measuring Cross-lingual Transfer in Bytes**
<br/>
<button class="copy-to-clipboard" title="Measuring Cross-lingual Transfer in Bytes" index=86>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 10  
Keywords: Scaling Law  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08191v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08191v1.pdf" filename="2404.08191v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual models also have a similar capability, but the mechanisms behind this transfer remain unclear. Some studies have explored factors like language contamination and syntactic similarity. An emerging line of research suggests that the representations learned by language models contain two components: a language-specific and a language-agnostic component. The latter is responsible for transferring a more universal knowledge. However, there is a lack of comprehensive exploration of these properties across diverse target languages. To investigate this hypothesis, we conducted an experiment inspired by the work on the <b>Scaling</b> <b>Laws</b> for Transfer. We measured the amount of data transferred from a source language to a target language and found that models initialized from diverse languages perform similarly to a target language in a cross-lingual setting. This was surprising because the amount of data transferred to 10 diverse target languages, such as Spanish, Korean, and Finnish, was quite similar. We also found evidence that this transfer is not related to language contamination or language proximity, which strengthens the hypothesis that the model also relies on language-agnostic knowledge. Our experiments have opened up new possibilities for measuring how much data represents the language-agnostic representations learned during pretraining.

{{</citation>}}


### (26/26 | 87/235) FastSpell: the LangId Magic Spell (Marta Bañón et al., 2024)

{{<citation>}}

Marta Bañón, Jaume Zaragoza-Bernabeu, Gema Ramírez-Sánchez, Sergio Ortiz-Rojas. (2024)  
**FastSpell: the LangId Magic Spell**
<br/>
<button class="copy-to-clipboard" title="FastSpell: the LangId Magic Spell" index=87>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08345v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08345v1.pdf" filename="2404.08345v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Language identification is a crucial component in the automated production of language resources, particularly in multilingual and big data contexts. However, commonly used language identifiers struggle to differentiate between similar or closely-related languages. This paper introduces FastSpell, a language identifier that combines fastText (a pre-trained language identifier tool) and Hunspell (a spell checker) with the aim of having a refined second-opinion before deciding which language should be assigned to a text. We provide a description of the FastSpell algorithm along with an explanation on how to use and configure it. To that end, we motivate the need of such a tool and present a <b>benchmark</b> including some popular language identifiers evaluated during the development of FastSpell. We show how FastSpell is useful not only to improve identification of similar languages, but also to identify new ones ignored by other tools.

{{</citation>}}


## cs.CR (12)



### (1/12 | 88/235) Empowering Malware Detection Efficiency within Processing-in-Memory Architecture (Sreenitha Kasarapu et al., 2024)

{{<citation>}}

Sreenitha Kasarapu, Sathwika Bavikadi, Sai Manoj Pudukotai Dinakarrao. (2024)  
**Empowering Malware Detection Efficiency within Processing-in-Memory Architecture**
<br/>
<button class="copy-to-clipboard" title="Empowering Malware Detection Efficiency within Processing-in-Memory Architecture" index=88>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-AR, cs-CR, cs.CR  
Keyword Score: 70  
Keywords: Convolutional Neural Network, Convolutional Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network, Deep Neural Network, Deep Neural Network, Malware, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08818v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08818v1.pdf" filename="2404.08818v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The widespread integration of embedded systems across various industries has facilitated seamless connectivity among devices and bolstered computational capabilities. Despite their extensive applications, embedded systems encounter significant <b>security</b> threats, with one of the most critical vulnerabilities being malicious software, commonly known as <b>malware.</b> In recent times, <b>malware</b> detection techniques leveraging Machine Learning have gained popularity. <b>Deep</b> <b>Neural</b> <b>Networks</b> <b>(DNNs)</b> and <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> have proven particularly efficient in image processing tasks. However, one major drawback of neural network architectures is their substantial computational resource requirements. Continuous training of <b>malware</b> detection models with updated <b>malware</b> and benign samples demands immense computational resources, presenting a challenge for real-world applications. In response to these concerns, we propose a Processing-in-Memory (PIM)-based architecture to mitigate memory access latency, thereby reducing the resources consumed during model updates. To further enhance throughput and minimize energy consumption, we incorporate precision scaling techniques tailored for <b>CNN</b> models. Our proposed PIM architecture exhibits a 1.09x higher throughput compared to existing Lookup Table (LUT)-based PIM architectures. Additionally, precision scaling combined with PIM enhances energy efficiency by 1.5x compared to full-precision operations, without sacrificing performance. This innovative approach offers a promising solution to the resource-intensive nature of <b>malware</b> detection model updates, paving the way for more efficient and sustainable cybersecurity practices.

{{</citation>}}


### (2/12 | 89/235) FCert: Certifiably Robust Few-Shot Classification in the Era of Foundation Models (Yanting Wang et al., 2024)

{{<citation>}}

Yanting Wang, Wei Zou, Jinyuan Jia. (2024)  
**FCert: Certifiably Robust Few-Shot Classification in the Era of Foundation Models**
<br/>
<button class="copy-to-clipboard" title="FCert: Certifiably Robust Few-Shot Classification in the Era of Foundation Models" index=89>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 53  
Keywords: Benchmarking, Few-shot, Foundation Model, Supervised Learning, Supervised Learning, PaLM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08631v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08631v1.pdf" filename="2404.08631v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Few-shot</b> classification with <b>foundation</b> <b>models</b> (e.g., CLIP, DINOv2, <b>PaLM-2)</b> enables users to build an accurate classifier with a few labeled training samples (called support samples) for a classification task. However, an attacker could perform data poisoning attacks by manipulating some support samples such that the classifier makes the attacker-desired, arbitrary prediction for a testing input. Empirical defenses cannot provide formal robustness guarantees, leading to a cat-and-mouse game between the attacker and defender. Existing certified defenses are designed for traditional <b>supervised</b> <b>learning,</b> resulting in sub-optimal performance when extended to <b>few-shot</b> classification. In our work, we propose FCert, the first certified defense against data poisoning attacks to <b>few-shot</b> classification. We show our FCert provably predicts the same label for a testing input under arbitrary data poisoning attacks when the total number of poisoned support samples is bounded. We perform extensive experiments on <b>benchmark</b> <b>few-shot</b> classification datasets with <b>foundation</b> <b>models</b> released by OpenAI, Meta, and Google in both vision and text domains. Our experimental results show our FCert: 1) maintains classification accuracy without attacks, 2) outperforms existing state-of-the-art certified defenses for data poisoning attacks, and 3) is efficient and general.

{{</citation>}}


### (3/12 | 90/235) JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models (Yingchaojie Feng et al., 2024)

{{<citation>}}

Yingchaojie Feng, Zhizhang Chen, Zhining Kang, Sijia Wang, Minfeng Zhu, Wei Zhang, Wei Chen. (2024)  
**JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models**
<br/>
<button class="copy-to-clipboard" title="JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models" index=90>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CL, cs-CR, cs-HC, cs.CR  
Keyword Score: 40  
Keywords: Large Language Model, Large Language Model, Prompt, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08793v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08793v1.pdf" filename="2404.08793v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The proliferation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has underscored concerns regarding their <b>security</b> vulnerabilities, notably against jailbreak attacks, where adversaries design jailbreak <b>prompts</b> to circumvent safety mechanisms for potential misuse. Addressing these concerns necessitates a comprehensive analysis of jailbreak <b>prompts</b> to evaluate <b>LLMs'</b> defensive capabilities and identify potential weaknesses. However, the complexity of evaluating jailbreak performance and understanding <b>prompt</b> characteristics makes this analysis laborious. We collaborate with domain experts to characterize problems and propose an <b>LLM-assisted</b> framework to streamline the analysis process. It provides automatic jailbreak assessment to facilitate performance evaluation and support analysis of components and keywords in <b>prompts.</b> Based on the framework, we design JailbreakLens, a visual analysis system that enables users to explore the jailbreak performance against the target model, conduct multi-level analysis of <b>prompt</b> characteristics, and refine <b>prompt</b> instances to verify findings. Through a case study, technical evaluations, and expert interviews, we demonstrate our system's effectiveness in helping users evaluate model <b>security</b> and identify model weaknesses.

{{</citation>}}


### (4/12 | 91/235) Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts (Tianyu Zhang et al., 2024)

{{<citation>}}

Tianyu Zhang, Zixuan Zhao, Jiaqi Huang, Jingyu Hua, Sheng Zhong. (2024)  
**Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts**
<br/>
<button class="copy-to-clipboard" title="Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts" index=91>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-AI, cs-CL, cs-CR, cs.CR  
Keyword Score: 40  
Keywords: Large Language Model, Large Language Model, Prompt, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08309v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08309v1.pdf" filename="2404.08309v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> of <b>Prompt</b> Jailbreaking are getting more and more attention, it is of great significance to raise a generalized research paradigm to evaluate attack strengths and a basic model to conduct subtler experiments. In this paper, we propose a novel approach by focusing on a set of target questions that are inherently more sensitive to jailbreak <b>prompts,</b> aiming to circumvent the limitations posed by enhanced <b>LLM</b> <b>security.</b> Through designing and analyzing these sensitive questions, this paper reveals a more effective method of identifying vulnerabilities in <b>LLMs,</b> thereby contributing to the advancement of <b>LLM</b> <b>security.</b> This research not only challenges existing jailbreaking methodologies but also fortifies <b>LLMs</b> against potential exploits.

{{</citation>}}


### (5/12 | 92/235) Optimizing Malware Detection in IoT Networks: Leveraging Resource-Aware Distributed Computing for Enhanced Security (Sreenitha Kasarapu et al., 2024)

{{<citation>}}

Sreenitha Kasarapu, Sanket Shukla, Sai Manoj Pudukotai Dinakarrao. (2024)  
**Optimizing Malware Detection in IoT Networks: Leveraging Resource-Aware Distributed Computing for Enhanced Security**
<br/>
<button class="copy-to-clipboard" title="Optimizing Malware Detection in IoT Networks: Leveraging Resource-Aware Distributed Computing for Enhanced Security" index=92>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-DC, cs.CR  
Keyword Score: 20  
Keywords: Malware, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.10012v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.10012v1.pdf" filename="2404.10012v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, networked IoT systems have revo- lutionized connectivity, portability, and functionality, offering a myriad of advantages. However, these systems are increasingly targeted by adversaries due to inherent <b>security</b> vulnerabilities and limited computational and storage resources. Malicious applications, commonly known as <b>malware,</b> pose a significant threat to IoT devices and networks. While numerous <b>malware</b> detection techniques have been proposed, existing approaches often overlook the resource constraints inherent in IoT environ- ments, assuming abundant resources for detection tasks. This oversight is compounded by ongoing workloads such as sens- ing and on-device computations, further diminishing available resources for <b>malware</b> detection. To address these challenges, we present a novel resource- and workload-aware <b>malware</b> detection framework integrated with distributed computing for IoT networks. Our approach begins by analyzing available resources for <b>malware</b> detection using a lightweight regression model. Depending on resource availability, ongoing workload executions, and communication costs, the <b>malware</b> detection task is dynamically allocated either on-device or offloaded to neighboring IoT nodes with sufficient resources. To safeguard data integrity and user privacy, rather than transferring the entire <b>malware</b> detection task, the classifier is partitioned and distributed across multiple nodes, and subsequently integrated at the parent node for comprehensive <b>malware</b> detection. Experimental analysis demonstrates the efficacy of our proposed technique, achieving a remarkable speed-up of 9.8x compared to on-device inference, while maintaining a high <b>malware</b> detection accuracy of 96.7%.

{{</citation>}}


### (6/12 | 93/235) Enhancing IoT Malware Detection through Adaptive Model Parallelism and Resource Optimization (Sreenitha Kasarapu et al., 2024)

{{<citation>}}

Sreenitha Kasarapu, Sanket Shukla, Sai Manoj Pudukotai Dinakarrao. (2024)  
**Enhancing IoT Malware Detection through Adaptive Model Parallelism and Resource Optimization**
<br/>
<button class="copy-to-clipboard" title="Enhancing IoT Malware Detection through Adaptive Model Parallelism and Resource Optimization" index=93>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-DC, cs.CR  
Keyword Score: 20  
Keywords: Malware, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08808v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08808v1.pdf" filename="2404.08808v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The widespread integration of IoT devices has greatly improved connectivity and computational capabilities, facilitating seamless communication across networks. Despite their global deployment, IoT devices are frequently targeted for <b>security</b> breaches due to inherent vulnerabilities. Among these threats, <b>malware</b> poses a significant risk to IoT devices. The lack of built-in <b>security</b> features and limited resources present challenges for implementing effective <b>malware</b> detection techniques on IoT devices. Moreover, existing methods assume access to all device resources for <b>malware</b> detection, which is often not feasible for IoT devices deployed in critical real-world scenarios. To overcome this challenge, this study introduces a novel approach to <b>malware</b> detection tailored for IoT devices, leveraging resource and workload awareness inspired by model parallelism. Initially, the device assesses available resources for <b>malware</b> detection using a lightweight regression model. Based on resource availability, ongoing workload, and communication costs, the <b>malware</b> detection task is dynamically allocated either on-device or offloaded to neighboring IoT nodes with sufficient resources. To uphold data integrity and user privacy, instead of transferring the entire <b>malware</b> detection task, the classifier is divided and distributed across multiple nodes, then integrated at the parent node for detection. Experimental results demonstrate that this proposed technique achieves a significant speedup of 9.8 x compared to on-device inference, while maintaining a high <b>malware</b> detection accuracy of 96.7%.

{{</citation>}}


### (7/12 | 94/235) Navigating Quantum Security Risks in Networked Environments: A Comprehensive Study of Quantum-Safe Network Protocols (Yaser Baseri et al., 2024)

{{<citation>}}

Yaser Baseri, Vikas Chouhan, Abdelhakim Hafid. (2024)  
**Navigating Quantum Security Risks in Networked Environments: A Comprehensive Study of Quantum-Safe Network Protocols**
<br/>
<button class="copy-to-clipboard" title="Navigating Quantum Security Risks in Networked Environments: A Comprehensive Study of Quantum-Safe Network Protocols" index=94>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 20  
Keywords: Stemming, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08232v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08232v1.pdf" filename="2404.08232v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The emergence of quantum computing poses a formidable <b>security</b> challenge to network protocols traditionally safeguarded by classical cryptographic algorithms. This paper provides an exhaustive analysis of vulnerabilities introduced by quantum computing in a diverse array of widely utilized <b>security</b> protocols across the layers of the TCP/IP model, including TLS, IPsec, SSH, PGP, and more. Our investigation focuses on precisely identifying vulnerabilities susceptible to exploitation by quantum adversaries at various migration stages for each protocol while also assessing the associated risks and consequences for secure communication. We delve deep into the impact of quantum computing on each protocol, emphasizing potential threats posed by quantum attacks and scrutinizing the effectiveness of post-quantum cryptographic solutions. Through carefully evaluating vulnerabilities and risks that network protocols face in the post-quantum era, this study provides invaluable insights to guide the development of appropriate countermeasures. Our findings contribute to a broader comprehension of quantum computing's influence on network <b>security</b> and offer practical guidance for protocol designers, implementers, and policymakers in addressing the challenges <b>stemming</b> from the advancement of quantum computing. This comprehensive study is a crucial step toward fortifying the <b>security</b> of networked environments in the quantum age.

{{</citation>}}


### (8/12 | 95/235) Identification of a replicable optical security element using laser speckle (A. M. Smolovich et al., 2024)

{{<citation>}}

A. M. Smolovich, A. V. Frolov, L. D. Klebanov, I. D. Laktaev, A. P. Orlov, P. A. Smolovich, O. V. Butov. (2024)  
**Identification of a replicable optical security element using laser speckle**
<br/>
<button class="copy-to-clipboard" title="Identification of a replicable optical security element using laser speckle" index=95>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR, physics-optics  
Keyword Score: 10  
Keywords: Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08723v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08723v1.pdf" filename="2404.08723v1.pdf">Download PDF</button>

---


**ABSTRACT**  
An optical <b>security</b> element containing an area of random rough relief is proposed. It combines the low cost of mass replication inherent in traditional <b>security</b> holograms with the impossibility of holographic copying, when the wave restored by the hologram is rewritten as a copy of this hologram. The proposed optical element is also protected from contact and photographic copying. Laboratory samples of optical elements were obtained by taking replicas of a rough surface. Identification of the authenticity of optical elements was demonstrated by calculating the cross-correlation of speckle patterns produced by coherent light scattered off different replicas. It is assumed that the proposed <b>security</b> elements can be mass-produced on standard equipment for embossing <b>security</b> holograms.

{{</citation>}}


### (9/12 | 96/235) Manifest V3 Unveiled: Navigating the New Era of Browser Extensions (Nikolaos Pantelaios et al., 2024)

{{<citation>}}

Nikolaos Pantelaios, Alexandros Kapravelos. (2024)  
**Manifest V3 Unveiled: Navigating the New Era of Browser Extensions**
<br/>
<button class="copy-to-clipboard" title="Manifest V3 Unveiled: Navigating the New Era of Browser Extensions" index=96>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 10  
Keywords: Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08310v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08310v1.pdf" filename="2404.08310v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Introduced over a decade ago, Chrome extensions now exceed 200,000 in number. In 2020, Google announced a shift in extension development with Manifest Version 3 (V3), aiming to replace the previous Version 2 (V2) by January 2023. This deadline was later extended to January 2025. The company's decision is grounded in enhancing three main pillars: privacy, <b>security,</b> and performance. This paper presents a comprehensive analysis of the Manifest V3 ecosystem. We start by investigating the adoption rate of V3, detailing the percentage of adoption from its announcement up until 2024. Our findings indicate, prior to the 2023 pause, less than 5% of all extensions had transitioned to V3, despite the looming deadline for the complete removal of V2, while currently nine out of ten new extensions are being uploaded in Manifest V3. Furthermore, we compare the <b>security</b> and privacy enhancements between V2 and V3 and we evaluate the improved <b>security</b> attributable to V3's safer APIs, examining how certain APIs, which were vulnerable or facilitated malicious behavior, have been deprecated or removed in V3. We dynamically execute 517 confirmed malicious extensions and we see a 87.8% removal of APIs related to malicious behavior due to the improvements of V3. We discover that only 154 (29.8%) of these extensions remain functional post-conversion. This analysis leads to the conclusion that V3 reduces the avenues for abuse of such APIs. However, despite the reduction in APIs associated with malicious activities, the new Manifest V3 protocol is not immune to such behavior. Our research demonstrates, through a proof of concept, the adaptability of malicious activities to V3. After the proof of concept changes are applied, we showcase 290 (56%) of the examined malicious extensions retain their capability to conduct harmful activities within the V3 framework.

{{</citation>}}


### (10/12 | 97/235) Securing Monolithic Kernels using Compartmentalization (Soo Yee Lim et al., 2024)

{{<citation>}}

Soo Yee Lim, Sidhartha Agrawal, Xueyuan Han, David Eyers, Dan O'Keeffe, Thomas Pasquier. (2024)  
**Securing Monolithic Kernels using Compartmentalization**
<br/>
<button class="copy-to-clipboard" title="Securing Monolithic Kernels using Compartmentalization" index=97>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-OS, cs.CR  
Keyword Score: 10  
Keywords: Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08716v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08716v1.pdf" filename="2404.08716v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Monolithic operating systems, where all kernel functionality resides in a single, shared address space, are the foundation of most mainstream computer systems. However, a single flaw, even in a non-essential part of the kernel (e.g., device drivers), can cause the entire operating system to fall under an attacker's control. Kernel hardening techniques might prevent certain types of vulnerabilities, but they fail to address a fundamental weakness: the lack of intra-kernel <b>security</b> that safely isolates different parts of the kernel. We survey kernel compartmentalization techniques that define and enforce intra-kernel boundaries and propose a taxonomy that allows the community to compare and discuss future work. We also identify factors that complicate comparisons among compartmentalized systems, suggest new ways to compare future approaches with existing work meaningfully, and discuss emerging research directions.

{{</citation>}}


### (11/12 | 98/235) Evaluation Framework for Quantum Security Risk Assessment: A Comprehensive Study for Quantum-Safe Migration (Yaser Baseri et al., 2024)

{{<citation>}}

Yaser Baseri, Vikas Chouhan, Ali Ghorbani, Aaron Chow. (2024)  
**Evaluation Framework for Quantum Security Risk Assessment: A Comprehensive Study for Quantum-Safe Migration**
<br/>
<button class="copy-to-clipboard" title="Evaluation Framework for Quantum Security Risk Assessment: A Comprehensive Study for Quantum-Safe Migration" index=98>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 10  
Keywords: Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08231v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08231v1.pdf" filename="2404.08231v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rise of large-scale quantum computing poses a significant threat to traditional cryptographic <b>security</b> measures. Quantum attacks undermine current asymmetric cryptographic algorithms, rendering them ineffective. Even symmetric key cryptography is vulnerable, albeit to a lesser extent, suggesting longer keys or extended hash functions for <b>security.</b> Thus, current cryptographic solutions are inadequate against emerging quantum threats. Organizations must transition to quantum-safe environments with robust continuity plans and meticulous risk management. This study explores the challenges of migrating to quantum-safe cryptographic states, introducing a comprehensive <b>security</b> risk assessment framework. We propose a <b>security</b> risk assessment framework that examines vulnerabilities across algorithms, certificates, and protocols throughout the migration process (pre-migration, during migration, post-migration). We link these vulnerabilities to the STRIDE threat model to assess their impact and likelihood. Then, we discuss practical mitigation strategies for critical components like algorithms, public key infrastructures, and protocols. Our study not only identifies potential attacks and vulnerabilities at each layer and migration stage but also suggests possible countermeasures and alternatives to enhance system resilience, empowering organizations to construct a secure infrastructure for the quantum era. Through these efforts, we establish the foundation for enduring <b>security</b> in networked systems amid the challenges of the quantum era.

{{</citation>}}


### (12/12 | 99/235) Lightweight Cryptanalysis of IoT Encryption Algorithms : Is Quota Sampling the Answer? (Jonathan Cook et al., 2024)

{{<citation>}}

Jonathan Cook, Sabih ur Rehman, M. Arif Khan. (2024)  
**Lightweight Cryptanalysis of IoT Encryption Algorithms : Is Quota Sampling the Answer?**
<br/>
<button class="copy-to-clipboard" title="Lightweight Cryptanalysis of IoT Encryption Algorithms : Is Quota Sampling the Answer?" index=99>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs.CR  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08165v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08165v1.pdf" filename="2404.08165v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Rapid growth in the number of small sensor devices known as the Internet of Things (IoT) has seen the development of lightweight encryption algorithms. Two well-known lightweight algorithms are SIMON and SIMECK which have been specifically designed for use on resource-constrained IoT devices. These lightweight encryption algorithms are based on the efficient Feistel block structure which is known to exhibit vulnerabilities to differential cryptanalysis. Consequently, it is necessary to test these algorithms for resilience against such attacks. While existing state-of-the-art research has demonstrated novel heuristic methods of differential cryptanalysis that improve time efficiency on previous techniques, the large state sizes of these encryption algorithms inhibit cryptanalysis time efficiency. In this paper, we introduce Versatile Investigative Sampling Technique for Advanced Cryptanalysis (VISTA-CRYPT) - a time-efficient enhancement of differential cryptanalysis of lightweight encryption algorithms. The proposed technique introduces a simple framework of quota sampling that produces state-of-the-art results with time reductions of up to $76\%$ over existing techniques. Further, we present a preliminary <b>graph-based</b> analysis of the output differentials for the identification of relationships within the data and future research opportunities to further enhance the performance of differential cryptanalysis. The code designed for this work and associated datasets will be available at https://github.com/johncook1979/simon-cryptanalysis.

{{</citation>}}


## cs.LG (36)



### (1/36 | 100/235) Reducing hallucination in structured outputs via Retrieval-Augmented Generation (Patrice Béchard et al., 2024)

{{<citation>}}

Patrice Béchard, Orlando Marquez Ayala. (2024)  
**Reducing hallucination in structured outputs via Retrieval-Augmented Generation**
<br/>
<button class="copy-to-clipboard" title="Reducing hallucination in structured outputs via Retrieval-Augmented Generation" index=100>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.LG  
Keyword Score: 70  
Keywords: Generative AI, Out-of-domain, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08189v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08189v1.pdf" filename="2404.08189v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A common and fundamental limitation of <b>Generative</b> <b>AI</b> (GenAI) is its propensity to hallucinate. While <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of <b>RAG,</b> our proposed system significantly reduces hallucinations in the output and improves the generalization of our <b>LLM</b> in <b>out-of-domain</b> settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying <b>LLM,</b> thereby making deployments of <b>LLM-based</b> systems less resource-intensive.

{{</citation>}}


### (2/36 | 101/235) Dataset Reset Policy Optimization for RLHF (Jonathan D. Chang et al., 2024)

{{<citation>}}

Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Kianté Brantley, Dipendra Misra, Jason D. Lee, Wen Sun. (2024)  
**Dataset Reset Policy Optimization for RLHF**
<br/>
<button class="copy-to-clipboard" title="Dataset Reset Policy Optimization for RLHF" index=101>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 60  
Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, GPT, GPT-4, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08495v3" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08495v3.pdf" filename="2404.08495v3.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Reinforcement</b> <b>Learning</b> (RL) from Human Preference-based feedback is a popular paradigm for <b>fine-tuning</b> generative models, which has produced impressive models such as <b>GPT-4</b> and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new <b>RLHF</b> algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR-PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR <b>summarization</b> and the Anthropic Helpful Harmful (HH) dataset, the generation from DR-PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of <b>GPT4</b> win-rate. Code for this work can be found at https://github.com/Cornell-RL/drpo.

{{</citation>}}


### (3/36 | 102/235) TSLANet: Rethinking Transformers for Time Series Representation Learning (Emadeldeen Eldele et al., 2024)

{{<citation>}}

Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Xiaoli Li. (2024)  
**TSLANet: Rethinking Transformers for Time Series Representation Learning**
<br/>
<button class="copy-to-clipboard" title="TSLANet: Rethinking Transformers for Time Series Representation Learning" index=102>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 55  
Keywords: Anomaly Detection, Convolution, Representation Learning, Self-supervised Learning, Self-supervised Learning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08472v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08472v1.pdf" filename="2404.08472v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Time series data, characterized by its intrinsic long and short-range dependencies, poses a unique challenge across analytical applications. While <b>Transformer-based</b> models excel at capturing long-range dependencies, they face limitations in noise sensitivity, computational efficiency, and overfitting with smaller datasets. In response, we introduce a novel Time Series Lightweight Adaptive Network (TSLANet), as a universal <b>convolutional</b> model for diverse time series tasks. Specifically, we propose an Adaptive Spectral Block, harnessing Fourier analysis to enhance feature <b>representation</b> <b>and</b> to capture both long-term and short-term interactions while mitigating noise via adaptive thresholding. Additionally, we introduce an Interactive <b>Convolution</b> Block and leverage <b>self-supervised</b> <b>learning</b> to refine the capacity of TSLANet for decoding complex temporal patterns and improve its robustness on different datasets. Our comprehensive experiments demonstrate that TSLANet outperforms state-of-the-art models in various tasks spanning classification, forecasting, and <b>anomaly</b> <b>detection,</b> showcasing its resilience and adaptability across a spectrum of noise levels and data sizes. The code is available at \url{https://github.com/emadeldeen24/TSLANet}

{{</citation>}}


### (4/36 | 103/235) HCL-MTSAD: Hierarchical Contrastive Consistency Learning for Accurate Detection of Industrial Multivariate Time Series Anomalies (Haili Sun et al., 2024)

{{<citation>}}

Haili Sun, Yan Huang, Lansheng Han, Cai Fu, Chunjie Zhou. (2024)  
**HCL-MTSAD: Hierarchical Contrastive Consistency Learning for Accurate Detection of Industrial Multivariate Time Series Anomalies**
<br/>
<button class="copy-to-clipboard" title="HCL-MTSAD: Hierarchical Contrastive Consistency Learning for Accurate Detection of Industrial Multivariate Time Series Anomalies" index=103>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CR, cs-IT, cs-LG, cs-SY, cs.LG, eess-SY, math-IT  
Keyword Score: 53  
Keywords: Anomaly Detection, Benchmarking, Contrastive Learning, Self-supervised Learning, Neural Machine Translation, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08224v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08224v1.pdf" filename="2404.08224v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Multivariate Time Series <b>(MTS)</b> <b>anomaly</b> <b>detection</b> focuses on pinpointing samples that diverge from standard operational patterns, which is crucial for ensuring the safety and <b>security</b> of industrial applications. The primary challenge in this domain is to develop representations capable of discerning anomalies effectively. The prevalent methods for <b>anomaly</b> <b>detection</b> in the literature are predominantly reconstruction-based and predictive in nature. However, they typically concentrate on a single-dimensional instance level, thereby not fully harnessing the complex associations inherent in industrial <b>MTS.</b> To address this issue, we propose a novel <b>self-supervised</b> hierarchical <b>contrastive</b> <b>consistency</b> learning method for detecting anomalies in <b>MTS,</b> named HCL-MTSAD. It innovatively leverages data consistency at multiple levels inherent in industrial <b>MTS,</b> systematically capturing consistent associations across four latent levels-measurement, sample, channel, and process. By developing a multi-layer <b>contrastive</b> <b>loss,</b> HCL-MTSAD can extensively mine data consistency and spatio-temporal association, resulting in more informative representations. Subsequently, an <b>anomaly</b> <b>discrimination</b> module, grounded in <b>self-supervised</b> hierarchical <b>contrastive</b> <b>learning,</b> is designed to detect timestamp-level anomalies by calculating multi-scale data consistency. Extensive experiments conducted on six diverse <b>MTS</b> datasets retrieved from real cyber-physical systems and server machines, in comparison with 20 baselines, indicate that HCL-MTSAD's <b>anomaly</b> <b>detection</b> capability outperforms the state-of-the-art <b>benchmark</b> models by an average of 1.8\% in terms of F1 score.

{{</citation>}}


### (5/36 | 104/235) The Illusion of State in State-Space Models (William Merrill et al., 2024)

{{<citation>}}

William Merrill, Jackson Petty, Ashish Sabharwal. (2024)  
**The Illusion of State in State-Space Models**
<br/>
<button class="copy-to-clipboard" title="The Illusion of State in State-Space Models" index=104>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CC, cs-CL, cs-FL, cs-LG, cs.LG  
Keyword Score: 50  
Keywords: Recurrent Neural Network, Recurrent Neural Network, Transformer, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08819v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08819v1.pdf" filename="2404.08819v1.pdf">Download PDF</button>

---


**ABSTRACT**  
State-space models (SSMs) have emerged as a potential alternative architecture for building <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> compared to the previously ubiquitous <b>transformer</b> architecture. One theoretical weakness of <b>transformers</b> is that they cannot express certain kinds of sequential computation and state tracking (Merrill and Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to <b>recurrent</b> <b>neural</b> <b>networks</b> <b>(RNNs).</b> But do SSMs truly have an advantage (over <b>transformers)</b> in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to <b>transformers:</b> SSMs cannot express computation outside the complexity class $\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its <b>recurrent</b> <b>formulation,</b> <b>the</b> "state" in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like <b>transformers,</b> which may fundamentally limit their ability to solve real-world state-tracking problems.

{{</citation>}}


### (6/36 | 105/235) CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models (Je-Yong Lee et al., 2024)

{{<citation>}}

Je-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, Azalia Mirhoseini. (2024)  
**CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models**
<br/>
<button class="copy-to-clipboard" title="CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models" index=105>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG  
Keyword Score: 50  
Keywords: Fine-tuning, LLaMA, Mistral, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08763v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08763v1.pdf" filename="2404.08763v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have dramatically advanced AI applications, yet their deployment remains challenging due to their immense inference costs. Recent studies ameliorate the computational costs of <b>LLMs</b> by increasing their activation sparsity but suffer from significant performance degradation on downstream tasks. In this work, we introduce a new framework for sparsifying the activations of base <b>LLMs</b> and reducing inference costs, dubbed Contextually Aware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to implement, and highly effective. At the heart of our framework is a new non-linear activation function. We demonstrate that CATS can be applied to various base models, including <b>Mistral-7B</b> and Llama2-7B, and outperforms existing sparsification techniques in downstream task performance. More precisely, CATS-based models often achieve downstream task performance within 1-2% of their base models without any <b>fine-tuning</b> and even at activation sparsity levels of 50%. Furthermore, CATS-based models converge faster and display better task performance than competing techniques when <b>fine-tuning</b> is applied. Finally, we develop a custom GPU kernel for efficient implementation of CATS that translates the activation of sparsity of CATS to real wall-clock time speedups. Our custom kernel implementation of CATS results in a ~15% improvement in wall-clock inference latency of token generation on both <b>Llama-7B</b> and <b>Mistral-7B.</b>

{{</citation>}}


### (7/36 | 106/235) RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs (Shreyas Chaudhari et al., 2024)

{{<citation>}}

Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande, Bruno Castro da Silva. (2024)  
**RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs**
<br/>
<button class="copy-to-clipboard" title="RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs" index=106>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 50  
Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08555v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08555v2.pdf" filename="2404.08555v2.pdf">Download PDF</button>

---


**ABSTRACT**  
State-of-the-art <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become indispensable tools for various tasks. However, training <b>LLMs</b> to serve as effective assistants for humans requires careful consideration. A promising approach is <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF),</b> which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Yet, an understanding of <b>RLHF</b> for <b>LLMs</b> is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework. In this paper, we analyze <b>RLHF</b> through the lens of <b>reinforcement</b> <b>learning</b> <b>principles</b> <b>to</b> <b>develop</b> an understanding of its fundamentals, dedicating substantial focus to the core component of <b>RLHF</b> -- the reward model. Our study investigates modeling choices, caveats of function approximation, and their implications on <b>RLHF</b> training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. We characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of <b>RLHF</b> and build upon existing efforts.

{{</citation>}}


### (8/36 | 107/235) Experimental Design for Active Transductive Inference in Large Language Models (Subhojyoti Mukherjee et al., 2024)

{{<citation>}}

Subhojyoti Mukherjee, Ge Liu, Aniket Deshmukh, Anusha Lalitha, Yifei Ma, Branislav Kveton. (2024)  
**Experimental Design for Active Transductive Inference in Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Experimental Design for Active Transductive Inference in Large Language Models" index=107>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG  
Keyword Score: 40  
Keywords: Few-shot, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08846v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08846v1.pdf" filename="2404.08846v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Transduction, the ability to include query-specific examples in the <b>prompt</b> at inference time, is one of the emergent abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> In this work, we propose a framework for adaptive <b>prompt</b> design called active transductive inference (ATI). We design the <b>LLM</b> <b>prompt</b> by adaptively choosing <b>few-shot</b> examples for a given inference query. The examples are initially unlabeled and we query the user to label the most informative ones, which maximally reduces the uncertainty in the <b>LLM</b> prediction. We propose two algorithms, GO and SAL, which differ in how the <b>few-shot</b> examples are chosen. We analyze these algorithms in linear models: first GO and then use its equivalence with SAL. We experiment with many different tasks and show that GO and SAL outperform other methods for choosing <b>few-shot</b> examples in the <b>LLM</b> <b>prompt</b> at inference time.

{{</citation>}}


### (9/36 | 108/235) Transfer Learning Study of Motion Transformer-based Trajectory Predictions (Lars Ullrich et al., 2024)

{{<citation>}}

Lars Ullrich, Alex McMaster, Knut Graichen. (2024)  
**Transfer Learning Study of Motion Transformer-based Trajectory Predictions**
<br/>
<button class="copy-to-clipboard" title="Transfer Learning Study of Motion Transformer-based Trajectory Predictions" index=108>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-RO, cs.LG  
Keyword Score: 40  
Keywords: Simulation, Simulator, Transfer Learning, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08271v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08271v1.pdf" filename="2404.08271v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Trajectory planning in autonomous driving is highly dependent on predicting the emergent behavior of other road users. Learning-based methods are currently showing impressive results in <b>simulation-based</b> challenges, with <b>transformer-based</b> architectures technologically leading the way. Ultimately, however, predictions are needed in the real world. In addition to the shifts from <b>simulation</b> to the real world, many vehicle- and country-specific shifts, i.e. differences in sensor systems, fusion and perception algorithms as well as traffic rules and laws, are on the agenda. Since models that can cover all system setups and design domains at once are not yet foreseeable, model adaptation plays a central role. Therefore, a <b>simulation-based</b> study on <b>transfer</b> <b>learning</b> techniques is conducted on basis of a <b>transformer-based</b> model. Furthermore, the study aims to provide insights into possible trade-offs between computational time and performance to support effective <b>transfers</b> <b>into</b> the real world.

{{</citation>}}


### (10/36 | 109/235) Advancing Forest Fire Prevention: Deep Reinforcement Learning for Effective Firebreak Placement (Lucas Murray et al., 2024)

{{<citation>}}

Lucas Murray, Tatiana Castillo, Jaime Carrasco, Andrés Weintraub, Richard Weber, Isaac Martín de Diego, José Ramón González, Jordi García-Gonzalo. (2024)  
**Advancing Forest Fire Prevention: Deep Reinforcement Learning for Effective Firebreak Placement**
<br/>
<button class="copy-to-clipboard" title="Advancing Forest Fire Prevention: Deep Reinforcement Learning for Effective Firebreak Placement" index=109>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 35  
Keywords: Convolutional Neural Network, Convolution, Convolutional Neural Network, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08523v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08523v1.pdf" filename="2404.08523v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Over the past decades, the increase in both frequency and intensity of large-scale wildfires due to climate change has emerged as a significant natural threat. The pressing need to design resilient landscapes capable of withstanding such disasters has become paramount, requiring the development of advanced decision-support tools. Existing methodologies, including Mixed Integer Programming, Stochastic Optimization, and Network Theory, have proven effective but are hindered by computational demands, limiting their applicability. In response to this challenge, we propose using artificial intelligence techniques, specifically Deep <b>Reinforcement</b> <b>Learning,</b> to address the complex problem of firebreak placement in the landscape. We employ value-function based approaches like Deep Q-Learning, Double Deep Q-Learning, and Dueling Double Deep Q-Learning. Utilizing the Cell2Fire fire spread simulator combined with <b>Convolutional</b> <b>Neural</b> <b>Networks,</b> we have successfully implemented a computational agent capable of learning firebreak locations within a forest environment, achieving good results. Furthermore, we incorporate a pre-training loop, initially teaching our agent to mimic a heuristic-based algorithm and observe that it consistently exceeds the performance of these solutions. Our findings underscore the immense potential of Deep <b>Reinforcement</b> <b>Learning</b> for operational research challenges, especially in fire prevention. Our approach demonstrates convergence with highly favorable results in problem instances as large as 40 x 40 cells, marking a significant milestone in applying <b>Reinforcement</b> <b>Learning</b> to this critical issue. To the best of our knowledge, this study represents a pioneering effort in using <b>Reinforcement</b> <b>Learning</b> to address the aforementioned problem, offering promising perspectives in fire prevention and landscape management

{{</citation>}}


### (11/36 | 110/235) Training a Vision Language Model as Smartphone Assistant (Nicolai Dorka et al., 2024)

{{<citation>}}

Nicolai Dorka, Janusz Marecki, Ammar Anwar. (2024)  
**Training a Vision Language Model as Smartphone Assistant**
<br/>
<button class="copy-to-clipboard" title="Training a Vision Language Model as Smartphone Assistant" index=110>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CV, cs-HC, cs-LG, cs.LG  
Keyword Score: 33  
Keywords: Benchmarking, Large Language Model, Large Language Model, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08755v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08755v1.pdf" filename="2404.08755v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Addressing the challenge of a digital assistant capable of executing a wide array of user tasks, our research focuses on the realm of instruction-based mobile device control. We leverage recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and present a visual language model (VLM) that can fulfill diverse tasks on mobile devices. Our model functions by interacting solely with the user interface (UI). It uses the visual input from the device screen and mimics human-like interactions, encompassing gestures such as tapping and swiping. This generality in the input and output space allows our agent to interact with any application on the device. Unlike previous methods, our model operates not only on a single screen image but on <b>vision-language</b> sentences created from sequences of past screenshots along with corresponding actions. Evaluating our method on the challenging Android in the Wild <b>benchmark</b> demonstrates its promising efficacy and potential.

{{</citation>}}


### (12/36 | 111/235) Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length (Xuezhe Ma et al., 2024)

{{<citation>}}

Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou. (2024)  
**Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length**
<br/>
<button class="copy-to-clipboard" title="Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length" index=111>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Graph Attention Networks, Transformer, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08801v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08801v2.pdf" filename="2404.08801v2.pdf">Download PDF</button>

---


**ABSTRACT**  
The quadratic complexity and weak length extrapolation of <b>Transformers</b> limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform <b>Transformers</b> in pretraining efficiency and downstream task accuracy. We introduce Megalodon, a neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with <b>gated</b> attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than <b>Transformer</b> in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code: https://github.com/XuezheMax/megalodon

{{</citation>}}


### (13/36 | 112/235) An improved tabular data generator with VAE-GMM integration (Patricia A. Apellániz et al., 2024)

{{<citation>}}

Patricia A. Apellániz, Juan Parras, Santiago Zazo. (2024)  
**An improved tabular data generator with VAE-GMM integration**
<br/>
<button class="copy-to-clipboard" title="An improved tabular data generator with VAE-GMM integration" index=112>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: I-2-1, cs-AI, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Autoencoder, Generative Adversarial Network, Variational Autoencoder  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08434v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08434v1.pdf" filename="2404.08434v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rising use of machine learning in various fields requires robust methods to create synthetic tabular data. Data should preserve key characteristics while addressing data scarcity challenges. Current approaches based on <b>Generative</b> <b>Adversarial</b> <b>Networks,</b> such as the state-of-the-art CTGAN model, struggle with the complex structures inherent in tabular data. These data often contain both continuous and discrete features with non-Gaussian distributions. Therefore, we propose a novel <b>Variational</b> <b>Autoencoder</b> (VAE)-based model that addresses these limitations. Inspired by the TVAE model, our approach incorporates a Bayesian Gaussian Mixture model (BGM) within the VAE architecture. This avoids the limitations imposed by assuming a strictly Gaussian latent space, allowing for a more accurate representation of the underlying data distribution during data generation. Furthermore, our model offers enhanced flexibility by allowing the use of various differentiable distributions for individual features, making it possible to handle both continuous and discrete data types. We thoroughly validate our model on three real-world datasets with mixed data types, including two medically relevant ones, based on their resemblance and utility. This evaluation demonstrates significant outperformance against CTGAN and TVAE, establishing its potential as a valuable tool for generating synthetic tabular data in various domains, particularly in healthcare.

{{</citation>}}


### (14/36 | 113/235) AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control Guarantees (William Fleshman et al., 2024)

{{<citation>}}

William Fleshman, Aleem Khan, Marc Marone, Benjamin Van Durme. (2024)  
**AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control Guarantees**
<br/>
<button class="copy-to-clipboard" title="AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control Guarantees" index=113>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Continual Learning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08417v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08417v1.pdf" filename="2404.08417v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are increasingly capable of completing knowledge intensive tasks by recalling information from a static pretraining corpus. Here we are concerned with <b>LLMs</b> in the context of evolving data requirements. For instance: batches of new data that are introduced periodically; subsets of data with user-based access controls; or requirements on dynamic removal of documents with guarantees that associated knowledge cannot be recalled. We wish to satisfy these requirements while at the same time ensuring a model does not forget old information when new data becomes available. To address these issues, we introduce AdapterSwap, a training and inference scheme that organizes knowledge from a data collection into a set of low-rank adapters, which are dynamically composed during inference. Our experiments demonstrate AdapterSwap's ability to support efficient <b>continual</b> <b>learning,</b> while also enabling organizations to have fine-grained control over data access and deletion.

{{</citation>}}


### (15/36 | 114/235) Exploring Contrastive Learning for Long-Tailed Multi-Label Text Classification (Alexandre Audibert et al., 2024)

{{<citation>}}

Alexandre Audibert, Aurélien Gauffre, Massih-Reza Amini. (2024)  
**Exploring Contrastive Learning for Long-Tailed Multi-Label Text Classification**
<br/>
<button class="copy-to-clipboard" title="Exploring Contrastive Learning for Long-Tailed Multi-Label Text Classification" index=114>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-IR, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Contrastive Learning, Supervised Learning, Text Classification  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08720v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08720v1.pdf" filename="2404.08720v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Learning an effective representation in multi-label <b>text</b> <b>classification</b> (MLTC) is a significant challenge in NLP. This challenge arises from the inherent complexity of the task, which is shaped by two key factors: the intricate connections between labels and the widespread long-tailed distribution of the data. To overcome this issue, one potential approach involves integrating <b>supervised</b> <b>contrastive</b> <b>learning</b> with classical <b>supervised</b> loss functions. Although <b>contrastive</b> <b>learning</b> has shown remarkable performance in multi-class classification, its impact in the multi-label framework has not been thoroughly investigated. In this paper, we conduct an in-depth study of <b>supervised</b> <b>contrastive</b> <b>learning</b> and its influence on representation in MLTC context. We emphasize the importance of considering long-tailed data distributions to build a robust representation space, which effectively addresses two critical challenges associated with <b>contrastive</b> <b>learning</b> that we identify: the "lack of positives" and the "attraction-repulsion imbalance". Building on this insight, we introduce a novel <b>contrastive</b> <b>loss</b> function for MLTC. It attains Micro-F1 scores that either match or surpass those obtained with other frequently employed loss functions, and demonstrates a significant improvement in Macro-F1 scores across three multi-label datasets.

{{</citation>}}


### (16/36 | 115/235) FastLogAD: Log Anomaly Detection with Mask-Guided Pseudo Anomaly Generation and Discrimination (Yifei Lin et al., 2024)

{{<citation>}}

Yifei Lin, Hanqiu Deng, Xingyu Li. (2024)  
**FastLogAD: Log Anomaly Detection with Mask-Guided Pseudo Anomaly Generation and Discrimination**
<br/>
<button class="copy-to-clipboard" title="FastLogAD: Log Anomaly Detection with Mask-Guided Pseudo Anomaly Generation and Discrimination" index=115>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 23  
Keywords: Anomaly Detection, Benchmarking, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08750v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08750v1.pdf" filename="2404.08750v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Nowadays large computers extensively output logs to record the runtime status and it has become crucial to identify any suspicious or malicious activities from the information provided by the realtime logs. Thus, fast log <b>anomaly</b> <b>detection</b> is a necessary task to be implemented for automating the infeasible manual detection. Most of the existing <b>unsupervised</b> methods are trained only on normal log data, but they usually require either additional abnormal data for hyperparameter selection or auxiliary datasets for discriminative model optimization. In this paper, aiming for a highly effective discriminative model that enables rapid <b>anomaly</b> <b>detection,we</b> propose FastLogAD, a generator-discriminator framework trained to exhibit the capability of generating pseudo-abnormal logs through the Mask-Guided <b>Anomaly</b> <b>Generation</b> (MGAG) model and efficiently identifying the anomalous logs via the Discriminative Abnormality Separation (DAS) model. Particularly, pseudo-abnormal logs are generated by replacing randomly masked tokens in a normal sequence with unlikely candidates. During the discriminative stage, FastLogAD learns a distinct separation between normal and pseudoabnormal samples based on their embedding norms, allowing the selection of a threshold without exposure to any test data and achieving competitive performance. Extensive experiments on several common <b>benchmarks</b> show that our proposed FastLogAD outperforms existing <b>anomaly</b> <b>detection</b> approaches. Furthermore, compared to previous methods, FastLogAD achieves at least x10 speed increase in <b>anomaly</b> <b>detection</b> over prior work. Our implementation is available at https://github.com/YifeiLin0226/FastLogAD.

{{</citation>}}


### (17/36 | 116/235) Graph data augmentation with Gromow-Wasserstein Barycenters (Andrea Ponti, 2024)

{{<citation>}}

Andrea Ponti. (2024)  
**Graph data augmentation with Gromow-Wasserstein Barycenters**
<br/>
<button class="copy-to-clipboard" title="Graph data augmentation with Gromow-Wasserstein Barycenters" index=116>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 23  
Keywords: Graph Classification, Graph, Data Augmentation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08376v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08376v1.pdf" filename="2404.08376v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Graphs</b> <b>are</b> ubiquitous in various fields, and deep learning methods have been successful applied in <b>graph</b> <b>classification</b> tasks. However, building large and diverse <b>graph</b> <b>datasets</b> for training can be expensive. While augmentation techniques exist for structured <b>data</b> <b>like</b> images or numerical <b>data,</b> <b>the</b> augmentation of <b>graph</b> <b>data</b> <b>remains</b> challenging. This is primarily due to the complex and non-Euclidean nature of <b>graph</b> <b>data.</b> <b>In</b> this paper, it has been proposed a novel augmentation strategy for <b>graphs</b> <b>that</b> operates in a non-Euclidean space. This approach leverages graphon estimation, which models the generative mechanism of networks sequences. Computational results demonstrate the effectiveness of the proposed augmentation framework in improving the performance of <b>graph</b> <b>classification</b> models. Additionally, using a non-Euclidean distance, specifically the Gromow-Wasserstein distance, results in better approximations of the graphon. This framework also provides a means to validate different graphon estimation approaches, particularly in real-world scenarios where the true graphon is unknown.

{{</citation>}}


### (18/36 | 117/235) Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes (Ali Younis et al., 2024)

{{<citation>}}

Ali Younis, Erik Sudderth. (2024)  
**Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes**
<br/>
<button class="copy-to-clipboard" title="Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes" index=117>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs-RO, cs.LG  
Keyword Score: 21  
Keywords: Deep Neural Network, Multi-modal, Multi-modal, Recurrent Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08789v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08789v1.pdf" filename="2404.08789v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods. Such generative models may be inaccurate or unavailable for high-dimensional observations like images. We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via <b>deep</b> <b>neural</b> <b>network</b> encoders. While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities. Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard <b>recurrent</b> <b>neural</b> <b>networks,</b> our mixture density particle filter represents <b>multimodal</b> uncertainty in continuous latent states, improving accuracy and robustness. On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, while also showing much greater stability across multiple training runs.

{{</citation>}}


### (19/36 | 118/235) Generating Synthetic Time Series Data for Cyber-Physical Systems (Alexander Sommers et al., 2024)

{{<citation>}}

Alexander Sommers, Somayeh Bakhtiari Ramezani, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure. (2024)  
**Generating Synthetic Time Series Data for Cyber-Physical Systems**
<br/>
<button class="copy-to-clipboard" title="Generating Synthetic Time Series Data for Cyber-Physical Systems" index=118>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Data Augmentation, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08601v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08601v1.pdf" filename="2404.08601v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Data</b> <b>augmentation</b> is an important facilitator of deep learning applications in the time series domain. A gap is identified in the literature, demonstrating sparse exploration of the <b>transformer,</b> the dominant sequence model, for <b>data</b> <b>augmentation</b> in time series. A architecture hybridizing several successful priors is put forth and tested using a powerful time domain similarity metric. Results suggest the challenge of this domain, and several valuable directions for future work.

{{</citation>}}


### (20/36 | 119/235) Decoding AI: The inside story of data analysis in ChatGPT (Ozan Evkaya et al., 2024)

{{<citation>}}

Ozan Evkaya, Miguel de Carvalho. (2024)  
**Decoding AI: The inside story of data analysis in ChatGPT**
<br/>
<button class="copy-to-clipboard" title="Decoding AI: The inside story of data analysis in ChatGPT" index=119>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG, stat-CO  
Keyword Score: 20  
Keywords: Generative AI, ChatGPT  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08480v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08480v1.pdf" filename="2404.08480v1.pdf">Download PDF</button>

---


**ABSTRACT**  
As a result of recent advancements in <b>generative</b> <b>AI,</b> the field of Data Science is prone to various changes. This review critically examines the Data Analysis (DA) capabilities of <b>ChatGPT</b> assessing its performance across a wide range of tasks. While DA provides researchers and practitioners with unprecedented analytical capabilities, it is far from being perfect, and it is important to recognize and address its limitations.

{{</citation>}}


### (21/36 | 120/235) OTTER: Improving Zero-Shot Classification via Optimal Transport (Changho Shin et al., 2024)

{{<citation>}}

Changho Shin, Jitian Zhao, Sonia Cromp, Harit Vishwakarma, Frederic Sala. (2024)  
**OTTER: Improving Zero-Shot Classification via Optimal Transport**
<br/>
<button class="copy-to-clipboard" title="OTTER: Improving Zero-Shot Classification via Optimal Transport" index=120>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Zero-shot, Text Classification  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08461v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08461v1.pdf" filename="2404.08461v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Popular <b>zero-shot</b> models suffer due to artifacts inherited from pretraining. A particularly detrimental artifact, caused by unbalanced web-scale pretraining data, is mismatched label distribution. Existing approaches that seek to repair the label distribution are not suitable in <b>zero-shot</b> settings, as they have incompatible requirements such as access to labeled downstream task data or knowledge of the true label balance in the pretraining distribution. We sidestep these challenges and introduce a simple and lightweight approach to adjust pretrained model predictions via optimal transport. Our technique requires only an estimate of the label distribution of a downstream task. Theoretically, we characterize the improvement produced by our procedure under certain mild conditions and provide bounds on the error caused by misspecification. Empirically, we validate our method in a wide array of <b>zero-shot</b> image and <b>text</b> <b>classification</b> tasks, improving accuracy by 4.8% and 15.9% on average, and beating baselines like Prior Matching -- often by significant margins -- in 17 out of 21 datasets.

{{</citation>}}


### (22/36 | 121/235) Anti-Byzantine Attacks Enabled Vehicle Selection for Asynchronous Federated Learning in Vehicular Edge Computing (Cui Zhang et al., 2024)

{{<citation>}}

Cui Zhang, Xiao Xu, Qiong Wu, Pingyi Fan, Qiang Fan, Huiling Zhu, Jiangzhou Wang. (2024)  
**Anti-Byzantine Attacks Enabled Vehicle Selection for Asynchronous Federated Learning in Vehicular Edge Computing**
<br/>
<button class="copy-to-clipboard" title="Anti-Byzantine Attacks Enabled Vehicle Selection for Asynchronous Federated Learning in Vehicular Edge Computing" index=121>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Federated Learning, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08444v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08444v1.pdf" filename="2404.08444v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In vehicle edge computing (VEC), asynchronous <b>federated</b> <b>learning</b> (AFL) is used, where the edge receives a local model and updates the global model, effectively reducing the global aggregation latency.Due to different amounts of local data,computing capabilities and locations of the vehicles, renewing the global model with same weight is inappropriate.The above factors will affect the local calculation time and upload time of the local model, and the vehicle may also be affected by Byzantine attacks, leading to the deterioration of the vehicle data. However, based on deep <b>reinforcement</b> <b>learning</b> (DRL), we can consider these factors comprehensively to eliminate vehicles with poor performance as much as possible and exclude vehicles that have suffered Byzantine attacks before AFL. At the same time, when aggregating AFL, we can focus on those vehicles with better performance to improve the accuracy and safety of the system. In this paper, we proposed a vehicle selection scheme based on DRL in VEC. In this scheme, vehicle s mobility, channel conditions with temporal variations, computational resources with temporal variations, different data amount, transmission channel status of vehicles as well as Byzantine attacks were taken into account.Simulation results show that the proposed scheme effectively improves the safety and accuracy of the global model.

{{</citation>}}


### (23/36 | 122/235) Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models (Zeyu Yang et al., 2024)

{{<citation>}}

Zeyu Yang, Peikun Guo, Khadija Zanna, Akane Sano. (2024)  
**Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models" index=122>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Diffusion Model, Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08254v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08254v1.pdf" filename="2404.08254v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Diffusion</b> <b>models</b> have emerged as a robust framework for various generative tasks, such as image and audio synthesis, and have also demonstrated a remarkable ability to generate mixed-type tabular data comprising both continuous and discrete variables. However, current approaches to training <b>diffusion</b> <b>models</b> on mixed-type tabular data tend to inherit the imbalanced distributions of features present in the training dataset, which can result in biased sampling. In this research, we introduce a fair <b>diffusion</b> <b>model</b> designed to generate balanced data on sensitive attributes. We present empirical evidence demonstrating that our method effectively mitigates the class imbalance in training data while maintaining the quality of the generated samples. Furthermore, we provide evidence that our approach outperforms existing methods for synthesizing tabular data in terms of performance and <b>fairness.</b>

{{</citation>}}


### (24/36 | 123/235) Computing distances and means on manifolds with a metric-constrained Eikonal approach (Daniel Kelshaw et al., 2024)

{{<citation>}}

Daniel Kelshaw, Luca Magri. (2024)  
**Computing distances and means on manifolds with a metric-constrained Eikonal approach**
<br/>
<button class="copy-to-clipboard" title="Computing distances and means on manifolds with a metric-constrained Eikonal approach" index=123>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CG, cs-LG, cs.LG, math-MG  
Keyword Score: 13  
Keywords: Clustering, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08754v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08754v1.pdf" filename="2404.08754v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Computing distances on Riemannian manifolds is a challenging problem with numerous applications, from physics, through statistics, to machine learning. In this paper, we introduce the metric-constrained Eikonal solver to obtain continuous, differentiable representations of distance functions on manifolds. The differentiable nature of these representations allows for the direct computation of globally length-minimising paths on the manifold. We showcase the use of metric-constrained Eikonal solvers for a range of manifolds and demonstrate the applications. First, we demonstrate that metric-constrained Eikonal solvers can be used to obtain the Fr\'echet mean on a manifold, employing the definition of a Gaussian mixture model, which has an analytical solution to verify the numerical results. Second, we demonstrate how the obtained distance function can be used to conduct <b>unsupervised</b> <b>clustering</b> on the manifold -- a task for which existing approaches are computationally prohibitive. This work opens opportunities for distance computations on manifolds.

{{</citation>}}


### (25/36 | 124/235) Generalized Population-Based Training for Hyperparameter Optimization in Reinforcement Learning (Hui Bai et al., 2024)

{{<citation>}}

Hui Bai, Ran Cheng. (2024)  
**Generalized Population-Based Training for Hyperparameter Optimization in Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Generalized Population-Based Training for Hyperparameter Optimization in Reinforcement Learning" index=124>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs-NE, cs.LG  
Keyword Score: 13  
Keywords: Benchmarking, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08233v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08233v1.pdf" filename="2404.08233v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Hyperparameter optimization plays a key role in the machine learning domain. Its significance is especially pronounced in <b>reinforcement</b> <b>learning</b> (RL), where agents continuously interact with and adapt to their environments, requiring dynamic adjustments in their learning trajectories. To cater to this dynamicity, the Population-Based Training (PBT) was introduced, leveraging the collective intelligence of a population of agents learning simultaneously. However, PBT tends to favor high-performing agents, potentially neglecting the explorative potential of agents on the brink of significant advancements. To mitigate the limitations of PBT, we present the Generalized Population-Based Training (GPBT), a refined framework designed for enhanced granularity and flexibility in hyperparameter adaptation. Complementing GPBT, we further introduce Pairwise Learning (PL). Instead of merely focusing on elite agents, PL employs a comprehensive pairwise strategy to identify performance differentials and provide holistic guidance to underperforming agents. By integrating the capabilities of GPBT and PL, our approach significantly improves upon traditional PBT in terms of adaptability and computational efficiency. Rigorous empirical evaluations across a range of RL <b>benchmarks</b> confirm that our approach consistently outperforms not only the conventional PBT but also its Bayesian-optimized variant.

{{</citation>}}


### (26/36 | 125/235) Hindsight PRIORs for Reward Learning from Human Preferences (Mudit Verma et al., 2024)

{{<citation>}}

Mudit Verma, Katherine Metcalf. (2024)  
**Hindsight PRIORs for Reward Learning from Human Preferences**
<br/>
<button class="copy-to-clipboard" title="Hindsight PRIORs for Reward Learning from Human Preferences" index=125>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-HC, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08828v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08828v1.pdf" filename="2404.08828v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Preference based <b>Reinforcement</b> <b>Learning</b> (PbRL) removes the need to hand specify a reward function by learning a reward from preference feedback over policy behaviors. Current approaches to PbRL do not address the credit assignment problem inherent in determining which parts of a behavior most contributed to a preference, which result in data intensive approaches and subpar reward functions. We address such limitations by introducing a credit assignment strategy (Hindsight PRIOR) that uses a world model to approximate state importance within a trajectory and then guides rewards to be proportional to state importance through an auxiliary predicted return redistribution objective. Incorporating state importance into reward learning improves the speed of policy learning, overall policy performance, and reward recovery on both locomotion and manipulation tasks. For example, Hindsight PRIOR recovers on average significantly (p<0.05) more reward on MetaWorld (20%) and DMC (15%). The performance gains and our ablations demonstrate the benefits even a simple credit assignment strategy can have on reward learning and that state importance in forward dynamics prediction is a strong proxy for a state's contribution to a preference decision. Code repository can be found at https://github.com/apple/ml-rlhf-hindsight-prior.

{{</citation>}}


### (27/36 | 126/235) Beyond One-Size-Fits-All: Adapting Counterfactual Explanations to User Objectives (Orfeas Menis Mastromichalakis et al., 2024)

{{<citation>}}

Orfeas Menis Mastromichalakis, Jason Liartis, Giorgos Stamou. (2024)  
**Beyond One-Size-Fits-All: Adapting Counterfactual Explanations to User Objectives**
<br/>
<button class="copy-to-clipboard" title="Beyond One-Size-Fits-All: Adapting Counterfactual Explanations to User Objectives" index=126>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Counter-factual  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08721v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08721v1.pdf" filename="2404.08721v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Explainable Artificial Intelligence (XAI) has emerged as a critical area of research aimed at enhancing the transparency and interpretability of AI systems. <b>Counterfactual</b> Explanations (CFEs) offer valuable insights into the decision-making processes of machine learning algorithms by exploring alternative scenarios where certain factors differ. Despite the growing popularity of CFEs in the XAI community, existing literature often overlooks the diverse needs and objectives of users across different applications and domains, leading to a lack of tailored explanations that adequately address the different use cases. In this paper, we advocate for a nuanced understanding of CFEs, recognizing the variability in desired properties based on user objectives and target applications. We identify three primary user objectives and explore the desired characteristics of CFEs in each case. By addressing these differences, we aim to design more effective and tailored explanations that meet the specific needs of users, thereby enhancing collaboration with AI systems.

{{</citation>}}


### (28/36 | 127/235) Lightweight Multi-System Multivariate Interconnection and Divergence Discovery (Mulugeta Weldezgina Asres et al., 2024)

{{<citation>}}

Mulugeta Weldezgina Asres, Christian Walter Omlin, Jay Dittmann, Pavel Parygin, Joshua Hiltbrand, Seth I. Cooper, Grace Cummings, David Yu. (2024)  
**Lightweight Multi-System Multivariate Interconnection and Divergence Discovery**
<br/>
<button class="copy-to-clipboard" title="Lightweight Multi-System Multivariate Interconnection and Divergence Discovery" index=127>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs-SY, cs.LG, eess-SY  
Keyword Score: 10  
Keywords: Information Retrieval  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08453v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08453v1.pdf" filename="2404.08453v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Identifying outlier behavior among sensors and subsystems is essential for discovering faults and facilitating diagnostics in large systems. At the same time, exploring large systems with numerous multivariate data sets is challenging. This study presents a lightweight interconnection and divergence discovery mechanism (LIDD) to identify abnormal behavior in multi-system environments. The approach employs a multivariate analysis technique that first estimates the similarity heatmaps among the sensors for each system and then applies <b>information</b> <b>retrieval</b> algorithms to provide relevant multi-level interconnection and discrepancy details. Our experiment on the readout systems of the Hadron Calorimeter of the Compact Muon Solenoid (CMS) experiment at CERN demonstrates the effectiveness of the proposed method. Our approach clusters readout systems and their sensors consistent with the expected calorimeter interconnection configurations, while capturing unusual behavior in divergent clusters and estimating their root causes.

{{</citation>}}


### (29/36 | 128/235) Federated Optimization with Doubly Regularized Drift Correction (Xiaowen Jiang et al., 2024)

{{<citation>}}

Xiaowen Jiang, Anton Rodomanov, Sebastian U. Stich. (2024)  
**Federated Optimization with Doubly Regularized Drift Correction**
<br/>
<button class="copy-to-clipboard" title="Federated Optimization with Doubly Regularized Drift Correction" index=128>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-OC  
Keyword Score: 10  
Keywords: Federated Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08447v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08447v1.pdf" filename="2404.08447v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Federated</b> <b>learning</b> is a distributed optimization paradigm that allows training machine learning models across decentralized devices while keeping the data localized. The standard method, FedAvg, suffers from client drift which can hamper performance and increase communication costs over centralized methods. Previous works proposed various strategies to mitigate drift, yet none have shown uniformly improved communication-computation trade-offs over vanilla gradient descent. In this work, we revisit DANE, an established method in distributed optimization. We show that (i) DANE can achieve the desired communication reduction under Hessian similarity constraints. Furthermore, (ii) we present an extension, DANE+, which supports arbitrary inexact local solvers and has more freedom to choose how to aggregate the local updates. We propose (iii) a novel method, FedRed, which has improved local computational complexity and retains the same communication complexity compared to DANE/DANE+. This is achieved by using doubly regularized drift correction.

{{</citation>}}


### (30/36 | 129/235) SIR-RL: Reinforcement Learning for Optimized Policy Control during Epidemiological Outbreaks in Emerging Market and Developing Economies (Maeghal Jain et al., 2024)

{{<citation>}}

Maeghal Jain, Ziya Uddin, Wubshet Ibrahim. (2024)  
**SIR-RL: Reinforcement Learning for Optimized Policy Control during Epidemiological Outbreaks in Emerging Market and Developing Economies**
<br/>
<button class="copy-to-clipboard" title="SIR-RL: Reinforcement Learning for Optimized Policy Control during Epidemiological Outbreaks in Emerging Market and Developing Economies" index=129>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, physics-soc-ph, q-bio-PE  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08423v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08423v1.pdf" filename="2404.08423v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The outbreak of COVID-19 has highlighted the intricate interplay between public health and economic stability on a global scale. This study proposes a novel <b>reinforcement</b> <b>learning</b> framework designed to optimize health and economic outcomes during pandemics. The framework leverages the SIR model, integrating both lockdown measures (via a stringency index) and vaccination strategies to simulate disease dynamics. The stringency index, indicative of the severity of lockdown measures, influences both the spread of the disease and the economic health of a country. Developing nations, which bear a disproportionate economic burden under stringent lockdowns, are the primary focus of our study. By implementing <b>reinforcement</b> <b>learning,</b> we aim to optimize governmental responses and strike a balance between the competing costs associated with public health and economic stability. This approach also enhances transparency in governmental decision-making by establishing a well-defined reward function for the <b>reinforcement</b> <b>learning</b> agent. In essence, this study introduces an innovative and ethical strategy to navigate the challenge of balancing public health and economic stability amidst infectious disease outbreaks.

{{</citation>}}


### (31/36 | 130/235) Enhancing Fairness and Performance in Machine Learning Models: A Multi-Task Learning Approach with Monte-Carlo Dropout and Pareto Optimality (Khadija Zanna et al., 2024)

{{<citation>}}

Khadija Zanna, Akane Sano. (2024)  
**Enhancing Fairness and Performance in Machine Learning Models: A Multi-Task Learning Approach with Monte-Carlo Dropout and Pareto Optimality**
<br/>
<button class="copy-to-clipboard" title="Enhancing Fairness and Performance in Machine Learning Models: A Multi-Task Learning Approach with Monte-Carlo Dropout and Pareto Optimality" index=130>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CY, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08230v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08230v1.pdf" filename="2404.08230v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper considers the need for generalizable bias mitigation techniques in machine learning due to the growing concerns of <b>fairness</b> and discrimination in data-driven decision-making procedures across a range of industries. While many existing methods for mitigating bias in machine learning have succeeded in specific cases, they often lack generalizability and cannot be easily applied to different data types or models. Additionally, the trade-off between accuracy and <b>fairness</b> remains a fundamental tension in the field. To address these issues, we propose a bias mitigation method based on multi-task learning, utilizing the concept of Monte-Carlo dropout and Pareto optimality from multi-objective optimization. This method optimizes accuracy and <b>fairness</b> while improving the model's explainability without using sensitive information. We test this method on three datasets from different domains and show how it can deliver the most desired trade-off between model <b>fairness</b> and performance. This allows for tuning in specific domains where one metric may be more important than another. With the framework we introduce in this paper, we aim to enhance the <b>fairness-performance</b> trade-off and offer a solution to bias mitigation methods' generalizability issues in machine learning.

{{</citation>}}


### (32/36 | 131/235) Conformal Prediction via Regression-as-Classification (Etash Guha et al., 2024)

{{<citation>}}

Etash Guha, Shlok Natarajan, Thomas Möllenhoff, Mohammad Emtiyaz Khan, Eugene Ndiaye. (2024)  
**Conformal Prediction via Regression-as-Classification**
<br/>
<button class="copy-to-clipboard" title="Conformal Prediction via Regression-as-Classification" index=131>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 9  
Keywords: Benchmarking, Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08168v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08168v1.pdf" filename="2404.08168v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Conformal prediction (CP) for regression can be challenging, especially when the output distribution is heteroscedastic, <b>multimodal,</b> or skewed. Some of the issues can be addressed by estimating a distribution over the output, but in reality, such approaches can be sensitive to estimation error and yield unstable intervals.~Here, we circumvent the challenges by converting regression to a classification problem and then use CP for classification to obtain CP sets for regression.~To preserve the ordering of the continuous-output space, we design a new loss function and make necessary modifications to the CP classification techniques.~Empirical results on many <b>benchmarks</b> shows that this simple approach gives surprisingly good results on many practical problems.

{{</citation>}}


### (33/36 | 132/235) Seismic First Break Picking in a Higher Dimension Using Deep Graph Learning (Hongtao Wang et al., 2024)

{{<citation>}}

Hongtao Wang, Li Long, Jiangshe Zhang, Xiaoli Wei, Chunxia Zhang, Zhenbo Guo. (2024)  
**Seismic First Break Picking in a Higher Dimension Using Deep Graph Learning**
<br/>
<button class="copy-to-clipboard" title="Seismic First Break Picking in a Higher Dimension Using Deep Graph Learning" index=132>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG, eess-SP, physics-geo-ph  
Keyword Score: 6  
Keywords: Graph, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08408v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08408v1.pdf" filename="2404.08408v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Contemporary automatic first break (FB) picking methods typically analyze 1D signals, 2D source gathers, or 3D source-receiver gathers. Utilizing higher-dimensional data, such as 2D or 3D, incorporates global features, improving the stability of local picking. Despite the benefits, high-dimensional data requires structured input and increases computational demands. Addressing this, we propose a novel approach using deep <b>graph</b> learning called DGL-FB, constructing a large <b>graph</b> to efficiently extract information. In this <b>graph,</b> each seismic trace is represented as a node, connected by edges that reflect similarities. To manage the size of the <b>graph,</b> we develop a subgraph sampling technique to streamline model training and inference. Our proposed framework, DGL-FB, leverages deep <b>graph</b> learning for FB picking. It encodes subgraphs into global features using a deep <b>graph</b> encoder. Subsequently, the encoded global features are combined with local node signals and fed into a ResUNet-based 1D segmentation network for FB detection. Field survey evaluations of DGL-FB show superior accuracy and stability compared to a 2D U-Net-based <b>benchmark</b> method.

{{</citation>}}


### (34/36 | 133/235) Regularized Gradient Clipping Provably Trains Wide and Deep Neural Networks (Matteo Tucat et al., 2024)

{{<citation>}}

Matteo Tucat, Anirbit Mukherjee. (2024)  
**Regularized Gradient Clipping Provably Trains Wide and Deep Neural Networks**
<br/>
<button class="copy-to-clipboard" title="Regularized Gradient Clipping Provably Trains Wide and Deep Neural Networks" index=133>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-OC  
Keyword Score: 5  
Keywords: Deep Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08624v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08624v1.pdf" filename="2404.08624v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we instantiate a regularized form of the gradient clipping algorithm and prove that it can converge to the global minima of <b>deep</b> <b>neural</b> <b>network</b> loss functions provided that the net is of sufficient width. We present empirical evidence that our theoretically founded regularized gradient clipping algorithm is also competitive with the state-of-the-art <b>deep-learning</b> <b>heuristics.</b> <b>Hence</b> the algorithm presented here constitutes a new approach to rigorous <b>deep</b> <b>learning.</b> <b>The</b> modification we do to standard gradient clipping is designed to leverage the PL* condition, a variant of the Polyak-Lojasiewicz inequality which was recently proven to be true for various neural networks for any depth within a neighborhood of the initialisation.

{{</citation>}}


### (35/36 | 134/235) Hyperbolic Delaunay Geometric Alignment (Aniss Aiman Medbouhi et al., 2024)

{{<citation>}}

Aniss Aiman Medbouhi, Giovanni Luca Marchetti, Vladislav Polianskii, Alexander Kravberg, Petra Poklukar, Anastasia Varava, Danica Kragic. (2024)  
**Hyperbolic Delaunay Geometric Alignment**
<br/>
<button class="copy-to-clipboard" title="Hyperbolic Delaunay Geometric Alignment" index=134>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08608v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08608v1.pdf" filename="2404.08608v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Hyperbolic machine learning is an emerging field aimed at representing data with a hierarchical structure. However, there is a lack of tools for evaluation and analysis of the resulting hyperbolic data representations. To this end, we propose Hyperbolic Delaunay Geometric Alignment (HyperDGA) -- a similarity score for comparing datasets in a hyperbolic space. The core idea is counting the edges of the hyperbolic Delaunay <b>graph</b> connecting datapoints across the given sets. We provide an empirical investigation on synthetic and real-life biological data and demonstrate that HyperDGA outperforms the hyperbolic version of classical distances between sets. Furthermore, we showcase the potential of HyperDGA for evaluating latent representations inferred by a Hyperbolic Variational Auto-Encoder.

{{</citation>}}


### (36/36 | 135/235) Fuxi-DA: A Generalized Deep Learning Data Assimilation Framework for Assimilating Satellite Observations (Xiaoze Xu et al., 2024)

{{<citation>}}

Xiaoze Xu, Xiuyu Sun, Wei Han, Xiaohui Zhong, Lei Chen, Hao Li. (2024)  
**Fuxi-DA: A Generalized Deep Learning Data Assimilation Framework for Assimilating Satellite Observations**
<br/>
<button class="copy-to-clipboard" title="Fuxi-DA: A Generalized Deep Learning Data Assimilation Framework for Assimilating Satellite Observations" index=135>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, physics-ao-ph  
Keyword Score: 3  
Keywords: Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08522v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08522v1.pdf" filename="2404.08522v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Data assimilation (DA), as an indispensable component within contemporary Numerical Weather Prediction (NWP) systems, plays a crucial role in generating the analysis that significantly impacts forecast performance. Nevertheless, the development of an efficient DA system poses significant challenges, particularly in establishing intricate relationships between the background data and the vast amount of multi-source observation data within limited time windows in operational settings. To address these challenges, researchers design complex pre-processing methods for each observation type, leveraging approximate modeling and the power of super-computing clusters to expedite solutions. The emergence of deep learning (DL) models has been a game-changer, offering unified <b>multi-modal</b> modeling, enhanced nonlinear representation capabilities, and superior parallelization. These advantages have spurred efforts to integrate DL models into various domains of weather modeling. Remarkably, DL models have shown promise in matching, even surpassing, the forecast accuracy of leading operational NWP models worldwide. This success motivates the exploration of DL-based DA frameworks tailored for weather forecasting models. In this study, we introduces FuxiDA, a generalized DL-based DA framework for assimilating satellite observations. By assimilating data from Advanced Geosynchronous Radiation Imager (AGRI) aboard Fengyun-4B, FuXi-DA consistently mitigates analysis errors and significantly improves forecast performance. Furthermore, through a series of single-observation experiments, Fuxi-DA has been validated against established atmospheric physics, demonstrating its consistency and reliability.

{{</citation>}}


## cs.IT (2)



### (1/2 | 136/235) Learning-Based Joint Antenna Selection and Precoding Design for Cell-Free MIMO Networks (Liangzhi Wang et al., 2024)

{{<citation>}}

Liangzhi Wang, Chen Chen, Carlo Fischione, Jie Zhang. (2024)  
**Learning-Based Joint Antenna Selection and Precoding Design for Cell-Free MIMO Networks**
<br/>
<button class="copy-to-clipboard" title="Learning-Based Joint Antenna Selection and Precoding Design for Cell-Free MIMO Networks" index=136>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, eess-SP, math-IT  
Keyword Score: 63  
Keywords: Convolutional Neural Network, Convolutional Neural Network, Graph, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08607v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08607v1.pdf" filename="2404.08607v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper considers a downlink cell-free multiple-input multiple-output (MIMO) network in which multiple multi-antenna base stations (BSs) serve multiple users via coherent joint transmission. In order to reduce the energy consumption by radio frequency components, each BS selects a subset of antennas for downlink data transmission after estimating the channel state information (CSI). We aim to maximize the sum spectral efficiency by jointly optimizing the antenna selection and precoding design. To alleviate the fronthaul overhead and enable real-time network operation, we propose a distributed scalable machine learning algorithm. In particular, at each BS, we deploy a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> for antenna selection and a <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> for precoding design. Different from conventional centralized solutions that require a large amount of CSI and signaling exchange among the BSs, the proposed distributed machine learning algorithm takes only locally estimated CSI as input. With well-trained learning models, it is shown that the proposed algorithm significantly outperforms the distributed baseline schemes and achieves a sum spectral efficiency comparable to its centralized counterpart.

{{</citation>}}


### (2/2 | 137/235) Joint Computation Offloading and Target Tracking in Integrated Sensing and Communication Enabled UAV Networks (Trinh Van Chien et al., 2024)

{{<citation>}}

Trinh Van Chien, Mai Dinh Cong, Nguyen Cong Luong, Tri Nhu Do, Dong In Kim, Symeon Chatzinotas. (2024)  
**Joint Computation Offloading and Target Tracking in Integrated Sensing and Communication Enabled UAV Networks**
<br/>
<button class="copy-to-clipboard" title="Joint Computation Offloading and Target Tracking in Integrated Sensing and Communication Enabled UAV Networks" index=137>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IT  
Categories: cs-IT, cs.IT, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08396v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08396v1.pdf" filename="2404.08396v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we investigate a joint computation offloading and target tracking in Integrated Sensing and Communication (ISAC)-enabled unmanned aerial vehicle (UAV) network. Therein, the UAV has a computing task that is partially offloaded to the ground UE for execution. Meanwhile, the UAV uses the offloading bit sequence to estimate the velocity of a ground target based on an autocorrelation function. The performance of the velocity estimation that is represented by Cramer-Rao lower bound (CRB) depends on the length of the offloading bit sequence and the UAV's location. Thus, we jointly optimize the task size for offloading and the UAV's location to minimize the overall computation latency and the CRB of the mean square error for velocity estimation subject to the UAV's budget. The problem is non-convex, and we propose a genetic algorithm to solve it. <b>Simulation</b> results are provided to demonstrate the effectiveness of the proposed algorithm.

{{</citation>}}


## cs.RO (13)



### (1/13 | 138/235) Inverse Kinematics for Neuro-Robotic Grasping with Humanoid Embodied Agents (Jan-Gerrit Habekost et al., 2024)

{{<citation>}}

Jan-Gerrit Habekost, Connor Gäde, Philipp Allgeuer, Stefan Wermter. (2024)  
**Inverse Kinematics for Neuro-Robotic Grasping with Humanoid Embodied Agents**
<br/>
<button class="copy-to-clipboard" title="Inverse Kinematics for Neuro-Robotic Grasping with Humanoid Embodied Agents" index=138>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-RO, cs.RO  
Keyword Score: 60  
Keywords: Simulation, Simulator, Zero-shot, human-in-the-loop, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08825v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08825v1.pdf" filename="2404.08825v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces a novel <b>zero-shot</b> motion planning method that allows users to quickly design smooth robot motions in Cartesian space. A B\'ezier curve-based Cartesian plan is transformed into a joint space trajectory by our neuro-inspired inverse kinematics (IK) method CycleIK, for which we enable platform independence by scaling it to arbitrary robot designs. The motion planner is evaluated on the physical hardware of the two humanoid robots NICO and NICOL in a <b>human-in-the-loop</b> grasping scenario. Our method is deployed with an embodied agent that is a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> at its core. We generalize the embodied agent, that was introduced for NICOL, to also be embodied by NICO. The agent can execute a discrete set of physical actions and allows the user to verbally instruct various different robots. We contribute a grasping primitive to its action space that allows for precise manipulation of household objects. The new CycleIK method is compared to popular numerical IK solvers and state-of-the-art neural IK methods in <b>simulation</b> and is shown to be competitive with or outperform all evaluated methods when the algorithm runtime is very short. The grasping primitive is evaluated on both NICOL and NICO robots with a reported grasp success of 72% to 82% for each robot, respectively.

{{</citation>}}


### (2/13 | 139/235) Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction in an Object Categorization Task (Hassan Ali et al., 2024)

{{<citation>}}

Hassan Ali, Philipp Allgeuer, Stefan Wermter. (2024)  
**Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction in an Object Categorization Task**
<br/>
<button class="copy-to-clipboard" title="Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction in an Object Categorization Task" index=139>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-HC, cs-RO, cs.RO  
Keyword Score: 56  
Keywords: Multi-modal, Multi-modal, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08424v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08424v1.pdf" filename="2404.08424v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Intention-based Human-Robot Interaction (HRI) systems allow robots to perceive and interpret user actions to proactively interact with humans and adapt to their behavior. Therefore, intention prediction is pivotal in creating a natural interactive collaboration between humans and robots. In this paper, we examine the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for inferring human intention during a collaborative object categorization task with a physical robot. We introduce a hierarchical approach for interpreting user non-verbal cues, like hand gestures, body poses, and facial expressions and combining them with environment states and user verbal cues captured using an existing <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> system. Our evaluation demonstrates the potential of <b>LLMs</b> to interpret non-verbal cues and to combine them with their context-understanding capabilities and real-world knowledge to support intention prediction during human-robot interaction.

{{</citation>}}


### (3/13 | 140/235) Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation (Hanlin Tian et al., 2024)

{{<citation>}}

Hanlin Tian, Kethan Reddy, Yuxiang Feng, Mohammed Quddus, Yiannis Demiris, Panagiotis Angeloudis. (2024)  
**Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation**
<br/>
<button class="copy-to-clipboard" title="Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation" index=140>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-LG, cs-RO, cs.RO  
Keyword Score: 50  
Keywords: Reinforcement Learning, Simulation, Simulator, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08570v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08570v1.pdf" filename="2404.08570v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces CRITICAL, a novel closed-loop framework for autonomous vehicle (AV) training and testing. CRITICAL stands out for its ability to generate diverse scenarios, focusing on critical driving situations that target specific learning and performance gaps identified in the <b>Reinforcement</b> <b>Learning</b> (RL) agent. The framework achieves this by integrating real-world traffic dynamics, driving behavior analysis, surrogate safety measures, and an optional <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> component. It is proven that the establishment of a closed feedback loop between the data generation pipeline and the training process can enhance the learning rate during training, elevate overall system performance, and augment safety resilience. Our evaluations, conducted using the Proximal Policy Optimization (PPO) and the HighwayEnv <b>simulation</b> environment, demonstrate noticeable performance improvements with the integration of critical case generation and <b>LLM</b> analysis, indicating CRITICAL's potential to improve the robustness of AV systems and streamline the generation of critical scenarios. This ultimately serves to hasten the development of AV agents, expand the general scope of RL training, and ameliorate validation efforts for AV safety.

{{</citation>}}


### (4/13 | 141/235) 'Don't forget to put the milk back!' Dataset for Enabling Embodied Agents to Detect Anomalous Situations (James F. Mullen Jr et al., 2024)

{{<citation>}}

James F. Mullen Jr, Prasoon Goyal, Robinson Piramuthu, Michael Johnston, Dinesh Manocha, Reza Ghanadan. (2024)  
**'Don't forget to put the milk back!' Dataset for Enabling Embodied Agents to Detect Anomalous Situations**
<br/>
<button class="copy-to-clipboard" title="'Don't forget to put the milk back!' Dataset for Enabling Embodied Agents to Detect Anomalous Situations" index=141>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-CV, cs-RO, cs.RO  
Keyword Score: 43  
Keywords: Graph, GPT, GPT-4, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08827v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08827v1.pdf" filename="2404.08827v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Home robots intend to make their users lives easier. Our work assists in this goal by enabling robots to inform their users of dangerous or unsanitary anomalies in their home. Some examples of these anomalies include the user leaving their milk out, forgetting to turn off the stove, or leaving poison accessible to children. To move towards enabling home robots with these abilities, we have created a new dataset, which we call SafetyDetect. The SafetyDetect dataset consists of 1000 anomalous home scenes, each of which contains unsafe or unsanitary situations for an agent to detect. Our approach utilizes <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> alongside both a <b>graph</b> representation of the scene and the relationships between the objects in the scene. Our key insight is that this connected scene <b>graph</b> and the object relationships it encodes enables the <b>LLM</b> to better reason about the scene -- especially as it relates to detecting dangerous or unsanitary situations. Our most promising approach utilizes <b>GPT-4</b> and pursues a categorization technique where object relations from the scene <b>graph</b> are classified as normal, dangerous, unsanitary, or dangerous for children. This method is able to correctly identify over 90% of anomalous scenarios in the SafetyDetect Dataset. Additionally, we conduct real world experiments on a ClearPath TurtleBot where we generate a scene <b>graph</b> from visuals of the real world scene, and run our approach with no modification. This setup resulted in little performance loss. The SafetyDetect Dataset and code will be released to the public upon this papers publication.

{{</citation>}}


### (5/13 | 142/235) WROOM: An Autonomous Driving Approach for Off-Road Navigation (Dvij Kalaria et al., 2024)

{{<citation>}}

Dvij Kalaria, Shreya Sharma, Sarthak Bhagat, Haoru Xue, John M. Dolan. (2024)  
**WROOM: An Autonomous Driving Approach for Off-Road Navigation**
<br/>
<button class="copy-to-clipboard" title="WROOM: An Autonomous Driving Approach for Off-Road Navigation" index=142>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-LG, cs-RO, cs.RO  
Keyword Score: 30  
Keywords: Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08855v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08855v1.pdf" filename="2404.08855v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Off-road navigation is a challenging problem both at the planning level to get a smooth trajectory and at the control level to avoid flipping over, hitting obstacles, or getting stuck at a rough patch. There have been several recent works using classical approaches involving depth map prediction followed by smooth trajectory planning and using a controller to track it. We design an end-to-end <b>reinforcement</b> <b>learning</b> (RL) system for an autonomous vehicle in off-road environments using a custom-designed simulator in the Unity game engine. We warm-start the agent by imitating a rule-based controller and utilize Proximal Policy Optimization (PPO) to improve the policy based on a reward that incorporates Control Barrier Functions (CBF), facilitating the agent's ability to generalize effectively to real-world scenarios. The training involves agents concurrently undergoing domain-randomized trials in various environments. We also propose a novel <b>simulation</b> environment to replicate off-road driving scenarios and deploy our proposed approach on a real buggy RC car. Videos and additional results: https://sites.google.com/view/wroom-utd/home

{{</citation>}}


### (6/13 | 143/235) Agile and versatile bipedal robot tracking control through reinforcement learning (Jiayi Li et al., 2024)

{{<citation>}}

Jiayi Li, Linqi Ye, Yi Cheng, Houde Liu, Bin Liang. (2024)  
**Agile and versatile bipedal robot tracking control through reinforcement learning**
<br/>
<button class="copy-to-clipboard" title="Agile and versatile bipedal robot tracking control through reinforcement learning" index=143>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-LG, cs-RO, cs.RO  
Keyword Score: 30  
Keywords: Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08246v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08246v1.pdf" filename="2404.08246v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The remarkable athletic intelligence displayed by humans in complex dynamic movements such as dancing and gymnastics suggests that the balance mechanism in biological beings is decoupled from specific movement patterns. This decoupling allows for the execution of both learned and unlearned movements under certain constraints while maintaining balance through minor whole-body coordination. To replicate this balance ability and body agility, this paper proposes a versatile controller for bipedal robots. This controller achieves ankle and body trajectory tracking across a wide range of gaits using a single small-scale neural network, which is based on a model-based IK solver and <b>reinforcement</b> <b>learning.</b> We consider a single step as the smallest control unit and design a universally applicable control input form suitable for any single-step variation. Highly flexible gait control can be achieved by combining these minimal control units with high-level policy through our extensible control interface. To enhance the trajectory-tracking capability of our controller, we utilize a three-stage training curriculum. After training, the robot can move freely between target footholds at varying distances and heights. The robot can also maintain static balance without repeated stepping to adjust posture. Finally, we evaluate the tracking accuracy of our controller on various bipedal tasks, and the effectiveness of our control framework is verified in the <b>simulation</b> environment.

{{</citation>}}


### (7/13 | 144/235) Multi-fingered Robotic Hand Grasping in Cluttered Environments through Hand-object Contact Semantic Mapping (Lei Zhang et al., 2024)

{{<citation>}}

Lei Zhang, Kaixin Bai, Guowen Huang, Zhaopeng Chen, Jianwei Zhang. (2024)  
**Multi-fingered Robotic Hand Grasping in Cluttered Environments through Hand-object Contact Semantic Mapping**
<br/>
<button class="copy-to-clipboard" title="Multi-fingered Robotic Hand Grasping in Cluttered Environments through Hand-object Contact Semantic Mapping" index=144>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Autoencoder, Variational Autoencoder  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08844v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08844v1.pdf" filename="2404.08844v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The integration of optimization method and generative models has significantly advanced dexterous manipulation techniques for five-fingered hand grasping. Yet, the application of these techniques in cluttered environments is a relatively unexplored area. To address this research gap, we have developed a novel method for generating five-fingered hand grasp samples in cluttered settings. This method emphasizes simulated grasp quality and the nuanced interaction between the hand and surrounding objects. A key aspect of our approach is our data generation method, capable of estimating contact spatial and semantic representations and affordance grasps based on object affordance information. Furthermore, our Contact Semantic Conditional <b>Variational</b> <b>Autoencoder</b> (CoSe-CVAE) network is adept at creating comprehensive contact maps from point clouds, incorporating both spatial and semantic data. We introduce a unique grasp detection technique that efficiently formulates mechanical hand grasp poses from these maps. Additionally, our evaluation model is designed to assess grasp quality and collision probability, significantly improving the practicality of five-fingered hand grasping in complex scenarios. Our data generation method outperforms previous datasets in grasp diversity, scene diversity, modality diversity. Our grasp generation method has demonstrated remarkable success, outperforming established baselines with 81.0% average success rate in real-world single-object grasping and 75.3% success rate in multi-object grasping. The dataset and supplementary materials can be found at https://sites.google.com/view/ffh-clutteredgrasping, and we will release the code upon publication.

{{</citation>}}


### (8/13 | 145/235) Safe Start Regions for Medical Steerable Needle Automation (Janine Hoelscher et al., 2024)

{{<citation>}}

Janine Hoelscher, Inbar Fried, Spiros Tsalikis, Jason Akulian, Robert J. Webster III, Ron Alterovitz. (2024)  
**Safe Start Regions for Medical Steerable Needle Automation**
<br/>
<button class="copy-to-clipboard" title="Safe Start Regions for Medical Steerable Needle Automation" index=145>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08558v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08558v1.pdf" filename="2404.08558v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Steerable needles are minimally invasive devices that enable novel medical procedures by following curved paths to avoid critical anatomical obstacles. Planning algorithms can be used to find a steerable needle motion plan to a target. Deployment typically consists of a physician manually inserting the steerable needle into tissue at the motion plan's start pose and handing off control to a robot, which then autonomously steers it to the target along the plan. The handoff between human and robot is critical for procedure success, as even small deviations from the start pose change the steerable needle's workspace and there is no guarantee that the target will still be reachable. We introduce a metric that evaluates the robustness to such start pose deviations. When measuring this robustness to deviations, we consider the tradeoff between being robust to changes in position versus changes in orientation. We evaluate our metric through <b>simulation</b> in an abstract, a liver, and a lung planning scenario. Our evaluation shows that our metric can be combined with different motion planners and that it efficiently determines large, safe start regions.

{{</citation>}}


### (9/13 | 146/235) Collective Bayesian Decision-Making in a Swarm of Miniaturized Robots for Surface Inspection (Thiemen Siemensma et al., 2024)

{{<citation>}}

Thiemen Siemensma, Darren Chiu, Sneha Ramshanker, Radhika Nagpal, Bahar Haghighat. (2024)  
**Collective Bayesian Decision-Making in a Swarm of Miniaturized Robots for Surface Inspection**
<br/>
<button class="copy-to-clipboard" title="Collective Bayesian Decision-Making in a Swarm of Miniaturized Robots for Surface Inspection" index=146>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs-SY, cs.RO, eess-SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08390v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08390v1.pdf" filename="2404.08390v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Robot swarms can effectively serve a variety of sensing and inspection applications. Certain inspection tasks require a binary classification decision. This work presents an experimental setup for a surface inspection task based on vibration sensing and studies a Bayesian two-outcome decision-making algorithm in a swarm of miniaturized wheeled robots. The robots are tasked with individually inspecting and collectively classifying a 1mx1m tiled surface consisting of vibrating and non-vibrating tiles based on the majority type of tiles. The robots sense vibrations using onboard IMUs and perform collision avoidance using a set of IR sensors. We develop a <b>simulation</b> and optimization framework leveraging the Webots robotic simulator and a Particle Swarm Optimization (PSO) method. We consider two existing information sharing strategies and propose a new one that allows the swarm to rapidly reach accurate classification decisions. We first find optimal parameters that allow efficient sampling in <b>simulation</b> and then evaluate our proposed strategy against the two existing ones using 100 randomized <b>simulation</b> and 10 real experiments. We find that our proposed method compels the swarm to make decisions at an accelerated rate, with an improvement of up to 20.52% in mean decision time at only 0.78% loss in accuracy.

{{</citation>}}


### (10/13 | 147/235) High-Speed Interception Multicopter Control by Image-based Visual Servoing (Kun Yang et al., 2024)

{{<citation>}}

Kun Yang, Chenggang Bai, Zhikun She, Quan Quan. (2024)  
**High-Speed Interception Multicopter Control by Image-based Visual Servoing**
<br/>
<button class="copy-to-clipboard" title="High-Speed Interception Multicopter Control by Image-based Visual Servoing" index=147>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08296v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08296v1.pdf" filename="2404.08296v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, reports of illegal drones threatening public safety have increased. For the invasion of fully autonomous drones, traditional methods such as radio frequency interference and GPS shielding may fail. This paper proposes a scheme that uses an autonomous multicopter with a strapdown camera to intercept a maneuvering intruder UAV. The interceptor multicopter can autonomously detect and intercept intruders moving at high speed in the air. The strapdown camera avoids the complex mechanical structure of the electro-optical pod, making the interceptor multicopter compact. However, the coupling of the camera and multicopter motion makes interception tasks difficult. To solve this problem, an Image-Based Visual Servoing (IBVS) controller is proposed to make the interception fast and accurate. Then, in response to the time delay of sensor imaging and image processing relative to attitude changes in high-speed scenarios, a Delayed Kalman Filter (DKF) observer is generalized to predict the current image position and increase the update frequency. Finally, Hardware-in-the-Loop (HITL) <b>simulations</b> and outdoor flight experiments verify that this method has a high interception accuracy and success rate. In the flight experiments, a high-speed interception is achieved with a terminal speed of 20 m/s.

{{</citation>}}


### (11/13 | 148/235) Non-impulsive Contact-Implicit Motion Planning for Morpho-functional Loco-manipulation (Adarsh Salagame et al., 2024)

{{<citation>}}

Adarsh Salagame, Kruthika Gangaraju, Harin Kumar Nallaguntla, Eric Sihite, Gunar Schirner, Alireza Ramezani. (2024)  
**Non-impulsive Contact-Implicit Motion Planning for Morpho-functional Loco-manipulation**
<br/>
<button class="copy-to-clipboard" title="Non-impulsive Contact-Implicit Motion Planning for Morpho-functional Loco-manipulation" index=148>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs-SY, cs.RO, eess-SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08714v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08714v1.pdf" filename="2404.08714v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Object manipulation has been extensively studied in the context of fixed base and mobile manipulators. However, the overactuated locomotion modality employed by snake robots allows for a unique blend of object manipulation through locomotion, referred to as loco-manipulation. The following work presents an optimization approach to solving the loco-manipulation problem based on non-impulsive implicit contact path planning for our snake robot COBRA. We present the mathematical framework and show high fidelity <b>simulation</b> results for fixed-shape lateral rolling trajectories that demonstrate the object manipulation.

{{</citation>}}


### (12/13 | 149/235) Loco-Manipulation with Nonimpulsive Contact-Implicit Planning in a Slithering Robot (Adarsh Salagame et al., 2024)

{{<citation>}}

Adarsh Salagame, Kruthika Gangaraju, Harin Kumar Nallaguntla, Eric Sihite, Gunar Schirner, Alireza Ramezani. (2024)  
**Loco-Manipulation with Nonimpulsive Contact-Implicit Planning in a Slithering Robot**
<br/>
<button class="copy-to-clipboard" title="Loco-Manipulation with Nonimpulsive Contact-Implicit Planning in a Slithering Robot" index=149>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs-SY, cs.RO, eess-SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08174v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08174v1.pdf" filename="2404.08174v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Object manipulation has been extensively studied in the context of fixed base and mobile manipulators. However, the overactuated locomotion modality employed by snake robots allows for a unique blend of object manipulation through locomotion, referred to as loco-manipulation. The following work presents an optimization approach to solving the loco-manipulation problem based on non-impulsive implicit contact path planning for our snake robot COBRA. We present the mathematical framework and show high-fidelity <b>simulation</b> results and experiments to demonstrate the effectiveness of our approach.

{{</citation>}}


### (13/13 | 150/235) A Passively Bendable, Compliant Tactile Palm with RObotic Modular Endoskeleton Optical (ROMEO) Fingers (Sandra Q. Liu et al., 2024)

{{<citation>}}

Sandra Q. Liu, Edward H. Adelson. (2024)  
**A Passively Bendable, Compliant Tactile Palm with RObotic Modular Endoskeleton Optical (ROMEO) Fingers**
<br/>
<button class="copy-to-clipboard" title="A Passively Bendable, Compliant Tactile Palm with RObotic Modular Endoskeleton Optical (ROMEO) Fingers" index=150>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-RO, cs.RO  
Keyword Score: 10  
Keywords: PaLM  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08227v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08227v1.pdf" filename="2404.08227v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Many robotic hands currently rely on extremely dexterous robotic fingers and a thumb joint to envelop themselves around an object. Few hands focus on the <b>palm</b> even though human hands greatly benefit from their central fold and soft surface. As such, we develop a novel structurally compliant soft <b>palm,</b> which enables more surface area contact for the objects that are pressed into it. Moreover, this design, along with the development of a new low-cost, flexible illumination system, is able to incorporate a high-resolution tactile sensing system inspired by the GelSight sensors. Concurrently, we design RObotic Modular Endoskeleton Optical (ROMEO) fingers, which are underactuated two-segment soft fingers that are able to house the new illumination system, and we integrate them into these various <b>palm</b> configurations. The resulting robotic hand is slightly bigger than a baseball and represents one of the first soft robotic hands with actuated fingers and a passively compliant <b>palm,</b> all of which have high-resolution tactile sensing. This design also potentially helps researchers discover and explore more soft-rigid tactile robotic hand designs with greater capabilities in the future. The supplementary video can be found here: https://youtu.be/RKfIFiewqsg

{{</citation>}}


## eess.SY (6)



### (1/6 | 151/235) A Novel Vision Transformer based Load Profile Analysis using Load Images as Inputs (Hyeonjin Kim et al., 2024)

{{<citation>}}

Hyeonjin Kim, Yi Hu, Kai Ye, Ning Lu. (2024)  
**A Novel Vision Transformer based Load Profile Analysis using Load Images as Inputs**
<br/>
<button class="copy-to-clipboard" title="A Novel Vision Transformer based Load Profile Analysis using Load Images as Inputs" index=151>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 60  
Keywords: Vision Transformer, Self-supervised Learning, Simulation, Simulator, Transformer, Vision Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08175v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08175v1.pdf" filename="2404.08175v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces ViT4LPA, an innovative <b>Vision</b> <b>Transformer</b> (ViT) based approach for Load Profile Analysis (LPA). We transform time-series load profiles into load images. This allows us to leverage the ViT architecture, originally designed for image processing, as a pre-trained image encoder to uncover latent patterns within load data. ViT is pre-trained using an extensive load image dataset, comprising 1M load images derived from smart meter data collected over a two-year period from 2,000 residential users. The training methodology is <b>self-supervised,</b> masked image modeling, wherein masked load images are restored to reveal hidden relationships among image patches. The pre-trained ViT encoder is then applied to various downstream tasks, including the identification of electric vehicle (EV) charging loads and behind-the-meter solar photovoltaic (PV) systems and load disaggregation. <b>Simulation</b> results illustrate ViT4LPA's superior performance compared to existing neural network models in downstream tasks. Additionally, we conduct an in-depth analysis of the attention weights within the ViT4LPA model to gain insights into its information flow mechanisms.

{{</citation>}}


### (2/6 | 152/235) Maximal electric power generation from varying ocean waves with LC-tuned reactive PTO force (Jingxin Zhang et al., 2024)

{{<citation>}}

Jingxin Zhang, Uzair Bin Tahir, Richard Manasseh. (2024)  
**Maximal electric power generation from varying ocean waves with LC-tuned reactive PTO force**
<br/>
<button class="copy-to-clipboard" title="Maximal electric power generation from varying ocean waves with LC-tuned reactive PTO force" index=152>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08360v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08360v1.pdf" filename="2404.08360v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The reactive Power Take Off (PTO) force is the key to maximizing mechanical power absorption and electric power generation of Wave Energy Converters (WECs) from ocean waves with variable frequency, but its study is limited due to its difficulty in physical realization. This paper presents a simple yet effective $LC$-tuned WEC that generates a tunable reactive PTO force from tunable inductor $L$ and capacitor $C$ in the WEC. A complete closed loop system model of the WEC is derived first, then three quantitative rules are obtained from analyzing the model. These rules are used to tune the $LC$ network, and hence the reactive PTO force that drives the WEC, to resonate with the input wave force and generate maximal electric power over a range of wave frequencies. Mathematical analysis of the WEC and tuning rules reveals the analytical and quantitative descriptions of the WEC's mechanical power absorption, active and reactive electric power generation and power factor, optimal electric resistance load, and the generator and $LC$ capacity requirements. <b>Simulation</b> results show the effectiveness and advantages of the proposed WEC and verify the analysis results.

{{</citation>}}


### (3/6 | 153/235) Prescribing Optimal Health-Aware Operation for Urban Air Mobility with Deep Reinforcement Learning (Mina Montazeri et al., 2024)

{{<citation>}}

Mina Montazeri, Chetan Kulkarni, Olga Fink. (2024)  
**Prescribing Optimal Health-Aware Operation for Urban Air Mobility with Deep Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Prescribing Optimal Health-Aware Operation for Urban Air Mobility with Deep Reinforcement Learning" index=153>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08497v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08497v1.pdf" filename="2404.08497v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Urban Air Mobility (UAM) aims to expand existing transportation networks in metropolitan areas by offering short flights either to transport passengers or cargo. Electric vertical takeoff and landing aircraft powered by lithium-ion battery packs are considered promising for such applications. Efficient mission planning is cru-cial, maximizing the number of flights per battery charge while ensuring completion even under unforeseen events. As batteries degrade, precise mission planning becomes challenging due to uncertainties in the end-of-discharge prediction. This often leads to adding safety margins, reducing the number or duration of po-tential flights on one battery charge. While predicting the end of discharge can support decision-making, it remains insufficient in case of unforeseen events, such as adverse weather conditions. This necessitates health-aware real-time control to address any unexpected events and extend the time until the end of charge while taking the current degradation state into account. This paper addresses the joint problem of mission planning and health-aware real-time control of opera-tional parameters to prescriptively control the duration of one discharge cycle of the battery pack. We pro-pose an algorithm that proactively prescribes operational parameters to extend the discharge cycle based on the battery's current health status while optimizing the mission. The proposed deep <b>reinforcement</b> <b>learn-ing</b> algorithm facilitates operational parameter optimization and path planning while accounting for the degradation state, even in the presence of uncertainties. Evaluation of simulated flights of a NASA concep-tual multirotor aircraft model, collected from Hardware-in-the-loop experiments, demonstrates the algo-rithm's near-optimal performance across various operational scenarios, allowing adaptation to changed en-vironmental conditions.

{{</citation>}}


### (4/6 | 154/235) Numerical Discretization Methods for Linear Quadratic Control Problems with Time Delays (Zhanhao Zhang et al., 2024)

{{<citation>}}

Zhanhao Zhang, Steen Hørsholt, John Bagterp Jørgensen. (2024)  
**Numerical Discretization Methods for Linear Quadratic Control Problems with Time Delays**
<br/>
<button class="copy-to-clipboard" title="Numerical Discretization Methods for Linear Quadratic Control Problems with Time Delays" index=154>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Continuous Time, Continuous Time  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08440v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08440v1.pdf" filename="2404.08440v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents the numerical discretization methods of the <b>continuous-time</b> <b>linear-quadratic</b> optimal control problems (LQ-OCPs) with time delays. We describe the weight matrices of the LQ-OCPs as differential equations systems, allowing us to derive the discrete equivalent of the <b>continuous-time</b> <b>LQ-OCPs.</b> Three numerical methods are introduced for solving proposed differential equations systems: 1) the ordinary differential equation (ODE) method, 2) the matrix exponential method, and 3) the step-doubling method. We implement a <b>continuous-time</b> <b>model</b> predictive control (CT-MPC) on a simulated cement mill system, and the objective function of the CT-MPC is discretized using the proposed LQ discretization scheme. The closed-loop results indicate that the CT-MPC successfully stabilizes and controls the simulated cement mill system, ensuring the viability and effectiveness of LQ discretization.

{{</citation>}}


### (5/6 | 155/235) Data-driven Interval MDP for Robust Control Synthesis (Rudi Coppola et al., 2024)

{{<citation>}}

Rudi Coppola, Andrea Peruffo, Licio Romao, Alessandro Abate, Manuel Mazo Jr. (2024)  
**Data-driven Interval MDP for Robust Control Synthesis**
<br/>
<button class="copy-to-clipboard" title="Data-driven Interval MDP for Robust Control Synthesis" index=155>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Discrete Time, Discrete Time  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08344v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08344v1.pdf" filename="2404.08344v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The abstraction of dynamical systems is a powerful tool that enables the design of feedback controllers using a correct-by-design framework. We investigate a novel scheme to obtain data-driven abstractions of <b>discrete-time</b> <b>stochastic</b> processes in terms of richer <b>discrete</b> <b>stochastic</b> models, whose actions lead to nondeterministic transitions over the space of probability measures. The data-driven component of the proposed methodology lies in the fact that we only assume samples from an unknown probability distribution. We also rely on the model of the underlying dynamics to build our abstraction through backward reachability computations. The nondeterminism in the probability space is captured by a collection of Markov Processes, and we identify how this model can improve upon existing abstraction techniques in terms of satisfying temporal properties, such as safety or reach-avoid. The connection between the <b>discrete</b> <b>and</b> the underlying dynamics is made formal through the use of the scenario approach theory. Numerical experiments illustrate the advantages and main limitations of the proposed techniques with respect to existing approaches.

{{</citation>}}


### (6/6 | 156/235) Joint Design of Self-Tuning UHF RFID Antenna and Microfluidic Channel for Liquid Sensing (Giulio Maria Bianco et al., 2024)

{{<citation>}}

Giulio Maria Bianco, Gaetano Marrocco. (2024)  
**Joint Design of Self-Tuning UHF RFID Antenna and Microfluidic Channel for Liquid Sensing**
<br/>
<button class="copy-to-clipboard" title="Joint Design of Self-Tuning UHF RFID Antenna and Microfluidic Channel for Liquid Sensing" index=156>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08268v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08268v2.pdf" filename="2404.08268v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Microfluidic has been an enabling technology for over a decade, particularly in the field of medical and wearable devices, allowing for the manipulation of small amounts of fluid in confined spaces. Micro-channels can also be used for wireless sensing thanks to the variations in antenna properties when the fluid flows near it. However, up to now, microfluidic channels and sensing antennas have always been designed separately; instead, since the liquid flow and the antenna <b>geometry</b> both contribute to the overall performance, they should be considered simultaneously when optimizing the antenna-microfluidic system. In this paper, the joint design of the antenna and microfluidic channels is investigated for liquid quantification. Self-tuning RFID microchips are exploited to minimize communication degradation due to the increase of lossy liquid amount over the sensing antenna while digitalizing the impedance mismatch itself. To experimentally corroborate the joint design technique, two different geometries are obtained and prototyped starting from a given antenna-microfluidic layout by setting different goals for an optimization function. The two flexible RFID prototypes returned performance in agreement with the simulated ones, achieving a maximum sensitivity of about 20 units of the digital metric per milligram increase of water.

{{</citation>}}


## eess.IV (9)



### (1/9 | 157/235) Benchmarking the Cell Image Segmentation Models Robustness under the Microscope Optical Aberrations (Boyuan Peng et al., 2024)

{{<citation>}}

Boyuan Peng, Jiaju Chen, Qihui Ye, Minjiang Chen, Peiwu Qin, Chenggang Yan, Dongmei Yu, Zhenglin Chen. (2024)  
**Benchmarking the Cell Image Segmentation Models Robustness under the Microscope Optical Aberrations**
<br/>
<button class="copy-to-clipboard" title="Benchmarking the Cell Image Segmentation Models Robustness under the Microscope Optical Aberrations" index=157>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV, physics-bio-ph  
Keyword Score: 51  
Keywords: Convolutional Neural Network, Convolutional Neural Network, Benchmarking, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Deep Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08549v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08549v1.pdf" filename="2404.08549v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Cell segmentation is essential in biomedical research for analyzing cellular morphology and behavior. Deep learning methods, particularly <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> have revolutionized cell segmentation by extracting intricate features from images. However, the robustness of these methods under microscope optical aberrations remains a critical challenge. This study comprehensively evaluates the performance of cell instance segmentation models under simulated aberration conditions using the DynamicNuclearNet <b>(DNN)</b> and LIVECell datasets. Aberrations, including Astigmatism, Coma, Spherical, and Trefoil, were simulated using Zernike polynomial equations. Various segmentation models, such as Mask R-CNN with different network heads (FPN, C3) and backbones (ResNet, VGG19, SwinS), were trained and tested under aberrated conditions. Results indicate that FPN combined with SwinS demonstrates superior robustness in handling simple cell images affected by minor aberrations. Conversely, Cellpose2.0 proves effective for complex cell images under similar conditions. Our findings provide insights into selecting appropriate segmentation models based on cell morphology and aberration severity, enhancing the reliability of cell segmentation in biomedical applications. Further research is warranted to validate these methods with diverse aberration types and emerging segmentation models. Overall, this research aims to guide researchers in effectively utilizing cell segmentation models in the presence of minor optical aberrations.

{{</citation>}}


### (2/9 | 158/235) Multi-Branch Generative Models for Multichannel Imaging with an Application to PET/CT Joint Reconstruction (Noel Jeffrey Pinton et al., 2024)

{{<citation>}}

Noel Jeffrey Pinton, Alexandre Bousse, Catherine Cheze-Le-Rest, Dimitris Visvikis. (2024)  
**Multi-Branch Generative Models for Multichannel Imaging with an Application to PET/CT Joint Reconstruction**
<br/>
<button class="copy-to-clipboard" title="Multi-Branch Generative Models for Multichannel Imaging with an Application to PET/CT Joint Reconstruction" index=158>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV, physics-med-ph  
Keyword Score: 50  
Keywords: MNIST, Autoencoder, Generative Adversarial Network, Generative Adversarial Network, Variational Autoencoder  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08748v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08748v1.pdf" filename="2404.08748v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a proof-of-concept approach for learned synergistic reconstruction of medical images using multi-branch <b>generative</b> <b>models.</b> <b>Leveraging</b> <b>variational</b> <b>autoencoders</b> (VAEs) and <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs),</b> our models learn from pairs of images simultaneously, enabling effective denoising and reconstruction. Synergistic image reconstruction is achieved by incorporating the trained models in a regularizer that evaluates the distance between the images and the model, in a similar fashion to multichannel dictionary learning (DiL). We demonstrate the efficacy of our approach on both Modified National Institute of Standards and Technology <b>(MNIST)</b> and positron emission tomography (PET)/computed tomography (CT) datasets, showcasing improved image quality and information sharing between modalities. Despite challenges such as patch decomposition and model limitations, our results underscore the potential of <b>generative</b> <b>models</b> <b>for</b> enhancing medical imaging reconstruction.

{{</citation>}}


### (3/9 | 159/235) Convolutional neural network classification of cancer cytopathology images: taking breast cancer as an example (MingXuan Xiao et al., 2024)

{{<citation>}}

MingXuan Xiao, Yufeng Li, Xu Yan, Min Gao, Weimin Wang. (2024)  
**Convolutional neural network classification of cancer cytopathology images: taking breast cancer as an example**
<br/>
<button class="copy-to-clipboard" title="Convolutional neural network classification of cancer cytopathology images: taking breast cancer as an example" index=159>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 50  
Keywords: Convolutional Neural Network, Convolutional Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transfer Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08279v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08279v1.pdf" filename="2404.08279v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Breast cancer is a relatively common cancer among gynecological cancers. Its diagnosis often relies on the pathology of cells in the lesion. The pathological diagnosis of breast cancer not only requires professionals and time, but also sometimes involves subjective judgment. To address the challenges of dependence on pathologists expertise and the time-consuming nature of achieving accurate breast pathological image classification, this paper introduces an approach utilizing <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> for the rapid categorization of pathological images, aiming to enhance the efficiency of breast pathological image detection. And the approach enables the rapid and automatic classification of pathological images into benign and malignant groups. The methodology involves utilizing a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> model leveraging the Inceptionv3 architecture and <b>transfer</b> <b>learning</b> algorithm for extracting features from pathological images. Utilizing a neural network with fully connected layers and employing the SoftMax function for image classification. Additionally, the concept of image partitioning is introduced to handle high-resolution images. To achieve the ultimate classification outcome, the classification probabilities of each image block are aggregated using three algorithms: summation, product, and maximum. Experimental validation was conducted on the BreaKHis public dataset, resulting in accuracy rates surpassing 0.92 across all four magnification coefficients (40X, 100X, 200X, and 400X). It demonstrates that the proposed method effectively enhances the accuracy in classifying pathological images of breast cancer.

{{</citation>}}


### (4/9 | 160/235) Structured Model Pruning for Efficient Inference in Computational Pathology (Mohammed Adnan et al., 2024)

{{<citation>}}

Mohammed Adnan, Qinle Ba, Nazim Shaikh, Shivam Kalra, Satarupa Mukherjee, Auranuch Lorsakul. (2024)  
**Structured Model Pruning for Efficient Inference in Computational Pathology**
<br/>
<button class="copy-to-clipboard" title="Structured Model Pruning for Efficient Inference in Computational Pathology" index=160>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 40  
Keywords: Model Compression, Model Pruning, Pruning, Scaling Law  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08831v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08831v1.pdf" filename="2404.08831v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Recent years have seen significant efforts to adopt Artificial Intelligence (AI) in healthcare for various use cases, from computer-aided diagnosis to ICU triage. However, the size of AI <b>models</b> <b>has</b> been rapidly growing due to <b>scaling</b> <b>laws</b> and the success of foundational <b>models,</b> <b>which</b> poses an increasing challenge to leverage advanced <b>models</b> <b>in</b> practical applications. It is thus imperative to develop efficient <b>models,</b> <b>especially</b> for deploying AI solutions under resource-constrains or with time sensitivity. One potential solution is to perform <b>model</b> <b>compression,</b> a set of techniques that remove less important <b>model</b> <b>components</b> or reduce parameter precision, to reduce <b>model</b> <b>computation</b> demand. In this work, we demonstrate that <b>model</b> <b>pruning,</b> as a <b>model</b> <b>compression</b> technique, can effectively reduce inference cost for computational and digital pathology based analysis with a negligible loss of analysis performance. To this end, we develop a methodology for <b>pruning</b> the widely used U-Net-style architectures in biomedical imaging, with which we evaluate multiple <b>pruning</b> heuristics on nuclei instance segmentation and classification, and empirically demonstrate that <b>pruning</b> can compress <b>models</b> <b>by</b> at least 70% with a negligible drop in performance.

{{</citation>}}


### (5/9 | 161/235) Lossy Image Compression with Foundation Diffusion Models (Lucas Relic et al., 2024)

{{<citation>}}

Lucas Relic, Roberto Azevedo, Markus Gross, Christopher Schroers. (2024)  
**Lossy Image Compression with Foundation Diffusion Models**
<br/>
<button class="copy-to-clipboard" title="Lossy Image Compression with Foundation Diffusion Models" index=161>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 40  
Keywords: Diffusion Model, Fine-tuning, Foundation Model, Quantization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08580v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08580v1.pdf" filename="2404.08580v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Incorporating <b>diffusion</b> <b>models</b> in the image compression domain has the potential to produce realistic and detailed reconstructions, especially at extremely low bitrates. Previous methods focus on using <b>diffusion</b> <b>models</b> as expressive decoders robust to <b>quantization</b> errors in the conditioning signals, yet achieving competitive results in this manner requires costly training of the <b>diffusion</b> <b>model</b> and long inference times due to the iterative generative process. In this work we formulate the removal of <b>quantization</b> error as a denoising task, using <b>diffusion</b> <b>to</b> recover lost information in the transmitted image latent. Our approach allows us to perform less than 10\% of the full <b>diffusion</b> <b>generative</b> process and requires no architectural changes to the <b>diffusion</b> <b>model,</b> enabling the use of <b>foundation</b> <b>models</b> as a strong prior without additional fine tuning of the backbone. Our proposed codec outperforms previous methods in quantitative realism metrics, and we verify that our reconstructions are qualitatively preferred by end users, even when other methods use twice the bitrate.

{{</citation>}}


### (6/9 | 162/235) Simulation of a Vision Correction Display System (Vidya Sunil et al., 2024)

{{<citation>}}

Vidya Sunil, Renu M Rameshan. (2024)  
**Simulation of a Vision Correction Display System**
<br/>
<button class="copy-to-clipboard" title="Simulation of a Vision Correction Display System" index=162>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08238v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08238v1.pdf" filename="2404.08238v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Eyes serve as our primary sensory organs, responsible for processing up to 80\% of our sensory input. However, common visual aberrations like myopia and hyperopia affect a significant portion of the global population. This paper focuses on simulating a Vision Correction Display (VCD) to enhance the visual experience of individuals with various visual impairments. Utilising Blender, we digitally model the functionality of a VCD in correcting refractive errors such as myopia and hyperopia. With these <b>simulations</b> we can see potential improvements in visual acuity and comfort. These <b>simulations</b> provide valuable insights for the design and development of future VCD technologies, ultimately advancing accessibility and usability for individuals with visual challenges.

{{</citation>}}


### (7/9 | 163/235) Real-time guidewire tracking and segmentation in intraoperative x-ray (Baochang Zhang et al., 2024)

{{<citation>}}

Baochang Zhang, Mai Bui, Cheng Wang, Felix Bourier, Heribert Schunkert, Nassir Navab. (2024)  
**Real-time guidewire tracking and segmentation in intraoperative x-ray**
<br/>
<button class="copy-to-clipboard" title="Real-time guidewire tracking and segmentation in intraoperative x-ray" index=163>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess.IV  
Keyword Score: 10  
Keywords: Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08805v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08805v1.pdf" filename="2404.08805v1.pdf">Download PDF</button>

---


**ABSTRACT**  
During endovascular interventions, physicians have to perform accurate and immediate operations based on the available real-time information, such as the shape and position of guidewires observed on the fluoroscopic images, haptic information and the patients' physiological signals. For this purpose, real-time and accurate guidewire segmentation and tracking can enhance the visualization of guidewires and provide visual feedback for physicians during the intervention as well as for robot-assisted interventions. Nevertheless, this task often comes with the challenge of elongated deformable structures that present themselves with low contrast in the noisy fluoroscopic image sequences. To address these issues, a two-stage deep learning framework for real-time guidewire segmentation and tracking is proposed. In the first stage, a Yolov5s detector is trained, using the original X-ray images as well as synthetic ones, which is employed to output the bounding boxes of possible target guidewires. More importantly, a refinement module based on spatiotemporal constraints is incorporated to robustly localize the guidewire and remove false detections. In the second stage, a novel and efficient network is proposed to segment the guidewire in each detected bounding box. The network contains two major modules, namely a hessian-based enhancement embedding module and a dual <b>self-attention</b> module. Quantitative and qualitative evaluations on clinical intra-operative images demonstrate that the proposed approach significantly outperforms our baselines as well as the current state of the art and, in comparison, shows higher robustness to low quality images.

{{</citation>}}


### (8/9 | 164/235) Self-Supervised k-Space Regularization for Motion-Resolved Abdominal MRI Using Neural Implicit k-Space Representation (Veronika Spieker et al., 2024)

{{<citation>}}

Veronika Spieker, Hannah Eichhorn, Jonathan K. Stelter, Wenqi Huang, Rickmer F. Braren, Daniel Rückert, Francisco Sahli Costabal, Kerstin Hammernik, Claudia Prieto, Dimitrios C. Karampinos, Julia A. Schnabel. (2024)  
**Self-Supervised k-Space Regularization for Motion-Resolved Abdominal MRI Using Neural Implicit k-Space Representation**
<br/>
<button class="copy-to-clipboard" title="Self-Supervised k-Space Regularization for Motion-Resolved Abdominal MRI Using Neural Implicit k-Space Representation" index=164>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, cs-LG, eess-IV, eess-SP, eess.IV, physics-med-ph  
Keyword Score: 10  
Keywords: Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08350v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08350v1.pdf" filename="2404.08350v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Neural implicit k-space representations have shown promising results for dynamic MRI at high temporal resolutions. Yet, their exclusive training in k-space limits the application of common image regularization methods to improve the final reconstruction. In this work, we introduce the concept of parallel imaging-inspired self-consistency (PISCO), which we incorporate as novel <b>self-supervised</b> k-space regularization enforcing a consistent neighborhood relationship. At no additional data cost, the proposed regularization significantly improves neural implicit k-space reconstructions on simulated data. Abdominal in-vivo reconstructions using PISCO result in enhanced spatio-temporal image quality compared to state-of-the-art methods. Code is available at https://github.com/vjspi/PISCO-NIK.

{{</citation>}}


### (9/9 | 165/235) A Mutual Inclusion Mechanism for Precise Boundary Segmentation in Medical Images (Yizhi Pan et al., 2024)

{{<citation>}}

Yizhi Pan, Junyi Xin, Tianhua Yang, Teeradaj Racharak, Le-Minh Nguyen, Guanqun Sun. (2024)  
**A Mutual Inclusion Mechanism for Precise Boundary Segmentation in Medical Images**
<br/>
<button class="copy-to-clipboard" title="A Mutual Inclusion Mechanism for Precise Boundary Segmentation in Medical Images" index=165>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 3  
Keywords: Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08201v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08201v1.pdf" filename="2404.08201v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In medical imaging, accurate image segmentation is crucial for quantifying diseases, assessing prognosis, and evaluating treatment outcomes. However, existing methods lack an in-depth integration of global and local features, failing to pay special attention to abnormal regions and boundary details in medical images. To this end, we present a novel deep learning-based approach, MIPC-Net, for precise boundary segmentation in medical images. Our approach, inspired by radiologists' working patterns, features two distinct modules: (i) \textbf{Mutual Inclusion of Position and Channel Attention (MIPC) module}: To enhance the precision of boundary segmentation in medical images, we introduce the MIPC module, which enhances the focus on channel information when extracting position features and vice versa; (ii) \textbf{GL-MIPC-Residue}: To improve the restoration of medical images, we propose the GL-MIPC-Residue, a global residual connection that enhances the integration of the encoder and decoder by filtering out invalid information and restoring the most effective information lost during the feature extraction process. We evaluate the performance of the proposed model using metrics such as Dice coefficient (DSC) and Hausdorff Distance (HD) on three publicly accessible datasets: Synapse, ISIC2018-Task, and Segpc. Our ablation study shows that each module contributes to improving the quality of segmentation results. Furthermore, with the assistance of both modules, our approach outperforms state-of-the-art methods across all metrics on the <b>benchmark</b> datasets, notably achieving a 2.23mm reduction in HD on the Synapse dataset, strongly evidencing our model's enhanced capability for precise image boundary segmentation. Codes will be available at https://github.com/SUN-1024/MIPC-Net.

{{</citation>}}


## physics.flu-dyn (1)



### (1/1 | 166/235) PiRD: Physics-informed Residual Diffusion for Flow Field Reconstruction (Siming Shan et al., 2024)

{{<citation>}}

Siming Shan, Pengkai Wang, Song Chen, Jiaxu Liu, Chao Xu, Shengze Cai. (2024)  
**PiRD: Physics-informed Residual Diffusion for Flow Field Reconstruction**
<br/>
<button class="copy-to-clipboard" title="PiRD: Physics-informed Residual Diffusion for Flow Field Reconstruction" index=166>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.flu-dyn  
Categories: cs-AI, physics-flu-dyn, physics.flu-dyn  
Keyword Score: 50  
Keywords: Convolutional Neural Network, Convolutional Neural Network, Diffusion Model, Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08412v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08412v1.pdf" filename="2404.08412v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The use of machine learning in fluid dynamics is becoming more common to expedite the computation when solving forward and inverse problems of partial differential equations. Yet, a notable challenge with existing <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)-based</b> methods for data fidelity enhancement is their reliance on specific low-fidelity data patterns and distributions during the training phase. In addition, the <b>CNN-based</b> method essentially treats the flow reconstruction task as a computer vision task that prioritizes the element-wise precision which lacks a physical and mathematical explanation. This dependence can dramatically affect the models' effectiveness in real-world scenarios, especially when the low-fidelity input deviates from the training data or contains noise not accounted for during training. The introduction of <b>diffusion</b> <b>models</b> in this context shows promise for improving performance and generalizability. Unlike direct mapping from a specific low-fidelity to a high-fidelity distribution, <b>diffusion</b> <b>models</b> learn to transition from any low-fidelity distribution towards a high-fidelity one. Our proposed model - Physics-informed Residual <b>Diffusion,</b> <b>demonstrates</b> the capability to elevate the quality of data from both standard low-fidelity inputs, to low-fidelity inputs with injected Gaussian noise, and randomly collected samples. By integrating physics-based insights into the objective function, it further refines the accuracy and the fidelity of the inferred high-quality data. Experimental results have shown that our approach can effectively reconstruct high-quality outcomes for two-dimensional turbulent flows from a range of low-fidelity input conditions without requiring retraining.

{{</citation>}}


## cs.NE (4)



### (1/4 | 167/235) NeuroLGP-SM: Scalable Surrogate-Assisted Neuroevolution for Deep Neural Networks (Fergal Stapleton et al., 2024)

{{<citation>}}

Fergal Stapleton, Edgar Galván. (2024)  
**NeuroLGP-SM: Scalable Surrogate-Assisted Neuroevolution for Deep Neural Networks**
<br/>
<button class="copy-to-clipboard" title="NeuroLGP-SM: Scalable Surrogate-Assisted Neuroevolution for Deep Neural Networks" index=167>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-AI, cs-NE, cs.NE  
Keyword Score: 45  
Keywords: Convolutional Neural Network, Autoencoder, Convolution, Convolutional Neural Network, Deep Neural Network, Deep Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08786v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08786v1.pdf" filename="2404.08786v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Evolutionary Algorithms (EAs) play a crucial role in the architectural configuration and training of Artificial <b>Deep</b> <b>Neural</b> <b>Networks</b> <b>(DNNs),</b> a process known as neuroevolution. However, neuroevolution is hindered by its inherent computational expense, requiring multiple generations, a large population, and numerous epochs. The most computationally intensive aspect lies in evaluating the fitness function of a single candidate solution. To address this challenge, we employ Surrogate-assisted EAs (SAEAs). While a few SAEAs approaches have been proposed in neuroevolution, none have been applied to truly large <b>DNNs</b> due to issues like intractable information usage. In this work, drawing inspiration from Genetic Programming semantics, we use phenotypic distance vectors, outputted from <b>DNNs,</b> alongside Kriging Partial Least Squares (KPLS), an approach that is effective in handling these large vectors, making them suitable for search. Our proposed approach, named Neuro-Linear Genetic Programming surrogate model (NeuroLGP-SM), efficiently and accurately estimates <b>DNN</b> fitness without the need for complete evaluations. NeuroLGP-SM demonstrates competitive or superior results compared to 12 other methods, including NeuroLGP without SM, <b>convolutional</b> <b>neural</b> <b>networks,</b> support vector machines, and <b>autoencoders.</b> Additionally, it is worth noting that NeuroLGP-SM is 25% more energy-efficient than its NeuroLGP counterpart. This efficiency advantage adds to the overall appeal of our proposed NeuroLGP-SM in optimising the configuration of large <b>DNNs.</b>

{{</citation>}}


### (2/4 | 168/235) An Integrated Toolbox for Creating Neuromorphic Edge Applications (Lars Niedermeier et al., 2024)

{{<citation>}}

Lars Niedermeier, Jeffrey L. Krichmar. (2024)  
**An Integrated Toolbox for Creating Neuromorphic Edge Applications**
<br/>
<button class="copy-to-clipboard" title="An Integrated Toolbox for Creating Neuromorphic Edge Applications" index=168>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-AI, cs-NE, cs.NE  
Keyword Score: 45  
Keywords: Deep Neural Network, Generative AI, Simulation, Simulator, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08726v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08726v1.pdf" filename="2404.08726v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Spiking Neural Networks (SNNs) and neuromorphic models are more efficient and have more biological realism than the activation functions typically used in <b>deep</b> <b>neural</b> <b>networks,</b> <b>transformer</b> models and <b>generative</b> <b>AI.</b> SNNs have local learning rules, are able to learn on small data sets, and can adapt through neuromodulation. Although research has shown their advantages, there are still few compelling practical applications, especially at the edge where sensors and actuators need to be processed in a timely fashion. One reason for this might be that SNNs are much more challenging to understand, build, and operate due to their intrinsic properties. For instance, the mathematical foundation involves differential equations rather than basic activation functions. To address these challenges, we have developed CARLsim++. It is an integrated toolbox that enables fast and easy creation of neuromorphic applications. It encapsulates the mathematical intrinsics and low-level C++ programming by providing a graphical user interface for users who do not have a background in software engineering but still want to create neuromorphic models. Developers can easily configure inputs and outputs to devices and robots. These can be accurately simulated before deploying on physical devices. CARLsim++ can lead to rapid development of neuromorphic applications for <b>simulation</b> or edge processing.

{{</citation>}}


### (3/4 | 169/235) RLEMMO: Evolutionary Multimodal Optimization Assisted By Deep Reinforcement Learning (Hongqiao Lian et al., 2024)

{{<citation>}}

Hongqiao Lian, Zeyuan Ma, Hongshu Guo, Ting Huang, Yue-Jiao Gong. (2024)  
**RLEMMO: Evolutionary Multimodal Optimization Assisted By Deep Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="RLEMMO: Evolutionary Multimodal Optimization Assisted By Deep Reinforcement Learning" index=169>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-AI, cs-NE, cs.NE  
Keyword Score: 19  
Keywords: Benchmarking, Multi-modal, Multi-modal, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08242v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08242v1.pdf" filename="2404.08242v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Solving <b>multimodal</b> optimization problems (MMOP) requires finding all optimal solutions, which is challenging in limited function evaluations. Although existing works strike the balance of exploration and exploitation through hand-crafted adaptive strategies, they require certain expert knowledge, hence inflexible to deal with MMOP with different properties. In this paper, we propose RLEMMO, a Meta-Black-Box Optimization framework, which maintains a population of solutions and incorporates a <b>reinforcement</b> <b>learning</b> agent for flexibly adjusting individual-level searching strategies to match the up-to-date optimization status, hence boosting the search performance on MMOP. Concretely, we encode landscape properties and evolution path information into each individual and then leverage attention networks to advance population information sharing. With a novel reward mechanism that encourages both quality and diversity, RLEMMO can be effectively trained using a policy gradient algorithm. The experimental results on the CEC2013 MMOP <b>benchmark</b> underscore the competitive optimization performance of RLEMMO against several strong baselines.

{{</citation>}}


### (4/4 | 170/235) Auto-configuring Exploration-Exploitation Tradeoff in Evolutionary Computation via Deep Reinforcement Learning (Zeyuan Ma et al., 2024)

{{<citation>}}

Zeyuan Ma, Jiacheng Chen, Hongshu Guo, Yining Ma, Yue-Jiao Gong. (2024)  
**Auto-configuring Exploration-Exploitation Tradeoff in Evolutionary Computation via Deep Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Auto-configuring Exploration-Exploitation Tradeoff in Evolutionary Computation via Deep Reinforcement Learning" index=170>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-AI, cs-NE, cs.NE  
Keyword Score: 18  
Keywords: Benchmarking, Black Box, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08239v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08239v1.pdf" filename="2404.08239v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Evolutionary computation (EC) algorithms, renowned as powerful <b>black-box</b> <b>optimizers,</b> leverage a group of individuals to cooperatively search for the optimum. The exploration-exploitation tradeoff (EET) plays a crucial role in EC, which, however, has traditionally been governed by manually designed rules. In this paper, we propose a deep <b>reinforcement</b> <b>learning-based</b> framework that autonomously configures and adapts the EET throughout the EC search process. The framework allows different individuals of the population to selectively attend to the global and local exemplars based on the current search state, maximizing the cooperative search outcome. Our proposed framework is characterized by its simplicity, effectiveness, and generalizability, with the potential to enhance numerous existing EC algorithms. To validate its capabilities, we apply our framework to several representative EC algorithms and conduct extensive experiments on the augmented CEC2021 <b>benchmark.</b> The results demonstrate significant improvements in the performance of the backbone algorithms, as well as favorable generalization across diverse problem classes, dimensions, and population sizes. Additionally, we provide an in-depth analysis of the EET issue by interpreting the learned behaviors of EC.

{{</citation>}}


## cs.AI (5)



### (1/5 | 171/235) Assessing Economic Viability: A Comparative Analysis of Total Cost of Ownership for Domain-Adapted Large Language Models versus State-of-the-art Counterparts in Chip Design Coding Assistance (Amit Sharma et al., 2024)

{{<citation>}}

Amit Sharma, Teodor-Dumitru Ene, Kishor Kunal, Mingjie Liu, Zafar Hasan, Haoxing Ren. (2024)  
**Assessing Economic Viability: A Comparative Analysis of Total Cost of Ownership for Domain-Adapted Large Language Models versus State-of-the-art Counterparts in Chip Design Coding Assistance**
<br/>
<button class="copy-to-clipboard" title="Assessing Economic Viability: A Comparative Analysis of Total Cost of Ownership for Domain-Adapted Large Language Models versus State-of-the-art Counterparts in Chip Design Coding Assistance" index=171>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-CE, cs-LG, cs.AI  
Keyword Score: 40  
Keywords: ChatGPT, Claude, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08850v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08850v1.pdf" filename="2404.08850v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper presents a comparative analysis of total cost of ownership (TCO) and performance between domain-adapted <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> and state-of-the-art (SoTA) <b>LLMs</b> , with a particular emphasis on tasks related to coding assistance for chip design. We examine the TCO and performance metrics of a domain-adaptive <b>LLM,</b> ChipNeMo, against two leading <b>LLMs,</b> <b>Claude</b> 3 Opus and <b>ChatGPT-4</b> Turbo, to assess their efficacy in chip design coding generation. Through a detailed evaluation of the accuracy of the model, training methodologies, and operational expenditures, this study aims to provide stakeholders with critical information to select the most economically viable and performance-efficient solutions for their specific needs. Our results underscore the benefits of employing domain-adapted models, such as ChipNeMo, that demonstrate improved performance at significantly reduced costs compared to their general-purpose counterparts. In particular, we reveal the potential of domain-adapted <b>LLMs</b> to decrease TCO by approximately 90%-95%, with the cost advantages becoming increasingly evident as the deployment scale expands. With expansion of deployment, the cost benefits of ChipNeMo become more pronounced, making domain-adaptive <b>LLMs</b> an attractive option for organizations with substantial coding needs supported by <b>LLMs</b>

{{</citation>}}


### (2/5 | 172/235) Memory Traces: Are Transformers Tulving Machines? (Jean-Marie Chauvet, 2024)

{{<citation>}}

Jean-Marie Chauvet. (2024)  
**Memory Traces: Are Transformers Tulving Machines?**
<br/>
<button class="copy-to-clipboard" title="Memory Traces: Are Transformers Tulving Machines?" index=172>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: I-2-4, cs-AI, cs.AI  
Keyword Score: 30  
Keywords: Foundation Model, Transformer, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08543v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08543v1.pdf" filename="2404.08543v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Memory traces--changes in the memory system that result from the perception and encoding of an event--were measured in pioneering studies by Endel Tulving and Michael J. Watkins in 1975. These and further experiments informed the maturation of Tulving's memory model, from the GAPS (General Abstract Processing System} to the SPI (Serial-Parallel Independent) model. Having current top of the line <b>LLMs</b> revisit the original Tulving-Watkins tests may help in assessing whether <b>foundation</b> <b>models</b> completely instantiate or not this class of psychological models.

{{</citation>}}


### (3/5 | 173/235) Complexity of Probabilistic Reasoning for Neurosymbolic Classification Techniques (Arthur Ledaguenel et al., 2024)

{{<citation>}}

Arthur Ledaguenel, Céline Hudelot, Mostepha Khouadjia. (2024)  
**Complexity of Probabilistic Reasoning for Neurosymbolic Classification Techniques**
<br/>
<button class="copy-to-clipboard" title="Complexity of Probabilistic Reasoning for Neurosymbolic Classification Techniques" index=173>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-CC, cs-LG, cs-SC, cs.AI  
Keyword Score: 30  
Keywords: Supervised Learning, Probabilistic Reasoning, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08404v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08404v1.pdf" filename="2404.08404v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Neurosymbolic artificial intelligence is a growing field of research aiming to combine neural network learning capabilities with the <b>reasoning</b> abilities of symbolic systems. Informed multi-label classification is a sub-field of neurosymbolic AI which studies how to leverage prior knowledge to improve neural classification systems. A well known family of neurosymbolic techniques for informed classification use <b>probabilistic</b> <b>reasoning</b> to integrate this knowledge during learning, inference or both. Therefore, the asymptotic complexity of <b>probabilistic</b> <b>reasoning</b> is of cardinal importance to assess the scalability of such techniques. However, this topic is rarely tackled in the neurosymbolic literature, which can lead to a poor understanding of the limits of <b>probabilistic</b> <b>neurosymbolic</b> techniques. In this paper, we introduce a formalism for informed <b>supervised</b> classification tasks and techniques. We then build upon this formalism to define three abstract neurosymbolic techniques based on <b>probabilistic</b> <b>reasoning.</b> Finally, we show computational complexity results on several representation languages for prior knowledge commonly found in the neurosymbolic literature.

{{</citation>}}


### (4/5 | 174/235) Handling Reward Misspecification in the Presence of Expectation Mismatch (Sarath Sreedharan et al., 2024)

{{<citation>}}

Sarath Sreedharan, Malek Mechergui. (2024)  
**Handling Reward Misspecification in the Presence of Expectation Mismatch**
<br/>
<button class="copy-to-clipboard" title="Handling Reward Misspecification in the Presence of Expectation Mismatch" index=174>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-LG, cs.AI  
Keyword Score: 13  
Keywords: Benchmarking, Markov Decision Process  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08791v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08791v1.pdf" filename="2404.08791v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Detecting and handling misspecified objectives, such as reward functions, has been widely recognized as one of the central challenges within the domain of Artificial Intelligence (AI) safety research. However, even with the recognition of the importance of this problem, we are unaware of any works that attempt to provide a clear definition for what constitutes (a) misspecified objectives and (b) successfully resolving such misspecifications. In this work, we use the theory of mind, i.e., the human user's beliefs about the AI agent, as a basis to develop a formal explanatory framework called Expectation Alignment (EAL) to understand the objective misspecification and its causes. Our \EAL\ framework not only acts as an explanatory framework for existing works but also provides us with concrete insights into the limitations of existing methods to handle reward misspecification and novel solution strategies. We use these insights to propose a new interactive algorithm that uses the specified reward to infer potential user expectations about the system behavior. We show how one can efficiently implement this algorithm by mapping the inference problem into linear programs. We evaluate our method on a set of standard <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) <b>benchmarks.</b>

{{</citation>}}


### (5/5 | 175/235) Study of Emotion Concept Formation by Integrating Vision, Physiology, and Word Information using Multilayered Multimodal Latent Dirichlet Allocation (Kazuki Tsurumaki et al., 2024)

{{<citation>}}

Kazuki Tsurumaki, Chie Hieida, Kazuki Miyazawa. (2024)  
**Study of Emotion Concept Formation by Integrating Vision, Physiology, and Word Information using Multilayered Multimodal Latent Dirichlet Allocation**
<br/>
<button class="copy-to-clipboard" title="Study of Emotion Concept Formation by Integrating Vision, Physiology, and Word Information using Multilayered Multimodal Latent Dirichlet Allocation" index=175>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-HC, cs-LG, cs-RO, cs-SC, cs.AI  
Keyword Score: 6  
Keywords: Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08295v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08295v1.pdf" filename="2404.08295v1.pdf">Download PDF</button>

---


**ABSTRACT**  
How are emotions formed? Through extensive debate and the promulgation of diverse theories , the theory of constructed emotion has become prevalent in recent research on emotions. According to this theory, an emotion concept refers to a category formed by interoceptive and exteroceptive information associated with a specific emotion. An emotion concept stores past experiences as knowledge and can predict unobserved information from acquired information. Therefore, in this study, we attempted to model the formation of emotion concepts using a constructionist approach from the perspective of the constructed emotion theory. Particularly, we constructed a model using multilayered <b>multimodal</b> latent Dirichlet allocation , which is a probabilistic generative model. We then trained the model for each subject using vision, physiology, and word information obtained from multiple people who experienced different visual emotion-evoking stimuli. To evaluate the model, we verified whether the formed categories matched human subjectivity and determined whether unobserved information could be predicted via categories. The verification results exceeded chance level, suggesting that emotion concept formation can be explained by the proposed model.

{{</citation>}}


## stat.ML (7)



### (1/7 | 176/235) Sliding down the stairs: how correlated latent variables accelerate learning with neural networks (Lorenzo Bardone et al., 2024)

{{<citation>}}

Lorenzo Bardone, Sebastian Goldt. (2024)  
**Sliding down the stairs: how correlated latent variables accelerate learning with neural networks**
<br/>
<button class="copy-to-clipboard" title="Sliding down the stairs: how correlated latent variables accelerate learning with neural networks" index=176>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cond-mat-stat-mech, cs-LG, math-PR, math-ST, stat-ML, stat-TH, stat.ML  
Keyword Score: 40  
Keywords: Simulation, Simulator, Stochastic Gradient Descent, Stochastic Gradient Descent  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08602v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08602v1.pdf" filename="2404.08602v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Neural networks extract features from data using <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD).</b> In particular, higher-order input cumulants (HOCs) are crucial for their performance. However, extracting information from the $p$th cumulant of $d$-dimensional inputs is computationally hard: the number of samples required to recover a single direction from an order-$p$ tensor (tensor PCA) using online <b>SGD</b> grows as $d^{p-1}$, which is prohibitive for high-dimensional inputs. This result raises the question of how neural networks extract relevant directions from the HOCs of their inputs efficiently. Here, we show that correlations between latent variables along the directions encoded in different input cumulants speed up learning from higher-order correlations. We show this effect analytically by deriving nearly sharp thresholds for the number of samples required by a single neuron to weakly-recover these directions using online <b>SGD</b> from a random start in high dimensions. Our analytical results are confirmed in <b>simulations</b> of two-layer neural networks and unveil a new mechanism for hierarchical learning in neural networks.

{{</citation>}}


### (2/7 | 177/235) Differentially Private Log-Location-Scale Regression Using Functional Mechanism (Jiewen Sheng et al., 2024)

{{<citation>}}

Jiewen Sheng, Xiaolei Fang. (2024)  
**Differentially Private Log-Location-Scale Regression Using Functional Mechanism**
<br/>
<button class="copy-to-clipboard" title="Differentially Private Log-Location-Scale Regression Using Functional Mechanism" index=177>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-CR, cs-LG, stat-AP, stat-ML, stat.ML  
Keyword Score: 33  
Keywords: Sample Size, Simulation, Simulator, Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08715v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08715v1.pdf" filename="2404.08715v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This article introduces differentially private log-location-scale (DP-LLS) regression models, which incorporate <b>differential</b> <b>privacy</b> into LLS regression through the functional mechanism. The proposed models are established by injecting noise into the log-likelihood function of LLS regression for perturbed parameter estimation. We will derive the sensitivities utilized to determine the magnitude of the injected noise and prove that the proposed DP-LLS models satisfy $\epsilon$-differential privacy. In addition, we will conduct <b>simulations</b> and case studies to evaluate the performance of the proposed models. The findings suggest that predictor dimension, training <b>sample</b> <b>size,</b> and privacy budget are three key factors impacting the performance of the proposed DP-LLS regression models. Moreover, the results indicate that a sufficiently large training dataset is needed to simultaneously ensure decent performance of the proposed models and achieve a satisfactory level of privacy protection.

{{</citation>}}


### (3/7 | 178/235) On the Independence Assumption in Neurosymbolic Learning (Emile van Krieken et al., 2024)

{{<citation>}}

Emile van Krieken, Pasquale Minervini, Edoardo M. Ponti, Antonio Vergari. (2024)  
**On the Independence Assumption in Neurosymbolic Learning**
<br/>
<button class="copy-to-clipboard" title="On the Independence Assumption in Neurosymbolic Learning" index=178>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-AI, cs-LG, stat-ML, stat.ML  
Keyword Score: 30  
Keywords: Probabilistic Model, Probabilistic Reasoning, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08458v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08458v1.pdf" filename="2404.08458v1.pdf">Download PDF</button>

---


**ABSTRACT**  
State-of-the-art neurosymbolic learning systems use <b>probabilistic</b> <b>reasoning</b> to guide neural networks towards predictions that conform to logical constraints over symbols. Many such systems assume that the probabilities of the considered symbols are conditionally independent given the input to simplify learning and <b>reasoning.</b> We study and criticise this assumption, highlighting how it can hinder optimisation and prevent uncertainty quantification. We prove that loss functions bias conditionally independent neural networks to become overconfident in their predictions. As a result, they are unable to represent uncertainty over multiple valid options. Furthermore, we prove that these loss functions are difficult to optimise: they are non-convex, and their minima are usually highly disconnected. Our theoretical analysis gives the foundation for replacing the conditional independence assumption and designing more expressive neurosymbolic <b>probabilistic</b> <b>models.</b>

{{</citation>}}


### (4/7 | 179/235) Language Model Prompt Selection via Simulation Optimization (Haoting Zhang et al., 2024)

{{<citation>}}

Haoting Zhang, Jinghai He, Rhonda Righter, Zeyu Zheng. (2024)  
**Language Model Prompt Selection via Simulation Optimization**
<br/>
<button class="copy-to-clipboard" title="Language Model Prompt Selection via Simulation Optimization" index=179>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-AI, cs-CL, cs-LG, stat-ML, stat.ML  
Keyword Score: 30  
Keywords: Simulation, Simulator, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08164v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08164v1.pdf" filename="2404.08164v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the advancement in generative language models, the selection of <b>prompts</b> has gained significant attention in recent years. A <b>prompt</b> is an instruction or description provided by the user, serving as a guide for the generative language model in content generation. Despite existing methods for <b>prompt</b> selection that are based on human labor, we consider facilitating this selection through <b>simulation</b> optimization, aiming to maximize a pre-defined score for the selected <b>prompt.</b> Specifically, we propose a two-stage framework. In the first stage, we determine a feasible set of <b>prompts</b> in sufficient numbers, where each <b>prompt</b> is represented by a moderate-dimensional vector. In the subsequent stage for evaluation and selection, we construct a surrogate model of the score regarding the moderate-dimensional vectors that represent the <b>prompts.</b> We propose sequentially selecting the <b>prompt</b> for evaluation based on this constructed surrogate model. We prove the consistency of the sequential evaluation procedure in our framework. We also conduct numerical experiments to demonstrate the efficacy of our proposed framework, providing practical instructions for implementation.

{{</citation>}}


### (5/7 | 180/235) Combining Statistical Depth and Fermat Distance for Uncertainty Quantification (Hai-Vy Nguyen et al., 2024)

{{<citation>}}

Hai-Vy Nguyen, Fabrice Gamboa, Reda Chhaibi, Sixin Zhang, Serge Gratton, Thierry Giaccone. (2024)  
**Combining Statistical Depth and Fermat Distance for Uncertainty Quantification**
<br/>
<button class="copy-to-clipboard" title="Combining Statistical Depth and Fermat Distance for Uncertainty Quantification" index=180>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-AI, cs-LG, math-PR, stat-AP, stat-ML, stat.ML  
Keyword Score: 10  
Keywords: Out-of-domain  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08476v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08476v1.pdf" filename="2404.08476v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We measure the <b>Out-of-domain</b> uncertainty in the prediction of Neural Networks using a statistical notion called ``Lens Depth'' (LD) combined with Fermat Distance, which is able to capture precisely the ``depth'' of a point with respect to a distribution in feature space, without any assumption about the form of distribution. Our method has no trainable parameter. The method is applicable to any classification model as it is applied directly in feature space at test time and does not intervene in training process. As such, it does not impact the performance of the original model. The proposed method gives excellent qualitative result on toy datasets and can give competitive or better uncertainty estimation on standard deep learning datasets compared to strong baseline methods.

{{</citation>}}


### (6/7 | 181/235) State-Space Systems as Dynamic Generative Models (Juan-Pablo Ortega et al., 2024)

{{<citation>}}

Juan-Pablo Ortega, Florian Rossmannek. (2024)  
**State-Space Systems as Dynamic Generative Models**
<br/>
<button class="copy-to-clipboard" title="State-Space Systems as Dynamic Generative Models" index=181>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: 37H05, 37N35, 62M10, 68T05, cs-LG, math-DS, math-PR, math-ST, stat-ML, stat-TH, stat.ML  
Keyword Score: 10  
Keywords: Discrete Time, Discrete Time  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08717v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08717v1.pdf" filename="2404.08717v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A probabilistic framework to study the dependence structure induced by deterministic <b>discrete-time</b> <b>state-space</b> systems between input and output processes is introduced. General sufficient conditions are formulated under which output processes exist and are unique once an input process has been fixed, a property that in the deterministic state-space literature is known as the echo state property. When those conditions are satisfied, the given state-space system becomes a generative model for probabilistic dependences between two sequence spaces. Moreover, those conditions guarantee that the output depends continuously on the input when using the Wasserstein metric. The output processes whose existence is proved are shown to be causal in a specific sense and to generalize those studied in purely deterministic situations. The results in this paper constitute a significant stochastic generalization of sufficient conditions for the deterministic echo state property to hold, in the sense that the stochastic echo state property can be satisfied under contractivity conditions that are strictly weaker than those in deterministic situations. This means that state-space systems can induce a purely probabilistic dependence structure between input and output sequence spaces even when there is no functional relation between those two spaces.

{{</citation>}}


### (7/7 | 182/235) Observation-specific explanations through scattered data approximation (Valentina Ghidini et al., 2024)

{{<citation>}}

Valentina Ghidini, Michael Multerer, Jacopo Quizi, Rohan Sen. (2024)  
**Observation-specific explanations through scattered data approximation**
<br/>
<button class="copy-to-clipboard" title="Observation-specific explanations through scattered data approximation" index=182>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-AI, cs-LG, cs-NA, math-NA, stat-ML, stat.ML  
Keyword Score: 5  
Keywords: Black Box  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08747v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08747v1.pdf" filename="2404.08747v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This work introduces the definition of observation-specific explanations to assign a score to each data point proportional to its importance in the definition of the prediction process. Such explanations involve the identification of the most influential observations for the <b>black-box</b> <b>model</b> of interest. The proposed method involves estimating these explanations by constructing a surrogate model through scattered data approximation utilizing the orthogonal matching pursuit algorithm. The proposed approach is validated on both simulated and real-world datasets.

{{</citation>}}


## cs.DB (1)



### (1/1 | 183/235) Can LLMs substitute SQL? Comparing Resource Utilization of Querying LLMs versus Traditional Relational Databases (Xiang Zhang et al., 2024)

{{<citation>}}

Xiang Zhang, Khatoon Khedri, Reza Rawassizadeh. (2024)  
**Can LLMs substitute SQL? Comparing Resource Utilization of Querying LLMs versus Traditional Relational Databases**
<br/>
<button class="copy-to-clipboard" title="Can LLMs substitute SQL? Comparing Resource Utilization of Querying LLMs versus Traditional Relational Databases" index=183>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DB  
Categories: 68-04, H-2-m, cs-AI, cs-CL, cs-DB, cs.DB  
Keyword Score: 40  
Keywords: Quantization, Mistral, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08727v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08727v1.pdf" filename="2404.08727v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can automate or substitute different types of tasks in the software engineering process. This study evaluates the resource utilization and accuracy of <b>LLM</b> in interpreting and executing natural language queries against traditional SQL within relational database management systems. We empirically examine the resource utilization and accuracy of nine <b>LLMs</b> varying from 7 to 34 Billion parameters, including Llama2 7B, Llama2 13B, <b>Mistral,</b> Mixtral, Optimus-7B, SUS-chat-34B, platypus-yi-34b, NeuralHermes-2.5-Mistral-7B and Starling-LM-7B-alpha, using a small transaction dataset. Our findings indicate that using <b>LLMs</b> for database queries incurs significant energy overhead (even small and <b>quantized</b> models), making it an environmentally unfriendly approach. Therefore, we advise against replacing relational databases with <b>LLMs</b> due to their substantial resource utilization.

{{</citation>}}


## cs.SE (3)



### (1/3 | 184/235) Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward (Xuan Xie et al., 2024)

{{<citation>}}

Xuan Xie, Jiayang Song, Zhehua Zhou, Yuheng Huang, Da Song, Lei Ma. (2024)  
**Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward**
<br/>
<button class="copy-to-clipboard" title="Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward" index=184>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs-SE, cs.SE  
Keyword Score: 33  
Keywords: Benchmarking, Fairness, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08517v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08517v1.pdf" filename="2404.08517v1.pdf">Download PDF</button>

---


**ABSTRACT**  
While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have seen widespread applications across numerous fields, their limited interpretability poses concerns regarding their safe operations from multiple aspects, e.g., truthfulness, robustness, and <b>fairness.</b> Recent research has started developing quality assurance methods for <b>LLMs,</b> introducing techniques such as offline detector-based or uncertainty estimation methods. However, these approaches predominantly concentrate on post-generation analysis, leaving the online safety analysis for <b>LLMs</b> during the generation phase an unexplored area. To bridge this gap, we conduct in this work a comprehensive evaluation of the effectiveness of existing online safety analysis methods on <b>LLMs.</b> We begin with a pilot study that validates the feasibility of detecting unsafe outputs in the early generation process. Following this, we establish the first publicly available <b>benchmark</b> of online safety analysis for <b>LLMs,</b> including a broad spectrum of methods, models, tasks, datasets, and evaluation metrics. Utilizing this <b>benchmark,</b> we extensively analyze the performance of state-of-the-art online safety analysis methods on both open-source and closed-source <b>LLMs.</b> This analysis reveals the strengths and weaknesses of individual methods and offers valuable insights into selecting the most appropriate method based on specific application scenarios and task requirements. Furthermore, we also explore the potential of using hybridization methods, i.e., combining multiple methods to derive a collective safety conclusion, to enhance the efficacy of online safety analysis for <b>LLMs.</b> Our findings indicate a promising direction for the development of innovative and trustworthy quality assurance methodologies for <b>LLMs,</b> facilitating their reliable deployments across diverse domains.

{{</citation>}}


### (2/3 | 185/235) Using Information Flow to estimate interference between developers same method contributions (Roberto Souto Maior de Barros Filho et al., 2024)

{{<citation>}}

Roberto Souto Maior de Barros Filho, Paulo Borba. (2024)  
**Using Information Flow to estimate interference between developers same method contributions**
<br/>
<button class="copy-to-clipboard" title="Using Information Flow to estimate interference between developers same method contributions" index=185>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 13  
Keywords: Graph, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08619v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08619v1.pdf" filename="2404.08619v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This work's main goal is to understand if Information Flow Control (IFC), a <b>security</b> technique used for discovering leaks in software, could be used to indicate the presence of dynamic semantic conflicts between developers contributions in merge scenarios. However, as defining if a dynamic semantic conflict exists involves understanding the expected behaviour of a system, and as such behavioural specifications are often hard to capture, formalize and reason about, we instead try to detect a code level adaptation of the notion of interference from Goguen and Meseguer. We limit our scope to interference caused by developers contributions on the same method. Therefore, we conduct an evaluation to understand if information flow may be used to estimate interference. In particular, we use Java Object-sensitive Analysis (JOANA) to do the IFC for Java programs. JOANA does the IFC of Java programs by using a System Dependence <b>Graph</b> (SDG), a directed <b>graph</b> representing the information flow through a program. Additionally, we bring evidence that information flow between developers same-method contributions occurred for around 64% of the scenarios we evaluated. Finally, we conducted a manual analysis, on 35 scenarios with information flow between developers same-method contributions, to understand the limitations of using information flow to estimate interference between same-method contributions. From the 35 analysed scenarios, for only 15 we considered that an interference in fact existed. We found three different major reasons for detecting information flow and no interference: cases related to the nature of changes, to excessive annotation from our strategy and to the conservativeness of the flows identified by JOANA. We conclude that information flow may be used to estimate interference, but, ideally, the number of false positives should be reduced.

{{</citation>}}


### (3/3 | 186/235) Automatic Recommendations for Evolving Relational Databases Schema (Anne Etien et al., 2024)

{{<citation>}}

Anne Etien, Nicolas Anquetil. (2024)  
**Automatic Recommendations for Evolving Relational Databases Schema**
<br/>
<button class="copy-to-clipboard" title="Automatic Recommendations for Evolving Relational Databases Schema" index=186>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 10  
Keywords: Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08525v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08525v1.pdf" filename="2404.08525v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Relational databases play a central role in many information systems. Their schema contains structural (e.g. tables and columns) and behavioral (e.g. stored procedures or views) entity descriptions. Then, just like for ``normal'' software, changes in legislation, offered functionalities, or functional contexts, impose to evolve databases and their schemas. But in some scenarios, it is not so easy to deconstruct a wished evolution of the schema into a precise sequence of operations. Changing a database schema may impose manually dropping and recreating dependent entities, or manually searching for dependencies in stored procedures. This is important because getting even the order of application of the operators can be difficult and have profound consequences. This meta-model allows us to compute the impact of planned changes and recommend additional changes that will ensure that the RDBMS constraints are always verified. The <b>recommendations</b> can then be compiled into a valid SQL patch actually updating the database schema in an orderly way. We replicated a past evolution showing that, without detailed knowledge of the database, we could perform the same change in 75\% less time than the expert database architect. We also exemplify the use of our approach on other planned changes.

{{</citation>}}


## cs.IR (7)



### (1/7 | 187/235) LazyDP: Co-Designing Algorithm-Software for Scalable Training of Differentially Private Recommendation Models (Juntaek Lim et al., 2024)

{{<citation>}}

Juntaek Lim, Youngeun Kwon, Ranggi Hwang, Kiwan Maeng, G. Edward Suh, Minsoo Rhu. (2024)  
**LazyDP: Co-Designing Algorithm-Software for Scalable Training of Differentially Private Recommendation Models**
<br/>
<button class="copy-to-clipboard" title="LazyDP: Co-Designing Algorithm-Software for Scalable Training of Differentially Private Recommendation Models" index=187>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-CR, cs-IR, cs-LG, cs.IR  
Keyword Score: 30  
Keywords: Recommendation, Recommender System, Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08847v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08847v1.pdf" filename="2404.08847v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Differential</b> <b>privacy</b> (DP) is widely being employed in the industry as a practical standard for privacy protection. While private training of computer vision or natural language processing applications has been studied extensively, the computational challenges of training of <b>recommender</b> <b>systems</b> (RecSys) with DP have not been explored. In this work, we first present our detailed characterization of private RecSys training using DP-SGD, root-causing its several performance bottlenecks. Specifically, we identify DP-SGD's noise sampling and noisy gradient update stage to suffer from a severe compute and memory bandwidth limitation, respectively, causing significant performance overhead in training private RecSys. Based on these findings, we propose LazyDP, an algorithm-software co-design that addresses the compute and memory challenges of training RecSys with DP-SGD. Compared to a state-of-the-art DP-SGD training system, we demonstrate that LazyDP provides an average 119x training throughput improvement while also ensuring mathematically equivalent, differentially private RecSys models to be trained.

{{</citation>}}


### (2/7 | 188/235) The Elephant in the Room: Rethinking the Usage of Pre-trained Language Model in Sequential Recommendation (Zekai Qu et al., 2024)

{{<citation>}}

Zekai Qu, Ruobing Xie, Chaojun Xiao, Xingwu Sun, Zhanhui Kang. (2024)  
**The Elephant in the Room: Rethinking the Usage of Pre-trained Language Model in Sequential Recommendation**
<br/>
<button class="copy-to-clipboard" title="The Elephant in the Room: Rethinking the Usage of Pre-trained Language Model in Sequential Recommendation" index=188>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs.IR  
Keyword Score: 30  
Keywords: Recommendation, Pre-trained Language Model, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08796v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08796v1.pdf" filename="2404.08796v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Sequential <b>recommendation</b> (SR) has seen significant advancements with the help of <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs).</b> Some <b>PLM-based</b> SR models directly use <b>PLM</b> to encode user historical behavior's text sequences to learn user representations, while there is seldom an in-depth exploration of the capability and suitability of <b>PLM</b> in behavior sequence modeling. In this work, we first conduct extensive model analyses between <b>PLMs</b> and <b>PLM-based</b> SR models, discovering great underutilization and parameter redundancy of <b>PLMs</b> in behavior sequence modeling. Inspired by this, we explore different lightweight usages of <b>PLMs</b> in SR, aiming to maximally stimulate the ability of <b>PLMs</b> for SR while satisfying the efficiency and usability demands of practical systems. We discover that adopting behavior-tuned <b>PLMs</b> for item initializations of conventional ID-based SR models is the most economical framework of <b>PLM-based</b> SR, which would not bring in any additional inference cost but could achieve a dramatic performance boost compared with the original version. Extensive experiments on five datasets show that our simple and universal framework leads to significant improvement compared to classical SR and SOTA <b>PLM-based</b> SR models without additional inference costs.

{{</citation>}}


### (3/7 | 189/235) Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking (Tianyu Zhu et al., 2024)

{{<citation>}}

Tianyu Zhu, Myong Chol Jung, Jesse Clark. (2024)  
**Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking**
<br/>
<button class="copy-to-clipboard" title="Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking" index=189>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-CV, cs-IR, cs-LG, cs.IR  
Keyword Score: 23  
Keywords: Graph Contrastive Learning, Contrastive Learning, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08535v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08535v1.pdf" filename="2404.08535v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Contrastive</b> <b>learning</b> has gained widespread adoption for retrieval tasks due to its minimal requirement for manual annotations. However, popular <b>contrastive</b> <b>frameworks</b> typically learn from binary relevance, making them ineffective at incorporating direct fine-grained rankings. In this paper, we curate a large-scale dataset featuring detailed relevance scores for each query-document pair to facilitate future research and evaluation. Subsequently, we propose Generalized <b>Contrastive</b> <b>Learning</b> for <b>Multi-Modal</b> Retrieval and Ranking <b>(GCL),</b> which is designed to learn from fine-grained rankings beyond binary relevance scores. Our results show that <b>GCL</b> achieves a 94.5% increase in NDCG@10 for in-domain and 26.3 to 48.8% increases for cold-start evaluations, all relative to the CLIP baseline and involving ground truth rankings.

{{</citation>}}


### (4/7 | 190/235) Measuring the Predictability of Recommender Systems using Structural Complexity Metrics (Alfonso Valderrama et al., 2024)

{{<citation>}}

Alfonso Valderrama, Andrés Abeliuk. (2024)  
**Measuring the Predictability of Recommender Systems using Structural Complexity Metrics**
<br/>
<button class="copy-to-clipboard" title="Measuring the Predictability of Recommender Systems using Structural Complexity Metrics" index=190>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-IR, cs-IT, cs-LG, cs.IR, math-IT  
Keyword Score: 10  
Keywords: Recommender System  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08829v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08829v1.pdf" filename="2404.08829v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Recommender</b> <b>systems</b> (RS) are central to the filtering and curation of online content. These algorithms predict user ratings for unseen items based on past preferences. Despite their importance, the innate predictability of RS has received limited attention. This study introduces data-driven metrics to measure the predictability of RS based on the structural complexity of the user-item rating matrix. A low predictability score indicates complex and unpredictable user-item interactions, while a high predictability score reveals less complex patterns with predictive potential. We propose two strategies that use singular value decomposition (SVD) and matrix factorization (MF) to measure structural complexity. By perturbing the data and evaluating the prediction of the perturbed version, we explore the structural consistency indicated by the SVD singular vectors. The assumption is that a random perturbation of highly structured data does not change its structure. Empirical results show a high correlation between our metrics and the accuracy of the best-performing prediction algorithms on real data sets.

{{</citation>}}


### (5/7 | 191/235) A Conceptual Framework for Conversational Search and Recommendation: Conceptualizing Agent-Human Interactions During the Conversational Search Process (Leif Azzopardi et al., 2024)

{{<citation>}}

Leif Azzopardi, Mateusz Dubiel, Martin Halvey, Jeffery Dalton. (2024)  
**A Conceptual Framework for Conversational Search and Recommendation: Conceptualizing Agent-Human Interactions During the Conversational Search Process**
<br/>
<button class="copy-to-clipboard" title="A Conceptual Framework for Conversational Search and Recommendation: Conceptualizing Agent-Human Interactions During the Conversational Search Process" index=191>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-AI, cs-IR, cs.IR  
Keyword Score: 10  
Keywords: Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08630v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08630v1.pdf" filename="2404.08630v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The conversational search task aims to enable a user to resolve information needs via natural language dialogue with an agent. In this paper, we aim to develop a conceptual framework of the actions and intents of users and agents explaining how these actions enable the user to explore the search space and resolve their information need. We outline the different actions and intents, before discussing key decision points in the conversation where the agent needs to decide how to steer the conversational search process to a successful and/or satisfactory conclusion. Essentially, this paper provides a conceptualization of the conversational search process between an agent and user, which provides a framework and a starting point for research, development and evaluation of conversational search agents.

{{</citation>}}


### (6/7 | 192/235) Accessibility in Information Retrieval (Leif Azzopardi et al., 2024)

{{<citation>}}

Leif Azzopardi, Vishwa Vinay. (2024)  
**Accessibility in Information Retrieval**
<br/>
<button class="copy-to-clipboard" title="Accessibility in Information Retrieval" index=192>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-DL, cs-IR, cs.IR  
Keyword Score: 10  
Keywords: Information Retrieval  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08628v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08628v1.pdf" filename="2404.08628v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces the concept of accessibility from the field of transportation planning and adopts it within the context of <b>Information</b> <b>Retrieval</b> (IR). An analogy is drawn between the fields, which motivates the development of document accessibility measures for IR systems. Considering the accessibility of documents within a collection given an IR System provides a different perspective on the analysis and evaluation of such systems which could be used to inform the design, tuning and management of current and future IR systems.

{{</citation>}}


### (7/7 | 193/235) Large-Scale Multi-Domain Recommendation: an Automatic Domain Feature Extraction and Personalized Integration Framework (Dongbo Xi et al., 2024)

{{<citation>}}

Dongbo Xi, Zhen Chen, Yuexian Wang, He Cui, Chong Peng, Fuzhen Zhuang, Peng Yan. (2024)  
**Large-Scale Multi-Domain Recommendation: an Automatic Domain Feature Extraction and Personalized Integration Framework**
<br/>
<button class="copy-to-clipboard" title="Large-Scale Multi-Domain Recommendation: an Automatic Domain Feature Extraction and Personalized Integration Framework" index=193>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-AI, cs-IR, cs.IR  
Keyword Score: 10  
Keywords: Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08361v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08361v2.pdf" filename="2404.08361v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Feed <b>recommendation</b> is currently the mainstream mode for many real-world applications (e.g., TikTok, Dianping), it is usually necessary to model and predict user interests in multiple scenarios (domains) within and even outside the application. Multi-domain learning is a typical solution in this regard. While considerable efforts have been made in this regard, there are still two long-standing challenges: (1) Accurately depicting the differences among domains using domain features is crucial for enhancing the performance of each domain. However, manually designing domain features and models for numerous domains can be a laborious task. (2) Users typically have limited impressions in only a few domains. Extracting features automatically from other domains and leveraging them to improve the predictive capabilities of each domain has consistently posed a challenging problem. In this paper, we propose an Automatic Domain Feature Extraction and Personalized Integration (DFEI) framework for the large-scale multi-domain <b>recommendation.</b> The framework automatically transforms the behavior of each individual user into an aggregation of all user behaviors within the domain, which serves as the domain features. Unlike offline feature engineering methods, the extracted domain features are higher-order representations and directly related to the target label. Besides, by personalized integration of domain features from other domains for each user and the innovation in the training mode, the DFEI framework can yield more accurate conversion identification. Experimental results on both public and industrial datasets, consisting of over 20 domains, clearly demonstrate that the proposed framework achieves significantly better performance compared with SOTA baselines. Furthermore, we have released the source code of the proposed framework at https://github.com/xidongbo/DFEI.

{{</citation>}}


## cs.DC (4)



### (1/4 | 194/235) Communication-Efficient Model Aggregation with Layer Divergence Feedback in Federated Learning (Liwei Wang et al., 2024)

{{<citation>}}

Liwei Wang, Jun Li, Wen Chen, Qingqing Wu, Ming Ding. (2024)  
**Communication-Efficient Model Aggregation with Layer Divergence Feedback in Federated Learning**
<br/>
<button class="copy-to-clipboard" title="Communication-Efficient Model Aggregation with Layer Divergence Feedback in Federated Learning" index=194>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: cs-DC, cs.DC  
Keyword Score: 30  
Keywords: Federated Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08324v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08324v1.pdf" filename="2404.08324v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Federated</b> <b>Learning</b> (FL) facilitates collaborative machine learning by training models on local datasets, and subsequently aggregating these local models at a central server. However, the frequent exchange of model parameters between clients and the central server can result in significant communication overhead during the FL training process. To solve this problem, this paper proposes a novel FL framework, the Model Aggregation with Layer Divergence Feedback mechanism (FedLDF). Specifically, we calculate model divergence between the local model and the global model from the previous round. Then through model layer divergence feedback, the distinct layers of each client are uploaded and the amount of data transferred is reduced effectively. Moreover, the convergence bound reveals that the access ratio of clients has a positive correlation with model performance. <b>Simulation</b> results show that our algorithm uploads local models with reduced communication overhead while upholding a superior global model performance.

{{</citation>}}


### (2/4 | 195/235) FlowWalker: A Memory-efficient and High-performance GPU-based Dynamic Graph Random Walk Framework (Junyi Mei et al., 2024)

{{<citation>}}

Junyi Mei, Shixuan Sun, Chao Li, Cheng Xu, Cheng Chen, Yibo Liu, Jing Wang, Cheng Zhao, Xiaofeng Hou, Minyi Guo, Bingsheng He, Xiaoliang Cong. (2024)  
**FlowWalker: A Memory-efficient and High-performance GPU-based Dynamic Graph Random Walk Framework**
<br/>
<button class="copy-to-clipboard" title="FlowWalker: A Memory-efficient and High-performance GPU-based Dynamic Graph Random Walk Framework" index=195>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: cs-DC, cs.DC  
Keyword Score: 23  
Keywords: Graph, Graph Neural Network, Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08364v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08364v2.pdf" filename="2404.08364v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Dynamic <b>graph</b> random walk (DGRW) emerges as a practical tool for capturing structural relations within a <b>graph.</b> Effectively executing DGRW on GPU presents certain challenges. First, existing sampling methods demand a pre-processing buffer, causing substantial space complexity. Moreover, the power-law distribution of <b>graph</b> vertex degrees introduces workload imbalance issues, rendering DGRW embarrassed to parallelize. In this paper, we propose FlowWalker, a GPU-based dynamic <b>graph</b> random walk framework. FlowWalker implements an efficient parallel sampling method to fully exploit the GPU parallelism and reduce space complexity. Moreover, it employs a sampler-centric paradigm alongside a dynamic scheduling strategy to handle the huge amounts of walking queries. FlowWalker stands as a memory-efficient framework that requires no auxiliary data structures in GPU global memory. We examine the performance of FlowWalker extensively on ten datasets, and experiment results show that FlowWalker achieves up to 752.2x, 72.1x, and 16.4x speedup compared with existing CPU, GPU, and FPGA random walk frameworks, respectively. Case study shows that FlowWalker diminishes random walk time from 35% to 3% in a pipeline of ByteDance friend <b>recommendation</b> <b>GNN</b> training.

{{</citation>}}


### (3/4 | 196/235) Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction (Haoran Qiu et al., 2024)

{{<citation>}}

Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbigniew T. Kalbarczyk, Tamer Başar, Ravishankar K. Iyer. (2024)  
**Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction**
<br/>
<button class="copy-to-clipboard" title="Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction" index=196>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: cs-CL, cs-DC, cs-LG, cs.DC  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08509v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08509v1.pdf" filename="2404.08509v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been driving a new wave of interactive AI applications across numerous domains. However, efficiently serving <b>LLM</b> inference requests is challenging due to their unpredictable execution times originating from the autoregressive nature of generative models. Existing <b>LLM</b> serving systems exploit first-come-first-serve (FCFS) scheduling, suffering from head-of-line blocking issues. To address the non-deterministic nature of <b>LLMs</b> and enable efficient interactive <b>LLM</b> serving, we present a speculative shortest-job-first (SSJF) scheduler that uses a light proxy model to predict <b>LLM</b> output sequence lengths. Our open-source SSJF implementation does not require changes to memory management or batching strategies. Evaluations on real-world datasets and production workload traces show that SSJF reduces average job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x compared to FCFS schedulers, across no batching, dynamic batching, and continuous batching settings.

{{</citation>}}


### (4/4 | 197/235) Efficient GPU Implementation of Static and Incrementally Expanding DF-P PageRank for Dynamic Graphs (Subhajit Sahu, 2024)

{{<citation>}}

Subhajit Sahu. (2024)  
**Efficient GPU Implementation of Static and Incrementally Expanding DF-P PageRank for Dynamic Graphs**
<br/>
<button class="copy-to-clipboard" title="Efficient GPU Implementation of Static and Incrementally Expanding DF-P PageRank for Dynamic Graphs" index=197>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DC  
Categories: G-2-2; I-5-3, cs-DC, cs-SI, cs.DC  
Keyword Score: 13  
Keywords: Graph, Pruning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08299v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08299v1.pdf" filename="2404.08299v1.pdf">Download PDF</button>

---


**ABSTRACT**  
PageRank is a widely used centrality measure that "ranks" vertices in a <b>graph</b> by considering the connections and their importance. In this report, we first introduce one of the most efficient GPU implementations of Static PageRank, which recomputes PageRank scores from scratch. It uses a synchronous pull-based atomics-free PageRank computation, with the low and high in-degree vertices being partitioned and processed by two separate kernels. Next, we present our GPU implementation of incrementally expanding (and contracting) Dynamic Frontier with <b>Pruning</b> (DF-P) PageRank, which processes only a subset of vertices likely to change ranks. It is based on Static PageRank, and uses an additional partitioning between low and high out-degree vertices for incremental expansion of the set of affected vertices with two additional kernels. On a server with an NVIDIA A100 GPU, our Static PageRank outperforms Hornet and Gunrock's PageRank implementations by 31x and 5.9x respectively. On top of the above, DF-P PageRank outperforms Static PageRank by 2.1x on real-world dynamic <b>graphs,</b> and by 3.1x on large static <b>graphs</b> with random batch updates.

{{</citation>}}


## cs.HC (5)



### (1/5 | 198/235) GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality (Jaewook Lee et al., 2024)

{{<citation>}}

Jaewook Lee, Jun Wang, Elizabeth Brown, Liam Chu, Sebastian S. Rodriguez, Jon E. Froehlich. (2024)  
**GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality**
<br/>
<button class="copy-to-clipboard" title="GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality" index=198>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 26  
Keywords: Augmented Reality (AR), Multi-modal, Multi-modal, Disambiguation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08213v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08213v1.pdf" filename="2404.08213v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Voice assistants (VAs) like Siri and Alexa are transforming human-computer interaction; however, they lack awareness of users' spatiotemporal context, resulting in limited performance and unnatural dialogue. We introduce GazePointAR, a fully-functional context-aware VA for wearable <b>augmented</b> <b>reality</b> that leverages eye gaze, pointing gestures, and conversation history to disambiguate speech queries. With GazePointAR, users can ask "what's over there?" or "how do I solve this math problem?" simply by looking and/or pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1) comparing GazePointAR to two commercial systems; (2) examining GazePointAR's pronoun <b>disambiguation</b> across three tasks; (3) and an open-ended phase where participants could suggest and try their own context-sensitive queries. Participants appreciated the naturalness and human-like nature of pronoun-driven queries, although sometimes pronoun use was counter-intuitive. We then iterated on GazePointAR and conducted a first-person diary study examining how GazePointAR performs in-the-wild. We conclude by enumerating limitations and design considerations for future context-aware VAs.

{{</citation>}}


### (2/5 | 199/235) VizGroup: An AI-Assisted Event-Driven System for Real-Time Collaborative Programming Learning Analytics (Xiaohang Tang et al., 2024)

{{<citation>}}

Xiaohang Tang, Sam Wong, Kevin Pu, Xi Chen, Yalong Yang, Yan Chen. (2024)  
**VizGroup: An AI-Assisted Event-Driven System for Real-Time Collaborative Programming Learning Analytics**
<br/>
<button class="copy-to-clipboard" title="VizGroup: An AI-Assisted Event-Driven System for Real-Time Collaborative Programming Learning Analytics" index=199>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08743v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08743v1.pdf" filename="2404.08743v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Programming instructors often conduct collaborative learning activities, like Peer Instruction, to foster a deeper understanding in students and enhance their engagement with learning. These activities, however, may not always yield productive outcomes due to the diversity of student mental models and their ineffective collaboration. In this work, we introduce VizGroup, an AI-assisted system that enables programming instructors to easily oversee students' real-time collaborative learning behaviors during <b>large</b> <b>programming</b> <b>courses.</b> VizGroup leverages <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to recommend event specifications for instructors so that they can simultaneously track and receive alerts about key correlation patterns between various collaboration metrics and ongoing coding tasks. We evaluated VizGroup with 12 instructors using a dataset collected from a Peer Instruction activity that was conducted in a <b>large</b> <b>programming</b> <b>lecture.</b> The results showed that compared to a version of VizGroup without the suggested units, VizGroup with suggested units helped instructors create additional monitoring units on previously undetected patterns on their own, covered a more diverse range of metrics, and influenced the participants' following notification creation strategies.

{{</citation>}}


### (3/5 | 200/235) The Clarkston AR Gateways Project: Anchoring Refugee Presence and Narratives in a Small Town (Joshua A. Fisher et al., 2024)

{{<citation>}}

Joshua A. Fisher, Fernando Rochaix. (2024)  
**The Clarkston AR Gateways Project: Anchoring Refugee Presence and Narratives in a Small Town**
<br/>
<button class="copy-to-clipboard" title="The Clarkston AR Gateways Project: Anchoring Refugee Presence and Narratives in a Small Town" index=200>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-ET, cs-HC, cs.HC  
Keyword Score: 20  
Keywords: Augmented Reality (AR), Augmented Reality (AR)  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08179v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08179v1.pdf" filename="2404.08179v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper outlines the Clarkston <b>AR</b> Gateways Project, a speculative process and artifact entering its second phase, where <b>Augmented</b> <b>Reality</b> <b>(AR)</b> will be used to amplify the diverse narratives of Clarkston, Georgia's refugee community. Focused on anchoring their stories and presence into the town's physical and digital landscapes, the project employs a participatory co-design approach, engaging directly with community members. This placemaking effort aims to uplift refugees by teaching them <b>AR</b> development skills that help them more autonomously express and elevate their voices through public art. The result is hoped to be <b>AR</b> experiences that not only challenge prevailing narratives but also celebrate the tapestry of cultures in the small town. This work is supported through <b>AR's</b> unique affordance for users to situate their experiences as interactive narratives within public spaces. Such site-specific <b>AR</b> interactive stories can encourage interactions within those spaces that shift how they are conceived, perceived, and experienced. This process of refugee-driven <b>AR</b> creation reflexively alters the space and affirms their presence and agency. The project's second phase aims to establish a model adaptable to diverse, refugee-inclusive communities, demonstrating how <b>AR</b> storytelling can be a powerful tool for cultural orientation and celebration.

{{</citation>}}


### (4/5 | 201/235) A Typology of Decision-Making Tasks for Visualization (Camelia D. Brumar et al., 2024)

{{<citation>}}

Camelia D. Brumar, Sam Molnar, Gabriel Appleby, Kristi Potter, Remco Chang. (2024)  
**A Typology of Decision-Making Tasks for Visualization**
<br/>
<button class="copy-to-clipboard" title="A Typology of Decision-Making Tasks for Visualization" index=201>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 10  
Keywords: Knowledge Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08812v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08812v1.pdf" filename="2404.08812v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite decision-making being a vital goal of data visualization, little work has been done to differentiate the decision-making tasks within our field. While visualization task taxonomies and typologies exist, they are often too granular for describing complex decision goals and decision-making processes, thus limiting their potential use in designing decision-support tools. In this paper, we contribute a typology of decision-making tasks that were iteratively refined from a list of design goals <b>distilled</b> from a literature review. Our typology is concise and consists of only three tasks: choose, activate, and create. Originally proposed by the scientific community, we extend and provide definitions for these tasks that are suitable for the visualization community. Our proposed typology offers two benefits. First, it facilitates the composition of decisions using these three tasks, allowing for flexible and clear descriptions across varying complexities and domains. Second, diagrams created using this typology encourage productive discourse between visualization designers and domain experts by abstracting the intricacies of data, thereby promoting clarity and rigorous analysis of decision-making processes. We motivate the use of our typology through four case studies and demonstrate the benefits of our approach through semi-structured interviews conducted with experienced members of the visualization community, comprising academic and industry experts, who have contributed to developing or publishing decision support systems for domain experts. Our interviewees composed diagrams using our typology to delineate the decision-making processes that drive their decision-support tools, demonstrating its descriptive capacity and effectiveness.

{{</citation>}}


### (5/5 | 202/235) Mixing Modes: Active and Passive Integration of Speech, Text, and Visualization for Communicating Data Uncertainty (Chase Stokes et al., 2024)

{{<citation>}}

Chase Stokes, Chelsea Sanker, Bridget Cogley, Vidya Setlur. (2024)  
**Mixing Modes: Active and Passive Integration of Speech, Text, and Visualization for Communicating Data Uncertainty**
<br/>
<button class="copy-to-clipboard" title="Mixing Modes: Active and Passive Integration of Speech, Text, and Visualization for Communicating Data Uncertainty" index=202>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 6  
Keywords: Multi-modal, Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08623v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08623v1.pdf" filename="2404.08623v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Interpreting uncertain data can be difficult, particularly if the data presentation is complex. We investigate the efficacy of different modalities for representing data and how to combine the strengths of each modality to facilitate the communication of data uncertainty. We implemented two <b>multimodal</b> prototypes to explore the design space of integrating speech, text, and visualization elements. A preliminary evaluation with 20 participants from academic and industry communities demonstrates that there exists no one-size-fits-all approach for uncertainty communication strategies; rather, the effectiveness of conveying uncertain data is intertwined with user preferences and situational context, necessitating a more refined, <b>multimodal</b> strategy for future interface design.

{{</citation>}}


## math.NA (4)



### (1/4 | 203/235) Modeling Melt Pool Geometry in Metal Additive Manufacturing Using Goldak's Semi-Ellipsoidal Heat Source: A Data-driven Computational Approach (Mohsen Asghari Ilani et al., 2024)

{{<citation>}}

Mohsen Asghari Ilani, Yaser Mike Banad. (2024)  
**Modeling Melt Pool Geometry in Metal Additive Manufacturing Using Goldak's Semi-Ellipsoidal Heat Source: A Data-driven Computational Approach**
<br/>
<button class="copy-to-clipboard" title="Modeling Melt Pool Geometry in Metal Additive Manufacturing Using Goldak's Semi-Ellipsoidal Heat Source: A Data-driven Computational Approach" index=203>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: cs-NA, math-NA, math.NA  
Keyword Score: 25  
Keywords: Geometry, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08834v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08834v1.pdf" filename="2404.08834v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This analytical solution, based on Goldak's Semi-Ellipsoidal Heat Source model, captures the dynamic temperature evolution from a semi-ellipsoidal power density moving heat source within a semi-infinite body. It tackles the convection-diffusion heat transfer equation by integrating an instantaneous point heat source across the volume of the ellipsoidal shape. The model's precision is validated by the excellent match between the predicted transient temperatures and empirical data from bead-on-plate specimens, enhancing its capability to accurately predict in-process temperature profiles in laser-based metal additive manufacturing (AM) operations. Developed in Python, the model offers customized calculations from setup to boundary conditions, adapting to variations in material properties under intense heat gradients. It considers the temperature dependency of thermal material properties and AM-process parameters, accounting for significant temperature gradients and changes in heat transfer mechanisms. The model also includes phase changes of melting or solidification with adjusted heat capacity to accurately reflect these transformations. Additionally, it considers the effects of variable laser power, scanning speed, and timing across each scanning pattern segment, acknowledging the thermal interactions between successive layers and their impact on heat transfer. This comprehensive analytical model is ready for applications including thermal stress analysis, microstructure modeling, and <b>simulation</b> of AM processes, predicting residual stresses and distortions.

{{</citation>}}


### (2/4 | 204/235) Complex variable solution on over-/under-break shallow tunnelling in gravitational geomaterial with reasonable far-field displacement (Luo-bin Lin et al., 2024)

{{<citation>}}

Luo-bin Lin, Fu-quan Chen, Jin-ping Zhuang. (2024)  
**Complex variable solution on over-/under-break shallow tunnelling in gravitational geomaterial with reasonable far-field displacement**
<br/>
<button class="copy-to-clipboard" title="Complex variable solution on over-/under-break shallow tunnelling in gravitational geomaterial with reasonable far-field displacement" index=204>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: cs-NA, math-CV, math-NA, math.NA  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08852v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08852v1.pdf" filename="2404.08852v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Over-/under-break excavation is a common phenomenon in shallow tunnelling, which is nonetheless not generally considered in existing complex variable solutions. In this paper, a new equilibrium mechanical model on over-/under-break shallow tunnelling in gravitational geomaterial is established by fixing far-field ground surface to form a corresponding mixed boundary problem. With integration of a newly proposed bidirectional composite conformal mapping using Charge <b>Simulation</b> Method, a complex variable solution of infinite complex potential series is subsequently derived using analytic continuation to tranform the mixed boundaries into a homogenerous Riemann-Hilbert problem, which is iteratively solved to obtain the stress and displacement in geomaterial. The infinite complex potential series of the complex variable solution are truncated to obtain numerical results, which is rectified by Lanczos filtering to reduce the oscillation of Gibbs phenomena. The bidirectional conformal mapping is discussed and validated via several numerical cases, and the subsequent complex variable solution is verified by examining the Lanczos filtering and solution convergence, and comparing with corresponding finite element solution and existing analytical solution. Further discussions are made to disclose possible defects of the proposed solution for objectivity.

{{</citation>}}


### (3/4 | 205/235) A backward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations (Lorenc Kapllani et al., 2024)

{{<citation>}}

Lorenc Kapllani, Long Teng. (2024)  
**A backward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations**
<br/>
<button class="copy-to-clipboard" title="A backward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations" index=205>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: 65C30, 68T07, 60H07, 91G20, cs-LG, cs-NA, math-NA, math.NA, q-fin-CP  
Keyword Score: 10  
Keywords: Deep Neural Network, Deep Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08456v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08456v1.pdf" filename="2404.08456v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we propose a novel backward differential <b>deep</b> <b>learning-based</b> <b>algorithm</b> for solving high-dimensional nonlinear backward stochastic differential equations (BSDEs), where the <b>deep</b> <b>neural</b> <b>network</b> <b>(DNN)</b> models are trained not only on the inputs and labels but also the differentials of the corresponding labels. This is motivated by the fact that differential <b>deep</b> <b>learning</b> <b>can</b> provide an efficient approximation of the labels and their derivatives with respect to inputs. The BSDEs are reformulated as differential <b>deep</b> <b>learning</b> <b>problems</b> by using Malliavin calculus. The Malliavin derivatives of solution to a BSDE satisfy themselves another BSDE, resulting thus in a system of BSDEs. Such formulation requires the estimation of the solution, its gradient, and the Hessian matrix, represented by the triple of processes $\left(Y, Z, \Gamma\right).$ All the integrals within this system are discretized by using the Euler-Maruyama method. Subsequently, <b>DNNs</b> are employed to approximate the triple of these unknown processes. The <b>DNN</b> parameters are backwardly optimized at each time step by minimizing a differential learning type loss function, which is defined as a weighted sum of the dynamics of the discretized BSDE system, with the first term providing the dynamics of the process $Y$ and the other the process $Z$. An error analysis is carried out to show the convergence of the proposed algorithm. Various numerical experiments up to $50$ dimensions are provided to demonstrate the high efficiency. Both theoretically and numerically, it is demonstrated that our proposed scheme is more efficient compared to other contemporary <b>deep</b> <b>learning-based</b> <b>methodologies,</b> especially in the computation of the process $\Gamma$.

{{</citation>}}


### (4/4 | 206/235) An Interior Penalty coupling strategy for Isogeometric non-conformal Kirchhoff-Love shell patches (Giuliano Guarino et al., 2024)

{{<citation>}}

Giuliano Guarino, Pablo Antolin, Alberto Milazzo, Annalisa Buffa. (2024)  
**An Interior Penalty coupling strategy for Isogeometric non-conformal Kirchhoff-Love shell patches**
<br/>
<button class="copy-to-clipboard" title="An Interior Penalty coupling strategy for Isogeometric non-conformal Kirchhoff-Love shell patches" index=206>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: cs-NA, math-NA, math.NA  
Keyword Score: 5  
Keywords: Geometry  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08485v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08485v1.pdf" filename="2404.08485v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This work focuses on the coupling of trimmed shell patches using Isogeometric Analysis, based on higher continuity splines that seamlessly meet the $C^1$ requirement of Kirchhoff-Love-based discretizations. Weak enforcement of coupling conditions is achieved through the symmetric interior penalty method, where the fluxes are computed using their correct variationally consistent expression that was only recently proposed and is unprecedentedly adopted herein in the context of coupling conditions. The constitutive relationships account for generically laminated materials, although the proposed tests are conducted under the assumption of uniform thickness and lamination sequence. Numerical experiments assess the method for an isotropic and a laminated plate, as well as an isotropic hyperbolic paraboloid shell from the new shell obstacle course. The boundary conditions and domain force are chosen to reproduce manufactured analytical solutions, which are taken as reference to compute rigorous convergence curves in the $L^2$, $H^1$, and $H^2$ norms, that closely approach optimal ones predicted by theory. Additionally, we conduct a final test on a complex structure comprising five intersecting laminated cylindrical shells, whose <b>geometry</b> is directly imported from a STEP file. The results exhibit excellent agreement with those obtained through commercial software, showcasing the method's potential for real-world industrial applications.

{{</citation>}}


## stat.ME (1)



### (1/1 | 207/235) Multiply-Robust Causal Change Attribution (Victor Quintas-Martinez et al., 2024)

{{<citation>}}

Victor Quintas-Martinez, Mohammad Taha Bahadori, Eduardo Santiago, Jeff Mu, Dominik Janzing, David Heckerman. (2024)  
**Multiply-Robust Causal Change Attribution**
<br/>
<button class="copy-to-clipboard" title="Multiply-Robust Causal Change Attribution" index=207>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ME  
Categories: cs-LG, econ-EM, stat-ME, stat-ML, stat.ME  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08839v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08839v1.pdf" filename="2404.08839v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Comparing two samples of data, we observe a change in the distribution of an outcome variable. In the presence of multiple explanatory variables, how much of the change can be explained by each possible cause? We develop a new estimation strategy that, given a causal model, combines regression and re-weighting methods to quantify the contribution of each causal mechanism. Our proposed methodology is multiply robust, meaning that it still recovers the target parameter under partial misspecification. We prove that our estimator is consistent and asymptotically normal. Moreover, it can be incorporated into existing frameworks for causal attribution, such as Shapley values, which will inherit the consistency and large-sample distribution properties. Our method demonstrates excellent performance in Monte Carlo <b>simulations,</b> and we show its usefulness in an empirical application.

{{</citation>}}


## cs.PF (1)



### (1/1 | 208/235) Strongly Tail-Optimal Scheduling in the Light-Tailed M/G/1 (George Yu et al., 2024)

{{<citation>}}

George Yu, Ziv Scully. (2024)  
**Strongly Tail-Optimal Scheduling in the Light-Tailed M/G/1**
<br/>
<button class="copy-to-clipboard" title="Strongly Tail-Optimal Scheduling in the Light-Tailed M/G/1" index=208>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.PF  
Categories: cs-PF, cs.PF, math-PR  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08826v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08826v1.pdf" filename="2404.08826v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We study the problem of scheduling jobs in a queueing system, specifically an M/G/1 with light-tailed job sizes, to asymptotically optimize the response time tail. This means scheduling to make $\mathbf{P}[T > t]$, the chance a job's response time exceeds $t$, decay as quickly as possible in the $t \to \infty$ limit. For some time, the best known policy was First-Come First-Served (FCFS), which has an asymptotically exponential tail: $\mathbf{P}[T > t] \sim C e^{-\gamma t}$. FCFS achieves the optimal *decay rate* $\gamma$, but its *tail constant* $C$ is suboptimal. Only recently have policies that improve upon FCFS's tail constant been discovered. But it is unknown what the optimal tail constant is, let alone what policy might achieve it. In this paper, we derive a closed-form expression for the optimal tail constant $C$, and we introduce *$\gamma$-Boost*, a new policy that achieves this optimal tail constant. Roughly speaking, $\gamma$-Boost operates similarly to FCFS, but it pretends that small jobs arrive earlier than their true arrival times. This significantly reduces the response time of small jobs without unduly delaying large jobs, improving upon FCFS's tail constant by up to 50% with only moderate job size variability, with even larger improvements for higher variability. While these results are for systems with full job size information, we also introduce and analyze a version of $\gamma$-Boost that works in settings with partial job size information, showing it too achieves significant gains over FCFS. Finally, we show via <b>simulation</b> that $\gamma$-Boost has excellent practical performance.

{{</citation>}}


## cs.ET (1)



### (1/1 | 209/235) Reducing the Barriers to Entry for Foundation Model Training (Paolo Faraboschi et al., 2024)

{{<citation>}}

Paolo Faraboschi, Ellis Giles, Justin Hotard, Konstanty Owczarek, Andrew Wheeler. (2024)  
**Reducing the Barriers to Entry for Foundation Model Training**
<br/>
<button class="copy-to-clipboard" title="Reducing the Barriers to Entry for Foundation Model Training" index=209>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.ET  
Categories: cs-AI, cs-AR, cs-ET, cs-LG, cs.ET  
Keyword Score: 20  
Keywords: Foundation Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08811v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08811v1.pdf" filename="2404.08811v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The world has recently witnessed an unprecedented acceleration in demands for Machine Learning and Artificial Intelligence applications. This spike in demand has imposed tremendous strain on the underlying technology stack in supply chain, GPU-accelerated hardware, software, datacenter power density, and energy consumption. If left on the current technological trajectory, future demands show insurmountable spending trends, further limiting market players, stifling innovation, and widening the technology gap. To address these challenges, we propose a fundamental change in the AI training infrastructure throughout the technology ecosystem. The changes require advancements in supercomputing and novel AI training approaches, from high-end software to low-level hardware, microprocessor, and chip design, while advancing the energy efficiency required by a sustainable infrastructure. This paper presents the analytical framework that quantitatively highlights the challenges and points to the opportunities to reduce the barriers to entry for training <b>large</b> <b>language</b> <b>models.</b>

{{</citation>}}


## eess.SP (4)



### (1/4 | 210/235) Mitigating Receiver Impact on Radio Frequency Fingerprint Identification via Domain Adaptation (Liu Yang et al., 2024)

{{<citation>}}

Liu Yang, Qiang Li, Xiaoyang Ren, Yi Fang, Shafei Wang. (2024)  
**Mitigating Receiver Impact on Radio Frequency Fingerprint Identification via Domain Adaptation**
<br/>
<button class="copy-to-clipboard" title="Mitigating Receiver Impact on Radio Frequency Fingerprint Identification via Domain Adaptation" index=210>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SP  
Categories: cs-LG, eess-SP, eess.SP  
Keyword Score: 20  
Keywords: Domain Adaptation, Security  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08566v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08566v1.pdf" filename="2404.08566v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Radio Frequency Fingerprint Identification (RFFI), which exploits non-ideal hardware-induced unique distortion resident in the transmit signals to identify an emitter, is emerging as a means to enhance the <b>security</b> of communication systems. Recently, machine learning has achieved great success in developing state-of-the-art RFFI models. However, few works consider cross-receiver RFFI problems, where the RFFI model is trained and deployed on different receivers. Due to altered receiver characteristics, direct deployment of RFFI model on a new receiver leads to significant performance degradation. To address this issue, we formulate the cross-receiver RFFI as a model adaptation problem, which adapts the trained model to unlabeled signals from a new receiver. We first develop a theoretical generalization error bound for the adaptation model. Motivated by the bound, we propose a novel method to solve the cross-receiver RFFI problem, which includes <b>domain</b> <b>alignment</b> and adaptive pseudo-labeling. The former aims at finding a feature space where both <b>domains</b> <b>exhibit</b> similar distributions, effectively reducing the <b>domain</b> <b>discrepancy.</b> Meanwhile, the latter employs a dynamic pseudo-labeling scheme to implicitly transfer the label information from the labeled receiver to the new receiver. Experimental results indicate that the proposed method can effectively mitigate the receiver impact and improve the cross-receiver RFFI performance.

{{</citation>}}


### (2/4 | 211/235) Semantic Communication for Cooperative Multi-Task Processing over Wireless Networks (Ahmad Halimi Razlighi et al., 2024)

{{<citation>}}

Ahmad Halimi Razlighi, Carsten Bockelmann, Armin Dekorsy. (2024)  
**Semantic Communication for Cooperative Multi-Task Processing over Wireless Networks**
<br/>
<button class="copy-to-clipboard" title="Semantic Communication for Cooperative Multi-Task Processing over Wireless Networks" index=211>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SP  
Categories: cs-IT, cs-LG, eess-SP, eess.SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08483v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08483v1.pdf" filename="2404.08483v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we have expanded the current status of semantic communication limited to processing one task to a more general system that can handle multiple tasks concurrently. In pursuit of this, we first introduced our definition of the "semantic source", enabling the interpretation of multiple semantics based on a single observation. A semantic encoder design is then introduced, featuring the division of the encoder into a common unit and multiple specific units enabling cooperative multi-task processing. <b>Simulation</b> results demonstrate the effectiveness of the proposed semantic source and the system design. Our approach employs information maximization (infomax) and end-to-end design principles.

{{</citation>}}


### (3/4 | 212/235) Optimized Detection with Analog Beamforming for Monostatic Integrated Sensing and Communication (Rodrigo Hernangómez et al., 2024)

{{<citation>}}

Rodrigo Hernangómez, Jochen Fink, Renato L. G. Cavalcante, Zoran Utkovski, Sławomir Stańczak. (2024)  
**Optimized Detection with Analog Beamforming for Monostatic Integrated Sensing and Communication**
<br/>
<button class="copy-to-clipboard" title="Optimized Detection with Analog Beamforming for Monostatic Integrated Sensing and Communication" index=212>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SP  
Categories: cs-IT, eess-SP, eess.SP, math-IT  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08455v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08455v2.pdf" filename="2404.08455v2.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we formalize an optimization framework for analog beamforming in the context of monostatic integrated sensing and communication (ISAC), where we also address the problem of self-interference in the analog domain. As a result, we derive semidefinite programs to approach detection-optimal transmit and receive beamformers, and we devise a superiorized iterative projection algorithm to approximate them. Our <b>simulations</b> show that this approach outperforms the detection performance of well-known design techniques for ISAC beamforming, while it achieves satisfactory self-interference suppression.

{{</citation>}}


### (4/4 | 213/235) Introducing Graph Learning over Polytopic Uncertain Graph (Masako Kishida et al., 2024)

{{<citation>}}

Masako Kishida, Shunsuke Ono. (2024)  
**Introducing Graph Learning over Polytopic Uncertain Graph**
<br/>
<button class="copy-to-clipboard" title="Introducing Graph Learning over Polytopic Uncertain Graph" index=213>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SP  
Categories: cs-LG, eess-SP, eess.SP  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08176v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08176v1.pdf" filename="2404.08176v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This extended abstract introduces a class of <b>graph</b> learning applicable to cases where the underlying <b>graph</b> has polytopic uncertainty, i.e., the <b>graph</b> is not exactly known, but its parameters or properties vary within a known range. By incorporating this assumption that the <b>graph</b> lies in a polytopic set into two established <b>graph</b> learning frameworks, we find that our approach yields better results with less computation.

{{</citation>}}


## cs.LO (1)



### (1/1 | 214/235) Almost-Sure Termination by Guarded Refinement (Simon Oddershede Gregersen et al., 2024)

{{<citation>}}

Simon Oddershede Gregersen, Alejandro Aguirre, Philipp G. Haselwarter, Joseph Tassarotti, Lars Birkedal. (2024)  
**Almost-Sure Termination by Guarded Refinement**
<br/>
<button class="copy-to-clipboard" title="Almost-Sure Termination by Guarded Refinement" index=214>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LO  
Categories: cs-LO, cs-PL, cs.LO  
Keyword Score: 20  
Keywords: Probabilistic Model, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08494v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08494v1.pdf" filename="2404.08494v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Almost-sure termination is an important correctness property for <b>probabilistic</b> <b>programs,</b> and a number of program logics have been developed for establishing it. However, these logics have mostly been developed for first-order programs written in languages with specific syntactic patterns for looping. In this paper, we consider almost-sure termination for higher-order <b>probabilistic</b> <b>programs</b> with general references. This combination of features allows for recursion and looping to be encoded through a variety of patterns. Therefore, rather than developing proof rules for <b>reasoning</b> about particular recursion patterns, we instead propose an approach based on proving refinement between a higher-order program and a simpler <b>probabilistic</b> <b>model,</b> in such a way that the refinement preserves termination behavior. By proving a refinement, almost-sure termination behavior of the program can then be established by analyzing the simpler model. We present this approach in the form of Caliper, a higher-order separation logic for proving termination-preserving refinements. Caliper uses <b>probabilistic</b> <b>couplings</b> to carry out relational <b>reasoning</b> between a program and a model. To handle the range of recursion patterns found in higher-order programs, Caliper uses guarded recursion, in particular the principle of L\"ob induction. A technical novelty is that Caliper does not require the use of transfinite step indexing or other technical restrictions found in prior work on guarded recursion for termination-preservation refinement. We demonstrate the flexibility of this approach by proving almost-sure termination of several examples, including first-order loop constructs, a random list generator, treaps, and a sampler for Galton-Watson trees that uses higher-order store. All the results have been mechanized in the Coq proof assistant.

{{</citation>}}


## cs.GT (2)



### (1/2 | 215/235) The Squared Kemeny Rule for Averaging Rankings (Patrick Lederer et al., 2024)

{{<citation>}}

Patrick Lederer, Dominik Peters, Tomasz Wąs. (2024)  
**The Squared Kemeny Rule for Averaging Rankings**
<br/>
<button class="copy-to-clipboard" title="The Squared Kemeny Rule for Averaging Rankings" index=215>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.GT  
Categories: cs-GT, cs.GT, econ-TH  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08474v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08474v1.pdf" filename="2404.08474v1.pdf">Download PDF</button>

---


**ABSTRACT**  
For the problem of aggregating several rankings into one ranking, Kemeny (1959) proposed two methods: the median rule which selects the ranking with the smallest total swap distance to the input rankings, and the mean rule which minimizes the squared swap distances to the input rankings. The median rule has been extensively studied since and is now known simply as Kemeny's rule. It exhibits majoritarian properties, so for example if more than half of the input rankings are the same, then the output of the rule is the same ranking. We observe that this behavior is undesirable in many rank aggregation settings. For example, when we rank objects by different criteria (quality, price, etc.) and want to aggregate them with specified weights for the criteria, then a criterion with weight 51% should have 51% influence on the output instead of 100%. We show that the Squared Kemeny rule (i.e., the mean rule) behaves this way, by establishing a bound on the distance of the output ranking to any input rankings, as a function of their weights. Furthermore, we give an axiomatic characterization of the Squared Kemeny rule, which mirrors the existing characterization of the Kemeny rule but replaces the majoritarian Condorcet axiom by a proportionality axiom. Finally, we discuss the computation of the rule and show its behavior in a <b>simulation</b> study.

{{</citation>}}


### (2/2 | 216/235) QI-DPFL: Quality-Aware and Incentive-Boosted Federated Learning with Differential Privacy (Wenhao Yuan et al., 2024)

{{<citation>}}

Wenhao Yuan, Xuehe Wang. (2024)  
**QI-DPFL: Quality-Aware and Incentive-Boosted Federated Learning with Differential Privacy**
<br/>
<button class="copy-to-clipboard" title="QI-DPFL: Quality-Aware and Incentive-Boosted Federated Learning with Differential Privacy" index=216>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.GT  
Categories: cs-GT, cs.GT  
Keyword Score: 20  
Keywords: Federated Learning, Differential Privacy  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08261v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08261v1.pdf" filename="2404.08261v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Federated</b> <b>Learning</b> (FL) has increasingly been recognized as an innovative and secure distributed model training paradigm, aiming to coordinate multiple edge clients to collaboratively train a shared model without uploading their private datasets. The challenge of encouraging mobile edge devices to participate zealously in FL model training procedures, while mitigating the privacy leakage risks during wireless transmission, remains comparatively unexplored so far. In this paper, we propose a novel approach, named QI-DPFL (Quality-Aware and Incentive-Boosted <b>Federated</b> <b>Learning</b> with <b>Differential</b> <b>Privacy),</b> to address the aforementioned intractable issue. To select clients with high-quality datasets, we first propose a quality-aware client selection mechanism based on the Earth Mover's Distance (EMD) metric. Furthermore, to attract high-quality data contributors, we design an incentive-boosted mechanism that constructs the interactions between the central server and the selected clients as a two-stage Stackelberg game, where the central server designs the time-dependent reward to minimize its cost by considering the trade-off between accuracy loss and total reward allocated, and each selected client decides the privacy budget to maximize its utility. The Nash Equilibrium of the Stackelberg game is derived to find the optimal solution in each global iteration. The extensive experimental results on different real-world datasets demonstrate the effectiveness of our proposed FL framework, by realizing the goal of privacy protection and incentive compatibility.

{{</citation>}}


## q-bio.QM (1)



### (1/1 | 217/235) VADA: a Data-Driven Simulator for Nanopore Sequencing (Jonas Niederle et al., 2024)

{{<citation>}}

Jonas Niederle, Simon Koop, Marc Pagès-Gallego, Vlado Menkovski. (2024)  
**VADA: a Data-Driven Simulator for Nanopore Sequencing**
<br/>
<button class="copy-to-clipboard" title="VADA: a Data-Driven Simulator for Nanopore Sequencing" index=217>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: q-bio.QM  
Categories: cs-LG, q-bio-QM, q-bio.QM  
Keyword Score: 20  
Keywords: Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08722v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08722v1.pdf" filename="2404.08722v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Nanopore sequencing offers the ability for real-time analysis of long DNA sequences at a low cost, enabling new applications such as early detection of cancer. Due to the complex nature of nanopore measurements and the high cost of obtaining ground truth datasets, there is a need for nanopore simulators. Existing simulators rely on handcrafted rules and parameters and do not learn an internal representation that would allow for analysing underlying biological factors of interest. Instead, we propose VADA, a purely data-driven method for simulating nanopores based on an autoregressive latent variable model. We embed subsequences of DNA and introduce a conditional prior to address the challenge of a collapsing conditioning. We introduce an auxiliary regressor on the latent variable to encourage our model to learn an informative latent representation. We empirically demonstrate that our model achieves competitive <b>simulation</b> performance on experimental nanopore data. Moreover, we show we have learned an informative latent representation that is predictive of the DNA labels. We hypothesize that other biological factors of interest, beyond the DNA labels, can potentially be extracted from such a learned latent representation.

{{</citation>}}


## cs.MM (1)



### (1/1 | 218/235) Guided Masked Self-Distillation Modeling for Distributed Multimedia Sensor Event Analysis (Masahiro Yasuda et al., 2024)

{{<citation>}}

Masahiro Yasuda, Noboru Harada, Yasunori Ohishi, Shoichiro Saito, Akira Nakayama, Nobutaka Ono. (2024)  
**Guided Masked Self-Distillation Modeling for Distributed Multimedia Sensor Event Analysis**
<br/>
<button class="copy-to-clipboard" title="Guided Masked Self-Distillation Modeling for Distributed Multimedia Sensor Event Analysis" index=218>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.MM  
Categories: cs-CV, cs-MM, cs.MM, eess-AS  
Keyword Score: 20  
Keywords: Knowledge Distillation, Self-Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08264v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08264v1.pdf" filename="2404.08264v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Observations with distributed sensors are essential in analyzing a series of human and machine activities (referred to as 'events' in this paper) in complex and extensive real-world environments. This is because the information obtained from a single sensor is often missing or fragmented in such an environment; observations from multiple locations and modalities should be integrated to analyze events comprehensively. However, a learning method has yet to be established to extract joint representations that effectively combine such distributed observations. Therefore, we propose Guided Masked <b>sELf-Distillation</b> modeling (Guided-MELD) for inter-sensor relationship modeling. The basic idea of Guided-MELD is to learn to supplement the information from the masked sensor with information from other sensors needed to detect the event. Guided-MELD is expected to enable the system to effectively <b>distill</b> the fragmented or redundant target event information obtained by the sensors without being overly dependent on any specific sensors. To validate the effectiveness of the proposed method in novel tasks of distributed multimedia sensor event analysis, we recorded two new datasets that fit the problem setting: MM-Store and MM-Office. These datasets consist of human activities in a convenience store and an office, recorded using distributed cameras and microphones. Experimental results on these datasets show that the proposed Guided-MELD improves event tagging and detection performance and outperforms conventional inter-sensor relationship modeling methods. Furthermore, the proposed method performed robustly even when sensors were reduced.

{{</citation>}}


## cs.DL (1)



### (1/1 | 219/235) Toward FAIR Semantic Publishing of Research Dataset Metadata in the Open Research Knowledge Graph (Raia Abu Ahmad et al., 2024)

{{<citation>}}

Raia Abu Ahmad, Jennifer D'Souza, Matthäus Zloch, Wolfgang Otto, Georg Rehm, Allard Oelen, Stefan Dietze, Sören Auer. (2024)  
**Toward FAIR Semantic Publishing of Research Dataset Metadata in the Open Research Knowledge Graph**
<br/>
<button class="copy-to-clipboard" title="Toward FAIR Semantic Publishing of Research Dataset Metadata in the Open Research Knowledge Graph" index=219>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DL  
Categories: cs-DL, cs-IR, cs.DL  
Keyword Score: 18  
Keywords: Graph, Knowledge Graph, Information Retrieval  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08443v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08443v1.pdf" filename="2404.08443v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Search engines these days can serve datasets as search results. Datasets get picked up by search technologies based on structured descriptions on their official web pages, informed by metadata ontologies such as the Dataset content type of schema.org. Despite this promotion of the content type dataset as a first-class citizen of search results, a vast proportion of datasets, particularly research datasets, still need to be made discoverable and, therefore, largely remain unused. This is due to the sheer volume of datasets released every day and the inability of metadata to reflect a dataset's content and context accurately. This work seeks to improve this situation for a specific class of datasets, namely research datasets, which are the result of research endeavors and are accompanied by a scholarly publication. We propose the ORKG-Dataset content type, a specialized branch of the Open Research <b>Knowledge</b> <b>Graoh</b> (ORKG) platform, which provides descriptive <b>information</b> <b>and</b> a semantic model for research datasets, integrating them with their accompanying scholarly publications. This work aims to establish a standardized framework for recording and reporting research datasets within the ORKG-Dataset content type. This, in turn, increases research dataset transparency on the web for their improved discoverability and applied use. In this paper, we present a proposal -- the minimum FAIR, comparable, semantic description of research datasets in terms of salient properties of their supporting publication. We design a specific application of the ORKG-Dataset semantic model based on 40 diverse research datasets on scientific <b>information</b> <b>extraction.</b>

{{</citation>}}


## physics.ao-ph (2)



### (1/2 | 220/235) Uncertainty Aware Tropical Cyclone Wind Speed Estimation from Satellite Data (Nils Lehmann et al., 2024)

{{<citation>}}

Nils Lehmann, Nina Maria Gottschling, Stefan Depeweg, Eric Nalisnick. (2024)  
**Uncertainty Aware Tropical Cyclone Wind Speed Estimation from Satellite Data**
<br/>
<button class="copy-to-clipboard" title="Uncertainty Aware Tropical Cyclone Wind Speed Estimation from Satellite Data" index=220>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.ao-ph  
Categories: cs-LG, physics-ao-ph, physics.ao-ph, stat-AP  
Keyword Score: 15  
Keywords: Black Box, Deep Neural Network, Deep Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08325v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08325v1.pdf" filename="2404.08325v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Deep</b> <b>neural</b> <b>networks</b> <b>(DNNs)</b> have been successfully applied to earth observation (EO) data and opened new research avenues. Despite the theoretical and practical advances of these techniques, <b>DNNs</b> are still considered <b>black</b> <b>box</b> tools and by default are designed to give point predictions. However, the majority of EO applications demand reliable uncertainty estimates that can support practitioners in critical decision making tasks. This work provides a theoretical and quantitative comparison of existing uncertainty quantification methods for <b>DNNs</b> applied to the task of wind speed estimation in satellite imagery of tropical cyclones. We provide a detailed evaluation of predictive uncertainty estimates from state-of-the-art uncertainty quantification (UQ) methods for <b>DNNs.</b> We find that predictive uncertainties can be utilized to further improve accuracy and analyze the predictive uncertainties of different methods across storm categories.

{{</citation>}}


### (2/2 | 221/235) Diffusion-Based Joint Temperature and Precipitation Emulation of Earth System Models (Katie Christensen et al., 2024)

{{<citation>}}

Katie Christensen, Lyric Otto, Seth Bassetti, Claudia Tebaldi, Brian Hutchinson. (2024)  
**Diffusion-Based Joint Temperature and Precipitation Emulation of Earth System Models**
<br/>
<button class="copy-to-clipboard" title="Diffusion-Based Joint Temperature and Precipitation Emulation of Earth System Models" index=221>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.ao-ph  
Categories: cs-LG, physics-ao-ph, physics-geo-ph, physics.ao-ph  
Keyword Score: 10  
Keywords: Diffusion Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08797v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08797v1.pdf" filename="2404.08797v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Earth system models (ESMs) are the principal tools used in climate science to generate future climate projections under various atmospheric emissions scenarios on a global or regional scale. Generative deep learning approaches are suitable for emulating these tools due to their computational efficiency and ability, once trained, to generate realizations in a fraction of the time required by ESMs. We extend previous work that used a generative probabilistic <b>diffusion</b> <b>model</b> to emulate ESMs by targeting the joint emulation of multiple variables, temperature and precipitation, by a single <b>diffusion</b> <b>model.</b> Joint generation of multiple variables is critical to generate realistic samples of phenomena resulting from the interplay of multiple variables. The <b>diffusion</b> <b>model</b> emulator takes in the monthly mean-maps of temperature and precipitation and produces the daily values of each of these variables that exhibit statistical properties similar to those generated by ESMs. Our results show the outputs from our extended model closely resemble those from ESMs on various climate metrics including dry spells and hot streaks, and that the joint distribution of temperature and precipitation in our sample closely matches those of ESMs.

{{</citation>}}


## cs.NI (1)



### (1/1 | 222/235) Routing and Spectrum Allocation in Broadband Quantum Entanglement Distribution (Rohan Bali et al., 2024)

{{<citation>}}

Rohan Bali, Ashley N. Tittelbaugh, Shelbi L. Jenkins, Anuj Agrawal, Jerry Horgan, Marco Ruffini, Daniel C. Kilper, Boulat A. Bash. (2024)  
**Routing and Spectrum Allocation in Broadband Quantum Entanglement Distribution**
<br/>
<button class="copy-to-clipboard" title="Routing and Spectrum Allocation in Broadband Quantum Entanglement Distribution" index=222>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-ET, cs-NI, cs.NI, quant-ph  
Keyword Score: 13  
Keywords: Graph, Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08744v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08744v1.pdf" filename="2404.08744v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We investigate resource allocation for quantum entanglement distribution over an optical network. We characterize and model a network architecture that employs a single quasi-deterministic time-frequency heralded Einstein-Podolsky-Rosen (EPR) pair source, and develop a routing scheme for distributing entangled photon pairs over such a network. We focus on max-min <b>fairness</b> in entanglement distribution and compare the performance of various spectrum allocation schemes by examining the max-min and median number of EPR-pairs assigned by them, and the Jain index associated with this assignment. Since this presents an NP-hard problem, we identify two approximation algorithms that outperform others in minimum and mean EPR-pair rate distribution and are comparable to others in the Jain index. We also analyze how the network size and connectivity affect these metrics using Watts-Strogatz random <b>graphs.</b> We find that a spectrum allocation approach that achieves high minimum EPR-pair rate can perform significantly worse when the median EPR-pair rate, Jain index, and runtimes are considered.

{{</citation>}}


## cs.SI (2)



### (1/2 | 223/235) BOND: Bootstrapping From-Scratch Name Disambiguation with Multi-task Promoting (Yuqing Cheng et al., 2024)

{{<citation>}}

Yuqing Cheng, Bo Chen, Fanjin Zhang, Jie Tang. (2024)  
**BOND: Bootstrapping From-Scratch Name Disambiguation with Multi-task Promoting**
<br/>
<button class="copy-to-clipboard" title="BOND: Bootstrapping From-Scratch Name Disambiguation with Multi-task Promoting" index=223>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: H-3-7; H-3-3, cs-AI, cs-SI, cs.SI  
Keyword Score: 13  
Keywords: Clustering, Disambiguation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08322v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08322v1.pdf" filename="2404.08322v1.pdf">Download PDF</button>

---


**ABSTRACT**  
From-scratch name <b>disambiguation</b> is an essential task for establishing a reliable foundation for academic platforms. It involves partitioning documents authored by identically named individuals into groups representing distinct real-life experts. Canonically, the process is divided into two decoupled tasks: locally estimating the pairwise similarities between documents followed by globally grouping these documents into appropriate clusters. However, such a decoupled approach often inhibits optimal information exchange between these intertwined tasks. Therefore, we present BOND, which bootstraps the local and global informative signals to promote each other in an end-to-end regime. Specifically, BOND harnesses local pairwise similarities to drive global <b>clustering,</b> subsequently generating pseudo-clustering labels. These global signals further refine local pairwise characterizations. The experimental results establish BOND's superiority, outperforming other advanced baselines by a substantial margin. Moreover, an enhanced version, BOND+, incorporating ensemble and post-match techniques, rivals the top methods in the WhoIsWho competition.

{{</citation>}}


### (2/2 | 224/235) Interest Maximization in Social Networks (Rahul Kumar Gautam et al., 2024)

{{<citation>}}

Rahul Kumar Gautam, Anjeneya Swami Kare, S. Durga Bhavani. (2024)  
**Interest Maximization in Social Networks**
<br/>
<button class="copy-to-clipboard" title="Interest Maximization in Social Networks" index=224>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SI  
Categories: cs-SI, cs.SI  
Keyword Score: 13  
Keywords: Diffusion Model, Benchmarking  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08236v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08236v1.pdf" filename="2404.08236v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Nowadays, organizations use viral marketing strategies to promote their products through social networks. It is expensive to directly send the product promotional information to all the users in the network. In this context, Kempe et al. \cite{kempe2003maximizing} introduced the Influence Maximization (IM) problem, which identifies $k$ most influential nodes (spreader nodes), such that the maximum number of people in the network adopts the promotional message. Many variants of the IM problem have been studied in the literature, namely, Perfect Evangelising Set (PES), Perfect Awareness Problem (PAP), etc. In this work, we propose a maximization version of PAP called the \IM{} problem. Different people have different levels of interest in a particular product. This is modeled by assigning an interest value to each node in the network. Then, the problem is to select $k$ initial spreaders such that the sum of the interest values of the people (nodes) who become aware of the message is maximized. We study the \IM{} problem under two popular <b>diffusion</b> <b>models:</b> the Linear Threshold Model (LTM) and the Independent Cascade Model (ICM). We show that the \IM{} problem is NP-Hard under LTM. We give linear programming formulation for the problem under LTM. We propose four heuristic algorithms for the \IM{} problem: \LBE{} (\LB{}), Maximum Degree First Heuristic (\MD{}), \PBE{} (\PB{}), and Maximum Profit Based Greedy Heuristic (\MP{}). Extensive experimentation has been carried out on many real-world <b>benchmark</b> data sets for both <b>diffusion</b> <b>models.</b> The results show that among the proposed heuristics, \MP{} performs better in maximizing the interest value.

{{</citation>}}


## cs.CE (3)



### (1/3 | 225/235) Clustering Analysis of US COVID-19 Rates, Vaccine Participation, and Socioeconomic Factors (Morteza Maleki, 2024)

{{<citation>}}

Morteza Maleki. (2024)  
**Clustering Analysis of US COVID-19 Rates, Vaccine Participation, and Socioeconomic Factors**
<br/>
<button class="copy-to-clipboard" title="Clustering Analysis of US COVID-19 Rates, Vaccine Participation, and Socioeconomic Factors" index=225>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CE  
Categories: cs-CE, cs.CE, physics-soc-ph  
Keyword Score: 13  
Keywords: Clustering, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08186v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08186v1.pdf" filename="2404.08186v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The COVID-19 pandemic has presented unprecedented challenges worldwide, with its impact varying significantly across different geographic and socioeconomic contexts. This study employs a <b>clustering</b> analysis to examine the diversity of responses to the pandemic within the United States, aiming to provide nuanced insights into the effectiveness of various strategies. We utilize an <b>unsupervised</b> machine learning approach, specifically K-Means <b>clustering,</b> to analyze county-level data that includes variables such as infection rates, death rates, demographic profiles, and socio-economic factors. Our analysis identifies distinct clusters of counties based on their pandemic responses and outcomes, facilitating a detailed examination of "high-performing" and "lower-performing" groups. These classifications are informed by a combination of COVID-specific datasets and broader socio-economic data, allowing for a comprehensive understanding of the factors that contribute to differing levels of pandemic impact. The findings underscore the importance of tailored public health responses that consider local conditions and capabilities. Additionally, this study introduces an innovative visualization tool that aids in hypothesis testing and further research, enhancing the ability of policymakers and public health officials to deploy more effective and targeted interventions in future health crises.

{{</citation>}}


### (2/3 | 226/235) Code Generation and Performance Engineering for Matrix-Free Finite Element Methods on Hybrid Tetrahedral Grids (Fabian Böhm et al., 2024)

{{<citation>}}

Fabian Böhm, Daniel Bauer, Nils Kohl, Christie Alappat, Dominik Thönnes, Marcus Mohr, Harald Köstler, Ulrich Rüde. (2024)  
**Code Generation and Performance Engineering for Matrix-Free Finite Element Methods on Hybrid Tetrahedral Grids**
<br/>
<button class="copy-to-clipboard" title="Code Generation and Performance Engineering for Matrix-Free Finite Element Methods on Hybrid Tetrahedral Grids" index=226>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CE  
Categories: 65F50, 65N30, 65N55, 65Y20, 65F10, cs-CE, cs.CE  
Keyword Score: 10  
Keywords: Code Generation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08371v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08371v1.pdf" filename="2404.08371v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper introduces a <b>code</b> <b>generator</b> designed for node-level optimized, extreme-scalable, matrix-free finite element operators on hybrid tetrahedral grids. It optimizes the local evaluation of bilinear forms through various techniques including tabulation, relocation of loop invariants, and inter-element vectorization - implemented as transformations of an abstract syntax tree. A key contribution is the development, analysis, and generation of efficient loop patterns that leverage the local structure of the underlying tetrahedral grid. These significantly enhance cache locality and arithmetic intensity, mitigating bandwidth-pressure associated with compute-sparse, low-order operators. The paper demonstrates the generator's capabilities through a comprehensive educational cycle of performance analysis, bottleneck identification, and emission of dedicated optimizations. For three differential operators ($-\Delta$, $-\nabla \cdot (k(\mathbf{x})\, \nabla\,)$, $\alpha(\mathbf{x})\, \mathbf{curl}\ \mathbf{curl} + \beta(\mathbf{x}) $), we determine the set of most effective optimizations. Applied by the generator, they result in speed-ups of up to 58$\times$ compared to reference implementations. Detailed node-level performance analysis yields matrix-free operators with a throughput of 1.3 to 2.1 GDoF/s, achieving up to 62% peak performance on a 36-core Intel Ice Lake socket. Finally, the solution of the curl-curl problem with more than a trillion ($ 10^{12}$) degrees of freedom on 21504 processes in less than 50 seconds demonstrates the generated operators' performance and extreme-scalability as part of a full multigrid solver.

{{</citation>}}


### (3/3 | 227/235) A splitting, discontinuous Galerkin solver for the cell-by-cell electroneutral Nernst-Planck framework (Ada J. Ellingsrud et al., 2024)

{{<citation>}}

Ada J. Ellingsrud, Miroslav Kuchta. (2024)  
**A splitting, discontinuous Galerkin solver for the cell-by-cell electroneutral Nernst-Planck framework**
<br/>
<button class="copy-to-clipboard" title="A splitting, discontinuous Galerkin solver for the cell-by-cell electroneutral Nernst-Planck framework" index=227>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CE  
Categories: 65M60, 65F10, 65M55, 68U20, 92-08, 92C37, cs-CE, cs.CE  
Keyword Score: 5  
Keywords: Black Box  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08320v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08320v1.pdf" filename="2404.08320v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Mathematical models for excitable tissue with explicit representation of individual cells are highly detailed and can, unlike classical homogenized models, represent complex cellular geometries and local membrane variations. However, these cell-based models are challenging to approximate numerically, partly due to their mixed-dimensional nature with unknowns both in the bulk and at the lower-dimensional cellular membranes. We here develop and evaluate a novel solution strategy for the cell-based KNP-EMI model describing ionic electrodiffusion in and between intra- and extracellular compartments with explicit representation of individual cells. The strategy is based on operator splitting, a multiplier-free formulation of the coupled dynamics across sub-regions, and a discontinuous Galerkin discretization. In addition to desirable theoretical properties, such as local mass conservation, the scheme is practical as it requires no specialized functionality in the finite element assembly and order optimal solvers for the resulting linear systems can be realized with <b>black-box</b> <b>algebraic</b> multigrid preconditioners. Numerical investigations show that the proposed solution strategy is accurate, robust with respect to discretization parameters, and that the parallel scalability of the solver is close to optimal - both for idealized and realistic two and three dimensional geometries.

{{</citation>}}


## cs.CY (1)



### (1/1 | 228/235) Scarce Resource Allocations That Rely On Machine Learning Should Be Randomized (Shomik Jain et al., 2024)

{{<citation>}}

Shomik Jain, Kathleen Creel, Ashia Wilson. (2024)  
**Scarce Resource Allocations That Rely On Machine Learning Should Be Randomized**
<br/>
<button class="copy-to-clipboard" title="Scarce Resource Allocations That Rely On Machine Learning Should Be Randomized" index=228>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CY  
Categories: K-4-0, cs-CY, cs.CY  
Keyword Score: 10  
Keywords: Fairness  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08592v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08592v1.pdf" filename="2404.08592v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Contrary to traditional deterministic notions of algorithmic <b>fairness,</b> this paper argues that fairly allocating scarce resources using machine learning often requires randomness. We address why, when, and how to randomize by proposing stochastic procedures that more adequately account for all of the claims that individuals have to allocations of social goods or opportunities.

{{</citation>}}


## physics.chem-ph (1)



### (1/1 | 229/235) Kinematics Modeling of Peroxy Free Radicals: A Deep Reinforcement Learning Approach (Subhadarsi Nayak et al., 2024)

{{<citation>}}

Subhadarsi Nayak, Hrithwik Shalu, Joseph Stember. (2024)  
**Kinematics Modeling of Peroxy Free Radicals: A Deep Reinforcement Learning Approach**
<br/>
<button class="copy-to-clipboard" title="Kinematics Modeling of Peroxy Free Radicals: A Deep Reinforcement Learning Approach" index=229>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.chem-ph  
Categories: cs-CE, cs-LG, physics-chem-ph, physics.chem-ph  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.10010v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.10010v1.pdf" filename="2404.10010v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Tropospheric ozone, known as a concerning air pollutant, has been associated with health issues including asthma, bronchitis, and impaired lung function. The rates at which peroxy radicals react with NO play a critical role in the overall formation and depletion of tropospheric ozone. However, obtaining comprehensive kinetic data for these reactions remains challenging. Traditional approaches to determine rate constants are costly and technically intricate. Fortunately, the emergence of machine learning-based models offers a less resource and time-intensive alternative for acquiring kinetics information. In this study, we leveraged deep <b>reinforcement</b> <b>learning</b> to predict ranges of rate constants (\textit{k}) with exceptional accuracy, achieving a testing set accuracy of 100%. To analyze reactivity trends based on the molecular structure of peroxy radicals, we employed 51 global descriptors as input parameters. These descriptors were derived from optimized minimum energy geometries of peroxy radicals using the quantum composite G3B3 method. Through the application of Integrated Gradients (IGs), we gained valuable insights into the significance of the various descriptors in relation to reaction rates. We successfully validated and contextualized our findings by conducting cross-comparisons with established trends in the existing literature. These results establish a solid foundation for pioneering advancements in chemistry, where computer analysis serves as an inspirational source driving innovation.

{{</citation>}}


## cs.DS (1)



### (1/1 | 230/235) Destroying Densest Subgraphs is Hard (Cristina Bazgan et al., 2024)

{{<citation>}}

Cristina Bazgan, André Nichterlein, Sofia Vazquez Alferez. (2024)  
**Destroying Densest Subgraphs is Hard**
<br/>
<button class="copy-to-clipboard" title="Destroying Densest Subgraphs is Hard" index=230>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DS  
Categories: cs-DS, cs.DS  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08599v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08599v1.pdf" filename="2404.08599v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We analyze the computational complexity of the following computational problems called Bounded-Density Edge Deletion and Bounded-Density Vertex Deletion: Given a <b>graph</b> $G$, a budget $k$ and a target density $\tau_\rho$, are there $k$ edges ($k$ vertices) whose removal from $G$ results in a <b>graph</b> where the densest subgraph has density at most $\tau_\rho$? Here, the density of a <b>graph</b> is the number of its edges divided by the number of its vertices. We prove that both problems are polynomial-time solvable on trees and cliques but are NP-complete on planar bipartite <b>graphs</b> and split <b>graphs.</b> From a parameterized point of view, we show that both problems are fixed-parameter tractable with respect to the vertex cover number but W[1]-hard with respect to the solution size. Furthermore, we prove that Bounded-Density Edge Deletion is W[1]-hard with respect to the feedback edge number, demonstrating that the problem remains hard on very sparse <b>graphs.</b>

{{</citation>}}


## math.CO (4)



### (1/4 | 231/235) Approximating the volume of a truncated relaxation of the independence polytope (Ferenc Bencs et al., 2024)

{{<citation>}}

Ferenc Bencs, Guus Regts. (2024)  
**Approximating the volume of a truncated relaxation of the independence polytope**
<br/>
<button class="copy-to-clipboard" title="Approximating the volume of a truncated relaxation of the independence polytope" index=231>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.CO  
Categories: 05C31, 52B55, 05C69, cs-DM, cs-DS, math-CO, math.CO  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08577v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08577v1.pdf" filename="2404.08577v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Answering a question of Gamarnik and Smedira, we give a polynomial time algorithm that approximately computes the volume of a truncation of a relaxation of the independent set polytope, improving on their quasi-polynomial time algorithm. Our algorithm is obtained by viewing the volume as an evaluation of a <b>graph</b> polynomial and we approximate this evaluation using Barvinok's interpolation method.

{{</citation>}}


### (2/4 | 232/235) An improved spectral lower bound of treewidth (Tatsuya Gima et al., 2024)

{{<citation>}}

Tatsuya Gima, Tesshu Hanaka, Kohei Noro, Hirotaka Ono, Yota Otachi. (2024)  
**An improved spectral lower bound of treewidth**
<br/>
<button class="copy-to-clipboard" title="An improved spectral lower bound of treewidth" index=232>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.CO  
Categories: cs-DS, math-CO, math.CO  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08520v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08520v1.pdf" filename="2404.08520v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We show that for every $n$-vertex <b>graph</b> with at least one edge, its treewidth is greater than or equal to $n \lambda_{2} / (\Delta + \lambda_{2}) - 1$, where $\Delta$ and $\lambda_{2}$ are the maximum degree and the second smallest Laplacian eigenvalue of the <b>graph,</b> respectively. This lower bound improves the one by Chandran and Subramanian [Inf. Process. Lett., 2003] and the subsequent one by the authors of the present paper [IEICE Trans. Inf. Syst., 2024]. The new lower bound is almost tight in the sense that there is an infinite family of <b>graphs</b> such that the lower bound is only $1$ less than the treewidth for each <b>graph</b> in the family. Additionally, using similar techniques, we also present a lower bound of treewidth in terms of the largest and the second smallest Laplacian eigenvalues.

{{</citation>}}


### (3/4 | 233/235) Asymptotics of relaxed $k$-ary trees (Manosij Ghosh Dastidar et al., 2024)

{{<citation>}}

Manosij Ghosh Dastidar, Michael Wallner. (2024)  
**Asymptotics of relaxed $k$-ary trees**
<br/>
<button class="copy-to-clipboard" title="Asymptotics of relaxed $k$-ary trees" index=233>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.CO  
Categories: 05C30, 05A16, 05C20, 05C05, cs-DM, cs-DS, math-CO, math.CO  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08415v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08415v1.pdf" filename="2404.08415v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A relaxed $k$-ary tree is an ordered directed acyclic <b>graph</b> with a unique source and sink in which every node has out-degree $k$. These objects arise in the compression of trees in which some repeated subtrees are factored and repeated appearances are replaced by pointers. We prove an asymptotic theta-result for the number of relaxed $k$-ary tree with $n$ nodes for $n \to \infty$. This generalizes the previously proved binary case to arbitrary finite arity, and shows that the seldom observed phenomenon of a stretched exponential term $e^{c n^{1/3}}$ appears in all these cases. We also derive the recurrences for compacted $k$-ary trees in which all subtrees are unique and minimal deterministic finite automata accepting a finite language over a finite alphabet.

{{</citation>}}


### (4/4 | 234/235) Radio number for the Cartesian product of a tree and a complete graph (Payal Vasoya et al., 2024)

{{<citation>}}

Payal Vasoya, Devsi Bantva. (2024)  
**Radio number for the Cartesian product of a tree and a complete graph**
<br/>
<button class="copy-to-clipboard" title="Radio number for the Cartesian product of a tree and a complete graph" index=234>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.CO  
Categories: 05C78, 05C15, 05C12, cs-DM, math-CO, math.CO  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08400v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08400v1.pdf" filename="2404.08400v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A radio labelling of a <b>graph</b> $G$ is a mapping $f : V(G) \rightarrow \{0, 1, 2,\ldots\}$ such that $|f(u)-f(v)|\geq diam(G) + 1 - d(u,v)$ for every pair of distinct vertices $u,v$ of $G$, where $diam(G)$ is the diameter of $G$ and $d(u,v)$ is the distance between $u$ and $v$ in $G$. The radio number $rn(G)$ of $G$ is the smallest integer $k$ such that $G$ admits a radio labelling $f$ with $\max\{f(v):v \in V(G)\} = k$. In this paper, we give a lower bound for the radio number of the Cartesian product of a tree and a complete <b>graph</b> and give two necessary and sufficient conditions to achieve the lower bound. We also give three sufficient conditions to achieve the lower bound. We determine the radio number for the Cartesian product of a level-wise regular trees and a complete <b>graph</b> which attains the lower bound. The radio number for the Cartesian product of a path and a complete <b>graph</b> derived in [Radio number for the product of a path and a complete <b>graph,</b> J. Comb. Optim., 30 (2015), 139-149] can be obtained using our results in a short way.

{{</citation>}}


## physics.soc-ph (1)



### (1/1 | 235/235) Opinion dynamics on signed graphs and graphons: Beyond the piece-wise constant case (Paolo Frasca et al., 2024)

{{<citation>}}

Paolo Frasca, Federica Garin, Raoul Prisant. (2024)  
**Opinion dynamics on signed graphs and graphons: Beyond the piece-wise constant case**
<br/>
<button class="copy-to-clipboard" title="Opinion dynamics on signed graphs and graphons: Beyond the piece-wise constant case" index=235>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: physics.soc-ph  
Categories: cs-SY, eess-SY, physics-soc-ph, physics.soc-ph  
Keyword Score: 3  
Keywords: Graph  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2404.08372v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2404.08372v1.pdf" filename="2404.08372v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper we make use of graphon theory to study opinion dynamics on large networks. The opinion dynamics models that we take into consideration allow for negative interactions between the individual, i.e. competing entities whose opinions can grow apart. We consider both the repelling model and the opposing model that are studied in the literature. We define the repelling and the opposing dynamics on graphons and we show that their initial value problem's solutions exist and are unique. We then show that the graphon dynamics well approximate the dynamics on large <b>graphs</b> that converge to a graphon. This result applies to large random <b>graphs</b> that are sampled according to a graphon. All these facts are illustrated in an extended numerical example.

{{</citation>}}
