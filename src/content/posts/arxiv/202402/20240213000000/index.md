---
draft: false
title: "arXiv @ 2024.02.13"
date: 2024-02-13
author: "akitenkrad"
description: ""
tags: ["arXiv", "Published:2024"]
menu:
  sidebar:
    name: "arXiv @ 2024.02.13"
    identifier: arxiv_20240213
    parent: 202402_arxiv
    weight: 10
math: true
---

<figure style="border:none; width:100%; display:flex; justify-content: center">
    <iframe src="pie.html" width=900 height=620 style="border:none"></iframe>
</figure>


## Primary Categories

- [cond-mat.soft (1)](#cond-matsoft-1)
- [cs.AI (11)](#csai-11)
- [cs.CE (1)](#csce-1)
- [cs.CL (9)](#cscl-9)
- [cs.CR (1)](#cscr-1)
- [cs.CV (12)](#cscv-12)
- [cs.DB (1)](#csdb-1)
- [cs.HC (2)](#cshc-2)
- [cs.IR (1)](#csir-1)
- [cs.LG (23)](#cslg-23)
- [cs.NE (1)](#csne-1)
- [cs.NI (1)](#csni-1)
- [cs.RO (2)](#csro-2)
- [cs.SE (3)](#csse-3)
- [eess.IV (4)](#eessiv-4)
- [eess.SY (1)](#eesssy-1)
- [math.NA (1)](#mathna-1)
- [q-bio.GN (1)](#q-biogn-1)
- [stat.ML (2)](#statml-2)

## Keywords

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>keyword</th>
      <th>cs.AI</th>
      <th>cs.CV</th>
      <th>cs.LG</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Active Learning</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Autoencoder</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>BERT</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Bandit Algorithm</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>ChatGPT</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Contrastive Learning</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Convolution</td>
      <td></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Convolutional Neural Network</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Emotion Recognition</td>
      <td>2</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Face Recognition</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Fairness</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Few-shot</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Fine-tuning</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Foundation Model</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>GPT</td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Gaussian Process</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Graph Convolutional Network</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Graph Neural Network</td>
      <td>1</td>
      <td></td>
      <td>4</td>
    </tr>
    <tr>
      <td>Human Intervention</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Information Retrieval</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Instruction Following</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Knowledge Distillation</td>
      <td></td>
      <td>2</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Large Language Model</td>
      <td>11</td>
      <td>4</td>
      <td>6</td>
    </tr>
    <tr>
      <td>Logistic Regression</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Message-Passing</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Multi-modal</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Node Classification</td>
      <td>1</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Out-of-distribution</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Outlier Detection</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Prompt</td>
      <td></td>
      <td></td>
      <td>3</td>
    </tr>
    <tr>
      <td>Quantization</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Question Answering</td>
      <td>2</td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Reasoning</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Recommendation</td>
      <td></td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <td>Reinforcement Learning</td>
      <td>2</td>
      <td></td>
      <td>9</td>
    </tr>
    <tr>
      <td>Self-supervised Learning</td>
      <td>4</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Simulation</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Simulator</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Stochastic Gradient Descent</td>
      <td></td>
      <td></td>
      <td>2</td>
    </tr>
    <tr>
      <td>Supervised Learning</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Text Classification</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Text Generation</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Transfer Learning</td>
      <td>1</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Transformer</td>
      <td>4</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Vision-and-Language</td>
      <td></td>
      <td>2</td>
      <td></td>
    </tr>
    <tr>
      <td>Visual Question Answering</td>
      <td></td>
      <td>3</td>
      <td></td>
    </tr>
    <tr>
      <td>Zero-shot</td>
      <td>1</td>
      <td>1</td>
      <td></td>
    </tr>
  </tbody>
</table>

<script>
$(function() {
  $("table").addClass("keyword-table table-bordered border-success");
  $("table thead").addClass("sticky-top");
  $("table tbody td").css("text-align", "");
});
</script>


## cs.CL (9)



### (1/78) How do Large Language Models Navigate Conflicts between Honesty and Helpfulness? (Ryan Liu et al., 2024)

{{<citation>}}

Ryan Liu, Theodore R. Sumers, Ishita Dasgupta, Thomas L. Griffiths. (2024)  
**How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?**
<br/>
<button class="copy-to-clipboard" title="How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?" index=1>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 100  
Keywords: Reinforcement Learning, Zero-shot, GPT, GPT-4, GPT-4 turbo, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07282v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07282v2.pdf" filename="2402.07282v2.pdf">Download PDF</button>

---


**ABSTRACT**  
In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze <b>LLMs.</b> We test a range of <b>LLMs</b> and explore how optimization for human preferences or inference-time <b>reasoning</b> affects these trade-offs. We find that <b>reinforcement</b> <b>learning</b> from human feedback improves both honesty and helpfulness, while <b>chain-of-thought</b> <b>prompting</b> skews <b>LLMs</b> towards helpfulness over honesty. Finally, <b>GPT-4</b> <b>Turbo</b> demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by <b>LLMs</b> and suggest that even these abstract values can, to a degree, be steered by <b>zero-shot</b> <b>prompting.</b>

{{</citation>}}


### (2/78) TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation (Peng Wang et al., 2024)

{{<citation>}}

Peng Wang, Xiang Wei, Fangxu Hu, Wenjuan Han. (2024)  
**TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation**
<br/>
<button class="copy-to-clipboard" title="TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation" index=2>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs.CL  
Keyword Score: 53  
Keywords: Fine-tuning, Multi-modal, Recommendation, Transformer, Neural Machine Translation, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07233v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07233v1.pdf" filename="2402.07233v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Natural language processing (NLP) is a key component of intelligent transportation systems (ITS), but it faces many challenges in the transportation domain, such as domain-specific knowledge and data, and <b>multi-modal</b> inputs and outputs. This paper presents TransGPT, a novel <b>(multi-modal)</b> <b>large</b> <b>language</b> <b>model</b> for the transportation domain, which consists of two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for <b>multi-modal</b> data. TransGPT-SM is <b>finetuned</b> on a single-modal Transportation dataset (STD) that contains textual data from various sources in the transportation domain. TransGPT-MM is <b>finetuned</b> on a <b>multi-modal</b> Transportation dataset <b>(MTD)</b> that we manually collected from three areas of the transportation domain: driving tests, traffic signs, and landmarks. We evaluate TransGPT on several benchmark datasets for different tasks in the transportation domain, and show that it outperforms baseline models on most tasks. We also showcase the potential applications of TransGPT for traffic analysis and modeling, such as generating synthetic traffic scenarios, explaining traffic phenomena, answering traffic-related questions, providing traffic <b>recommendations,</b> and generating traffic reports. This work advances the state-of-the-art of NLP in the transportation domain and provides a useful tool for ITS researchers and practitioners.

{{</citation>}}


### (3/78) Natural Language Reinforcement Learning (Xidong Feng et al., 2024)

{{<citation>}}

Xidong Feng, Ziyu Wan, Mengyue Yang, Ziyan Wang, Girish A. Koushiks, Yali Du, Ying Wen, Jun Wang. (2024)  
**Natural Language Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Natural Language Reinforcement Learning" index=3>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-AI, cs-CL, cs-LG, cs.CL  
Keyword Score: 50  
Keywords: Reinforcement Learning, GPT, GPT-4, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07157v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07157v1.pdf" filename="2402.07157v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Reinforcement</b> <b>Learning</b> (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language <b>Reinforcement</b> <b>Learning</b> (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>GPT-4.</b> Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.

{{</citation>}}


### (4/78) Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation (Haonan Chen et al., 2024)

{{<citation>}}

Haonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan Liu, Ziliang Zhao. (2024)  
**Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation**
<br/>
<button class="copy-to-clipboard" title="Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation" index=4>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-IR, cs.CL  
Keyword Score: 50  
Keywords: Contrastive Learning, Data Augmentation, Dense Retrieval, Zero-shot, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07092v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07092v1.pdf" filename="2402.07092v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational <b>dense</b> <b>retrieval</b> models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe <b>data</b> <b>sparsity</b> problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational <b>dense</b> <b>retrieval</b> via <b>LLM-cognition</b> <b>data</b> <b>Augmentation</b> (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A <b>contrastive</b> <b>learning</b> objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and <b>zero-shot</b> settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug.

{{</citation>}}


### (5/78) Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models (Zhibo Hu et al., 2024)

{{<citation>}}

Zhibo Hu, Chen Wang, Yanfeng Shu, Helen, Paik, Liming Zhu. (2024)  
**Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models" index=5>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: I-2-7; H-3-3, cs-CL, cs-IR, cs.CL  
Keyword Score: 40  
Keywords: Text Generation, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07179v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07179v1.pdf" filename="2402.07179v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The robustness of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of <b>text</b> <b>generation</b> from <b>LLMs.</b> However, how the outputs from RAG-based <b>LLMs</b> are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the <b>prompt</b> leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided <b>Prompt</b> Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based <b>LLMs</b> to targeted wrong answers. It can also cope with instructions in the <b>prompts</b> requesting to ignore irrelevant context. We also exploit <b>LLMs'</b> neuron activation difference between <b>prompts</b> with and without GGPP perturbations to give a method that improves the robustness of RAG-based <b>LLMs</b> through a highly effective detector trained on neuron activation triggered by GGPP generated <b>prompts.</b> Our evaluation on open-sourced <b>LLMs</b> demonstrates the effectiveness of our methods.

{{</citation>}}


### (6/78) Previously on the Stories: Recap Snippet Identification for Story Reading (Jiangnan Li et al., 2024)

{{<citation>}}

Jiangnan Li, Qiujing Wang, Liyan Xu, Wenjie Pang, Mo Yu, Zheng Lin, Weiping Wang, Jie Zhou. (2024)  
**Previously on the Stories: Recap Snippet Identification for Story Reading**
<br/>
<button class="copy-to-clipboard" title="Previously on the Stories: Recap Snippet Identification for Story Reading" index=6>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs.CL  
Keyword Score: 20  
Keywords: Large Language Model, Pre-trained Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07271v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07271v1.pdf" filename="2402.07271v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Similar to the "previously-on" scenes in TV shows, recaps can help book reading by recalling the readers' memory about the important elements in previous texts to better understand the ongoing plot. Despite its usefulness, this application has not been well studied in the NLP community. We propose the first benchmark on this useful task called Recap Snippet Identification with a hand-crafted evaluation dataset. Our experiments show that the proposed task is challenging to <b>PLMs,</b> <b>LLMs,</b> and proposed methods as the task requires a deep understanding of the plot correlation between snippets.

{{</citation>}}


### (7/78) American Sign Language Video to Text Translation (Parsheeta Roy et al., 2024)

{{<citation>}}

Parsheeta Roy, Ji-Eun Han, Srishti Chouhan, Bhaavanaa Thumu. (2024)  
**American Sign Language Video to Text Translation**
<br/>
<button class="copy-to-clipboard" title="American Sign Language Video to Text Translation" index=7>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-CV, cs.CL  
Keyword Score: 20  
Keywords: Label Smoothing, BLEU  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07255v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07255v1.pdf" filename="2402.07255v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Sign language to text is a crucial technology that can break down communication barriers for individuals with hearing difficulties. We replicate and try to improve on a recently published study. We evaluate models using <b>BLEU</b> and rBLEU metrics to ensure translation quality. During our ablation study, we found that the model's performance is significantly influenced by optimizers, activation functions, and <b>label</b> <b>smoothing.</b> Further research aims to refine visual feature capturing, enhance decoder utilization, and integrate pre-trained decoders for better translation outcomes. Our source code is available to facilitate replication of our results and encourage future research.

{{</citation>}}


### (8/78) Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi (Mithun Das et al., 2024)

{{<citation>}}

Mithun Das, Saurabh Kumar Pandey, Shivansh Sethi, Punyajoy Saha, Animesh Mukherjee. (2024)  
**Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi**
<br/>
<button class="copy-to-clipboard" title="Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi" index=8>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-HC, cs.CL  
Keyword Score: 10  
Keywords: Low-Resource  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07262v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07262v1.pdf" filename="2402.07262v1.pdf">Download PDF</button>

---


**ABSTRACT**  
With the rise of online abuse, the NLP community has begun investigating the use of neural architectures to generate counterspeech that can "counter" the vicious tone of such abusive speech and dilute/ameliorate their rippling effect over the social network. However, most of the efforts so far have been primarily focused on English. To bridge the gap for <b>low-resource</b> languages such as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive speech/counterspeech pairs, of which 2,460 pairs are in Bengali and 2,602 pairs are in Hindi. We implement several baseline models considering various interlingual transfer mechanisms with different configurations to generate suitable counterspeech to set up an effective benchmark. We observe that the monolingual setup yields the best performance. Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family.

{{</citation>}}


### (9/78) Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education (Nischal Ashok Kumar et al., 2024)

{{<citation>}}

Nischal Ashok Kumar, Andrew Lan. (2024)  
**Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education**
<br/>
<button class="copy-to-clipboard" title="Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education" index=9>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CL  
Categories: cs-CL, cs-SE, cs.CL  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07081v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07081v1.pdf" filename="2402.07081v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In computer science education, test cases are an integral part of programming assignments since they can be used as assessment items to test students' programming knowledge and provide personalized feedback on student-written code. The goal of our work is to propose a fully automated approach for test case generation that can accurately measure student knowledge, which is important for two reasons. First, manually constructing test cases requires expert knowledge and is a labor-intensive process. Second, developing test cases for students, especially those who are novice programmers, is significantly different from those oriented toward professional-level software developers. Therefore, we need an automated process for test case generation to assess student knowledge and provide feedback. In this work, we propose a <b>large</b> <b>language</b> <b>model-based</b> approach to automatically generate test cases and show that they are good measures of student knowledge, using a publicly available dataset that contains student-written Java code. We also discuss future research directions centered on using test cases to help students.

{{</citation>}}


## cs.LG (23)



### (10/78) Explainable Global Wildfire Prediction Models using Graph Neural Networks (Dayou Chen et al., 2024)

{{<citation>}}

Dayou Chen, Sibo Cheng, Jinwei Hu, Matthew Kasoar, Rossella Arcucci. (2024)  
**Explainable Global Wildfire Prediction Models using Graph Neural Networks**
<br/>
<button class="copy-to-clipboard" title="Explainable Global Wildfire Prediction Models using Graph Neural Networks" index=10>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 90  
Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07152v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07152v1.pdf" filename="2402.07152v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Wildfire prediction has become increasingly crucial due to the escalating impacts of climate change. Traditional <b>CNN-based</b> wildfire prediction models struggle with handling missing oceanic data and addressing the long-range dependencies across distant regions in meteorological data. In this paper, we introduce an innovative <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)-based</b> model for global wildfire prediction. We propose a hybrid model that combines the spatial prowess of <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> with the temporal depth of Long Short-Term Memory (LSTM) networks. Our approach uniquely transforms global climate and wildfire data into a <b>graph</b> <b>representation,</b> <b>addressing</b> challenges such as null oceanic data locations and long-range dependencies inherent in traditional models. Benchmarking against established architectures using an unseen ensemble of JULES-INFERNO <b>simulations,</b> our model demonstrates superior predictive accuracy. Furthermore, we emphasise the model's explainability, unveiling potential wildfire correlation clusters through community detection and elucidating feature importance via Integrated Gradient analysis. Our findings not only advance the methodological domain of wildfire prediction but also underscore the importance of model transparency, offering valuable insights for stakeholders in wildfire management.

{{</citation>}}


### (11/78) Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine (Shayan Meshkat Alsadat et al., 2024)

{{<citation>}}

Shayan Meshkat Alsadat, Jean-Raphael Gaglione, Daniel Neider, Ufuk Topcu, Zhe Xu. (2024)  
**Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine**
<br/>
<button class="copy-to-clipboard" title="Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine" index=11>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 50  
Keywords: Few-shot, Reinforcement Learning, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07069v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07069v1.pdf" filename="2402.07069v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present LARL-RM <b>(Large</b> <b>language</b> <b>model-generated</b> Automaton for <b>Reinforcement</b> <b>Learning</b> with Reward Machine) algorithm in order to encode high-level knowledge into <b>reinforcement</b> <b>learning</b> using automaton to expedite the <b>reinforcement</b> <b>learning.</b> Our method uses <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> to obtain high-level domain-specific knowledge using <b>prompt</b> engineering instead of providing the <b>reinforcement</b> <b>learning</b> algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and <b>few-shot</b> methods for <b>prompt</b> engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop <b>reinforcement</b> <b>learning</b> without the need for an expert to guide and supervise the learning since LARL-RM can use the <b>LLM</b> directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-RM speeds up the convergence by 30% by implementing our method in two case studies.

{{</citation>}}


### (12/78) Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs (Bilal Chughtai et al., 2024)

{{<citation>}}

Bilal Chughtai, Alan Cooney, Neel Nanda. (2024)  
**Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs**
<br/>
<button class="copy-to-clipboard" title="Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs" index=12>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 40  
Keywords: Transformer, Large Language Model, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07321v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07321v1.pdf" filename="2402.07321v1.pdf">Download PDF</button>

---


**ABSTRACT**  
How do <b>transformer-based</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in <b>prompts</b> of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pair of two separate additive updates from different source tokens.

{{</citation>}}


### (13/78) Rethinking Graph Masked Autoencoders through Alignment and Uniformity (Liang Wang et al., 2024)

{{<citation>}}

Liang Wang, Xiang Tao, Qiang Liu, Shu Wu, Liang Wang. (2024)  
**Rethinking Graph Masked Autoencoders through Alignment and Uniformity**
<br/>
<button class="copy-to-clipboard" title="Rethinking Graph Masked Autoencoders through Alignment and Uniformity" index=13>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 40  
Keywords: Autoencoder, Contrastive Learning, Self-supervised Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07225v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07225v1.pdf" filename="2402.07225v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Self-supervised</b> <b>learning</b> on graphs can be bifurcated into <b>contrastive</b> <b>and</b> generative methods. <b>Contrastive</b> <b>methods,</b> also known as graph <b>contrastive</b> <b>learning</b> (GCL), have dominated graph <b>self-supervised</b> <b>learning</b> in the past few years, but the recent advent of graph masked <b>autoencoder</b> (GraphMAE) rekindles the momentum behind generative methods. Despite the empirical success of GraphMAE, there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and <b>contrastive</b> <b>methods</b> have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between GraphMAE and GCL, and prove that the node-level reconstruction objective in GraphMAE implicitly performs context-level GCL. Based on our theoretical analysis, we further identify the limitations of the GraphMAE from the perspectives of alignment and uniformity, which have been considered as two key properties of high-quality representations in GCL. We point out that GraphMAE's alignment performance is restricted by the masking strategy, and the uniformity is not strictly guaranteed. To remedy the aforementioned limitations, we propose an Alignment-Uniformity enhanced Graph Masked <b>AutoEncoder,</b> named AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy to provide hard-to-align samples, which improves the alignment performance. Meanwhile, we introduce an explicit uniformity regularizer to ensure the uniformity of the learned representations. Experimental results on benchmark datasets demonstrate the superiority of our model over existing state-of-the-art methods.

{{</citation>}}


### (14/78) A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference (Chenlu Ye et al., 2024)

{{<citation>}}

Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, Tong Zhang. (2024)  
**A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference**
<br/>
<button class="copy-to-clipboard" title="A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference" index=14>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, stat-ML  
Keyword Score: 30  
Keywords: Reinforcement Learning, Large Language Model, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07314v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07314v1.pdf" filename="2402.07314v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Reinforcement</b> <b>Learning</b> from Human Feedback (RLHF) learns from the preference signal provided by a probabilistic preference model, which takes a <b>prompt</b> and two responses as input, and produces a score indicating the preference of one response against another. So far, the most popular RLHF paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. However, the existence of a reward function is a strong assumption and the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference. In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive <b>LLMs.</b> The learning objective is to find a policy that consistently generates responses preferred over any competing policy while staying close to the initial model. The objective is defined as the Nash equilibrium (NE) of the KL-regularized preference model. We aim to make the first attempt to study the theoretical learnability of the KL-regularized NLHF by considering both offline and online settings. For the offline learning from a pre-collected dataset, we propose algorithms that are efficient under suitable coverage conditions of the dataset. For batch online learning from iterative interactions with a preference oracle, our proposed algorithm enjoys a finite sample guarantee under the structural condition of the underlying preference model. Our results connect the new NLHF paradigm with traditional RL theory, and validate the potential of reward-model-free learning under general preference.

{{</citation>}}


### (15/78) Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning (Mohak Chadha et al., 2024)

{{<citation>}}

Mohak Chadha, Pulkit Khera, Jianfeng Gu, Osama Abboud, Michael Gerndt. (2024)  
**Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning**
<br/>
<button class="copy-to-clipboard" title="Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning" index=15>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-DC, cs-LG, cs.LG  
Keyword Score: 30  
Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07295v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07295v1.pdf" filename="2402.07295v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> in this paper. Towards this, we propose novel optimized serverless workflows for two popular conventional federated <b>KD</b> techniques, i.e., FedMD and FedDF. We implement these workflows by introducing several extensions to an open-source serverless FL system called FedLess. Moreover, we comprehensively evaluate the two strategies on multiple datasets across varying levels of client data heterogeneity using heterogeneous client models with respect to accuracy, fine-grained training times, and costs. Results from our experiments demonstrate that serverless FedDF is more robust to extreme non-IID data distributions, is faster, and leads to lower costs than serverless FedMD. In addition, compared to the original implementation, our optimizations for particular steps in FedMD and FedDF lead to an average speedup of 3.5x and 1.76x across all datasets.

{{</citation>}}


### (16/78) Physics-Informed Neural Networks with Hard Linear Equality Constraints (Hao Chen et al., 2024)

{{<citation>}}

Hao Chen, Gonzalo E. Constante Flores, Can Li. (2024)  
**Physics-Informed Neural Networks with Hard Linear Equality Constraints**
<br/>
<button class="copy-to-clipboard" title="Physics-Informed Neural Networks with Hard Linear Equality Constraints" index=16>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-OC  
Keyword Score: 30  
Keywords: Knowledge Distillation, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07251v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07251v1.pdf" filename="2402.07251v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Surrogate modeling is used to replace computationally expensive <b>simulations.</b> Neural networks have been widely applied as surrogate models that enable efficient evaluations over complex physical systems. Despite this, neural networks are data-driven models and devoid of any physics. The incorporation of physics into neural networks can improve generalization and data efficiency. The physics-informed neural network (PINN) is an approach to leverage known physical constraints present in the data, but it cannot strictly satisfy them in the predictions. This work proposes a novel physics-informed neural network, KKT-hPINN, which rigorously guarantees hard linear equality constraints through projection layers derived from KKT conditions. Numerical experiments on Aspen models of a continuous stirred-tank reactor (CSTR) unit, an extractive <b>distillation</b> subsystem, and a chemical plant demonstrate that this model can further enhance the prediction accuracy.

{{</citation>}}


### (17/78) Rethinking the Capacity of Graph Neural Networks for Branching Strategy (Ziang Chen et al., 2024)

{{<citation>}}

Ziang Chen, Jialin Liu, Xiaohan Chen, Xinshang Wang, Wotao Yin. (2024)  
**Rethinking the Capacity of Graph Neural Networks for Branching Strategy**
<br/>
<button class="copy-to-clipboard" title="Rethinking the Capacity of Graph Neural Networks for Branching Strategy" index=17>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-OC  
Keyword Score: 30  
Keywords: Message-Passing, Graph Neural Network, Graph Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07099v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07099v1.pdf" filename="2402.07099v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of <b>GNNs</b> to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm. Although <b>message-passing</b> <b>GNN</b> (MP-GNN), as the simplest <b>GNN</b> structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power -- there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. In addition, we establish a universal approximation theorem for another <b>GNN</b> structure called the second-order folklore <b>GNN</b> (2-FGNN). We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability. A small-scale numerical experiment is conducted to directly validate our theoretical findings.

{{</citation>}}


### (18/78) ODIN: Disentangled Reward Mitigates Hacking in RLHF (Lichang Chen et al., 2024)

{{<citation>}}

Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro. (2024)  
**ODIN: Disentangled Reward Mitigates Hacking in RLHF**
<br/>
<button class="copy-to-clipboard" title="ODIN: Disentangled Reward Mitigates Hacking in RLHF" index=18>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-CL, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Reinforcement Learning, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07319v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07319v1.pdf" filename="2402.07319v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this work, we study the issue of reward hacking on the response length, a challenge emerging in <b>Reinforcement</b> <b>Learning</b> from Human Feedback (RLHF) on <b>LLMs.</b> A well-formatted, verbose but less helpful response from the <b>LLMs</b> can often deceive <b>LLMs</b> or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between <b>LLM</b> evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the other trained to decorrelate with length and therefore focus more on the actual content. We then discard the length head in RL to prevent reward hacking on length. Experiments demonstrate that our approach almost eliminates the reward correlation with length, and improves the obtained policy by a significant margin.

{{</citation>}}


### (19/78) HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs (Adrin Bazaga et al., 2024)

{{<citation>}}

Adrin Bazaga, Pietro Li, Gos Micklem. (2024)  
**HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs**
<br/>
<button class="copy-to-clipboard" title="HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs" index=19>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG, stat-ML  
Keyword Score: 20  
Keywords: Node Classification, BERT  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07309v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07309v2.pdf" filename="2402.07309v2.pdf">Download PDF</button>

---


**ABSTRACT**  
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of <b>node</b> <b>classification</b> on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the <b>nodes</b> <b>attributes,</b> which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained <b>BERT</b> model with specialized hypergraph-aware layers for the task of <b>node</b> <b>classification.</b> Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model which simultaneously models hypergraph relational structure while maintaining the high-quality text encoding capabilities of a pre-trained <b>BERT.</b> Notably, HyperBERT presents results that achieve a new state-of-the-art on five challenging text-attributed hypergraph <b>node</b> <b>classification</b> benchmarks.

{{</citation>}}


### (20/78) Power Transformer Fault Prediction Based on Knowledge Graphs (Chao Wang et al., 2024)

{{<citation>}}

Chao Wang, Zhuo Chen, Ziyan Zhang, Chiyi Li, Kai Song. (2024)  
**Power Transformer Fault Prediction Based on Knowledge Graphs**
<br/>
<button class="copy-to-clipboard" title="Power Transformer Fault Prediction Based on Knowledge Graphs" index=20>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CL, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Logistic Regression, Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07283v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07283v1.pdf" filename="2402.07283v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we address the challenge of learning with limited fault data for power <b>transformers.</b> Traditional operation and maintenance tools lack effective predictive capabilities for potential faults. The scarcity of extensive fault data makes it difficult to apply machine learning techniques effectively. To solve this problem, we propose a novel approach that leverages the knowledge graph (KG) technology in combination with gradient boosting decision trees (GBDT). This method is designed to efficiently learn from a small set of high-dimensional data, integrating various factors influencing <b>transformer</b> faults and historical operational data. Our approach enables accurate safe state assessments and fault analyses of power <b>transformers</b> despite the limited fault characteristic data. Experimental results demonstrate that this method outperforms other learning approaches in prediction accuracy, such as artificial neural networks (ANN) and <b>logistic</b> <b>regression</b> (LR). Furthermore, it offers significant improvements in progressiveness, practicality, and potential for widespread application.

{{</citation>}}


### (21/78) More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning (Kaiwen Wang et al., 2024)

{{<citation>}}

Kaiwen Wang, Owen Oertell, Alekh Agarwal, Nathan Kallus, Wen Sun. (2024)  
**More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning" index=21>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Bandit Algorithm, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07198v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07198v1.pdf" filename="2402.07198v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we prove that Distributional <b>Reinforcement</b> <b>Learning</b> (DistRL), which learns the return distribution, can obtain second-order bounds in both online and offline RL in general settings with function approximation. Second-order bounds are instance-dependent bounds that scale with the variance of return, which we prove are tighter than the previously known small-loss bounds of distributional RL. To the best of our knowledge, our results are the first second-order bounds for low-rank MDPs and for offline RL. When specializing to contextual <b>bandits</b> (one-step RL problem), we show that a distributional learning based optimism algorithm achieves a second-order worst-case regret bound, and a second-order gap dependent bound, simultaneously. We also empirically demonstrate the benefit of DistRL in contextual <b>bandits</b> on real-world datasets. We highlight that our analysis with DistRL is relatively simple, follows the general framework of optimism in the face of uncertainty and does not require weighted regression. Our results suggest that DistRL is a promising framework for obtaining second-order bounds in general RL settings, thus further reinforcing the benefits of DistRL.

{{</citation>}}


### (22/78) The Implicit Bias of Gradient Noise: A Symmetry Perspective (Liu Ziyin et al., 2024)

{{<citation>}}

Liu Ziyin, Mingze Wang, Lei Wu. (2024)  
**The Implicit Bias of Gradient Noise: A Symmetry Perspective**
<br/>
<button class="copy-to-clipboard" title="The Implicit Bias of Gradient Noise: A Symmetry Perspective" index=22>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG, math-OC, stat-ML  
Keyword Score: 20  
Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07193v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07193v1.pdf" filename="2402.07193v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We characterize the learning dynamics of <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> when continuous symmetry exists in the loss function, where the divergence between <b>SGD</b> and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, <b>SGD</b> naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, <b>SGD</b> will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, matrix factorization, and the use of warmup.

{{</citation>}}


### (23/78) Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations (Muhammad Bilal Shahid et al., 2024)

{{<citation>}}

Muhammad Bilal Shahid, Cody Fleming. (2024)  
**Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations**
<br/>
<button class="copy-to-clipboard" title="Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations" index=23>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Gaussian Process, Recommendation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07139v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07139v1.pdf" filename="2402.07139v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The selection of the target variable is important while learning parameters of the classical car following models like GIPPS, IDM, etc. There is a vast body of literature on which target variable is optimal for classical car following models, but there is no study that empirically evaluates the selection of optimal target variables for black-box models, such as LSTM, etc. The black-box models, like LSTM and <b>Gaussian</b> <b>Process</b> (GP) are increasingly being used to model car following behavior without wise selection of target variables. The current work tests different target variables, like acceleration, velocity, and headway, for three black-box models, i.e., GP, LSTM, and Kernel Ridge Regression. These models have different objective functions and work in different vector spaces, e.g., GP works in function space, and LSTM works in parameter space. The experiments show that the optimal target variable <b>recommendations</b> for black-box models differ from classical car following models depending on the objective function and the vector space. It is worth mentioning that models and datasets used during evaluation are diverse in nature: the datasets contained both automated and human-driven vehicle trajectories; the black-box models belong to both parametric and non-parametric classes of models. This diversity is important during the analysis of variance, wherein we try to find the interaction between datasets, models, and target variables. It is shown that the models and target variables interact and recommended target variables don't depend on the dataset under consideration.

{{</citation>}}


### (24/78) Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning (Alex Christopher Stutts et al., 2024)

{{<citation>}}

Alex Christopher Stutts, Danilo Erricolo, Theja Tulabandhula, Amit Ranjan Trivedi. (2024)  
**Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning" index=24>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 20  
Keywords: Out-of-distribution, Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07107v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07107v2.pdf" filename="2402.07107v2.pdf">Download PDF</button>

---


**ABSTRACT**  
We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional <b>reinforcement</b> <b>learning</b> involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of <b>out-of-distribution</b> (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously evaluate uncertainty improves exploration strategies and can serve as a blueprint for other algorithms requiring uncertainty awareness.

{{</citation>}}


### (25/78) Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study (Santonu Sarkar et al., 2024)

{{<citation>}}

Santonu Sarkar, Shanay Mehta, Nicole Fernandes, Jyotirmoy Sarkar, Snehanshu Saha. (2024)  
**Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study**
<br/>
<button class="copy-to-clipboard" title="Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study" index=25>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Outlier Detection  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07281v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07281v1.pdf" filename="2402.07281v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured. A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events. This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study. The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and <b>outlier</b> <b>detection</b> methods. The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios. The paper dispels the deep learning myth, demonstrating that though powerful, deep learning is not a universal solution in this case. We observed that recently proposed tree-based evolutionary algorithms outperform in many scenarios. We noticed that tree-based approaches catch a singleton anomaly in a dataset where deep learning methods fail. On the other hand, classical SVM performs the best on datasets with more than 10% anomalies, implying that such scenarios can be best modeled as a classification problem rather than anomaly detection. To our knowledge, such a study on a large number of state-of-the-art algorithms using diverse data sets, with the objective of guiding researchers and practitioners in making informed algorithmic choices, has not been attempted earlier.

{{</citation>}}


### (26/78) Towards Generalized Inverse Reinforcement Learning (Chaosheng Dong et al., 2024)

{{<citation>}}

Chaosheng Dong, Yijia Wang. (2024)  
**Towards Generalized Inverse Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Towards Generalized Inverse Reinforcement Learning" index=26>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07246v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07246v1.pdf" filename="2402.07246v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This paper studies generalized inverse <b>reinforcement</b> <b>learning</b> (GIRL) in Markov decision processes (MDPs), that is, the problem of learning the basic components of an MDP given observed behavior (policy) that might not be optimal. These components include not only the reward function and transition probability matrices, but also the action space and state space that are not exactly known but are known to belong to given uncertainty sets. We address two key challenges in GIRL: first, the need to quantify the discrepancy between the observed policy and the underlying optimal policy; second, the difficulty of mathematically characterizing the underlying optimal policy when the basic components of an MDP are unobservable or partially observable. Then, we propose the mathematical formulation for GIRL and develop a fast heuristic algorithm. Numerical results on both finite and infinite state problems show the merit of our formulation and algorithm.

{{</citation>}}


### (27/78) GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains (Yan Lin et al., 2024)

{{<citation>}}

Yan Lin, Jilin Hu, Shengnan Guo, Bin Yang, Christian S. Jensen, Youfang Lin, Huaiyu Wan. (2024)  
**GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains**
<br/>
<button class="copy-to-clipboard" title="GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains" index=27>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07232v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07232v1.pdf" filename="2402.07232v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Trajectories are sequences of timestamped location samples. In sparse trajectories, the locations are sampled infrequently; and while such trajectories are prevalent in real-world settings, they are challenging to use to enable high-quality transportation-related applications. Current methodologies either assume densely sampled and accurately map-matched trajectories, or they rely on two-stage schemes, yielding sub-optimal applications. To extend the utility of sparse trajectories, we propose a novel sparse trajectory learning framework, GenSTL. The framework is pre-trained to form connections between sparse trajectories and dense counterparts using auto-regressive generation of feature domains. GenSTL can subsequently be applied directly in downstream tasks, or it can be <b>fine-tuned</b> first. This way, GenSTL eliminates the reliance on the availability of large-scale dense and map-matched trajectory data. The inclusion of a well-crafted feature domain encoding layer and a hierarchical masked trajectory encoder enhances GenSTL's learning capabilities and adaptability. Experiments on two real-world trajectory datasets offer insight into the framework's ability to contend with sparse trajectories with different sampling intervals and its versatility across different downstream tasks, thus offering evidence of its practicality in real-world applications.

{{</citation>}}


### (28/78) Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning (Willem Rpke et al., 2024)

{{<citation>}}

Willem Rpke, Mathieu Reymond, Patrick Mannion, Diederik M. Roijers, Ann Now, Roxana Rdulescu. (2024)  
**Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning**
<br/>
<button class="copy-to-clipboard" title="Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning" index=28>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07182v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07182v1.pdf" filename="2402.07182v1.pdf">Download PDF</button>

---


**ABSTRACT**  
A significant challenge in multi-objective <b>reinforcement</b> <b>learning</b> is obtaining a Pareto front of policies that attain optimal performance under different preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), a principled algorithm that decomposes the task of finding the Pareto front into a sequence of single-objective problems for which various solution methods exist. This enables us to establish convergence guarantees while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step. Empirical evaluations demonstrate that IPRO matches or outperforms methods that require additional domain knowledge. By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective <b>reinforcement</b> <b>learning,</b> such as in pathfinding and optimisation.

{{</citation>}}


### (29/78) GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring (Madhav Khirwar et al., 2024)

{{<citation>}}

Madhav Khirwar, Ankur Narang. (2024)  
**GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring**
<br/>
<button class="copy-to-clipboard" title="GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring" index=29>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CV, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07164v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07164v1.pdf" filename="2402.07164v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Air pollution represents a pivotal environmental challenge globally, playing a major role in climate change via greenhouse gas emissions and negatively affecting the health of billions. However predicting the spatial and temporal patterns of pollutants remains challenging. The scarcity of ground-based monitoring facilities and the dependency of air pollution modeling on comprehensive datasets, often inaccessible for numerous areas, complicate this issue. In this work, we introduce GeoFormer, a compact model that combines a vision <b>transformer</b> module with a highly efficient time-series <b>transformer</b> module to predict surface-level nitrogen dioxide (NO2) concentrations from Sentinel-5P satellite imagery. We train the proposed model to predict surface-level NO2 measurements using a dataset we constructed with Sentinel-5P images of ground-level monitoring stations, and their corresponding NO2 concentration readings. The proposed model attains high accuracy (MAE 5.65), demonstrating the efficacy of combining vision and time-series <b>transformer</b> architectures to harness satellite-derived data for enhanced GHG emission insights, proving instrumental in advancing climate change monitoring and emission regulation efforts globally.

{{</citation>}}


### (30/78) Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments (Jeongyeol Kwon et al., 2024)

{{<citation>}}

Jeongyeol Kwon, Liu Yang, Robert Nowak, Josiah Hanna. (2024)  
**Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments**
<br/>
<button class="copy-to-clipboard" title="Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments" index=30>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-AI, cs-LG, cs.LG  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07102v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07102v1.pdf" filename="2402.07102v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Learning a good history representation is one of the core challenges of <b>reinforcement</b> <b>learning</b> (RL) in partially observable environments. Recent works have shown the advantages of various auxiliary tasks for facilitating representation learning. However, the effectiveness of such auxiliary tasks has not been fully convincing, especially in partially observable environments that require long-term memorization and inference. In this empirical study, we investigate the effectiveness of future prediction for learning the representations of histories, possibly of extensive length, in partially observable environments. We first introduce an approach that decouples the task of learning history representations from policy optimization via future prediction. Then, our main contributions are two-fold: (a) we demonstrate that the performance of <b>reinforcement</b> <b>learning</b> is strongly correlated with the prediction accuracy of future observations in partially observable environments, and (b) our approach can significantly improve the overall end-to-end approach by preventing high-variance noisy signals from <b>reinforcement</b> <b>learning</b> objectives to influence the representation learning. We illustrate our claims on three types of benchmarks that necessitate the ability to process long histories for high returns.

{{</citation>}}


### (31/78) Refined Sample Complexity for Markov Games with Independent Linear Function Approximation (Yan Dai et al., 2024)

{{<citation>}}

Yan Dai, Qiwen Cui, Simon S. Du. (2024)  
**Refined Sample Complexity for Markov Games with Independent Linear Function Approximation**
<br/>
<button class="copy-to-clipboard" title="Refined Sample Complexity for Markov Games with Independent Linear Function Approximation" index=31>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-GT, cs-LG, cs.LG, stat-ML  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07082v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07082v1.pdf" filename="2402.07082v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Markov Games (MG) is an important model for Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL). It was long believed that the "curse of multi-agents" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023. While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023). This paper first refines the `AVLPR` framework by Wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent linear function approximations, we propose novel *action-dependent bonuses* to cover occasionally extreme estimation errors. With the help of state-of-the-art techniques from the single-agent RL literature, we give the first algorithm that tackles the curse of multi-agents, attains the optimal $O(T^{-1/2})$ convergence rate, and avoids $\text{poly}(A_{\max})$ dependency simultaneously.

{{</citation>}}


### (32/78) The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey (Taojie Kuang et al., 2024)

{{<citation>}}

Taojie Kuang, Pengfei Liu, Zhixiang Ren. (2024)  
**The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey**
<br/>
<button class="copy-to-clipboard" title="The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey" index=32>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.LG  
Categories: cs-CE, cs-LG, cs.LG, q-bio-BM  
Keyword Score: 3  
Keywords: Multi-modal  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07249v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07249v1.pdf" filename="2402.07249v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The precise prediction of molecular properties is essential for advancements in drug development, particularly in virtual screening and compound optimization. The recent introduction of numerous deep learning-based methods has shown remarkable potential in enhancing molecular property prediction (MPP), especially improving accuracy and insights into molecular structures. Yet, two critical questions arise: does the integration of domain knowledge augment the accuracy of molecular property prediction and does employing <b>multi-modal</b> data fusion yield more precise results than unique data source methods? To explore these matters, we comprehensively review and quantitatively analyze recent deep learning methods based on various benchmarks. We discover that integrating molecular information will improve both MPP regression and classification tasks by upto 3.98% and 1.72%, respectively. We also discover that the utilizing 3-dimensional information with 1-dimensional and 2-dimensional information simultaneously can substantially enhance MPP upto 4.2%. The two consolidated insights offer crucial guidance for future advancements in drug discovery.

{{</citation>}}


## eess.IV (4)



### (33/78) Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation (Ziyang Wang et al., 2024)

{{<citation>}}

Ziyang Wang, Chao Ma. (2024)  
**Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation**
<br/>
<button class="copy-to-clipboard" title="Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation" index=33>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 80  
Keywords: Contrastive Learning, Convolution, Convolutional Neural Network, Convolutional Neural Network, Self-supervised Learning, Semi-Supervised Learning, Transformer, Self-Attention  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07245v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07245v1.pdf" filename="2402.07245v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Medical image segmentation is essential in diagnostics, treatment planning, and healthcare, with deep learning offering promising advancements. Notably, <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> excel in capturing local image features, whereas Vision <b>Transformer</b> (ViT) adeptly model long-range dependencies through multi-head <b>self-attention</b> mechanisms. Despite their strengths, both <b>CNN</b> and ViT face challenges in efficiently processing long-range dependencies within medical images, often requiring substantial computational resources. This issue, combined with the high cost and limited availability of expert annotations, poses significant obstacles to achieving precise segmentation. To address these challenges, this paper introduces the Semi-Mamba-UNet, which integrates a visual mamba-based UNet architecture with a conventional UNet into a <b>semi-supervised</b> <b>learning</b> (SSL) framework. This innovative SSL approach leverages dual networks to jointly generate pseudo labels and cross supervise each other, drawing inspiration from consistency regularization techniques. Furthermore, we introduce a <b>self-supervised</b> pixel-level <b>contrastive</b> <b>learning</b> strategy, employing a projector pair to further enhance feature learning capabilities. Our comprehensive evaluation on a publicly available MRI cardiac segmentation dataset, comparing against various SSL frameworks with different UNet-based segmentation networks, highlights the superior performance of Semi-Mamba-UNet. The source code has been made publicly accessible.

{{</citation>}}


### (34/78) KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos (Yiting Lu et al., 2024)

{{<citation>}}

Yiting Lu, Xin Li, Yajing Pei, Kun Yuan, Qizhi Xie, Yunpeng Qu, Ming Sun, Chao Zhou, Zhibo Chen. (2024)  
**KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos**
<br/>
<button class="copy-to-clipboard" title="KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos" index=34>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Visual Question Answering, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07220v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07220v1.pdf" filename="2402.07220v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Short-form UGC video platforms, like Kwai and TikTok, have been an emerging and irreplaceable mainstream media form, thriving on user-friendly engagement, and kaleidoscope creation, etc. However, the advancing content-generation modes, e.g., special effects, and sophisticated processing workflows, e.g., de-artifacts, have introduced significant challenges to recent UGC video quality assessment: (i) the ambiguous contents hinder the identification of quality-determined regions. (ii) the diverse and complicated hybrid distortions are hard to distinguish. To tackle the above challenges and assist in the development of short-form videos, we establish the first large-scale Kaleidoscope short Video database for Quality assessment, termed KVQ, which comprises 600 user-uploaded short videos and 3600 processed videos through the diverse practical processing workflows, including pre-processing, transcoding, and enhancement. Among them, the absolute quality score of each video and partial ranking score among indistinguishable samples are provided by a team of professional researchers specializing in image processing. Based on this database, we propose the first short-form video quality evaluator, i.e., KSVQE, which enables the quality evaluator to identify the quality-determined semantics with the content understanding of large vision language models (i.e., CLIP) and distinguish the distortions with the distortion understanding module. Experimental results have shown the effectiveness of KSVQE on our KVQ database and popular <b>VQA</b> databases.

{{</citation>}}


### (35/78) Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations (H. Fabelo et al., 2024)

{{<citation>}}

H. Fabelo, S. Ortega, D. Ravi, B. R. Kiran, C. Sosa, D. Bulters, G. M. Callico, H. Bulstrode, A. Szolna, J. F. Pineiro, S. Kabwama, D. Madronal, R. Lazcano, A. J. OShanahan, S. Bisshopp, M. Hernandez, A. Baez-Quevedo, G. Z. Yang, B. Stanciulescu, R. Salvador, E. Juarez, R. Sarmiento. (2024)  
**Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations**
<br/>
<button class="copy-to-clipboard" title="Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations" index=35>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 20  
Keywords: Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07192v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07192v1.pdf" filename="2402.07192v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Surgery for brain cancer is a major problem in neurosurgery. The diffuse infiltration into the surrounding normal brain by these tumors makes their accurate identification by the naked eye difficult. Since surgery is the common treatment for brain cancer, an accurate radical resection of the tumor leads to improved survival rates for patients. However, the identification of the tumor boundaries during surgery is challenging. Hyperspectral imaging is a noncontact, non-ionizing and non-invasive technique suitable for medical diagnosis. This study presents the development of a novel classification method taking into account the spatial and spectral characteristics of the hyperspectral images to help neurosurgeons to accurately determine the tumor boundaries in surgical-time during the resection, avoiding excessive excision of normal tissue or unintentionally leaving residual tumor. The algorithm proposed in this study to approach an efficient solution consists of a hybrid framework that combines both <b>supervised</b> and <b>unsupervised</b> machine learning methods. To evaluate the proposed approach, five hyperspectral images of surface of the brain affected by glioblastoma tumor in vivo from five different patients have been used. The final classification maps obtained have been analyzed and validated by specialists. These preliminary results are promising, obtaining an accurate delineation of the tumor area.

{{</citation>}}


### (36/78) Supervised Reconstruction for Silhouette Tomography (Evan Bell et al., 2024)

{{<citation>}}

Evan Bell, Michael T. McCann, Marc Klasky. (2024)  
**Supervised Reconstruction for Silhouette Tomography**
<br/>
<button class="copy-to-clipboard" title="Supervised Reconstruction for Silhouette Tomography" index=36>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.IV  
Categories: cs-CV, eess-IV, eess.IV  
Keyword Score: 10  
Keywords: Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07298v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07298v1.pdf" filename="2402.07298v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we introduce silhouette tomography, a novel formulation of X-ray computed tomography that relies only on the geometry of the imaging system. We formulate silhouette tomography mathematically and provide a simple method for obtaining a particular solution to the problem, assuming that any solution exists. We then propose a <b>supervised</b> reconstruction approach that uses a deep neural network to solve the silhouette tomography problem. We present experimental results on a synthetic dataset that demonstrate the effectiveness of the proposed method.

{{</citation>}}


## cs.AI (11)



### (37/78) GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks (Mengmei Zhang et al., 2024)

{{<citation>}}

Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi. (2024)  
**GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks**
<br/>
<button class="copy-to-clipboard" title="GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks" index=37>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 70  
Keywords: Node Classification, Zero-shot, ChatGPT, Instruction Following, Question Answering, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07197v2" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07197v2.pdf" filename="2402.07197v2.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>ChatGPT,</b> exhibit powerful <b>zero-shot</b> and <b>instruction-following</b> <b>capabilities,</b> have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying <b>LLMs</b> to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with <b>LLM</b> as a <b>node</b> <b>feature</b> enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and <b>LLM</b> by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of <b>LLMs</b> to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along <b>node</b> <b>information,</b> neighbor information and model information. By treating the <b>node</b> <b>representation</b> as a type of language, the proposed GraphTranslator empowers an <b>LLM</b> to make predictions based on <b>node</b> <b>representation</b> and language <b>instructions,</b> <b>providing</b> a unified perspective for both pre-defined and open-ended tasks. Extensive results show that the proposed GraphTranslator effectively improves the results of <b>zero-shot</b> <b>node</b> <b>classification.</b> The graph <b>question</b> <b>answering</b> experiments reveal our GraphTranslator potential across a broad spectrum of open-ended applications through language instructions.

{{</citation>}}


### (38/78) Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers (Minoo Shayaninasab et al., 2024)

{{<citation>}}

Minoo Shayaninasab, Bagher Babaali. (2024)  
**Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers**
<br/>
<button class="copy-to-clipboard" title="Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers" index=38>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 66  
Keywords: Fine-tuning, Multi-modal, Multi-modal, Self-supervised Learning, Self-supervised Learning, Transfer Learning, Transformer, Emotion Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07327v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07327v1.pdf" filename="2402.07327v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Due to the complex nature of human <b>emotions</b> <b>and</b> the diversity of <b>emotion</b> <b>representation</b> methods in humans, <b>emotion</b> <b>recognition</b> is a challenging field. In this research, three input modalities, namely text, audio (speech), and video, are employed to generate <b>multimodal</b> feature vectors. For generating features for each of these modalities, pre-trained <b>Transformer</b> models with <b>fine-tuning</b> are utilized. In each modality, a <b>Transformer</b> model is used with <b>transfer</b> <b>learning</b> to extract feature and <b>emotional</b> <b>structure.</b> These features are then fused together, and <b>emotion</b> <b>recognition</b> is performed using a classifier. To select an appropriate fusion method and classifier, various feature-level and decision-level fusion techniques have been experimented with, and ultimately, the best model, which combines feature-level fusion by concatenating feature vectors and classification using a Support Vector Machine on the IEMOCAP <b>multimodal</b> dataset, achieves an accuracy of 75.42%. Keywords: <b>Multimodal</b> <b>Emotion</b> <b>Recognition,</b> IEMOCAP, <b>Self-Supervised</b> <b>Learning,</b> <b>Transfer</b> <b>Learning,</b> <b>Transformer.</b>

{{</citation>}}


### (39/78) Persian Speech Emotion Recognition by Fine-Tuning Transformers (Minoo Shayaninasab et al., 2024)

{{<citation>}}

Minoo Shayaninasab, Bagher Babaali. (2024)  
**Persian Speech Emotion Recognition by Fine-Tuning Transformers**
<br/>
<button class="copy-to-clipboard" title="Persian Speech Emotion Recognition by Fine-Tuning Transformers" index=39>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI, eess-AS  
Keyword Score: 60  
Keywords: Fine-tuning, Fine-tuning, Self-supervised Learning, Self-supervised Learning, Transformer, Emotion Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07326v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07326v1.pdf" filename="2402.07326v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Given the significance of speech <b>emotion</b> <b>recognition,</b> numerous methods have been developed in recent years to create effective and efficient systems in this domain. One of these methods involves the use of pretrained <b>transformers,</b> <b>fine-tuned</b> to address this specific problem, resulting in high accuracy. Despite extensive discussions and global-scale efforts to enhance these systems, the application of this innovative and effective approach has received less attention in the context of Persian speech <b>emotion</b> <b>recognition.</b> In this article, we review the field of speech <b>emotion</b> <b>recognition</b> and its background, with an emphasis on the importance of employing <b>transformers</b> in this context. We present two models, one based on spectrograms and the other on the audio itself, <b>fine-tuned</b> using the shEMO dataset. These models significantly enhance the accuracy of previous systems, increasing it from approximately 65% to 80% on the mentioned dataset. Subsequently, to investigate the effect of multilinguality on the <b>fine-tuning</b> process, these same models are <b>fine-tuned</b> twice. First, they are <b>fine-tuned</b> using the English IEMOCAP dataset, and then they are <b>fine-tuned</b> with the Persian shEMO dataset. This results in an improved accuracy of 82% for the Persian <b>emotion</b> <b>recognition</b> system. Keywords: Persian Speech <b>Emotion</b> <b>Recognition,</b> shEMO, <b>Self-Supervised</b> <b>Learning</b>

{{</citation>}}


### (40/78) CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain (Xin Tong et al., 2024)

{{<citation>}}

Xin Tong, Bo Jin, Zhi Lin, Binjun Wang, Ting Yu. (2024)  
**CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain**
<br/>
<button class="copy-to-clipboard" title="CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain" index=40>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 60  
Keywords: Information Retrieval, Question Answering, Text Classification, Text Generation, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07234v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07234v1.pdf" filename="2402.07234v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream <b>LLMs</b> in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of <b>LLMs</b> across four key dimensions: <b>text</b> <b>classification,</b> <b>information</b> <b>extraction,</b> <b>question</b> <b>answering,</b> and <b>text</b> <b>generation.</b> Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of <b>LLMs</b> in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized <b>LLM</b> models targeted at applications in this field.

{{</citation>}}


### (41/78) Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy (Zehao Dong et al., 2024)

{{<citation>}}

Zehao Dong, Yixin Chen, Hiram Gay, Yao Hao, Geoffrey D. Hugo, Pamela Samson, Tianyu Zhao. (2024)  
**Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy**
<br/>
<button class="copy-to-clipboard" title="Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy" index=41>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 60  
Keywords: Graph Neural Network, Convolutional Neural Network, Human Intervention, Transformer, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07167v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07167v1.pdf" filename="2402.07167v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Treatment planning is currently a patient specific, time-consuming, and resource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction plays a critical role in automating this process. The geometric relationship between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target volume (PTV) has been well established. This study explores the potential of deep learning models for predicting DVHs using images and subsequent <b>human</b> <b>intervention</b> facilitated by a <b>large-language</b> <b>model</b> <b>(LLM)</b> to enhance the planning quality. We propose a pipeline to convert unstructured images to a structured <b>graph</b> <b>consisting</b> <b>of</b> image-patch nodes and dose nodes. A novel Dose <b>Graph</b> <b>Neural</b> <b>Network</b> (DoseGNN) model is developed for predicting DVHs from the structured <b>graph.</b> <b>The</b> <b>proposed</b> DoseGNN is enhanced with the <b>LLM</b> to encode massive knowledge from prescriptions and interactive instructions from clinicians. In this study, we introduced an online <b>human-AI</b> <b>collaboration</b> (OHAC) system as a practical implementation of the concept proposed for the automation of intensity-modulated radiotherapy (IMRT) planning. In comparison to the widely-employed DL models used in radiotherapy, DoseGNN achieved mean square errors that were 80$\%$, 76$\%$ and 41.0$\%$ of those predicted by Swin U-Net <b>Transformer,</b> 3D U-Net <b>CNN</b> and vanilla MLP, respectively. Moreover, the <b>LLM-empowered</b> DoseGNN model facilitates seamless adjustment to treatment plans through interaction with clinicians using natural language.

{{</citation>}}


### (42/78) Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models (Yuyao Ge et al., 2024)

{{<citation>}}

Yuyao Ge, Shenghua Liu, Lingrui Mei, Lizhe Chen, Xueqi Cheng. (2024)  
**Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models**
<br/>
<button class="copy-to-clipboard" title="Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models" index=42>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 30  
Keywords: Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07140v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07140v1.pdf" filename="2402.07140v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In recent years, <b>Large</b> <b>Language</b> <b>Models</b> have reached state-of-the-art performance across multiple domains. However, the progress in the field of graph <b>reasoning</b> remains limited. Our work delves into this gap by thoroughly investigating graph <b>reasoning</b> with <b>LLM.</b> In this work, we reveal the impact of text sequence on <b>LLM</b> spatial understanding, finding that graph-descriptive text sequences significantly affect <b>LLM</b> <b>reasoning</b> performance on graphs. By altering the graph-descriptive text sequences, we enhance the performance of <b>LLM</b> from 42.22\% to 70\%. Furthermore, we evaluate the relationship between <b>LLM</b> performance and graph size, discovering that the <b>reasoning</b> performance of <b>LLM</b> does not monotonically decrease with the increase in graph size. Conclusively, we introduce the Scaled Graph <b>Reasoning</b> benchmark for assessing <b>LLM</b> performance across varied graph sizes.

{{</citation>}}


### (43/78) Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning (Yihong Tang et al., 2024)

{{<citation>}}

Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Kebing Hou, Dingyi Zhuang, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma. (2024)  
**Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning**
<br/>
<button class="copy-to-clipboard" title="Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning" index=43>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs-CL, cs-LG, cs.AI  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07204v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07204v1.pdf" filename="2402.07204v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to provide services that customize urban itineraries based on users' needs. Specifically, we develop an <b>LLM-based</b> pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage <b>LLM</b> in cooperation with an embedding-based module for retrieving candidate POIs from the user's POI database. Then, a spatial optimization module is used to order these POIs, followed by <b>LLM</b> crafting a personalized, spatially coherent itinerary. To the best of our knowledge, this study marks the first integration of <b>LLMs</b> to innovate itinerary planning solutions. Extensive experiments on offline datasets and online subjective evaluation have demonstrated the capacities of our system to deliver more responsive and spatially coherent itineraries than current <b>LLM-based</b> solutions. Our system has been deployed in production at the TuTu online travel service and has attracted thousands of users for their urban travel planning.

{{</citation>}}


### (44/78) Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL (Sungyoon Kim et al., 2024)

{{<citation>}}

Sungyoon Kim, Yunseon Choi, Daiki E. Matsunaga, Kee-Eung Kim. (2024)  
**Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL**
<br/>
<button class="copy-to-clipboard" title="Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL" index=44>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07226v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07226v1.pdf" filename="2402.07226v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Offline Goal-Conditioned <b>Reinforcement</b> <b>Learning</b> (Offline GCRL) is an important problem in RL that focuses on acquiring diverse goal-oriented skills solely from pre-collected behavior datasets. In this setting, the reward feedback is typically absent except when the goal is achieved, which makes it difficult to learn policies especially from a finite dataset of suboptimal behaviors. In addition, realistic scenarios involve long-horizon planning, which necessitates the extraction of useful skills within sub-trajectories. Recently, the conditional diffusion model has been shown to be a promising approach to generate high-quality long-horizon plans for RL. However, their practicality for the goal-conditioned setting is still limited due to a number of technical assumptions made by the methods. In this paper, we propose SSD (Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method that leverages the conditional diffusion model to address these limitations. In summary, we use the diffusion model that generates future plans conditioned on the target goal and value, with the target value estimated from the goal-relabeled offline dataset. We report state-of-the-art performance in the standard benchmark set of GCRL tasks, and demonstrate the capability to successfully stitch the segments of suboptimal trajectories in the offline data to generate high-quality plans.

{{</citation>}}


### (45/78) The Reasons that Agents Act: Intention and Instrumental Goals (Francis Rhys Ward et al., 2024)

{{<citation>}}

Francis Rhys Ward, Matt MacDermott, Francesco Belardinelli, Francesca Toni, Tom Everitt. (2024)  
**The Reasons that Agents Act: Intention and Instrumental Goals**
<br/>
<button class="copy-to-clipboard" title="The Reasons that Agents Act: Intention and Instrumental Goals" index=45>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07221v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07221v1.pdf" filename="2402.07221v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Intention is an important and challenging concept in AI. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents. Finally, we demonstrate how our definition can be used to infer the intentions of <b>reinforcement</b> <b>learning</b> agents and language models from their behaviour.

{{</citation>}}


### (46/78) A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense (Ryota Iijima et al., 2024)

{{<citation>}}

Ryota Iijima, Sayaka Shiota, Hitoshi Kiya. (2024)  
**A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense**
<br/>
<button class="copy-to-clipboard" title="A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense" index=46>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07183v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07183v1.pdf" filename="2402.07183v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs). In previous studies, the use of models encrypted with a secret key was demonstrated to be robust against white-box attacks, but not against black-box ones. In this paper, we propose a novel method using the vision <b>transformer</b> (ViT) that is a random ensemble of encrypted models for enhancing robustness against both white-box and black-box attacks. In addition, a benchmark attack method, called AutoAttack, is applied to models to test adversarial robustness objectively. In experiments, the method was demonstrated to be robust against not only white-box attacks but also black-box ones in an image classification task on the CIFAR-10 and ImageNet datasets. The method was also compared with the state-of-the-art in a standardized benchmark for adversarial robustness, RobustBench, and it was verified to outperform conventional defenses in terms of clean accuracy and robust accuracy.

{{</citation>}}


### (47/78) Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias (Arifa Khan et al., 2024)

{{<citation>}}

Arifa Khan, P. Saravanan, S. K Venkatesan. (2024)  
**Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias**
<br/>
<button class="copy-to-clipboard" title="Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias" index=47>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.AI  
Categories: cs-AI, cs.AI  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07166v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07166v1.pdf" filename="2402.07166v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in <b>Large</b> <b>Language</b> <b>Models.</b> The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.

{{</citation>}}


## cs.RO (2)



### (48/78) Does ChatGPT and Whisper Make Humanoid Robots More Relatable? (Xiaohui Chen et al., 2024)

{{<citation>}}

Xiaohui Chen, Katherine Luo, Trevor Gee, Mahla Nejati. (2024)  
**Does ChatGPT and Whisper Make Humanoid Robots More Relatable?**
<br/>
<button class="copy-to-clipboard" title="Does ChatGPT and Whisper Make Humanoid Robots More Relatable?" index=48>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-HC, cs-RO, cs.RO  
Keyword Score: 60  
Keywords: ChatGPT, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07095v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07095v1.pdf" filename="2402.07095v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Humanoid robots are designed to be relatable to humans for applications such as customer support and helpdesk services. However, many such systems, including Softbank's Pepper, fall short because they fail to communicate effectively with humans. The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> shows the potential to solve the communication barrier for humanoid robotics. This paper outlines the comparison of different <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> APIs, the integration of Whisper <b>ASR</b> and <b>ChatGPT</b> with the Pepper robot and the evaluation of the system (Pepper-GPT) tested by 15 human users. The comparison result shows that, compared to the Google <b>ASR</b> and Google Cloud <b>ASR,</b> the Whisper <b>ASR</b> performed best as its average Word Error Rate (1.716%) and processing time (2.639 s) are both the lowest. The participants' usability investigations show that 60% of the participants thought the performance of the Pepper-GPT was "excellent", while the rest rated this system as "good" in the subsequent experiments. It is proved that while some problems still need to be overcome, such as the robot's multilingual ability and facial tracking capacity, users generally responded positively to the system, feeling like talking to an actual human.

{{</citation>}}


### (49/78) Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation (Chrisantus Eze et al., 2024)

{{<citation>}}

Chrisantus Eze, Christopher Crick. (2024)  
**Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation**
<br/>
<button class="copy-to-clipboard" title="Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation" index=49>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.RO  
Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO  
Keyword Score: 20  
Keywords: Self-supervised Learning, Summarization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07127v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07127v1.pdf" filename="2402.07127v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through <b>self-supervised</b> techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for robotic manipulation. The survey <b>summarizes</b> video-based learning approaches, analyses their benefits over standard datasets, survey metrics, and benchmarks, and discusses open challenges and future directions in this nascent domain at the intersection of computer vision, natural language processing, and robot learning.

{{</citation>}}


## cs.CV (12)



### (50/78) A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs (Zicheng Zhang et al., 2024)

{{<citation>}}

Zicheng Zhang, Haoning Wu, Erli Zhang, Guangtao Zhai, Weisi Lin. (2024)  
**A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs**
<br/>
<button class="copy-to-clipboard" title="A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs" index=50>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 53  
Keywords: Foundation Model, Multi-modal, GPT, Question Answering, Visual Question Answering, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07116v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07116v1.pdf" filename="2402.07116v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The rapid development of Multi-modality <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile <b>foundational</b> <b>models.</b> However, evaluating MLLMs in low-level <b>visual</b> <b>perception</b> <b>and</b> understanding remains a yet-to-explore domain. To this end, we design benchmark settings to emulate human language responses related to low-level vision: the low-level <b>visual</b> <b>perception</b> <b>(A1)</b> via <b>visual</b> <b>question</b> <b>answering</b> related to low-level attributes (e.g. clarity, lighting); and the low-level <b>visual</b> <b>description</b> <b>(A2),</b> on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related <b>question-answering</b> <b>and</b> description evaluations of MLLMs from single images to image pairs. Specifically, for perception (A1), we carry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended <b>question</b> <b>about</b> its low-level features; for description (A2), we propose the LLDescribe+ dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predicting score, by employing a softmax-based approach to enable all MLLMs to generate quantifiable quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level <b>visual</b> <b>competencies</b> <b>on</b> single images, but only <b>GPT-4V</b> exhibits higher accuracy on pairwise comparisons than single image evaluations (like humans). We hope that our benchmark will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs. Datasets will be available at https://github.com/Q-Future/Q-Bench.

{{</citation>}}


### (51/78) Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy (Simon Ging et al., 2024)

{{<citation>}}

Simon Ging, Mara A. Bravo, Thomas Brox. (2024)  
**Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy**
<br/>
<button class="copy-to-clipboard" title="Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy" index=51>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CL, cs-CV, cs-LG, cs.CV  
Keyword Score: 50  
Keywords: Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07270v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07270v1.pdf" filename="2402.07270v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The evaluation of text-generative <b>vision-language</b> models is a challenging yet crucial endeavor. By addressing the limitations of existing <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel <b>VQA</b> benchmark based on well-known <b>visual</b> <b>classification</b> <b>datasets</b> which allows a granular evaluation of text-generative <b>vision-language</b> models and their comparison with discriminative <b>vision-language</b> models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up <b>questions</b> <b>about</b> the ground-truth category. Finally, we compare traditional NLP and <b>LLM-based</b> metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our benchmark to a suite of <b>vision-language</b> models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of <b>vision-language</b> modeling.

{{</citation>}}


### (52/78) Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks (Muqun Niu et al., 2024)

{{<citation>}}

Muqun Niu, Yuan Ren, Boyu Li, Chenchen Ding. (2024)  
**Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks**
<br/>
<button class="copy-to-clipboard" title="Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks" index=52>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-LG, cs-NE, cs.CV  
Keyword Score: 50  
Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Quantization, Quantization  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07200v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07200v1.pdf" filename="2402.07200v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Lightweight design of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> requires co-design efforts in the model architectures and compression techniques. As a novel design paradigm that separates training and inference, a structural re-parameterized (SR) network such as the representative RepVGG revitalizes the simple VGG-like network with a high accuracy comparable to advanced and often more complicated networks. However, the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in <b>quantization.</b> To address this, we propose an operator-level improvement for training called Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of limited bitwidths while upkeeping the inference accuracy, we develop a clustering-based non-uniform <b>quantization</b> framework for <b>Quantization-Aware</b> Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the <b>quantized</b> performance of RepVGG is largely enhanced, particularly when the bitwidth falls below 8.

{{</citation>}}


### (53/78) Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation (Binyan Hu et al., 2024)

{{<citation>}}

Binyan Hu, A. K. Qin. (2024)  
**Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation**
<br/>
<button class="copy-to-clipboard" title="Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation" index=53>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-NE, cs.CV  
Keyword Score: 40  
Keywords: Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Self-supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07119v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07119v1.pdf" filename="2402.07119v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Medical image segmentation has been significantly advanced by deep learning (DL) techniques, though the data scarcity inherent in medical applications poses a great challenge to DL-based segmentation methods. <b>Self-supervised</b> <b>learning</b> offers a solution by creating auxiliary learning tasks from the available dataset and then leveraging the <b>knowledge</b> <b>acquired</b> from solving auxiliary tasks to help better solve the target segmentation task. Different auxiliary tasks may have different properties and thus can help the target task to different extents. It is desired to leverage their complementary advantages to enhance the overall assistance to the target task. To achieve this, existing methods often adopt a joint training paradigm, which co-solves segmentation and auxiliary tasks by integrating their losses or intermediate gradients. However, direct coupling of losses or intermediate gradients risks undesirable interference because the <b>knowledge</b> <b>acquired</b> from solving each auxiliary task at every training step may not always benefit the target task. To address this issue, we propose a two-stage training approach. In the first stage, the target segmentation task will be independently co-solved with each auxiliary task in both joint training and pre-training modes, with the better model selected via validation performance. In the second stage, the models obtained with respect to each auxiliary task are converted into a single model using an ensemble <b>knowledge</b> <b>distillation</b> method. Our approach allows for making best use of each auxiliary task to create multiple elite segmentation models and then combine them into an even more powerful model. We employed five auxiliary tasks of different proprieties in our approach and applied it to train the U-Net model on an X-ray pneumothorax segmentation dataset. Experimental results demonstrate the superiority of our approach over several existing methods.

{{</citation>}}


### (54/78) The Bias of Harmful Label Associations in Vision-Language Models (Caner Hazirbas et al., 2024)

{{<citation>}}

Caner Hazirbas, Alicia Sun, Yonathan Efroni, Mark Ibrahim. (2024)  
**The Bias of Harmful Label Associations in Vision-Language Models**
<br/>
<button class="copy-to-clipboard" title="The Bias of Harmful Label Associations in Vision-Language Models" index=54>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 30  
Keywords: Fairness, Transformer, Vision-and-Language  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07329v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07329v1.pdf" filename="2402.07329v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Despite the remarkable performance of foundation <b>vision-language</b> models, the shared representation space for text and vision can also encode harmful label associations detrimental to <b>fairness.</b> While prior work has uncovered bias in <b>vision-language</b> models' (VLMs) classification performance across geography, work has been limited along the important axis of harmful label associations due to a lack of rich, labeled data. In this work, we investigate harmful label associations in the recently released Casual Conversations datasets containing more than 70,000 videos. We study bias in the frequency of harmful label associations across self-provided labels for age, gender, apparent skin tone, and physical adornments across several leading VLMs. We find that VLMs are $4-13$x more likely to harmfully classify individuals with darker skin tones. We also find scaling <b>transformer</b> encoder model size leads to higher confidence in harmful predictions. Finally, we find improvements on standard vision tasks across VLMs does not address disparities in harmful label associations.

{{</citation>}}


### (55/78) Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets (Ross Greer et al., 2024)

{{<citation>}}

Ross Greer, Mohan Trivedi. (2024)  
**Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets**
<br/>
<button class="copy-to-clipboard" title="Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets" index=55>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-AI, cs-CV, cs-LG, cs.CV  
Keyword Score: 20  
Keywords: Active Learning, Reasoning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07320v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07320v1.pdf" filename="2402.07320v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This research explores the integration of language embeddings for <b>active</b> <b>learning</b> in autonomous driving datasets, with a focus on novelty detection. Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level <b>reasoning</b> abilities. Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and <b>active</b> <b>learning.</b> The research presents a clustering experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties. We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted. From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting qualitative examples from the clustered results. Our results demonstrate the effectiveness of language-driven embeddings in identifying novel elements and generating explanations of data, and we further discuss potential applications in safe takeovers, data curation, and multi-task <b>active</b> <b>learning.</b>

{{</citation>}}


### (56/78) LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions (Atharva Pandey et al., 2024)

{{<citation>}}

Atharva Pandey, Vishal Yadav, Rajendra Nagar, Santanu Chaudhury. (2024)  
**LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions**
<br/>
<button class="copy-to-clipboard" title="LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions" index=56>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Supervised Learning, Supervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07301v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07301v1.pdf" filename="2402.07301v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Implicit 3D surface reconstruction of an object from its partial and noisy 3D point cloud scan is the classical geometry processing and 3D computer vision problem. In the literature, various 3D shape representations have been developed, differing in memory efficiency and shape retrieval effectiveness, such as volumetric, parametric, and implicit surfaces. Radial basis functions provide memory-efficient parameterization of the implicit surface. However, we show that training a neural network using the mean squared error between the ground-truth implicit surface and the linear basis-based implicit surfaces does not converge to the global solution. In this work, we propose locally supported compact radial basis functions for a linear representation of the implicit surface. This representation enables us to generate 3D shapes with arbitrary topologies at any resolution due to their continuous nature. We then propose a neural network architecture for learning the linear implicit shape representation of the 3D surface of an object. We learn linear implicit shapes within a <b>supervised</b> <b>learning</b> framework using ground truth Signed-Distance Field (SDF) data for guidance. The classical strategies face difficulties in finding linear implicit shapes from a given 3D point cloud due to numerical issues (requires solving inverse of a large matrix) in basis and query point selection. The proposed approach achieves better Chamfer distance and comparable F-score than the state-of-the-art approach on the benchmark dataset. We also show the effectiveness of the proposed approach by using it for the 3D shape completion task.

{{</citation>}}


### (57/78) GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting (Xiaoyu Zhou et al., 2024)

{{<citation>}}

Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang. (2024)  
**GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting**
<br/>
<button class="copy-to-clipboard" title="GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting" index=57>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07207v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07207v1.pdf" filename="2402.07207v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the <b>LLMs</b> to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at https://gala3d.github.io/.

{{</citation>}}


### (58/78) Deep Learning for Medical Image Segmentation with Imprecise Annotation (Binyan Hu et al., 2024)

{{<citation>}}

Binyan Hu, A. K. Qin. (2024)  
**Deep Learning for Medical Image Segmentation with Imprecise Annotation**
<br/>
<button class="copy-to-clipboard" title="Deep Learning for Medical Image Segmentation with Imprecise Annotation" index=58>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs-NE, cs.CV  
Keyword Score: 10  
Keywords: Fine-tuning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07330v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07330v1.pdf" filename="2402.07330v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Medical image segmentation (MIS) plays an instrumental role in medical image analysis, where considerable efforts have been devoted to automating the process. Currently, mainstream MIS approaches are based on deep neural networks (DNNs) which are typically trained on a dataset that contains annotation masks produced by doctors. However, in the medical domain, the annotation masks generated by different doctors can inherently vary because a doctor may unnecessarily produce precise and unique annotations to meet the goal of diagnosis. Therefore, the DNN model trained on the data annotated by certain doctors, often just a single doctor, could undesirably favour those doctors who annotate the training data, leading to the unsatisfaction of a new doctor who will use the trained model. To address this issue, this work investigates the utilization of multi-expert annotation to enhance the adaptability of the model to a new doctor and we conduct a pilot study on the MRI brain segmentation task. Experimental results demonstrate that the model trained on a dataset with multi-expert annotation can efficiently cater for a new doctor, after lightweight <b>fine-tuning</b> on just a few annotations from the new doctor.

{{</citation>}}


### (59/78) Trade-off Between Spatial and Angular Resolution in Facial Recognition (Muhammad Zeshan Alam et al., 2024)

{{<citation>}}

Muhammad Zeshan Alam, Sousso kelowani, Mohamed Elsaeidy. (2024)  
**Trade-off Between Spatial and Angular Resolution in Facial Recognition**
<br/>
<button class="copy-to-clipboard" title="Trade-off Between Spatial and Angular Resolution in Facial Recognition" index=59>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Face Recognition  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07263v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07263v1.pdf" filename="2402.07263v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Ensuring robustness in <b>face</b> <b>recognition</b> systems across various challenging conditions is crucial for their versatility. State-of-the-art methods often incorporate additional information, such as depth, thermal, or angular data, to enhance performance. However, light field-based <b>face</b> <b>recognition</b> approaches that leverage angular information <b>face</b> <b>computational</b> limitations. This paper investigates the fundamental trade-off between spatio-angular resolution in light field representation to achieve improved <b>face</b> <b>recognition</b> performance. By utilizing macro-pixels with varying angular resolutions while maintaining the overall image size, we aim to quantify the impact of angular information at the expense of spatial resolution, while considering computational constraints. Our experimental results demonstrate a notable performance improvement in <b>face</b> <b>recognition</b> systems by increasing the angular resolution, up to a certain extent, at the cost of spatial resolution.

{{</citation>}}


### (60/78) PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression (Jiahao Pang et al., 2024)

{{<citation>}}

Jiahao Pang, Kevin Bui, Dong Tian. (2024)  
**PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression**
<br/>
<button class="copy-to-clipboard" title="PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression" index=60>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV, eess-IV  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07243v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07243v1.pdf" filename="2402.07243v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The universality of the point cloud format enables many 3D applications, making the compression of point clouds a critical phase in practice. Sampled as discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D with a finite bit-depth. However, the point distribution of a practical point cloud changes drastically as its bit-depth increases, requiring different methodologies for effective consumption/analysis. In this regard, a heterogeneous point cloud compression (PCC) framework is proposed. We unify typical point cloud representations -- point-based, voxel-based, and tree-based representations -- and their associated backbones under a learning-based framework to compress an input point cloud at different bit-depth levels. Having recognized the importance of voxel-domain processing, we augment the framework with a proposed context-aware upsampling for decoding and an enhanced voxel <b>transformer</b> for feature aggregation. Extensive experimentation demonstrates the state-of-the-art performance of our proposal on a wide range of point clouds.

{{</citation>}}


### (61/78) A novel spatial-frequency domain network for zero-shot incremental learning (Jie Ren et al., 2024)

{{<citation>}}

Jie Ren, Yang Zhao, Weichuan Zhang, Changming Sun. (2024)  
**A novel spatial-frequency domain network for zero-shot incremental learning**
<br/>
<button class="copy-to-clipboard" title="A novel spatial-frequency domain network for zero-shot incremental learning" index=61>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CV  
Categories: cs-CV, cs.CV  
Keyword Score: 10  
Keywords: Zero-shot  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07216v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07216v1.pdf" filename="2402.07216v1.pdf">Download PDF</button>

---


**ABSTRACT**  
<b>Zero-shot</b> incremental learning aims to enable the model to generalize to new classes without forgetting previously learned classes. However, the semantic gap between old and new sample classes can lead to catastrophic forgetting. Additionally, existing algorithms lack capturing significant information from each sample image domain, impairing models' classification performance. Therefore, this paper proposes a novel Spatial-Frequency Domain Network (SFDNet) which contains a Spatial-Frequency Feature Extraction (SFFE) module and Attention Feature Alignment (AFA) module to improve the <b>Zero-Shot</b> Translation for Class Incremental algorithm. Firstly, SFFE module is designed which contains a dual attention mechanism for obtaining salient spatial-frequency feature information. Secondly, a novel feature fusion module is conducted for obtaining fused spatial-frequency domain features. Thirdly, the Nearest Class Mean classifier is utilized to select the most suitable category. Finally, iteration between tasks is performed using the <b>Zero-Shot</b> Translation model. The proposed SFDNet has the ability to effectively extract spatial-frequency feature representation from input images, improve the accuracy of image classification, and fundamentally alleviate catastrophic forgetting. Extensive experiments on the CUB 200-2011 and CIFAR100 datasets demonstrate that our proposed algorithm outperforms state-of-the-art incremental learning algorithms.

{{</citation>}}


## cond-mat.soft (1)



### (62/78) X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design (Eric L. Buehler et al., 2024)

{{<citation>}}

Eric L. Buehler, Markus J. Buehler. (2024)  
**X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design**
<br/>
<button class="copy-to-clipboard" title="X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design" index=62>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cond-mat.soft  
Categories: cond-mat-dis-nn, cond-mat-soft, cond-mat.soft, cs-AI, cs-CL, cs-LG, q-bio-QM  
Keyword Score: 50  
Keywords: Graph Attention Networks, Fine-tuning, Reasoning, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07148v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07148v1.pdf" filename="2402.07148v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We report a mixture of expert strategy to create <b>fine-tuned</b> <b>large</b> <b>language</b> <b>models</b> using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a <b>gating</b> strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced <b>reasoning</b> capability, focused on biomaterial analysis, protein mechanics and design. The impact of this work include access to readily expandable, adaptable and changeable models with strong domain knowledge and the capability to integrate across areas of knowledge. With the X-LoRA model featuring experts in biology, mathematics, <b>reasoning,</b> bio-inspired materials, mechanics and materials, chemistry, and protein mechanics we conduct a series of physics-focused case studies. We examine knowledge recall, protein mechanics forward/inverse tasks, protein design, and adversarial agentic modeling including ontological knowledge graphs. The model is capable not only of making quantitative predictions of nanomechanical properties of proteins, but also reasons over the results and correctly predicts likely mechanisms that explain distinct molecular behaviors.

{{</citation>}}


## cs.HC (2)



### (63/78) Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies (Zheng Ning et al., 2024)

{{<citation>}}

Zheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, Toby Li. (2024)  
**Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies**
<br/>
<button class="copy-to-clipboard" title="Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies" index=63>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-HC, cs.HC  
Keyword Score: 30  
Keywords: Attention Alignment, Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07304v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07304v1.pdf" filename="2402.07304v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Querying structured databases with natural language (NL2SQL) has remained a difficult problem for years. Recently, the advancement of machine learning (ML), natural language processing (NLP), and <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have led to significant improvements in performance, with the best model achieving ~85% percent accuracy on the benchmark Spider dataset. However, there is a lack of a systematic understanding of the types, causes, and effectiveness of error-handling mechanisms of errors for erroneous queries nowadays. To bridge the gap, a taxonomy of errors made by four representative NL2SQL models was built in this work, along with an in-depth analysis of the errors. Second, the causes of model errors were explored by analyzing the model-human <b>attention</b> <b>alignment</b> to the natural language query. Last, a within-subjects user study with 26 participants was conducted to investigate the effectiveness of three interactive error-handling mechanisms in NL2SQL. Findings from this paper shed light on the design of model structure and error discovery and repair strategies for natural language data query interfaces in the future.

{{</citation>}}


### (64/78) EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches (Pengcheng An et al., 2024)

{{<citation>}}

Pengcheng An, Jiawen Zhu, Zibo Zhang, Yifei Yin, Qingyuan Ma, Che Yan, Linghao Du, Jian Zhao. (2024)  
**EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches**
<br/>
<button class="copy-to-clipboard" title="EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches" index=64>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.HC  
Categories: cs-AI, cs-HC, cs.HC  
Keyword Score: 10  
Keywords: Knowledge Distillation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07174v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07174v1.pdf" filename="2402.07174v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content. This hinders the shared emotional experience at the pre-retrieval stage. Research scarcely explored "Emotional Teasers"-pre-retrieval cues offering a glimpse into an awaiting message's emotional tone without disclosing its content. We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions. EmoWear eases senders' choice by prioritizing emotions based on semantic and acoustic processing. EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages. The animated teasers were considered intuitive and valued for diverse expressions. Desirable interaction qualities and practical implications are <b>distilled</b> for future design. We thereby contribute both a novel system and empirical knowledge concerning emotional teasers for voice messaging.

{{</citation>}}


## cs.NI (1)



### (65/78) ML Framework for Wireless MAC Protocol Design (Navid Keshtiarast et al., 2024)

{{<citation>}}

Navid Keshtiarast, Marina Petrova. (2024)  
**ML Framework for Wireless MAC Protocol Design**
<br/>
<button class="copy-to-clipboard" title="ML Framework for Wireless MAC Protocol Design" index=65>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NI  
Categories: cs-NI, cs-SY, cs.NI, eess-SY  
Keyword Score: 30  
Keywords: Reinforcement Learning, Simulation, Simulator  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07208v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07208v1.pdf" filename="2402.07208v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Adaptivity, reconfigurability and intelligence are key features of the next-generation wireless networks to meet the increasingly diverse quality of service (QoS) requirements of the future applications. Conventional protocol designs, however, struggle to provide flexibility and agility to changing radio environments, traffic types and different user service requirements. In this paper, we explore the potential of deep <b>reinforcement</b> <b>learning</b> (DRL), in particular Proximal Policy Optimization (PPO), to design and configure intelligent and application-specific medium access control (MAC) protocols. We propose a framework that enables the addition, removal, or modification of protocol features to meet individual application needs. The DRL channel access policy design empowers the protocol to adapt and optimize in accordance with the network and radio environment. Through extensive <b>simulations,</b> we demonstrate the superior performance of the learned protocols over legacy IEEE 802.11ac in terms of throughput and latency.

{{</citation>}}


## cs.CE (1)



### (66/78) Joint Source-Channel Coding for Wireless Image Transmission: A Deep Compressed-Sensing Based Method (Mohammad Amin Jarrahi et al., 2024)

{{<citation>}}

Mohammad Amin Jarrahi, Eirina Bourtsoulatze, Vahid Abolghasemi. (2024)  
**Joint Source-Channel Coding for Wireless Image Transmission: A Deep Compressed-Sensing Based Method**
<br/>
<button class="copy-to-clipboard" title="Joint Source-Channel Coding for Wireless Image Transmission: A Deep Compressed-Sensing Based Method" index=66>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CE  
Categories: cs-CE, cs.CE  
Keyword Score: 30  
Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07162v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07162v1.pdf" filename="2402.07162v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Nowadays, the demand for image transmission over wireless networks has surged significantly. To meet the need for swift delivery of high-quality images through time-varying channels with limited bandwidth, the development of efficient transmission strategies and techniques for preserving image quality is of importance. This paper introduces an innovative approach to Joint Source-Channel Coding (JSCC) tailored for wireless image transmission. It capitalizes on the power of Compressed Sensing (CS) to achieve superior compression and resilience to channel noise. In this method, the process begins with the compression of images using a block-based CS technique implemented through a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> structure. Subsequently, the images are encoded by directly mapping image blocks to complex-valued channel input symbols. Upon reception, the data is decoded to recover the channel-encoded information, effectively removing the noise introduced during transmission. To finalize the process, a novel <b>CNN-based</b> reconstruction network is employed to restore the original image from the channel-decoded data. The performance of the proposed method is assessed using the CIFAR-10 and Kodak datasets. The results illustrate a substantial improvement over existing JSCC frameworks when assessed in terms of metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) across various channel Signal-to-Noise Ratios (SNRs) and channel bandwidth values. These findings underscore the potential of harnessing <b>CNN-based</b> CS for the development of deep JSCC algorithms tailored for wireless image transmission.

{{</citation>}}


## cs.CR (1)



### (67/78) Differentially Private Training of Mixture of Experts Models (Pierre Tholoniat et al., 2024)

{{<citation>}}

Pierre Tholoniat, Huseyin A. Inan, Janardhan Kulkarni, Robert Sim. (2024)  
**Differentially Private Training of Mixture of Experts Models**
<br/>
<button class="copy-to-clipboard" title="Differentially Private Training of Mixture of Experts Models" index=67>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.CR  
Categories: cs-CR, cs-LG, cs.CR  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07334v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07334v1.pdf" filename="2402.07334v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This position paper investigates the integration of Differential Privacy (DP) in the training of Mixture of Experts (MoE) models within the field of natural language processing. As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> scale to billions of parameters, leveraging expansive datasets, they exhibit enhanced linguistic capabilities and emergent abilities. However, this growth raises significant computational and privacy concerns. Our study addresses these issues by exploring the potential of MoE models, known for their computational efficiency, and the application of DP, a standard for privacy preservation. We present the first known attempt to train MoE models under the constraints of DP, addressing the unique challenges posed by their architecture and the complexities of DP integration. Our initial experimental studies demonstrate that MoE models can be effectively trained with DP, achieving performance that is competitive with their non-private counterparts. This initial study aims to provide valuable insights and ignite further research in the domain of privacy-preserving MoE models, softly laying the groundwork for prospective developments in this evolving field.

{{</citation>}}


## stat.ML (2)



### (68/78) Self-Consistent Conformal Prediction (Lars van der Laan et al., 2024)

{{<citation>}}

Lars van der Laan, Ahmed M. Alaa. (2024)  
**Self-Consistent Conformal Prediction**
<br/>
<button class="copy-to-clipboard" title="Self-Consistent Conformal Prediction" index=68>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-LG, stat-ME, stat-ML, stat.ML  
Keyword Score: 20  
Keywords: In-context Learning, Prompt  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07307v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07307v1.pdf" filename="2402.07307v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions <b>prompted</b> by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.

{{</citation>}}


### (69/78) Resampling methods for Private Statistical Inference (Karan Chadha et al., 2024)

{{<citation>}}

Karan Chadha, John Duchi, Rohith Kuditipudi. (2024)  
**Resampling methods for Private Statistical Inference**
<br/>
<button class="copy-to-clipboard" title="Resampling methods for Private Statistical Inference" index=69>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: stat.ML  
Categories: cs-CR, cs-LG, stat-ME, stat-ML, stat.ML  
Keyword Score: 13  
Keywords: Logistic Regression, Sample Size  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07131v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07131v1.pdf" filename="2402.07131v1.pdf">Download PDF</button>

---


**ABSTRACT**  
We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the <b>sample</b> <b>size</b> $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and <b>logistic</b> <b>regression</b> with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.

{{</citation>}}


## q-bio.GN (1)



### (70/78) Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer (Zehao Dong et al., 2024)

{{<citation>}}

Zehao Dong, Qihang Zhao, Philip R. O. Payne, Michael A Province, Carlos Cruchaga, Muhan Zhang, Tianyu Zhao, Yixin Chen, Fuhai Li. (2024)  
**Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer**
<br/>
<button class="copy-to-clipboard" title="Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer" index=70>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: q-bio.GN  
Categories: cs-AI, cs-LG, q-bio-GN, q-bio.GN  
Keyword Score: 20  
Keywords: Graph Neural Network, Graph Neural Network  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07268v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07268v1.pdf" filename="2402.07268v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Biomarker identification is critical for precise disease diagnosis and understanding disease pathogenesis in omics data analysis, like using fold change and regression analysis. <b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have been the dominant deep learning model for analyzing <b>graph-structured</b> <b>data.</b> <b>However,</b> we found two major limitations of existing <b>GNNs</b> in omics data analysis, i.e., limited-prediction (diagnosis) accuracy and limited-reproducible biomarker identification capacity across multiple datasets. The root of the challenges is the unique <b>graph</b> <b>structure</b> <b>of</b> biological signaling pathways, which consists of a large number of targets and intensive and complex signaling interactions among these targets. To resolve these two challenges, in this study, we presented a novel <b>GNN</b> model architecture, named PathFormer, which systematically integrate signaling network, priori knowledge and omics data to rank biomarkers and predict disease diagnosis. In the comparison results, PathFormer outperformed existing <b>GNN</b> models significantly in terms of highly accurate prediction capability ( 30% accuracy improvement in disease diagnosis compared with existing <b>GNN</b> models) and high reproducibility of biomarker ranking across different datasets. The improvement was confirmed using two independent Alzheimer's Disease (AD) and cancer transcriptomic datasets. The PathFormer model can be directly applied to other omics data analysis studies.

{{</citation>}}


## cs.SE (3)



### (71/78) Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces (Claudionor N. Coelho Jr et al., 2024)

{{<citation>}}

Claudionor N. Coelho Jr, Hanchen Xiong, Tushar Karayil, Sree Koratala, Rex Shang, Jacob Bollinger, Mohamed Shabar, Syam Nair. (2024)  
**Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces**
<br/>
<button class="copy-to-clipboard" title="Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces" index=71>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-LG, cs-SE, cs.SE  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07158v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07158v1.pdf" filename="2402.07158v1.pdf">Download PDF</button>

---


**ABSTRACT**  
The advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> has also resulted in an equivalent proliferation in its applications. Software design, being one, has gained tremendous benefits in using <b>LLMs</b> as an interface component that extends fixed user stories. However, inclusion of <b>LLM-based</b> AI agents in software design often poses unexpected challenges, especially in the estimation of development efforts. Through the example of UI-based user stories, we provide a comparison against traditional methods and propose a new way to enhance specifications of natural language-based questions that allows for the estimation of development effort by taking into account data sources, interfaces and algorithms.

{{</citation>}}


### (72/78) Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example (Malinda Dilhara et al., 2024)

{{<citation>}}

Malinda Dilhara, Abhiram Bellur, Timofey Bryksin, Danny Dig. (2024)  
**Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example**
<br/>
<button class="copy-to-clipboard" title="Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example" index=72>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-SE, cs.SE  
Keyword Score: 20  
Keywords: Large Language Model, Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07138v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07138v1.pdf" filename="2402.07138v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Software developers often repeat code changes, known as "code change patterns" (CPATs), within and across projects. Automating these CPATs accelerates development, but current Transformation by Example (TBE) techniques are limited by the input examples' quality and quantity, missing variations with different syntax or flow yet semantically similar. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> trained on vast code datasets, can overcome these limitations by generating semantically equivalent, unseen CPAT variants, enhancing TBE effectiveness. We identified best practices for using <b>LLMs</b> to generate code variants meeting criteria of correctness, usefulness, and applicability. Implementing these in PyCraft, combining static and dynamic analysis with <b>LLMs,</b> we achieved an F-measure of 96.6% in identifying correct variants, expanding inputs by 58x on average, and automating changes to increase target codes by up to 39x. Patches from PyCraft were submitted to projects like microsoft/DeepSpeed and IBM/inFairness, with an 83% acceptance rate, validating our approach's usefulness.

{{</citation>}}


### (73/78) On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study (Amir M. Mir et al., 2024)

{{<citation>}}

Amir M. Mir, Mehdi Keshani, Sebastian Proksch. (2024)  
**On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study**
<br/>
<button class="copy-to-clipboard" title="On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study" index=73>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.SE  
Categories: cs-AI, cs-LG, cs-PL, cs-SE, cs.SE  
Keyword Score: 10  
Keywords: Pruning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07294v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07294v1.pdf" filename="2402.07294v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Static call graph (CG) construction often over-approximates call relations, leading to sound, but imprecise results. Recent research has explored machine learning (ML)-based CG <b>pruning</b> as a means to enhance precision by eliminating false edges. However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses. Prior results were also not compared with advanced static CG construction techniques yet. This study tackles these issues. We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs. We leverage these CGs to explore conservative <b>pruning</b> strategies during the training and inference of ML-based CG pruners. We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both with and without <b>pruning.</b> We find that CG <b>pruning</b> is a difficult task for real-world Java projects and substantial improvements in the CG precision (+25%) meet reduced recall (-9%). However, our experiments show promising results: even when we favor recall over precision by using an F2 metric in our experiments, we can show that pruned CGs have comparable quality to a context-sensitive 1-CFA analysis while being computationally less demanding. Resulting CGs are much smaller (69%), and substantially faster (3.5x speed-up), with virtually unchanged results in our downstream analysis.

{{</citation>}}


## math.NA (1)



### (74/78) CodPy: a Python library for numerics, machine learning, and statistics (Philippe G. LeFloch et al., 2024)

{{<citation>}}

Philippe G. LeFloch, Jean-Marc Mercier, Shohruh Miryusupov. (2024)  
**CodPy: a Python library for numerics, machine learning, and statistics**
<br/>
<button class="copy-to-clipboard" title="CodPy: a Python library for numerics, machine learning, and statistics" index=74>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: math.NA  
Categories: cs-NA, math-NA, math-ST, math.NA, stat-TH  
Keyword Score: 20  
Keywords: Supervised Learning, Unsupervised Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07084v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07084v1.pdf" filename="2402.07084v1.pdf">Download PDF</button>

---


**ABSTRACT**  
This monograph offers an introduction to a collection of numerical algorithms implemented in the library CodPy (an acronym that stands for the Curse Of Dimensionality in PYthon), which has found widespread applications across various areas, including machine learning, statistics, and computational physics. We develop here a strategy based on the theory of reproducing kernel Hilbert spaces (RKHS) and the theory of optimal transport. Initially designed for mathematical finance, this library has since been enhanced and broadened to be applicable to problems arising in engineering and industry. In order to present the general principles and techniques employed in CodPy and its applications, we have structured this monograph into two main parts. First of all, we focus on the fundamental principles of kernel-based representations of data and solutions, also that the presentation therein is supplemented with illustrative examples only. Next, we discuss the application of these principles to many classes of concrete problems, spanning from the numerical approximation of partial differential equations to <b>(supervised,</b> <b>unsupervised)</b> machine learning, extending to generative methods with a focus on stochastic aspects.

{{</citation>}}


## cs.DB (1)



### (75/78) Intent-Based Access Control: Using LLMs to Intelligently Manage Access Control (Pranav Subramaniam et al., 2024)

{{<citation>}}

Pranav Subramaniam, Sanjay Krishnan. (2024)  
**Intent-Based Access Control: Using LLMs to Intelligently Manage Access Control**
<br/>
<button class="copy-to-clipboard" title="Intent-Based Access Control: Using LLMs to Intelligently Manage Access Control" index=75>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.DB  
Categories: cs-CR, cs-DB, cs.DB  
Keyword Score: 10  
Keywords: Large Language Model  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07332v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07332v1.pdf" filename="2402.07332v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In every enterprise database, administrators must define an access control policy that specifies which users have access to which assets. Access control straddles two worlds: policy (organization-level principles that define who should have access) and process (database-level primitives that actually implement the policy). Assessing and enforcing process compliance with a policy is a manual and ad-hoc task. This paper introduces a new paradigm for access control called Intent-Based Access Control for Databases (IBAC-DB). In IBAC-DB, access control policies are expressed more precisely using a novel format, the natural language access control matrix (NLACM). Database access control primitives are synthesized automatically from these NLACMs. These primitives can be used to generate new DB configurations and/or evaluate existing ones. This paper presents a reference architecture for an IBAC-DB interface, an initial implementation for PostgreSQL (which we call LLM4AC), and initial benchmarks that evaluate the accuracy and scope of such a system. We find that our chosen implementation, LLM4AC, vastly outperforms other baselines, achieving near-perfect F1 scores on our initial benchmarks.

{{</citation>}}


## eess.SY (1)



### (76/78) A Review of the GIC Blocker Placement Problem (Arthur K. Barnes et al., 2024)

{{<citation>}}

Arthur K. Barnes, Adam Mate, Russell Bent. (2024)  
**A Review of the GIC Blocker Placement Problem**
<br/>
<button class="copy-to-clipboard" title="A Review of the GIC Blocker Placement Problem" index=76>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: eess.SY  
Categories: cs-SY, eess-SY, eess.SY  
Keyword Score: 10  
Keywords: Transformer  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07302v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07302v1.pdf" filename="2402.07302v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Space weather poses a tremendous threat to power systems: geomagnetic disturbances could result in widespread disruptions and long-duration blackouts, including severe damage to system components. To mitigate their impacts, a handful of strategies exist, with the most promising being the deployment of <b>transformer</b> neutral blocking devices. The high cost of these devices, however, precludes their installation at all substations; this motivates the development of effective solutions for the cost-effective placement of such devices. While the current state-of-the-art in blocker placement methods is insufficient to be applied to real-sized power grids, ongoing research continues to increase the size of networks for which the placement problem remains tractable. Along these lines, the contributions of this paper are two fold: first, a comprehensive overview of the current state-of-the-art in blocker placement methods is provided; and second, a complete optimization formulation - implemented and benchmarked in an open-source software - for the blocker placement problem is presented.

{{</citation>}}


## cs.NE (1)



### (77/78) Optimizing Genetically-Driven Synaptogenesis (Tommaso Boccato et al., 2024)

{{<citation>}}

Tommaso Boccato, Matteo Ferrante, Nicola Toschi. (2024)  
**Optimizing Genetically-Driven Synaptogenesis**
<br/>
<button class="copy-to-clipboard" title="Optimizing Genetically-Driven Synaptogenesis" index=77>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.NE  
Categories: cs-NE, cs.NE, q-bio-NC  
Keyword Score: 10  
Keywords: Reinforcement Learning  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07242v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07242v1.pdf" filename="2402.07242v1.pdf">Download PDF</button>

---


**ABSTRACT**  
In this paper we introduce SynaptoGen, a novel framework that aims to bridge the gap between genetic manipulations and neuronal network behavior by simulating synaptogenesis and guiding the development of neuronal networks capable of solving predetermined computational tasks. Drawing inspiration from recent advancements in the field, we propose SynaptoGen as a bio-plausible approach to modeling synaptogenesis through differentiable functions. To validate SynaptoGen, we conduct a preliminary experiment using <b>reinforcement</b> <b>learning</b> as a benchmark learning framework, demonstrating its effectiveness in generating neuronal networks capable of solving the OpenAI Gym's Cart Pole task, compared to carefully designed baselines. The results highlight the potential of SynaptoGen to inspire further advancements in neuroscience and computational modeling, while also acknowledging the need for incorporating more realistic genetic rules and synaptic conductances in future research. Overall, SynaptoGen represents a promising avenue for exploring the intersection of genetics, neuroscience, and artificial intelligence.

{{</citation>}}


## cs.IR (1)



### (78/78) Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training (Haonan Chen et al., 2024)

{{<citation>}}

Haonan Chen, Zhicheng Dou, Xuetong Hao, Yunhao Tao, Shiren Song, Zhenli Sheng. (2024)  
**Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training**
<br/>
<button class="copy-to-clipboard" title="Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training" index=78>
  <span class="copy-to-clipboard-item">Copy Title<span>
</button>
<div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role="alert" aria-live="assertive" aria-atomic="true">
  <div class="d-flex">
    <div class="toast-body">
      Copied!
    </div>
  </div>
</div>

---
Primary Category: cs.IR  
Categories: cs-AI, cs-IR, cs.IR  
Keyword Score: 10  
Keywords: Data Augmentation  
<a type="button" class="btn btn-outline-primary" href="http://arxiv.org/abs/2402.07076v1" target="_blank" >Paper Link</a>
<button type="button" class="btn btn-outline-primary download-pdf" url="https://arxiv.org/pdf/2402.07076v1.pdf" filename="2402.07076v1.pdf">Download PDF</button>

---


**ABSTRACT**  
Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction <b>data.</b> <b>To</b> tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three <b>data</b> <b>augmentation</b> strategies and a contrastive pre-training objective to compensate for the imperfections in the available <b>data.</b> <b>Through</b> extensive experiments on a real-world dataset, we demonstrate that CAMA outperforms several strong baseline matching models significantly. Furthermore, we have deployed our matching framework on a system of Huawei Cloud. Our observations indicate an improvement of about 30% compared to the previous online model in terms of Conversion Rate (CVR), which demonstrates its great business value.

{{</citation>}}
